---
layout: post
title: "CS231n 10강. Recurrent Neural Networks"
subtitle: "Stanford CS231n Lecture 10.  Recurrent Neural Networks"
categories: data
tags: cs231
comments: true
---
Stanfoard [CS231n 2017](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=0) 10강을 요약한 글입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :)  
[RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/) 글과 [Pytorch를 활용한 RNN](https://zzsza.github.io/data/2018/03/17/pytorch-rnn/) 글을 함께 보면 좋을 것 같습니다!


## Recurrent Neural Networks Intro
- Process Sequences 
<img src="https://www.dropbox.com/s/5lvfnk3ahvyarmc/Screenshot%202018-05-27%2020.36.58.png?raw=1">

- RNN : handling variable sized sequence data that allow us to pretty naturally capture all of these different types of setups in our models
	- 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조며, 필요에 따라 다양하고 유연하게 구조를 만들 수 있음
- 히든 노드가 방향을 가진 엣지로 연결되어 순환 구조를 이루는 인공신경망의 한 종류
- fixed size input과 fixed size output을 가지는 몇 문제점이 있지만, RNN은 그래도 유용합니다
	- [![Watch the video](http://img.youtube.com/vi/Zt-7MI9eKEo/0.jpg)](https://youtu.be/Zt-7MI9eKEo) 
	- Sequential Processing of Non-Sequence Data
		- Classify images by taking a series of "glimpses"
		- glimpses : 언뜻 보다, 주어진 이미지의 주변부를 보고 판단
	- Generate images one piece at a time! 

## RNN
<img src="https://www.dropbox.com/s/dty6gz9b8ozv231/Screenshot%202018-05-27%2021.29.29.png?raw=1">

- Notice : function과 parameter들은 모든 time step마다 동일하게 사용됩니다

<img src="https://www.dropbox.com/s/g56om5s3vt62pyx/Screenshot%202018-05-27%2021.31.06.png?raw=1">

- $$W_{hh}$$는 previous hidden state에서 나오는 Weight Matrix
- 활성 함수로 tanh를 사용한 이유는?
	- [Tensorflow KR 게시글](https://www.facebook.com/groups/TensorFlowKR/permalink/478174102523653/) 

<img src="https://www.dropbox.com/s/ugvhfvughor9uhg/Screenshot%202018-05-27%2021.42.22.png?raw=1">

- 왼쪽부터 순차적으로 진행

<img src="https://www.dropbox.com/s/4te17dpv6jrvwzv/Screenshot%202018-05-27%2021.44.29.png?raw=1">

- $$y_{t}$$ : class scores

<img src="https://www.dropbox.com/s/3vh3et9gdfe4yai/Screenshot%202018-05-27%2021.44.49.png?raw=1">

- $$L_{t}$$ : loss
- $$L$$ : sum of individual losses

<img src="https://www.dropbox.com/s/v06pglkpd24d865/Screenshot%202018-05-27%2021.45.03.png?raw=1">

- final  hidden state는 전체 sequence의 모든 context를 요약한 정보를 가지고 있습니다

<img src="https://www.dropbox.com/s/9pkenubq82t1m0q/Screenshot%202018-05-27%2021.45.16.png?raw=1">


<img src="https://www.dropbox.com/s/wtxl8scyquxuaye/Screenshot%202018-05-27%2021.56.20.png?raw=1">

- Seq2Seq Model
- Machine Translation
- Many to one과 one to Many의 조합
- Encoder + Decoder

## Example : Character-level Language Model
- [텐서플로우(TensorFlow)를 이용해서 글자 생성(Text Generation) 해보기](http://solarisailab.com/archives/2487) 코드 참고
- Language model
	- 단어 시퀀스에 대한 확률 분포
	- $$m$$개의 단어가 주어졌을 때 $$m$$개의 단어 시퀀스가 나타날 확률, $$P(w_1, w_2,...,w_m)$$을 할당
	- [언어모델(Language Model)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/09/16/LM/) 참고
- <img src="https://www.dropbox.com/s/6dxp9oo4mnl59cn/Screenshot%202018-05-27%2023.42.56.png?raw=1">
	- Train 과정
- <img src="https://www.dropbox.com/s/f62wo0hebrikjaq/Screenshot%202018-05-28%2000.19.43.png?raw=1">
	- Test
	- 모든 Timestep에서 다음에 올 단어를 예측하고 오차를 Cross Entropy로 구함
- RNN은 매우 느리고 converge되지 않습니다. 많은 memory를 사용하는 것이 RNN의 단점

<img src="https://www.dropbox.com/s/82c25g7hj1ow826/Screenshot%202018-05-28%2001.03.22.png?raw=1">

- 느린 속도를 극복하기 위해 배치별로 나눠서 진행

- [karpathy Code](https://gist.github.com/karpathy/d4dee566867f8291f086)
 
## Image Captioning
<img src="https://www.dropbox.com/s/l7wd6547wpzoa9o/Screenshot%202018-05-28%2015.37.04.png?raw=1">

- caption : variably length sequence
- CNN에서 나온 것을 hidden layer의 초기값으로 설정

<img src="https://www.dropbox.com/s/h53dlkdzr8j5n7e/Screenshot%202018-05-28%2016.43.12.png?raw=1">


## Image Captioning with Attention
<img src="https://www.dropbox.com/s/1sabrzy8y3c0yo8/Screenshot%202018-05-28%2016.43.37.png?raw=1">

<img src="https://www.dropbox.com/s/vef1vpi3bxjt2rm/Screenshot%202018-05-28%2016.44.13.png?raw=1">

- 이미지의 특정 부분에 기반해 단어를 유추했다고 보여줌
- Attention, RNN, LSTM에 대한 내용은 CS231에선 많이 다루고 있지 않습니다. CS224를 참고하면 좋습니다

## Visual Question Answering
<img src="https://www.dropbox.com/s/xs7rgjrmksmffdf/Screenshot%202018-05-28%2016.45.16.png?raw=1">

<img src="https://www.dropbox.com/s/9qd2tet7pkz4uhe/Screenshot%202018-05-28%2016.45.00.png?raw=1">

- RNN과 Computer Vision의 조합은 Cool
- RNN에선 2~3개의 layer를 쌓는 것이 일반적입니다

## Vainilla RNN Gradient Flow
<img src="https://www.dropbox.com/s/l4r8c394rhhmrn5/Screenshot%202018-05-28%2016.49.43.png?raw=1">

- Vanilla RNN의 형태는 Exploding gradients/Vanishing gradients 문제를 가지고 있습니다
	- Exploding gradients의 경우 Gradient clipping을 사용
	- Gradient의 L2 norm이 기준 값을 초과할 때 (threshold/L2 norm)을 곱해주는 것

## Long Short Term Memory (LSTM)
<img src="https://www.dropbox.com/s/0ua2590myagvaaq/Screenshot%202018-05-28%2017.09.36.png?raw=1">


<img src="https://www.dropbox.com/s/ufx6fm508mieoiz/Screenshot%202018-05-28%2017.09.54.png?raw=1">

<img src="https://www.dropbox.com/s/cjex8xirvhcnfms/Screenshot%202018-05-28%2017.10.24.png?raw=1">

- forget gate가 매번 달라지기 때문에 vanishing gradient 문제가 발생하지 않음


## Other RNN Varinats
<img src="https://www.dropbox.com/s/802d14mkp5gdje6/Screenshot%202018-05-28%2017.17.46.png?raw=1">


## Summary
<img src="https://www.dropbox.com/s/2dn08eeekrdis08/Screenshot%202018-05-28%2017.10.41.png?raw=1">

	



## Reference
- [Stanfoard CS231n 2017](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=0)
- [RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
- [언어모델(Language Model)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/09/16/LM/)
- [텐서플로우(TensorFlow)를 이용해서 글자 생성(Text Generation) 해보기](http://solarisailab.com/archives/2487)
