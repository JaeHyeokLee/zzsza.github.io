---
layout: post
title:  "Interpretable Machine Learning"
subtitle: "Interpretable Machine Learning"
categories: data
tags: interpretable
comments: true
---

- 패스트캠퍼스 Alumni Special Seminar, Interpretable Machine Learning 세미나를 듣고 정리한 글입니다

---



### 총평
- 요새 Interpretable Machine Learning의 필요성을 느끼고 LIME이나 깃북을 보며 공부하고 있었는데, 연사님의 좋은 발표 덕에 Interpretable ML의 큰 그림을 더 잘 이해할 수 있게됨
- 많은 사람들이 비슷한 생각을 하고 있구나- 깨달았고, 회사 업무에 어떻게 적용할 수 있을지 고민해볼 예정
- 추후 참고하면 좋을 컨텐츠
	- [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/) : 전반적인 내용이 잘 나와있는 Gitbook
	- [Interpretable Machine Learning, ICML 2017](https://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf) : ICML 2017 Tutorial
	- [Interpretable ML Symposium](http://interpretable.ml/) : NIPS 2017
	- [모두의 연구소 Safe AI Lab](http://pay.modulabs.co.kr/lab_safeai/)
	- [LIME Github](https://github.com/marcotcr/lime)
	- Kaggle [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)
	- Paper
		- [Interpretable machine learning: definitions, methods, and applications](https://arxiv.org/abs/1901.04592) 
		- [Why Should I Trust You? Explaining the Prediction of Any classifier](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)
		- [Deep Lattice Networks and Partial Monotonic functions](https://arxiv.org/abs/1709.06680)
		- [Grad-CAM : Visual Explanations from Deep networks via Gradient-Based localization](https://arxiv.org/abs/1610.02391)
		- [Clinically applicable deep learning for diagnosis and referral in retinal disease](https://www.nature.com/articles/s41591-018-0107-6)
		- [Boolean Decision Rules via Column Generation](https://arxiv.org/abs/1805.09901)
		- [An Interpretable Model with Globally Consistent Explanations for Credit Risk](https://arxiv.org/abs/1811.12615)
		- [Please Stop Explaining Black Box Models for High Stakes Decisions](https://arxiv.org/abs/1811.10154)
	    - [Co-teaching : Robust Training of Deep Neural Networks with Extremely Noisy Labels](https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf)

    
    
---

- [패스트캠퍼스 Alumni Special Seminar - Interpretable Machine Learning](https://www.fastcampus.co.kr/data_seminar_fcalumni/)
- 연사님
	- 윤상웅님
	    - 장병탁 교수님 연구실
	    - SNU Robotics
	    - 해줌

### Intro
- 빌리언즈라는 드라마
- 2명의 트레이더
    - 의견 중재가 필요한 상황
    - 얼마나 확신하는지 물어봄
    - 이 때 답변을 못하면?
- 트레이더는 곧 ML Model
    - 한다면?
        - 과거 데이터 /시뮬레이션에 대해서 철저히 검증됨
        - 알고리즘이 기반한 아이디어가 말이 되고 납득이 됨
        - 리스크가 적음
    - 하지 않는다면?
        - 과거 데이터에 대해서만 검증(=시뮬레이션에 대해서만)
        - 알고리즘이 작동하는 원리가 말이 안됨
        - 리스크가 큼
- ML의 숙명 - 1
    - 에러가 반드시 발생함
        - Bayes Error
            - 정의 : Classification 문제에서 이론적으로 도달할 수 있는 최소한의 오분류율
            - 각 class의 확률분포가 겹치는 만큼 발생
            - Bayes Error보다 오분류율을 줄일 수 없으며, 많은 경우 Bayes Error ≠ 0
            - 새로운 정보를 가진 Feature가 추가되어야만 Bayes Error를 낮출 수 있음
    - 어떤 에러는 아주 치명적
        - loss(암 환자를 정상으로 분류) > loss(정상을 암환자로 분류)
- ML의 숙명 - 2
    - 예측값만 제공
        - 인간을 설득시키기엔 부족함
        - 알고리즘 작동으로 얻는 정보가 너무 적음
    - 일부 알고리즘은 불확실성(uncertainty) 혹은 확정도(confidence)도 함께 제공
        - 그러나 항상 제공하진 않음
- 결국 **신뢰의 문제**
    - ML 알고리즘을 믿고 의존할 수 있는가?
    - ML 알고리즘이 잘못되었을 때 인간이 바로잡을 수 있는가?
    - ML 알고리즘이 예측하지 못한 안 좋은 효과를 일으키지 않는가?


---

### Trusthworthy Machine Learning
- 신뢰할만한 머신러닝, 다양한 분야랑 연결됨
- 1) Interpretable ML
- 2) Causality : 상관관계가 아닌 인과관계를 학습
    - 코잘리티
    - X로부터 Y를 예측할 수 있다고 X가 Y의 원인인 것은 아님
        - X와 Y가 상관관계(correlation)이 있을 뿐
    - 관측 데이터로 인과관계를 알아내는 것은 매우 어려운 일
    - 외부 변수(Confounding variable) → 현실에선 항상 존재
    - 당연한 것(암 걸린 사람이 항암제 처방을 받았을 것이다)을 인과관계로 정의할 수 있는데, 그럼 bias... 그러면 안됨
    - 이론적으로 어디까지 가능한지 학회에서 연구중
- 3) Fairness : 알고리즘의 공정성
    - ML 알고리즘은 편향(bias)되기 쉬움
        - 예시
            - 흑인 사진을 고릴라라고 인식
            - 인도 신부 복장을 입은 사람을 댄서로 인식
        - 수집한 데이터의 편향
        - 데이터셋 제작자의 부주의
- 4) Privacy-preserving ML : 사용자 데이터 보호
    - ML 모델 개발 과정 및 서비스 중 확인을 위해 로그 데이터를 까보는데, 민감한 개인 정보가 노출됨
    - 암호화된 데이터로 학습할 수 있을까?
    - 애초에 데이터 수집 단계에서 Privacy를 보존할 수 있을까?
    - Differential Privacy
        - 설문지에 에이즈 보균자냐고 물어보면 정직하게 답변하지 않을 것
        - 동전을 던져서 앞면이 나오면 무조건 YES, 뒷면이 나오면 솔직한 답을 적음
        - 얻은 YES 응답률에서 0.5를 빼고 2배를 하면 실제 YES 응답 비율을 구할 수 있음
- 5) Adversarial attack : 해킹에 대한 보호
    - DNN의 입력에 작은 perturbation을 출력해 원하는대로 출력을 조절
    - 왜 발생하는지? 어떻게 방어할 것인지? 고민하고 있음

---

### Interpretable Machine Learning
- 예측 결과에 더해서 사람이 이해할 수 있는 형태로 추가적인 정보를 제공할 수 있는 머신러닝 알고리즘을 연구하는 분야
- 목표
    - Interpretability란 무엇인가?
    - Interpretability가 가능한가? 왜 필요한가?
    - UI 기획으로도 풀 수 있음
- 신뢰를 얻는 방법은 상황과 상대에 따라 다름
    - 문제 상황, 데이터셋, 알고리즘, 사용자의 상황에 따라 다르게 접근
    - 새로운 알고리즘만이 해결책은 아님
    - 여러 문제에 맞는 해결책을 **직접 고민해야 함**
        - 시각화는 어떻게? 등
- Interpretable ML의 효용 (From selvaragu, 2017)
    - 어디에 도움이 될 것인가
    - (1) 연구 개발 단계의 ML
        - 인간 수준 이하의 알고리즘의 경우(ex: VQA)
        - 왜 못하는지 ⇒ 확인 후 개선
    - (2) 상용화 수준의 ML
        - 인간 수준에 가깝거나 유용한 수준(ex: Image classification)
        - 사용자들에게 신뢰를 얻고 활용도를 증진
    - (3) 인간 이상의 수준 ML
        - 인간보다 압도적으로 뛰어난
        - Interpretability를 통해 인간을 가르칠 수 있음
- 널리 퍼져 있는 믿음
    - Linear model은 interpretable ( 학계의 정설)
        - Linear Regression, Logistic Regression
- 1) 개별 가중치를 파악
    - 전체적인 규칙을 알 수 있음 ⇒ Global interpretability
    - 함정
        - 다중 공선성
            - 입력 feature간 상관성이 높을 경우
            - weight 값이 직관과 다르게 얻어질 수 있음(그래프를 뿌려보면 양의 상관관계가 보이는데 실제로 나온 계수는 음수)
            - 상관성이 높은 feature를 제거
                - 차원 축소 혹은 feature delete ⇒ 휴리스틱
- 2) 개별 Data point에 대해 예측해야되는 순간
    - 나이, 거리, 등록기간에 따른 재등록 확률을 예측
    - $$w_{i} x_{i}$$ : log odds-ratio에 대한 기여도
    - Local interpretability
- 3) 확률값과 신뢰구간
    - 모델의 출력은 확률값 0 ~ 1
    - 1에 가까우면 확실하게 재등록
    - 0에 가까우면 확실히 이탈
    - 0.5면? → ??
    - 모델의 예측이 얼마나 확실한지 제공
        - Predictive Uncertainty / Confidence Score

- Interpretable ML의 세 방향
    - 1) Global interpretability
        - 모델이 전체적으로 어떤 원리로 예측하는지 설명
        - 분산 분석, 통계 테스트
    - 2) Local interpretability
        - 개별 data point에 대해 예측의 이유 설명
        - $$w_{i} x_{i}$$ 항들의 값을 비교
    - 3) Uncertainty
        - 예측값에 얼마나 확신이 있는지?
    - 1)과 2)는 아직 정립된 용어는 아님

### Global Interpretability

- 1) Monotonicity(단조성)
    - 대출 연체를 했는데 신용등급이 올라간다?
    - 소득이 늘었는데 신용 등급이 떨어진다?
    - 입력 변수와 예측 변수의 단조성을 강제로 넣어줌!
        - 상식과 벗어난 거동을 방지
        - 모델의 전체적인 작동을 설명하기 쉬움
        - 선형모델은 단조성이 보장됨
    - 굉장히 강력한 제한조건
        - 소수의 모델에만 적용 가능
    - Linear 모델은 Monotonic함
    - Gradient Boosting(XGB, LGBM)은 monotonic constraint option 존재
        - 원래 에러를 무조건 줄이라고 하면 사인 함수로 할텐데, 모노토닉 constraint 옵션을 주면 어디는 트리를 만들지 않고 단조 증가/단조 감소 하도록 예측
        - 실용적으론 제일 유용
    - Neural Networks
        - 신경망과 유사한 구조를 사용하며 단조성 조건 추가
        - Tensorflow 구현체 공개됨
            - TensorFlow Lattice
            - [https://github.com/tensorflow/lattice](https://github.com/tensorflow/lattice)
        - 1D Lattice
            - 한 레이어나 유닛
            - 일종의 Look-up Table
            - 5개의 점이 찍혀있는 좌표를 알면 그에 맞는 함수 생성
            - Key가 아닌 값(없는 좌표)을 받으면 linear interpolation
            - 파라미터 값은 gradient descent로 학습
            - 1차원 함수로 다 표현
            - Monotonicity
                - Key1 < Key2이면 Value1 ≤ Value2가 되도록 constraint
            - Activation function의 역할
                - ReLU의 선형결합과 동일
            - Exclusive or 문제
                - Linear는 풀 수 없음
                - 1,3 / 2,4 같은 클래스
                - 단조성 강제하면 모델이 가질 수 있는 함수의 Set이 줄어드니 함수가 약해지는 것이 맞음
                - 단조성이 선형성은 아님
                - 모든 변수에 대해서 모노토릭할 필요는 없음 ⇒ partial monotonic
        - KD Lattice
            - 2^{k}의 격자점
            - 3개의 입력을 받음
            - 격자점이 아닌 값이 들어오면 multilinear interpolation
            - Monotonicity가 필요할 경우 격자점들의 값에 제한조건 부여
        - Deep Lattice Network
            - 쌓아둠
            - Monotonic이라고 지정된 입력이 지나가는 layer는 모두 monotonic constraint
                - constraint를 만족하는 파라미터를 찾는 최적화 과정이 non-trivial
            - Classification/Regression loss를 최소화하도록 SGD를 이용해 학습
            - 단조적 연산만 하도록 강제
            - 모노토릭한 것을 하면 반쯤은 지켜야한다는 constraint가 생김 ⇒ 더 많은 연산량
- 2) Feature Importance
    - Rough한 정보지만 여러 종류 모델에 적용 가능
    - Feature selection과 밀접한 연관
        - 변수가 너무 많을 경우 어떤 변수를 예측모형에 사용할지 선택하는 작업
    - (1) Feature Ablation
        - Feature를 제거하고 돌렸을 때 성능이 얼마나 떨어지는지 확인
        - 성능이 많이 떨어지는 feature가 중요한 feature
        - 장점
            - 무식한 방법이지만 직관적으로 이해하기 쉬움
        - 단점
            - 연산이 많음
    - (2) Mutual information
        - 정보 이론에서 나옴
        - 두 확률변수 X, Y가 서로에게 얼마나 정보를 가지고 있는지 나타내는 값
        - MI(X, Y)=0이면 X와 Y가 독립
        - Corr과 다른 것
            - Corr은 선형 관계만 표시하는데, 이건 비선형도 사용 가능
        - MI가 큰 feature가 중요한 feature일 확률이 높음
        - Nearest neighbor, kernel density estimation 등을 이용한 비모수적(non-parameter) 추정 방법 가능
        - MI가 아니더라도 t-statistic 등 여러 지표로 측정 가능
    - (3) L1 constraint
        - 가중치의 L1 nort으로 regularization
        - 0이 많이 들어간 솔루션을 찾음
        - 중요하지 않은 feature는 weight는 0이 됨, Sparse solution

---

### Local Interpretability
- 이게 연구가 많이 됨
- 1) Local Surrogate
    - **LIME(Local interpretable model-agnostic explanations)**
    - 복잡한 모형을 국지적(local)으로 근사하는 선형 모형을 만듬
    - 특정 포인트 주변에 선형 모형을 fitting
    - 전체에 대해선 모르지만 특정 부분 근처엔 왼쪽은 ~, 오른쪽은 ~
    - [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)
    - Interpretable Data Representation
        - 이것에 대해 생각해볼 수 있음
        - 이 Feature가 항상 의미가 있을까?
        - 해석이 불가능한 것이 있을 수 있음. 이미지에서 개별 픽셀(452번째)이 중요하다고 ???
        - Feature Transform : 루트, 제곱 등 ⇒ 루트는 중요한데 제곱은 중요하지 않다?
        - Feature 자체로 의미가 있는 경우와 없는 경우를 구분
    - LIME
        - f(x) : black-box model based on features
        - g(z) : local explanation based on interpretable representation
        - $$\pi_{x}(z)$$ : 가중치
        - $$\Omega (g)$$ : regularization
        - g(z)가 f(x)를 잘 모사하면서 너무 복잡하지 않도록 학습
- 2) Counterfactual Explanation
    - Counterfactual = What if
        - 만약 이랬다면 다른 결과가 얻어졌을 것이다
        - 가상 혹은 진짜의 데이터 포인트를 제시
        - 입력값 중 하나, 두개 여러개를 바꿈 ⇒ 예측 결과가 달라짐
    - Counterfactual 끝판왕 = 입력을 바꿔서 돌려볼 수 있도록 공개
        - Simulator
            - 사용자가 직접 값을 바꿔보면 어떻게 되는지?
            - 헬스장 거리를 줄여보니 예측값이 좋아짐
        - 단점 : Adversarial attack 등 위험
            - classification을 모두 공개하면 attack할 수 있음
            - 예를 들어 보험료 산정 알고리즘이면 그걸 해킹해보고.. 특정 조건을 찾아서 내 보험료를 깎을 수 있음
- 3) Gradient-based Explanation
    - Grad CAM
        - 가장 성공적이고 널리 쓰이는 방법
        - Class activation 에 가장 기여하는 hidden neuron을 찾음
            - 가장 윗층의 conv layer 사용
                - 가장 high level 정보
                - 공간적인(Spatial) 정보를 보유하고 있음
        - 각 Feautre map의 기여도를 계산함(Global Average Pooling in CAM)
        - Class activation Map L을 얻음
        - L을 upsample(bilinear)하여 입력 image에 시각화
        - 장점
            - Classification만 했는데 Detection 가능
                - 여러 object 가능
            - 학습된 네트워크에 적용하기만 하면 됨
                - 재학습, 구조 변경이 없어서 예측 성능 유지
            - guided backpropagation과 함께 사용해 더 향상
            - ConvNet으로 시작하는 task는 모두 적용 가능
    - 예측 결과에 중요한 이미지 부분을 heatmap 형태로 제공
        - 일반적인 ConvNet에 대해
        - Image classification, captioning, visual QA 등 여러 종류의 task에 사용 가능
        - 네트워크에 변형도 가하지 않고 어떤 성능 저하도 없음
    - Grad CAM 실험
        - 인간 실험
            - 실제 사람들에게 설문조사 같은 것을 뿌림
            - 이 방법을 사용했을 때 더 신뢰를 느끼는가?
            - 결론 : Grad-CAM visualization이 더 믿을만한 알고리즘을 고르는데 도움이 됨
        - 모델 실패 사례 분석
            - vgg16이 잘못 예측하는 경우를 분석
            - 어째서 잘못된 예측을 하는가?
            - 관측된 사례
                - Class label의 모호함
                - 헷갈릴만한 경우
            - 잘못된 예측에 대해 엉뚱한 CAM이 나오면 모델 학습에 문제가 있다는 것
            - 하지만 구체적 개선방안이 언제나 명확한 것은 아님
- Class 레이블의 애매한 이유
    - 레이블러의 실수 때문(노이즈 때문) 이라면 통계적 접근을 할 수 있을..(딥뉴럴넷의 특징)
    - Co-teaching : Robust Training of Deep Neural Networks with Extremely Noisy Labels
    - [https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf](https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf)
    - 학습 과정 중간을 보면 레이블이 제대로 된 것부터 학습하고, 아닌 것에 나중에 fitting(나중에 training error가 줄어듬)
        - 잘못달린 것 같은데? 라고 의심할 수 있고 로버스트하게.. 제안
    - 이론적 근거가 확실하진 않지만 실험적으로 관찰되는 것

### Uncertainty

- 설명이 아닌 예측에 대해 얼마나 확신을 가지는지 제시할 수 있는 기법에 대해 이야기함
- Class 1, 2 분류하는 알고리즘
- 좋은 분류기라면 확실한 것은 확실하다 말하고 경계선은 약간의 불확실성이 있다고 말할 수 있을 듯
- Confidence Score
    - ML 알고리즘은 예측값만 출력하는 것이 일반적
    - 그러나 알고리즘에 따라 내부적인 변수를 활용해 예측에 얼마나 확신을 가지고 있는지 알 수 있는 경우가 있음
    - "확률"의 형태를 띄고 있지 않음 → 직관적으로 이해하기 어려움
        - SVM Margin
        - Ensemble Methods : 개별 tree들의 예측이 일치하면 high confidence, 불일치하면 low confidence
- Predictive Probability
    - 로지스틱 회귀나 뉴럴넷은 아웃풋을 확률의 형태로 출력할 수 있음
        - 확률값이 1에 가까우면 high confidence, 아니면 low confidence
    - 정량적으로 (Shannon) Entropy를 기준으로 판단할 수 있음
        - 3개의 클래스로 예측한 후
            - 결과 확률이 많이 차이가 안나는 경우 (비등비등)
            - 결과 확률이 차이가 크게 나는 경우
        - 엔트로피 계산 : 데이터가 얼마나 무질서에 가까운지 측정
            - 유니폼 분포라면 엔트로피가 높고, 극단적인 분포라면 엔트로피가 낮음
            - 엔트로피가 높으면 확신을 많이 하고있진 않겠구나..
    - 응용
        - Active Learning
            - 데이터셋을 수집하는 단계부터 생각
            - Label된 데이터가 조금 있고, 새로 취득할 때 어떤 데이터를 취득해야 모델의 학습에 도움이 될까?
            - Labeling은 비싼 작업이므로 최대한 효율적으로 데이터를 수집하는 것이 중요
            - Idea : 모델이 가장 헷갈려하는 데이터를 수집
            - 현재 데이터로 모델 학습
            - Unlabeled data들에 대해 예측을 수행
            - 예측 확률의 entropy가 가장 큰 sample들을 labeling
            - 학습 데이터셋에 추가
            - **어떤 것에 취득해야 할까 알고리즘 학습에 도움이 될까?**가 핵심
                - unlabeld 데이터에 모두 예측을 한 후, 엔트로피가 높은 것을 선택
    - 함정
        - 최근에 Report된 현상
        - 확률값이 부정확하단 말이 있음(not calibrated)
        - $$P(Y=1\vert X)$$가 0.5인 데이터들을 모아놓고 보면 
	        - $$P(Y=1\vert X) ≠ 0.5$$
        - 이 현상을 개선하기 위해 많은 연구가 이루어짐
- Bayesian Machine Learning
    - 불확실성을 위해 많이 사용하는 기법
    - Bayesian 통계에선 확률=확신의 세기
    - 모델의 parametere들을 distribution을 갖는 random variable로 취급
    - 단점
        - 계산이 어려움
        - closed-form solution이 없을 수 있어 근사가 필요
        - 필요한 계산 양이 많음
    - Gaussian Process Regression
        - 가장 깔끔하고 아름다운 머신러닝 알고리즘 중 한나
        - 통계학에서 확률 과정(Stochastic Process) 중 하나인 Gaussian Process를 확장해 ML에 사용할 수 있도록 한 것
        - 무한히 많은 개수일 때 어떻게 생각할 수 있을까?
        - 무한개 중 아무렇게나 N개를 골라도 가우시안 분포를 가진다고 정의
        - 머신러닝에선 예측에 쓸 수 있도록 조금 바꿈
            - 학습 데이터와 예측 대상이 되는 y가 모두 하나의 Gaussian distribution을 따른다고 가정
            - 평균이 0이고 covariance matrix가 커널 function으로 주어짐
            - conditional gaussian distribution의 공식에 의해 평균과 분산을 구함
        - Predictive uncertainty가 closed form으로 구해지는 몇 안되는 사례
        - Inverse 곱해서 N^{3}이라 몇천개 넘어가면 계산 속도가 급격하게 하락
        - 응용 : Bayesian Optimization
            - Auto ML을 시작시킨 개념
            - Active Learning과 유사
    - 베이지안 뉴럴넷을 다루고 싶었는데 시간 관계나 난이도상 못다루긴 했는데, 관심이 있다면 더 찾아보기
        - [https://arxiv.org/abs/1506.02142](https://arxiv.org/abs/1506.02142)

- 제공되는 형태에 초점을 맞춤
    - 모델 전체에 대해 interpretable을 줄 것인가? 개별에 대한 interpretable을 줄 것인가?

---

### 애초에 Interpretable ML이 가능할까?
- 애초에 가능하면 왜 이 사단이 났을까?
- 데이터엔 신호와 소음으로 나눠서 생각
    - 데이터마다 다 다름! 어떤 데이터는 신호가 낮고 소음이 많고 어떤 데이터는 신호가 높고 소음이 적음
    - 낮은 신호/높은 소음 : 베이즈 에러가 높다고 볼 수 있음 ⇒ 주식/금융 데이터, 사회과학 데이터
        - 단순한 모델
        - 소음이 높으면 오버피팅을 막아야 함
        - 데이터의 노이즈를 잘 무시하는 것이 관건
        - 단순한 모델이니 설명할 필요가 없음
    - 높은 신호/낮은 소음 : 베이즈 에러가 낮다고 볼 수 있음 ⇒ 사진, 음성, 텍스트, 바둑
        - 복잡한 모델
        - 언더피팅이 문제
        - 데이터가 많이 필요
        - 복잡한 패턴을 포착하는 것이 목표
- 고슴도치와 여우
    - 고슴도치
        - 초기에 설정한 규칙을 고수, 예측을 잘 못함
        - 근본적 아이디어가 모든 것을 결정한다고 생각
    - 여우
        - 사소한 생각과 증거들을 고려하고 면밀히 살핌
        - 여러 접근 방법을 동시에 적용
        - 한두마디로 설명할 수 없지만 예측력 상승
        - 앙상블 알고리즘, ML 알고리즘
- Human-(in/out of)-the Loop
    - 데이터를 이용해 할 수 있는 일
    - 1) 데이터 시각화
    - 2) 지식의 생산 (가설 검정)
    - 3) 예측
    - <img src="https://www.dropbox.com/s/99wxe8sbfrornb6/스크린샷 2019-04-14 15.45.09.png?raw=1">
    - Human in the Loop이 interpretable이 중요한 듯
- 신용평가 Credit Scoring 사례
    - 이 사람이 돈을 빌렸을 때 잘 갚을 수 있을까?
    - 굉장히 큰 임팩트를 주고 있다는 것을 깨달음
    - 역사
        - 1941년 미국에서 시작
        - 1950년대 후반 Fiar & Isaac에 의해 방법론 정립 후 상용화 ( Score card & Logistic regression)
        - 현재 FICO는 세계 최대의 신용정보 회사
            - FICO Score가 미국인들에겐 신용등급과 동일한 단어
    - 우리 나라는 NICE신용평가, KCB(Korea Credit Bureau)이 수행
    - <img src="https://www.dropbox.com/s/q8hajyc0hm5uu1j/스크린샷 2019-04-14 15.49.30.png?raw=1">
    - NIPS 2018 워크샵
        - [https://sites.google.com/view/feap-ai4fin-2018/](https://sites.google.com/view/feap-ai4fin-2018/)
        - [https://community.fico.com/s/explainable-machine-learning-challenge](https://community.fico.com/s/explainable-machine-learning-challenge)
        - Winner : IBM 리서치팀
            - [http://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation](http://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation)
            - 명시적인 룰을 뽑아내는 알고리즘을 만듬
        - 2등
            - Duke 대학팀
            - 왜 굳이 Black-Box 모델을 쓰고 그걸 설명하려고 하나?
            - 애초부터 transparent한 white-box 모델을 쓰자
            - 거의 유사한 성능을 낸다!
            - [http://dukedatasciencefico.cs.duke.edu/](http://dukedatasciencefico.cs.duke.edu/)
            - 웹 데모로 직관적으로 표현
            - 히든 뉴런에 이름이 달려있음
            - 발표자님이 이걸 좋아한 이유 : 단순히 알고리즘이 중요한 것이 아닐 수 있음! 보여주는 과정도 생각을
- Medical 사례
    - [https://nips.cc/Conferences/2018/Schedule?showEvent=12346](https://nips.cc/Conferences/2018/Schedule?showEvent=12346)
    - 이 부분을 보니 환자가 ~~ 병이야
    - 영국에서 만든 어플리케이션
        - 유방암 진단을 받은 사람이 얼마나 더 살 수 있는지?
        - [https://breast.predict.nhs.uk/index.html](https://breast.predict.nhs.uk/index.html)
        - 결과를 보여줄 때 생존 확률을 보여주고, 아이콘 형태로 100명을 가정할 때 몇명이 살아나는 확률입니다 (심리적으로 검증) 라고 보여줌
        - 어플리케이션 자체가 신뢰를 얻어야 함
        - 의사, 간호사, 환자, 환자 부모님, 연구자 등 다양한 이해 관계자가 설명을 얻어가는 포인트가 다름
- Communicating Uncertainty
    - 생각보다 사람은 확률을 잘 인지하지 못한다
    - 생존 확률이 0.92 → 와닿지 않음
    - 100명 중 92명이 생존 → 와닿음
    - Heart Age
        - 나의 심장질환 발병확률(위험)이 몇 세의 확률과 같은가?
- Explainable AI는 종합예술
    - 다면적인 설명이 제공될 필요가 있음
        - 인간은 본능적으로 거짓이 아닌 것은 여러 각도에서 보아도 참이라는 것을 알고 있음
        - 다양한 측면에서 제공된 정보가 모두 일치할 때 신뢰를 얻을 수 있음
    - 복합적인 요소가 작용
        - 심리, 통계, 윤리, 디자인, 언어
        - 사용자 스터디, 좋은 기획, 좋은 디자인이 필요
        - 끝판왕 알고리즘 하나로 해결되는 문제가 아님!
- 다하지 못한 이야기
    - Shapley Value
    - Bayesian Deep Learning
    - 하나 더 있었는데 발표자료 보고 작성


### 질문
- Self Attention과 Grad CAM의 차이는?
    - 아이디어 공유되는 것이 있음
    - Attention 기법은 모델을 설계할 때 애초에 입력한 데이터를 중점적으로 볼지 말지 취사선택 하는 모듈을 중간에 넣는 것
