var tipuesearch = {"pages": [{
    "title": "글 쓰는 개발자 모임 - 글또 4기 다짐",
    "text": "제가 운영하고 있는 글 쓰는 개발자 모임, 글또 4기 다짐 글입니다 글또 4기 벌써 4기째! 매 기수가 끝나고, 새로운 기수가 시작되면 다양한 생각이 들곤합니다 이번 기수는 저번 기수보다 더 많은 분들이 지원해주셨습니다. 그 중, 69명과 함께 합니다! 3기 회고 글에 4기에 80명 정도 함께하려나..? 했는데 그정도는 아니였네요! 사람들이 많아진 만큼, 서로에게 좋은 영향력을 끼치면 좋을 것 같네요 :) 4기를 시작하며 준비한 것 로고 우선 로고를 만들었습니다! 글또 1~3기를 하신 김나영님께서 이쁜 로고를 만들어주셨습니다 예전부터 만들고 싶었는데, 이제 로고가 생겨서 너무 기쁘네요-! 조만간 스티커를 만들 예정입니다 :) 글또 관련 문서 글또 관련 규칙을 발표 자료로 유지했는데, 이번에 Notion에 글을 작성했습니다! 글또 노션에서 확인할 수 있고, 가이드 문서, 자료 모음 등 다양한 부분을 계속 보강할 예정입니다 글또 활동을 하지 않아도, 이런 모임을 만들고 싶은 분들에게 좋은 영향력을 드리고 싶습니다 온라인 OT 과거 기수에선 계속 첫 모임은 오프라인!을 고집해왔습니다 하지만 이번엔 약 70명의 글또분들이 계시고, 코로나로 인해 모임을 하기 어렵다고 판단했습니다 따라서 유튜브 온라인에서 오리엔테이션을 진행했습니다 영상은 유튜브에서 확인할 수 있습니다 온라인으로 진행한 부분이 생각보다 신선하고 긍정적인 피드백이 있어서, 종종 온라인으로 같이 피드백, 의논 등을 해볼까 생각하고 있습니다 규칙 개정 및 업무 자동화 기수가 지나며 점점 개선해야 할 규칙들을 개정하고 있습니다 1) 다른 직군 피드백이 아닌, 같은 팀 내 피드백 하도록 변경 2) 많이 헷갈릴 수 있는 피드백 부분 개선 =&gt; 자동화팀에서 구현해주실 예정 3) 블로그 글 대신 유튜브 글도 허용하도록 규칙 개정 =&gt; 이 부분은 실제로 하실 분이 얼마나 계실지 모르겠지만! 일단 컨텐츠의 다양성을 존중하기로 했습니다 이번엔 무슨 글을 작성할 것인가? 사실 저는 글또 1~3기를 하며 한번도 예치금을 잃어본 적이 없어서(Pass도 한번도 쓰지 않은 것 같네요) 예치금 없이 Pass 꼭 해야지! 이런 생각은 없습니다 다만 이번엔 글의 퀄리티를 더 높이기 위한 노력을 하려고 합니다 다양한 요소를 엮은 통찰력 있는 글 작성 실제 업무하며 겪을 수 있는 내용을 일반화해서 글 작성 리더십, 동기부여에 관한 글 기술적 부분에 대해 작성하고 싶은 테마는 아래와 같습니다 여전히 MLOps. 아직도 다양한 방법론이 있고, 그걸 어떻게 블럭같이 엮을지는 시도가 필요한 상태 Operation Research와 Machine Learning의 결합 Graph 이론 그 외에 개발 분야가 아닌 책의 후기도 블로그에 남기고 싶습니다!(일단 읽기라도 해야..) 이번엔 유튜브를 진짜 시작할 것인가? 사실 유튜브를 진짜!!!!! 하려고 다짐했습니다 채널도 만들었습니다. 일명 카일스쿨 주로 이야기할 내용은 다음과 같습니다 1) 신입 데이터 분석가가 알면 좋을 내용(어떻게 일할 것인가, 업무의 방식, 데이터 시각화, 간단한 개발 상식 등) 머신러닝이나 통계학 내용은 인터넷에 매우 많은데, 이 부분은 부족한 것 같아 꾸준히 영상으로 남길 예정입니다 2) 데이터 관련 커리어에 대한 이야기(취업, 이직 등) 특히 회사를 어떻게 찾아보고, 어떻게 준비할 것인가에 대한 제 전략을 공유할 예정입니다. 실제로 꽤- 많이 효과보았고, 면접을 많이 보는 사람의 입장도 섞을 예정입니다 3) Google Cloud Platform GCP를 사용한지 벌써 4년이 넘었습니다. 이제 AWS보다 GCP가 더 익숙하네요 GCP 관련 유튜브 자료는 영어론 많지만, 한국어론 아직 적습니다. 지인은 글로벌하게 영어로 하라고 했지만 저는 일단 한국분들도 많이 GCP를 알 수 있도록 최대한 쉽게 설명하는 영상을 만드려고 합니다 특히 BigQuery부터 시작할 생각입니다 기타 MLOps나 Operation Research는 제 여건이 된다면.. 최소 2주에 영상 1개는 올리고 싶네요. 과연 어떻게 될지..! 취업 컨설팅 글또 4기분들 중 희망자에 한해서(주로 데이터 직군) 취업 컨설팅을 합니다 이미 희망자를 모두 받았고, 자신에 대해 작성하는 시간을 가졌습니다 유튜브 영상에도 이야기할 예정이지만, 그래도 글또분들의 성장을 지켜보고 싶어 따로 컨설팅하기로 했습니다 6개월 내 취업이란 목표를 가지고 있는데, 모두 좋은 곳으로 가실 수 있도록 열심히 돕도록 하겠습니다!",
    "tags": "diary",
    "url": "/diary/2020/02/29/geultto-4th/"
  },{
    "title": "What If Tool 논문 리뷰 및 사용 방법?",
    "text": "~~~ 도구인 What If Tool(WIT) 논문 리뷰 및 사용 방법에 대해 작성한 글입니다 https://www.groundai.com/project/the-what-if-tool-interactive-probing-of-machine-learning-models/1 여기서 보구 번역기 해도될듯 What If Tool이란? 머신러닝 시스템을 개발하고 deploy하는 큰 문제는 input의 다양함에 따라 성능이 어떻게 되는지 이해하는지가 중요함. 이 도전 때문에 What If Tool을 만들었고, 코딩을 최소화하고 데이터를 조사하고, 시각화하고, ML 시스템을 분석한다 What if Tool을 사용하면 실무자는 가상의 상황에서 성능을 테스트하고 다양한 데이터 feature의 중요성을 분석해 여러 모델 및 input 데이터 하위 집합에서 모델 동작을 시각화할 수 있음 실무자들은 ML 공정성(fairness) 지표에 따라 시스템을 측정할 수 있음. 도구의 디자인을 설명하고 다른 조직의 실제 사용량에 대해 보고함 1. Introduction 머신러닝 작업에서 그들의 모델 성능이 좋은지 나쁜지 이해하고 싶어함. 이 데이터 포인트가 내 예측에 얼마나 변화를 주는가? 내 모델은 여러 그룹에서 다양한 퍼포먼스를 내는지? WIT 툴은 모델을 이해할 수 있는 인터랙티브한 도구임 도구를 사용하기 위해서 트레인 모델과 샘플 데이터셋만 있으면 됨. Tool은 Tensorboard의 일부로 사용할 수 있고, Jupyter나 Colab에서 사용할 수 있음 WIT는 iterative what if 탐색을 제공한다. 유저는 counterfactual(만약 ~ 다면) 추론을 하고 dicision boundaries를 조사하고, 데이터 포인트에 대한 일반적인 변경 사항이 예측에 미치는 영향을 탐색해 모델이 어떻게 동작하는지 시뮬레이션 할 수 있음 입력 데이터 및 모델 성능의 유연한 시각화를 지원하고 사용자가 다른 관점으로 볼 수 있도록 해줌 이런 시각화의 중요한 motivation은 데이터 세트의 다른 feature를 조합해 조각화하는 것임. (여러 feature의 조합) intersectional analysis, 교차 분석이라 부르고 이런 분석은 모델의 fairness 조사와 관련된 문제를 이해하는데 중요함 WIT는 로컬(단일 데이터 포인트에 대한 decision 분석) 및 전체(전체에 대한 모델 동작 이해)를 모두 지원함. 또한 다양한 데이터 및 모델 유형을 지원함 이 paper에선 도구의 디자인에 대해 설명하고 ML 시스템 분석에 실제로 적용되는 방법을 시나리오로 안내함. 사용자는 실제 시스템에 대한 놀라운 사실을 발견할 수 있었음. 결과는 가설 탐색을 지원하는 것이 ML 시스템의 동작을 이해하는 강력한 방법임을 제안함 2. Related Work WIT는 ML 모델 이해하는 프레임워크와 유연한 시각화 플랫폼 2개의 영역을 가지고 있음 2.1 Model understaging frameworks WIT는 black box의 범주에 속함 모델의 내부에 의존하지 않지만 사용자가 입력 및 출력만 probe할 수 있도록 설계됨 이 제한은 실제 제약 조건(모델 내부에 항상 액세스 할 수 있는 것은 아니라는)을 반영하지만 매우 일반적인 것을 의미함 다른 여러 시스템이 블랙박스 방식을 취함 Uber의 Manifold는 WIT와 동일한 기능을 제공함. 예를 들어 두 모델을 비교할 수 있는 정교한 시각화 세트가 있음. EnsembleMatrix는 앙상블을 구성하는 모델을 비교하기 위해 고안된 반면 RuleMatrix는 간단한 규칙으로 ML 모델을 설명하려고 시도함 ModelTracker는 WIT와 유사한 시스템으로, 샘플 데이터에서 모델 동작을 풍부하게 시각화함 Procpector도 WIT와 유사한데, 블랙박스 모델을 눈으로 검사할 수 있는 도구. 특정 데이터 포인트를 드릴 다운해 feature 값을 조작해 모델 예측에 미치는 영향을 확인할 수 있음. Prospector는 단일 클래스 예측 모델에 대한 input feature의 직교성에 의존하지만, WIT는 잠재적으로 상효 연관되거나 혼동된 여러 기능에 대한 교차 분석을 제공함 기타 유사한 시스템은 GAMut 툴의 중요한 motivation은 모델 이해와 사전 시각화 작업을 일반화하려는 것임 WIT의 주요 기능은 모델에서 mL 공정성 메트릭을 계산할 수 있음. IBM Ai Fairness 360, Audit AI, GAMut 도 유사한 기능을 제공함. 인터랙티브하게 최적화 과정을 적용해 사후 훈련 분류 임계값을 조정해 메트릭을 향상시킴 2.2 Flexibla visualization platform WIT는 DAtapoint Editor tab이 있음. 이 부분은 Facets Dive으로 구성됨. 이걸 사용하면 여러 속성의 교차를 탐색해 input 데이터와 모델 결과의 사용자가 원하는대로 볼 수 있음 X축, Y축 색상에 대한 인코딩을 빠르게 보여주는 것은 tableau에서 제공하는 일부 기능의 단순한 버전이고, 동일한 기능을 부드러운 애니메이션을 사용해 사용자가 인코딩간 전환을 이해하는데 도움이 되는 Micrsoft pivot tool도 있음 Facets Dive는 local에 의존해 inmemory storage와 계산 기반으로 함. 민감한 데이터를 보호할 수 있음. 액세스가 제한되어 분석을 위해 원격 서버로 전송하는 것은 불가능함. 원활한 탐색도 가능 3. BACKGROUND AND OVERALL DESIGN 3.1 User Needs 1) 최소한의 코드로 다양한 가설을 테스트하고 싶음 2) 시각화를 모델 이해를 위한 매개체로 사용하고 싶음 3) 모드의 내부 작업에 액세스하지 않고 가상 테스트 WIT는 모델을 블랙박스로 다루어 “나이의 가치를 높이는 것이 모델의 예측 점수에 어떤 영향을 미칠까?” 또는 “데이터 포인트에서 다른 결과를 위해 무엇을 변경해야 하는가?”와 같은 질문에 답하기 위해 가상의 엔드투엔드 모델 행동에 대한 설명을 생성하도록 도와야 한다. 가설로 사용자는 하나 이상의 지정된 치수를 따라 데이터 지점의 동요에 대한 모델 성능을 테스트할 수 있다. 모델 내부를 이용할 수 없다면, 가설을 사용하여 생성된 설명은 모델에 구애받지 않고 동요에 일반화된다. 이는 설명과 표현의 유연성을 증가시킨다. 특히 평가되는 모델이 매우 복잡하기 때문에 의미 있게 해석할 수 없는 경우 [27]. 가설은 새로운 추측과 What-If 시나리오에 대한 모델 추론 결과의 조건 테스트 시 특히 강력하다. 4) 모델 성능에 대한 탐색적 교차 분석 수행 사용자는 종종 모형이 예기치 않게 수행되는 데이터의 하위 집합에 관심을 갖는다. 예를 들어, Buolamwini와 Gebru[9]는 특징만 아닌 교차로 부분군의 정확도를 조사하였고, 상업적으로 이용할 수 있는 이미지 분류기 3개가 시험 데이터에서 어두운 여성 부분군에서 가장 낮은 성능을 나타냈다고 밝혔다. 모델이 하위 그룹에서 높은 정확도를 달성하더라도, 잘못된 양의 비율과 거짓된 음의 비율은 크게 달라서 실제 결과를 초래할 수 있다[7]. 이러한 하위 집합을 정의하는 방법은 여러 가지가 있으므로 탐색 데이터 분석을 위한 시각화는 입력 데이터 유형, 모델 작업 및 최적화 전략에 대한 유연성, 확장성 및 사용자 지정을 유지해야 한다. 인스턴스(instance), 형상(feature), 부분 집합(subset) 및 결과 수준의 보완적인 뷰는 동일한 데이터 집합에서 여러 모델의 성능을 비교할 때 특히 통찰력이 뛰어나다. 5) 여러 모델에 대한 잠재적 성능 개선 평가 모델 개발에서는 분류 임계값을 변경하는 등 변화의 영향을 추적하기 어려울 수 있다. 예를 들어, 인구통계학적 패리티에 대한 임계값을 최적화하면 모든 그룹이 “유익한” 분류의 동일한 부분을 받도록 보장할 수 있으며, 반면 진정한 양의 비율이 낮은 하위 그룹은 불리하게 작용할 수 있다[13]. 훈련 데이터나 모델 하이퍼파라미터의 성능을 향상시키기 위해 변경하기 전에 다양한 비용에 대한 다양한 최적화 전략을 현지 및 전 세계적으로 테스트하고자 할 수 있다. 사용자는 분석을 오프라인으로 전환할 필요 없이 소규모 데이터 세트의 탐색 공간에서 바람직하지 않은 모델 동작을 완화하는 전략을 테스트하여 모델 성능을 대화형으로 디버그할 수 있어야 한다. 사용자가 모델의 변경을 시험하는 밀접하게 관련된 방법은 “제어” 모델의 성능을 벤치마크의 “실험” 버전에 비교하는 것이다. 흠 모델 하나 만들어봤는데 잘 안되네.. OTL 논문 https://arxiv.org/pdf/1907.04135.pdf Github https://github.com/PAIR-code/what-if-tool 아직 xgboost, sklearn 같은 곳에 직접 할 순 없음. ML Engine을 사용해야 할듯 공식 홈페이지의 What if tool 설명 https://www.tensorflow.org/tensorboard/what_if_tool ML Engine에서 What if Tool 사용하기 https://cloud.google.com/ml-engine/docs/using-what-if-tool?hl=ko Video https://www.youtube.com/watch?time_continue=6&amp;v=hpYl8WLYeKo&amp;feature=emb_title 송호연님 영상 https://www.youtube.com/watch?v=ZAS2FwlLTNE Notebook에서 사용하는 예제 https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/interactive_inference/README.md#notebook-mode-details Google 블로그 글 https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html 블로그 글 https://towardsdatascience.com/using-what-if-tool-to-investigate-machine-learning-models-913c7d4118f 모델 비교 Colab https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_Toxicity_Text_Model_Comparison.ipynb 예제 https://pair-code.github.io/what-if-tool/uci.html",
    "tags": "feature mlops",
    "url": "/mlops/2020/02/17/what-if-tool/"
  },{
    "title": "BigQuery non-partition Table을 partition Table로 옮기기",
    "text": "BigQuery의 non-partitioned Table을 partition Table로 옮기는 방법에 대해 작성한 글입니다 BigQuery Partitioned Tables BigQuery엔 파티션을 기반으로 저장하는 Table이 있음 데이터를 보다 쉽게 관리하고, 쿼리할 수 있게 해줌 파티션을 추가하는 경우 장점은 다음과 같음 Query 탐색 범위를 줄임(=쿼리 탐색 비용 감소) BigQuery는 쿼리가 데이터를 탐색하는 양만큼 비용을 부과함 파티션이 걸린 경우 해당 파티션 범위만 탐색하기 때문에 비용이 덜 부과될 수 있음 Query 속도가 빨라짐(=쿼리 성능 증가) BigQuery에서 데이터의 양에 따라 쿼리 소요 시간이 달라짐 데이터의 양이 적으면 더 빠르게 결과가 나타남 Partition 나누는 기준 1) Ingestion time(수집 시간) : 데이터 수집(로드) 날짜를 기준으로 파티션 나눔 2) DATE/TIMESTAMP : Date나 Timestamp로 파티션을 나눔 3) Integer(정수 범위) : 정수 값을 기준으로 파티션을 나눔 테이블 샤딩 날짜/타임스탬프로 파티션을 나눈 테이블이 아닌 Table 이름에 suffix를 붙여서 테이블을 샤딩할 수 있음 예: firebase 데이터가 저장되는 analytics_XXX 데이터셋의 Table들(analytics_XXX.events_20200211 등) 날짜/타임스탬프로 파티션을 나눈 테이블이 샤딩된 테이블보다 성능이 뛰어남 샤딩 테이블을 만들 땐 BigQuery가 각 테이블의 스키마 및 메타데이터 복사본을 유지해야 하고, 각 쿼리 대상 테이블의 권한을 확인해야 함 쿼리 오버헤드가 추가되고 쿼리 성능에 영향을 미침 따라서 샤딩보다 날짜/타임스탬프로 파티션을 나눈 테이블 사용하는 것이 좋음 샤딩된 테이블은 쿼리시 최대 1,000개의 테이블만 참조할 수 있고, 날짜/타임스탬프로 파티션을 나눈 테이블은 최대 4,000개의 파티션을 가질 수 있음 Non-partitioned Table을 Partitioned Table로 복사하기 특정 Task를 반복해서 수행하는 경우 예를 들어 서비스의 Raw 데이터를 가공해 테이블에 적재하는 경우 처음부터 만든다면 아예 Partitioned Table로 만들어서 Airflow 등에서 실행시키면 됨 단, 이미 서비스가 어느정도 운영한 후에 특정 시점에 데이터를 Migration하고 그 다음 날부터 Airflow DAG을 돌리고 싶은 경우 과거 데이터를 한번에 복사해야 함 과거 데이터를 복사할 때, BigQuery UI에서 쿼리를 날리고 Table로 저장하는 방법이 있음 그러나 “Incompatible table partitioning specification when copying to the column partitioned table” Error가 발생함 Partition Spec이 호환되지 않아 발생한 문제 터미널에서 bq query를 사용함 bq query가 길어질 것 같아 쉘스크립트 작성(query_to_partition_table.sh) bq query는 Query의 결과를 Table로 바로 저장해주고, 파티션 옵션도 설정할 수 있음 destination_table의 뒤를 수정하고 맨 아래 SELECT 부분을 수정하면 됨 time_partitioning_field는 date_kr이고, require_partition_filter=True를 주면 쿼리할 때 항상 WHERE 절에 파티션 필터를 걸어야 함 BigQuery를 안정적으로 운영하기 위해 이 옵션은 꼭 주는 것을 추천 #!/bin/bash bq --location=US query \\ --destination_table {dataset}.{table} \\ --time_partitioning_field date_kr \\ --require_partition_filter=True \\ --use_legacy_sql=false \\ ' SELECT *, date_kr FROM Table ' 실행 bash query_to_partition_table.sh 혹시 Error가 발생한다면 bq command가 제대로 작동하는지? 작동하지 않으면 Cloud SDK를 설치 공식 문서 참고해서 설치하면 됨 gcloud에 원하는 프로젝트 설정이 되어있는지? 아래 명령어를 통해 현재 로컬에 설치된 gcloud 설정을 확인 gcloud config configurations list 원하는 프로젝트의 설정이 없다면 gcloud init을 통해 새 설정을 추가 SELECT 쿼리에서 ‘‘을 사용하는 경우 예 : SELECT * FROM Table WHERE event_name=’user_engagement’ \"”로 수정해야 함. 쉘 스크립트에선 ‘‘와 ““이 다름 스택오버플로우의 Difference between single and double quotes in Bash 글에 자세히 나옴 Reference Google Cloud Platform Document Managing partitioned tables",
    "tags": "BigQuery gcp",
    "url": "/gcp/2020/02/11/bigquery_query_to_partition_table/"
  },{
    "title": "머신러닝 프레임워크 Hopsworks",
    "text": "머신러닝 프레임워크 Hopsworks를 설치하고 사용하며 작성한 글입니다 설치 이준범님의 블로그를 보면 AWS에서 설치하는 과정에 나옴 GCP에서 설치할 예정이며, Documentㄹ를 참고함 프로젝트 셋팅 GCP 계정을 만들고, 프로젝트 생성 Hopsworks Image 생성",
    "tags": "feature mlops",
    "url": "/mlops/2020/02/05/hopsworks/"
  },{
    "title": "Machine Learning의 Feature Store란?",
    "text": "머신러닝의 Feature Store에 대해 작성한 글입니다 Feature Store가 왜 필요한지?와 어떤 종류들이 있는지? 등에 대해 작성했습니다 머신러닝의 모델링 과정 머신러닝 모델링 업무의 큰 흐름은 다음과 같음 데이터 적재 데이터 EDA 및 분석 Feature Engineering 모델링(모델 선정, 하이퍼 파라미터 튜닝, metric 선정 과정 포함) Test Set에 성능 개선되었는지 확인 모델이 완성되면, 그 후에 실제 서비스에 활용하기 위해 아래와 같은 과정을 거침(Production, Serving 과정) 실시간 데이터 적재 확인 실시간 데이터에 기반한 Feature Engineering 모델 학습 필요시 모델 업데이트 Inference Production시 발생할 수 있는 문제 1) 데이터 스트리밍 파이프라인 존재 유무 데이터 ETL 파이프라인은 초반엔 배치성으로 생성해 관리하는 경우가 있음(더 쉽고, 빠르게 가능) 이 경우, 모델을 Production화하기 위해 데이터를 실시간으로 적재해야 함 Kafka, Redis, GCP Pub/Sub, REST API 등을 활용해 데이터를 적재 단, 예측하려는 문제에 따라 실시간보다 배치성으로 예측해도 괜찮은 경우가 있음 2) Feature Engineering 소요되는 시간 실시간으로 생성되는 데이터를 기반으로 Feature Engineering 수행 Spark나 BigQuery 같은 SQL 기반으로 전처리할 수도 있고, Python의 Pandas 등을 사용할 수도 있음 그러나 Feature Engneering의 연산이 복잡한 경우, Feature를 생성하는 시간이 오래 걸릴 수 있음 3) 오프라인과 온라인에서 사용한 Feature의 차이 오프라인 : 실시간이 아닌 데이터를 사용하는 환경(주로 배치성) 온라인 : 실시간으로 데이터가 스트리밍되는 환경 오프라인은 주로 배치성으로 적재되기 때문에, 그 과정에서 미리 복잡한 연산을 해서 저장할 수 있음. 그러나 온라인 적재는 복잡한 연산을 하지 못하거나(혹은 개발 리소스의 부족으로) 따로 처리해야할 수 있음 같은 로직으로 처리하려 했으나, 처리하는 시간에 따라 값이 다르게 계산되는 경우가 존재함 4) 데이터 프로토콜의 부재 데이터를 실시간으로 적재하는 부분은 데이터 엔지니어 직군이, 모델링을 위해 데이터 전처리하는 부분은 데이터 사이언티스트(혹은 머신러닝 엔지니어)가 진행하는데, 두 조직의 프로토콜이 없는 경우도 있음 5) 서비스에서 많은 머신러닝 모델이 생기는 경우 머신러닝 모델이 소수인 경우(1-2개) 단일 모델을 위해 시스템을 구축하는 것은 비효율적이라 판단할 수 있음 하지만 모델이 많아지고, 동일한 Feature를 사용할 경우엔 각각의 모델링 과정에서 Feature Engineering을 진행하면 모델의 개수만큼 연산을 진행해야 함 즉, 재사용성이 떨어짐 6) 과거 Feature를 사용해야 하는 경우 보통 시간과 관련된 Feature들이 많은데, 이 Feature들은 시간이 지나며 점점 값이 바뀜(1월 1일 시점에 최근 1시간 전 데이터와 1월 5일 시점에 최근 1시간 전 데이터는 다름) 이럴 경우 index가 있거나, Backfill 기능이 있어야 함 혹시 잠시 Feature 관련 코드가 장애날 경우에 과거 값을 계산해야할 수 있음 Backfill : 간단히 말해서 예전 데이터를 계산 Feature Store 발생할 수 있는 문제들을 위해 Feature Store를 구축 Feature Store Layer를 추가한다고 표현함 Feature Store의 오픈소스는 Hopsworks와 feast가 존재 데이터를 Train, Serving 할 때 모두 사용할 수 있도록 통합해서 저장함 Feature Store 예시 다양한 회사들이 자신들이 구축한 Feature Store에 대해 발표함 featurestore.org에 다양한 Feature Store에 대한 영상들이 있음 많은 회사들이 비슷한 개념으로 개발함 Hopsworks의 Feature Store Document WHITE PAPER White Paper(백서)를 참고하면 어떤 구조로 만들었는지 나옴 Offline Features와 Online Features를 저장함 Hopsworks의 End to End System Gojek의 Feast Github Offline은 BigQuery에 저장하고, Online은 BigTable에 저장함 Google Cloud Platform과 공동으로 개발한 프로그램 Introducing Feast: an open source feature store for machine learning 글에 나와있음 Uber의 Michelangelo 우버의 머신러닝 플랫폼으로 Feature Store 및 다양한 기능이 제공됨 Uber Techblog Meet Michelangelo: Uber’s Machine Learning Platform, Evolving Michelangelo Model Representation for Flexibility at Scale에 나와있음 AirBnB의 Zipline Zipline: Airbnb’s Machine Learning Data Management Platform Netflix의 Metaflow Github Airflow의 Dependency를 정의하는 것을 코드로 진행하는듯 featurestore.org에 나와있는 Feature Store Comparison 정리한 표 Hopsworks Homepage Hopsworks는 전체 프레임워크 단위고, Feature Store가 그 중에 한 부분 링크에 AWS, GCP, 단일 인스턴스에 설치하는 방법에 대해 나와있음 Feature Store Feature Store와 함께 프로젝트 생성 등록된 Feature Groups을 볼 수 있음. Cached, On Demand 타입인지, Version, 분포(파란색 버튼) 등을 볼 수 있음 Feature Unit Testing Page Feature의 Constraint를 추가할 수 있음. 이 조건들로 데이터 Validation을 실행함 Feature 추가하기 from hops import featurestore featurestore.insert_into_featuregroup(features_df, featuregroup_name) Feature 가져오기 from hops import featurestore features_df = featurestore.get_features([\"average_attendance\", \"average_player_age\"]) Airflow를 사용해 Validation 후, 작업하기도 함 Github 참고 validation = HopsworksLaunchOperator(dag=dag, project_name=PROJECT_NAME, # Arbitrary task name task_id=\"validation_job\", job_name=VALIDATION_JOB_NAME) # Fetch validation result result = HopsworksFeatureValidationResult(dag=dag, project_name=PROJECT_NAME, # Arbitrary task name task_id=\"parse_validation\", feature_group_name=FEATURE_GROUP_NAME) # Run first the validation job and then evaluate the result validation &gt;&gt; result 그 외에도 hopsworks는 데이터 공유하기, 노트북 환경 생성, 제플린, 텐서보드 등 다양한 기능을 제공함 Gojek의 feast 공식 문서 사용 예시 설치는 Docker Composer, Google Kubernetes Engine(GKE)에서 하는 방법에 대해 공식 문서에 나와있음 BigQuery, Dataflow, Cloud Storage 등 GCP 친화적으로 보임 어떻게 해야할까? 현재 주어진 요구사항에 따라 다른 접근이 필요 처음부터 Feature Store를 구축하면 좋으나, 지속적으로 모델링하며 어떤 요구사항이 있을지 알 수 없음 따라서 처음엔 나이브하게 시작하는 것도 좋다고 생각 자주 사용하는 Feature를 Airflow 등에서 스케쥴링 1시간별 수요를 Table로 N분 단위로 스케줄링해서 특정 Table에 저장 그러다가 점점 모델이 많아질 때 고도화하면 좋을듯 Feature Store도 중요하지만, Data가 들어올 때 Validation 체크하는 것도 매우 중요함 Hopsworks를 직접 실행시켜서 아이디어를 얻은 후, 직접 개발하는 것이 더 좋을 수 있음(Hopsworks는 엄청 큰데, 모두 회사에 필요할까?란 고민이 필요) 다음 글은 Hopsworks를 직접 띄워서, 어떤 구성 요소가 있고 어떻게 활용할지에 대해 작성하겠습니다 :) Reference featurestore.org The Feature Store - Jim Dowling : Hopsworks에 대한 영상 Accelerating Machine Learning with the Feature Store Service Introducing Feast: an open source feature store for machine learning Feature Stores: Components of a Data Science Factory Feature Store: The missing data layer in ML pipelines? Rethinking Feature Stores -gojek feast",
    "tags": "feature mlops",
    "url": "/mlops/2020/02/02/feature-store/"
  },{
    "title": "Voila를 사용해 Jupyter Notebook Dashboard 만들기",
    "text": "Jupyter notebook에서 voila를 사용해 대시보드 만드는 방법에 대해 작성한 글입니다 R의 Shiny처럼 Python Jupyter Notebook에선 Voila를 사용하면 빠르게 웹에 대시보드를 띄울 수 있습니다 Voila를 사용하게 된 이유 대시보드는 다양한 대체 수단이 많음 Tableau, 스프레드시트, Superset, Metabase 등 대부분의 경우엔 BI 도구를 활용해도 충분함 종종 발생하는 경우 1) 지도에 데이터를 뿌려야 할 경우, 지도 표현을 잘 제공한다면 문제되지 않지만 지도를 잘 지원해주는 BI 도구는 적음 2) 파이썬으로 전처리를 해야할 경우 위 두가지 경우 Jupyter Notebook에 코드를 작성하고 주기적으로 html으로 저장할 수 있음 그러나 html이 아닌 ipywidget 등을 사용해 특정 시점의 데이터를 볼 수 있도록 만들려면 노트북에서 결과를 한번 실행해야 함(html 추출시 ipywidget은 같이 추출되지 않음) 3) 노트북에서 시각화, 지표 확인을 모두 하고싶은 경우 Voila를 사용하면 ipywidget과 노트북을 같이 렌더링해줌 Dash나 julyterlab-dash도 있지만 쉬운 사용성은 아니라 생각해 Voila를 선택함 블로그 글에 따르면 Jupyter의 하위 프로젝트로 통합되었다고 함 아직 0.1.20 버전이지만 추후 더 발전할 가능성이 있다고 생각 Voila의 특징 1) Jupyter Notebook 결과를 쉽게 웹에 띄울 수 있음 2) Ipywidget, Ipyleaflet 등 사용 가능 3) Jupyter Notebook의 Extension 있음(=노트북에서 바로 대시보드로 변환 가능) 4) Python, Julia, C++ 코드 지원 5) 고유의 템플릿을 생성 가능 설치 Mac OS 기준 jupyter client 업그레이드 cannot import name ‘secure_write’ from ‘jupyter_core.paths’ 에러가 발생할 경우 jupyter_client 업그레이드 pip3 install --upgrade jupyter_client pip로 설치 pip3 install voila JupyterLab preview extension 설치(JupyterLab 사용시만) jupyter labextension install @jupyter-voila/jupyterlab-preview 혹시 jupyter-labextension not found Error가 발생한다면 jupyterlab을 설치하지 않은 것 pip3 install jupyterlab notebook이나 jupyter_server extension 설치 jupyter serverextension enable voila --sys-prefix 아래 명령어로 voila/extension이 enabled인지 확인 jupyter nbextension list # jupyter_server의 경우 jupyter serverextension list 사용 방법 간단한 Jupyter Notebook 파일 생성(이름 : voila_test) import ipywidgets as widgets slider = widgets.FloatSlider(description='$x$', value=4) text = widgets.FloatText(disabled=True, description='$x^2$') def compute(*ignore): text.value = str(slider.value ** 2) slider.observe(compute, 'value') widgets.VBox([slider, text]) import pandas as pd iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv') iris.tail() 1) voila로 노트북 렌더링(코드 안보임) voila voila_test.ipynb localhost:8866로 접근하면 아래 결과를 확인할 수 있음 2) voila로 노트북 렌더링(코드 보임) --strip_sources=False 조건을 주면 기존 노트북의 코드도 보임(셀 실행은 불가능) voila voila_test.ipynb --strip_sources=False 3) voila 폴더 실행 voila 현재 폴더에 있는 파일을 voila로 실행할 수 있음 4) Notebook의 Extension 클릭 노트북 실행 후 Voila 클릭하면 1)과 같은 결과가 바로 보임 옵션이 더 궁금하다면 터미널에서 아래 명령어 입력 voila --help 사용하며 체득한 Tip Voila 실행시 셀 타임아웃 제한이 있음 기본적으로 한 셀을 실행하는데 30초 소요되면 Timeout Error 발생(터미널 로그에만 남고 웹은 계속 진행중 처럼 나옴) voila 실행시 아래 명령어를 사용해 타임아웃을 늘릴 수 있음 참고한 Github Pull request voila --ExecutePreprocessor.timeout=180 보안 이슈 해결하기 해당 이슈를 보면 oauth 기능을 추가할 예정인듯 oauthenticator를 사용해 직접 만들까 고민하다 Notebook의 암호를 사용하는 방법을 생각함 암호 생성 터미널에서 Python3 실행 후, 아래 커맨드 입력 from IPython.lib import passwd passwd() ‘sha1:~~’ 복사하기 jupyter_notebook_config.py 생성 vi ~/.jupyter/jupyter_notebook_config.py c = get_config() c.NotebookApp.password = 'sha1:~~~ 붙여넣기' notebook을 재실행하면 암호 입력하게 됨 특정 nbextension이 404 발생하는 경우 voila로 직접 노트북 파일 실행시 특정 nbextension(pydeck쪽)이 404가 발생해서 아래 명령어로 해결 voila your_notebook.ipynb --enable_nbextensions=True jupyter notebook extension에서 voila를 실행할 경우엔 인자를 줄 수 없고, notebook 실행시 옵션을 줘야함. [참고 문서] jupyter notebook --ExecutePreprocessor.timeout=180 --VoilaConfiguration.enable_nbextensions=True 사용 예시 Voila Gallery에 여러 예시가 존재함 machine-learning-interactive-visualization이 눈에 띄어서 소스를 clone한 후 실행해봄 bqplot도 Voila를 사용해 쉽게 대시보드화 할 수 있음 volia-vuetify엔 VuetifyJS를 사용한 대시보드 템플릿을 볼 수 있음 배포 공식 문서를 보면 mybinder.org, Heroku, Google App Engine 등에서 배포하는 방법이 간단히 나와있음 Voila Architecture Reference Voila Github And voila Voilà is now a Jupyter subproject voila demo Dashboarding with Jupyter Notebooks, Voila and Widgets Turn any Notebook into a Deployable Dashboard : 이건 Panel 이야기긴한데, 비교할만한 대상인듯",
    "tags": "python development",
    "url": "/development/2020/01/06/jupyter_notebook_voila_dashboard/"
  },{
    "title": "Julia Language 프로그래밍 언어 기본 문법 정리",
    "text": "프로그래밍 언어인 Julia Language 기본 문법에 대한 글입니다 Julia Julia는 MIT에서 개발한 언어로 2012년에 처음 개발함 Two langauge problem Performance(fortan, c, asm) vs Productivity(python, ruby, matlab) vs Generality 이런 문제를 해결하고자 Julia를 만듬 looks like python, feels like lisp, runs like C Julia는 C만큼 빠름 Julia는 R, Matlab, Python 같이 high level로 수치 계산하기 좋고, 일반 범용 목적의 프로그래밍도 지원함 메타프로그래밍을 할 수 있음(macros, multiple dispatch) 수치 해석(Numerical Analysis), 최적화(Optimization) 문제 등 고성능이 필요한 수학적 계산 처리를 주로 품 index 숫자가 1부터 시작함 Python에서 Julia 실행 가능 : pyjulia 사용 Julia에서 Python 실행 가능 : PyCall 사용 Julia 사용 사례 Celeste Sloan Digital Sky Survey Data로 178 terabytes 천문학 이미지 분석하기 위해 Julia를 활용함 Parallel Supercomputing for Astronomy에 자세한 내용이 나옴 사용하는 회사 2018년 기준이고 지금은 더 많을듯 Julia 설치하기 Mac OS 기준 brew cask install julia 터미널에서 Julia를 입력한 후 아래와 같은 내용이 뜨면 성공 Jupyter Notebook에서 Julia 사용하기 IJulia 사용 터미널에서 Julia를 입력해서 Julia prompt로 이동 후 아래 명령어 입력 ENV[\"PYTHON\"] = \"\" ENV[\"JUPYTER\"] = \"\" IJulia 설치(Julia Prompt에서 실행) using Pkg Pkg.add(\"IJulia\") IJulia 실행(Julia Prompt에서 실행) using IJulia notebook() 이제 익숙한 Jupyter Notebook이 보이고 Julia 커널을 사용할 수 있게됨 최초 실행 이후엔 Jupyter Notebook으로 실행해도 줄리아 커널이 보임 Package 설치 패키지 Import는 using을 사용 Pkg를 사용해 패키지 설치 usisg Pkg Pkg.add(\"PackageName\") 기본 문법 함수의 Document를 보고 싶은 경우 함수 앞에 ?를 붙임 ?println 쉘 커맨드를 사용하고 싶은 경우 ;를 앞에 붙임 ;ls print println() 사용 Assert @assert하고 식을 씀. 파이썬에서 assert a == 1 이런 느낌인데 골뱅이만 추가됨 Variable type 확인하기 typeof() 사용 타입 변환은 파이썬처럼 float(365)로 할수도 있고, parse(Float64, \"365\")로 할수도 있음 신기한 부분은 이모지에 변수 할당할 수 있음(Super generic) String \"”나 “”” “\"”를 사용함 후자의 경우 문자열 내에서 따옴표를 사용할 수 있고, 전자는 불가능(Error 발생) '’는 character고 string이 아님 \"Here, we get an \"error\" because it's ambiguous where this string ends \" \"\"\"Look, Mom, no \"errors\"!!! \"\"\" typeof('a') String Interpolation Python의 F string같이 값을 넣고 흘려버리기 달러 기호 $를 사용함 $() 안에 연산도 할 수 있음 name = \"kyle\" num_fingers = 10 num_toes = 10 println(\"Hello, my name is $name.\") println(\"I have $num_fingers fingers and $num_toes toes.\") println(\"That is $(num_fingers + num_toes) digits in all!!\") String Concatenation string끼리 합칠 경우엔 string()을 사용 s3 = \"How many cats \"; s4 = \"is too many cats?\"; 😺 = 10 string(s3, s4) string(\"I don't know, but \", 😺, \" is too few.\") String 반복 Python에선 “hi”*100처럼 했지만 Julia에선 “hi”^100으로 가능함 Data Structures Tuple Tuple은 생성하고 바꿀 수 없음 ()로 만듬 순서가 지정된 요소를 묶어 튜플로 만들 수 있음 myfavoriteanimals = (\"penguins\", \"cats\", \"sugargliders\") myfavoriteanimals[1] NamedTuples 각각의 요소가 이름이 있는 Tuple myfavoriteanimals = (bird = \"penguins\", mammal = \"cats\", marsupial = \"sugargliders\") myfavoriteanimals[1] myfavoriteanimals.bird Dictionaries Python의 dictinary랑 비슷한듯. key-pair 존재 Dict()으로 만들고, =&gt;를 사용함 myphonebook = Dict(\"Jenny\" =&gt; \"867-5309\", \"Ghostbusters\" =&gt; \"555-2368\") myphonebook[\"Jenny\"] myphonebook[\"Kramer\"] = \"555-FILK\" pop!을 쓰면 값이 나옴(원본 dict엔 사라짐) 느낌표가 있다니 신기함 pop!(myphonebook, \"Kramer\") myphonebook Tuple이나 Array와 다르게 dictionaries는 정렬되지 않아서 index로 접근할 수 없음 Arrays Array는 mutable하고 dict과 다르게 순서가 있음 []로 만듬 myfriends = [\"Ted\", \"Robyn\", \"Barney\", \"Lily\", \"Marshall\"] mixture = [1, 1, 2, 3, \"Ted\", \"Robyn\"] push!와 pop! 사용 가능 ND Array도 생성할 수 있음 numbers = [[1, 2, 3], [4, 5], [6, 7, 8, 9]] rand(4, 3) rand(4, 3, 2) 복사는 copy()를 사용해 가능 array의 길이는 length() 사용 array의 요소 합은 sum() 사용 Loops while loops와 for loops이 있음 : 모두 end를 써야함 while loops 사용 방식 while *condition* *loop body* end 예시 : indent에 둔감함 n = 0 while n &lt; 10 n += 1 # n = n + 1 println(n) end n 세미 콜론(;)을 사용하면 줄바뀜 효과를 얻음 n = 0 while n &lt; 10; n += 1 println(n) end n myfriends = [\"Ted\", \"Robyn\", \"Barney\", \"Lily\", \"Marshall\"] i = 1 while i &lt;= length(myfriends) friend = myfriends[i] println(\"Hi $friend, it's great to see you!\") i += 1 end for loops 사용 방식 for *var* in *loop iterable* *loop body* end 예시(1:10은 python에서 range(1, 10)과 동일) for n in 1:10 println(n) end myfriends = [\"Ted\", \"Robyn\", \"Barney\", \"Lily\", \"Marshall\"] for friend in myfriends println(\"Hi $friend, it's great to see you!\") end 2중 for문을 더 쉽게 쓸 수 있음 m, n = 5, 5 A = fill(0, (m, n)) for i in 1:m for j in 1:n A[i, j] = i + j end end A B = fill(0, (m, n)) for i in 1:m, j in 1:n B[i, j] = i + j end B Array comprehension(Python의 list comprehension 느낌) =&gt; Julia 스타일 [ x+y for x in 1:10, y in 2:5] C = [i + j for i in 1:m, j in 1:n] Conditionals if문 if *condition 1* *option 1* elseif *condition 2* *option 2* else *option 3* end fizzbuzz 예시 &amp;&amp;는 AND를 뜻함 %는 나머지를 뜻함 N=15 if (N % 3 == 0) &amp;&amp; (N % 5 == 0) # `&amp;&amp;` means \"AND\"; % computes the remainder after division println(\"FizzBuzz\") elseif N % 3 == 0 println(\"Fizz\") elseif N % 5 == 0 println(\"Buzz\") else println(N) end ternary operators(삼항 연산) : 오 신기함 a ? b : c 이건 아래와 동의어임 if a b else c end 예시 x = 10 y = 5 if x &gt; y x else y end (x &gt; y) ? x : y short-circuit evaluation a &amp;&amp; b if a and b가 true일 경우 true를 return함 false &amp;&amp; (println(\"hi\"); true) true &amp;&amp; (println(\"hi\"); true) error()를 사용하면 Error 발생 true || println(\"hi\") || operator는 or을 뜻함 true || println(\"hi\") false || println(\"hi\") Functions 주요 Topic 함수 정의하는 방법 Duck-typing 하는 방법 : 타입을 미리 정의하는게 아닌 실행되었을 때 타입 정의. 덕타이핑이란? 글 참고 Mutating vs non-mutating function Some higher order functions 함수 정의 function sayhi(name) println(\"Hi $name, it's great to see you!\") end function f(x) x^2 end 1줄로 할 수도 있음 sayhi2(name) = println(\"Hi $name, it's great to see you!\") f2(x) = x^2 함수 인자 타입 지정 :: 사용하며 타입 선언이 없으면 기본적으로 Any 타입으로 지정됨 foo(x::Int, y::Int) = println(\"My inputs x and y are both integer!\") foo(3, 4) anonymous(익명) 함수 : = 를 사용해서 -&gt; 결과를 낸다. Tuple도 사용 가능 sayhi3 = name -&gt; println(\"Hi $name, it's great to see you!\") f3 = x -&gt; x^2 sayhi4 = (firstname, lastname) -&gt; println(\"Hi $firstname $lastname, it's greate to see you!\") Duck-typing Julia는 모든 입력이 의미가 있다고 생각함 string으로 들어갈 것이라 예상했던 것이 int가 들어가도 작동함 sayhi(55595472) A = rand(3, 3) f(A) f(\"hi\") # vector 연산은 ^ 연산이 구현되지 않아서 아래 명령어는 오류 발생 v = rand(3) f(v) Mutating vs non-mutating functions !를 붙이는 차이 sort는 원본 데이터는 변하지 않지만, sort!는 변함. 마치 pandas에서 replace=True가 기본적으로 들어간게 ! 붙인거 같은 느낌 v = [3, 5, 2] sort(v) v sort!(v) v Some higher order functions(고차 함수) map 해당 함수를 전달한 데이터의 모든 요소에 적용함 map(f, [1, 2, 3]) =&gt; [f(1), f(2), f(3)] 이렇게 됨 map(x -&gt; x^3, [1, 2, 3]) broadcast map의 일반적인 형태 broadcast(f, [1, 2, 3]) 함수 뒤에 .을 붙일 경우에도 broadcast됨 f.([1, 2, 3]) 연산 차이 A = [i + 3*j for j in 0:2, i in 1:3] f(A) # A^2 = A*A 행렬 곱 f.(A) # 각 요소들의 제곱 아래 두 코드는 동일한 결과가 나타남 broadcast(x -&gt; x + 2 * f(x) / x, A) A .+ 2 .* f.(A) ./ A Packages https://pkg.julialang.org/docs/에서 사용 가능한 패키지를 확인할 수 있음 패키지 설치(Pkg : 패키지 관리자) using Pkg Pkg.add(\"Example\") 패키지 사용 using \"package name\" using을 하면 그 안에 있는 함수를 사용할 수 있음 Palette 사용하는 예시 Pkg.add(\"Colors\") using Colors palette = distinguishable_colors(100) rand(palette, 3, 3) Plotting 그래프를 그리는 방식은 정말 다양함(PyPlot을 부를수도 있고, Plots 사용할수도 있음) Plots는 다양한 백엔드를 지원하는데, GR과 UnicodePlots을 사용해볼 예정 Plots Backent 참고 gr을 빌드할 때 너무 오래 걸려서 링크를 찾아봄 Using Plots시 Permission Error가 떠서 PyPlot을 사용함 import Pkg; Pkg.add(\"Plots\") using Plots globaltemperatures = [14.4, 14.5, 14.8, 15.2, 15.5, 15.8] numpirates = [45000, 20000, 15000, 5000, 400, 17]; gr() plot(numpirates, globaltemperatures, label=\"line\") scatter!(numpirates, globaltemperatures, label=\"points\") xlabel!(\"Number of Pirates [Approximate]\") ylabel!(\"Global Temperature (C)\") title!(\"Influence of pirate population on global warming\") xflip!() Pkg.add(\"PyPlot\") using PyPlot globaltemperatures = [14.4, 14.5, 14.8, 15.2, 15.5, 15.8] numpirates = [45000, 20000, 15000, 5000, 400, 17]; plot(numpirates, globaltemperatures, label=\"line\") scatter(numpirates, globaltemperatures, label=\"points\", color=\"red\") xlabel(\"Number of Pirates [Approximate]\") ylabel(\"Global Temperature (C)\") title(\"Influence of pirate population on global warming\") Multiple Dispatch 아마 생소한 개념. 위키피디아 참고 함수의 인자 타입에 따라 다른 함수 호출 foo(x::String, y::String) = println(\"My inputs x and y are both strings!\") foo(\"hello\", \"hi!\") foo(3, 4) # MethodError: no method matching foo(::Int64, ::Int64) foo(x::Int, y::Int) = println(\"My inputs x and y are both integers!\") foo(3, 4) method()를 사용하면 함수에 대한 내용이 나옴 generic function with 2 methods : 2가지 메서드를 가진 함수 객체를 뜻함 methods(foo) &gt;&gt;&gt; 2 methods for generic function foo: &gt;&gt;&gt; foo(x::Int64, y::Int64) in Main at In[8]:1 &gt;&gt;&gt; foo(x::String, y::String) in Main at In[5]:1 foo(x::Number, y::Number) = println(\"My inputs x and y are both numbers!\") foo(x, y) = println(\"I accept inputs of any type!\") v = rand(3) foo(v, v) method(+)를 입력해서 + 구현을 확인해볼 수 있음 generic function을 호출할 때 어떤 메소드가 전달되는지 확인하려면 @which 매크로를 사용할 수 있음 @which foo(3, 4) &gt;&gt;&gt; foo(x::Int64, y::Int64) in Main at In[8]:1 @which 3.0 + 3.0 &gt;&gt;&gt; +(x::Float64, y::Float64) in Base at float.jl:395 간단한 내용만 본거고, 더 궁금하면 Github 참고하면 좋음 Dataframe 데이터를 조작하기 위해 Dataframes.jl 활용 Pandas와 비슷한 느낌 import Pkg; Pkg.add(\"DataFrames\") using DataFramesusing DataFrames df = DataFrame(A = 1:4, B = [\"M\", \"F\", \"F\", \"M\"]) df.col 또는 df[!, :col]을 통해 열에 직접 액세스 가능 후자는 열 이름을 변수에 전달할 수 있어 유용함. 열 이름은 Symbol(“col”) names(df)로 이름을 확인할 수 있음 df[!, :col]은 복사하지 않고 원본을 컨트롤하고, df[:, :col]은 사본을 얻음 Dataframe을 만들고 하나씩 붙이는 것도 가능(Column끼리) df = DataFrame() df.A = 1:8 df.B = [\"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\"] Row끼리 합치기 : push! 사용 df = DataFrame(A = Int[], B = String[]) push!(df, (1, \"M\")) push!(df, [2, \"N\"]) push!(df, Dict(:B =&gt; \"F\", :A =&gt; 3)) 저장하기 using Pkg Pkg.add(\"CSV\") using CSV CSV.write(\"dataframe.csv\", df) Filtering : Pandas랑 느낌 비슷한데 점을 추가함(각 요소별 연산) : .&gt;, in. 등 df = DataFrame(A = 1:2:1000, B = repeat(1:10, inner=50), C = 1:500) df[df.A .&gt; 500, :] Describe describe(df)로 가능 df = DataFrame(A = 1:4, B = [\"M\", \"F\", \"F\", \"M\"]) describe(df) Aggregate : aggregate(df, 연산) df = DataFrame(A = 1:4, B = 4.0:-1.0:1.0) aggregate(df, sum) aggregate(df, [sum, prod]) 특정 값 변경하기 df.A[1] = 10 CSV Read using Pkg Pkg.add(\"CSV\") using CSV DataFrame(CSV.File(input)) DelimitedFiles(CSV 읽기) using DelimitedFiles P,H = readdlm(\"programminglanguages.csv\",header=true) Join join 인자 안에 kind = :inner, :left 등을 사용할 수 있음 people = DataFrame(ID = [20, 40], Name = [\"John Doe\", \"Jane Doe\"]) jobs = DataFrame(ID = [20, 40], Job = [\"Lawyer\", \"Doctor\"]) join(people, jobs, on = :ID) 그 외에도 Split-apply-combine을 위해 by 등을 사용함 최적화 라이브러리 JuliaOpt 쪽에 자료 많음 JuMP.jl CVX는 Convex.jl JuMP 예시 코드 using JuMP, GLPK # 모델 생성 m = Model(with_optimizer(GLPK.Optimizer)) # Variable 선언 @variable(m, 0&lt;= x1 &lt;=10) @variable(m, x2 &gt;=0) @variable(m, x3 &gt;=0) # 목적 함수 @objective(m, Max, x1 + 2x2 + 5x3) # 제약 조건 설정 @constraint(m, constraint1, -x1 + x2 + 3x3 &lt;= -5) @constraint(m, constraint2, x1 + 3x2 - 7x3 &lt;= 10) # Optimization Model 출력 print(m) # 최적화 문제 풀기 JuMP.optimize!(m) # Optimal Solution Print println(\"Optimal Solutions:\") println(\"x1 = \", JuMP.value(x1)) println(\"x2 = \", JuMP.value(x2)) println(\"x3 = \", JuMP.value(x3)) # Optimal dual variables Print println(\"Dual Variables:\") println(\"dual1 = \", JuMP.shadow_price(constraint1)) println(\"dual2 = \", JuMP.shadow_price(constraint2)) VSCode Extension julia-vscode Reference 권창현님의 Julia Programming for Operation Research Julia 공식 Document Intro to Julia tutorial (version 1.0) Github 이분의(교수되신듯) 강의 Intro to Julia",
    "tags": "julia development",
    "url": "/development/2019/12/27/julia_basic/"
  },{
    "title": "머신러닝 엔지니어의 2019년 회고",
    "text": "2019년을 되돌아보며 작성한 회고 글입니다 2019년 상반기 회고는 키워드 위주로 간단히 작성했고, 이번엔 월별 정리 + 키워드 정리를 하려고 합니다 제가 어떤 사람인지 궁금하시면 아래 자료들을 보시면 좋을듯 합니다 데이터 사이언티스트가 되기 위해 진행한 다양한 노력들 2017년 회고, 2018년 계획 2018년 회고, 2019년 다짐 Gap Year 및 쏘카 이직 이야기 2019년 상반기 회고 및 글또 3기 시작 2019년 월별 정리 2019년을 되돌아보니 “성장하는 서비스에서 다양한 업무를 빠르게 진행했고, 머신러닝이 아닌 최적화(Constraint Programming 등)라는 도구에 대해 알아간 해, 매니저 역량을 확인하고 팀원들의 성장을 고민한 해”라고 정의할 수 있을 것 같습니다 쏘카에 입사하고 본격적으로 일을 진행한 1년이였고(2018년엔 9월 입사라 준비 단계 정도라 생각하고..) 너무 빠르게 변하는 상황에서 나름 잘 적응하며 지냈습니다 월별 정리는 키워드 위주로 작성하겠습니다 1월 학습 시뮬레이터 개발 시계열 수요 예측 및 Serving 프로토타입 개발 AWS Lambda와 Terraform을 사용한 API 개발 Hashing을 사용한 AB Test Split 정서적 안정감 정서를 느낄 시간 없이 빠르게 지나감. 정서적 안정감 좋음 기타 일기장을 보니 책을 쓰려는 생각이 있어 여러 출판사를 만나고 다님 I Want to Study Data Science 글을 작성한 후, 출판사에게 많은 연락을 받음 정작 회사 일이 바빠서 쓰진 않았지만, 컨텐츠에 대해 고민해본 시기(과연 내 책이 팔릴까? 등) 딥모닝 : PR12(아침에 PR12 영상 1개씩 매일 보는 모임)을 웅원이와 만들고 진행함. 아침에 성실하게 살기 위한 노력이였음 2월 학습 일 잘하시는 팀원분이 최적화 개념을 알려주셔서 그 쪽을 주로 공부함(Mixed Integer Programming, Constraint Programming, Google OR-tools) 지리 데이터를 파악해야 해서 QGIS나 SHP 파일 다루는 쪽도 공부 시작 2월 말에 팀 개편으로 두 팀의 팀장이 됨 데이터 사이언스팀 + 타다데이터팀 데이터 그룹에선 3개월 수습 기간에 발표를 통해 정규직 전환이 결정되는데, 이 정규직 전환을 잘할 수 있도록 + 수습 발표 전 스트레스를 케어하기 위해 “멘탈 붕괴 방지 위원회”를 만들어 수습 발표 전에 사전 발표 및 피드백을 진행함 =&gt; 여태까지 계속 진행 중 정서적 안정감 2월 말에 너무 피곤하고 뭔가 번아웃 위기가 온 것 같아 고민해보니, 책 집필을 해보려고 초안 작성하는 시간에 스트레스를 너무 받음 블로그에 글 작성하는 것과 책은 정말 큰 차이가 있다는 것을 깨닫고, 시간적 여유가 생기면 책을 쓰기로 하고 잠정 중단 기타 AutoML 도구인 데이터로봇을 사용할 기회가 생겨서 많이 사용해보니, Tabular 데이터의 AutoML은 조만간 빠르게 대중화될 수 있다고 생각 패스트캠퍼스 시계열 예측 강의를 수강하며 전통적인 시계열 예측 방법론에 대해 익힘 GDG Campus에서 진행한 Daily 만년 Junior들의 이야기 - 델리만주에서 데이터 사이언스 입문에 대해 발표함 3월 학습 TF가 아닌 팀이 되었고, 팀장 역할에 대한 고민 팀장이 개인 성과를 내야하는걸까? 팀의 성과가 잘 나오도록 하면 되는게 아닐까? 팀원과 경쟁하지 말자, 팀원을 포용하자(내가 항상 옳진 않고 상대방의 의견이 맞을 수 있다. 왜 그렇게 생각하는지 맥락을 듣자) 성과를 가로채는 그런 느낌의 팀장이 되지 말자(해당 팀원이 진행한 일을 윗분에게 잘 어필하고, 인정받을 수 있도록 돕자) 팀 정기 회고를 진행해서 팀 운영 방식이나 개선할 수 있는 포인트를 찾아 계속 개선했음 GCP의 Composer(Airflow의 managed service)를 사용해 머신러닝 파이프라인 구축 시계열 데이터를 프러덕션할 때 주의할 점에 대한 깨달음 데이터 그룹의 학습하는 문화를 만들기 위해 파이썬 머신러닝 완벽 가이드 기반 스터디를 만듬 Github에 자세히 적어놨지만, 간단히 작성하면 누가 공부해와서 발표하는 방식이 아닌 일과 시간에 같이 모여서 책 읽고 토론하는 시간. 이 스터디는 데이터 분석에 대해서만 알고, 머신러닝에 대해선 잘 모르는 분들을 빠르게 베이스라인을 끌어올리기 위해 진행함 5월 말까지 꾸준히 진행했음 정서적 안정감 개인 업무에 집중하다가 공식적으로 팀장 역할을 하며 역할에 대해 고민한 시간이 많음. 이 시간이 정서적 안정감에 영향을 준 것은 아니지만 자신에 대한 방향성을 고민하게 해줌 기타 지인분들과 함께 딥놀 유튜브 시작! 혼자 하면 안할 확률이 컸는데, 함께해서 시너지나는 모임 마이크로소프트웨어 396호에 글쓰는 개발자 모임 - 글또 관련 내용을 기고. 잡지에 글을 써보는 경험도 처음이라 재미있었지만, 그만큼 신경을 더 써야해서 살짝 스트레스도 있었음 Write The Docs Seoul에서 글쓰는 개발자 모임, 글또의 주제로 발표함 4월 학습 Uber의 H3, Kepler.gl에 대해 깊게 알아보고, 블로그 글도 작성함 메타휴리스틱 방법론이 신기해서, Genetic Algorithm에 대해 공부하고 적용하려 했으나, Constraint Programming이 더 효율적이라 판단해 적용하진 않음 VCNC 데이터팀에 새로 입사하신 분이 계셔서, 케어하다보니 거의 3팀의 매니저 같은 역할을 함 Airflow를 사용해 로그를 BigQuery로 보내는 파이프라인 Interpretable Machine Learning에 대해 공부함 정서적 안정감 두 팀의 팀장 + 채용 등의 업무가 추가되서 정말 정신없었음. 하루에 기본 미팅(Daily)가 2개니 그냥 미팅 미팅의 연속 정서적 안정감이 하락하고 있는 것이 팀장 겸직인 것 같아 4월 말에 하나의 팀만 집중하는 것이 좋을 것 같다고 그룹장님에게 말씀드림. 그 결과 5월까진 천천히 차기 팀장님에게 인수인계+서포트 하고 6월부터 완전히 하나의 팀만 맡기로 기타 데이터 사이언티스트가 되기 위해 진행한 다양한 노력들에 대한 글을 작성함. 많은 분들이 궁금해하셔서 작성했는데, 구글에서 “데이터 사이언티스트” 검색시 상단에 나와서 뿌듯 5월 학습 데이터사이언스팀의 팀장님에게 인수인계하고, 5월엔 잘 운영될 수 있도록 서포트를 함. 남은 시간엔 빠르게 성장하고 있는 타다에 집중함 모빌리티에선 어떤 종류의 문제를 푸는지, 어떤 논문이 있는지 한번쯤 정리하면 좋을 것 같아 Awesome Mobility Machine Learning Contents에 정리함 앞선 시기에 진행한 업무들을 더 잘 &amp; 깊게 하도록 진행함 쏘카에 GIS 장인이신 신수현님이 데이터 분석한 것을 듣고 “지리 데이터에 기반한 분석도 새롭고 신선하다”고 생각함 날씨 데이터를 수집하기 위해 공공 데이터 부분을 자세하게 파악함 Serving을 어떻게 효율적으로 할까 고민하다 Kubeflow를 테스트해봄 아직 버전이 낮지만 추후엔 꽤 유용해질 것 같다는 생각을 함 다만 사용하기 위해 구성원들이 모두 Docker에 대해 잘 알고, 모델을 엄청 적극적으로 활용해야 유용할 것 같단 생각을 함 서버 코드를 읽기 위해 코틀린을 살짝 공부함 정서적 안정감 팀장 겸직이 해제되서 심리적으로 안정감이 다시 올라감. 여러 업무를 할 수 있어도, 그래도 하나의 집중하는 편이 좋다고 생각함 기타 웅원이와 함께 스타트업 머신러닝 플레이어(스머플) 모임을 만들어 진행함. 주로 스타트업에서 ML을 적용할 때 이슈, 채용은 어떻게 할까? 등에 대해 논의함 웅원이가 있는 당근마켓에 가서 BigQuery의 모든 것을 발표하고 옴. 271쪽을 3시간만에 끝내려는건 역시 무리였으나! 핵심은 다 전달하고 옴 해당 자료는 19년 12월 기준 조회수 11,000회 정도 6월 학습 GCP에서 Instance Template을 활용해 인스턴스를 다량으로 생성하고 실행하는 쉘 스크립트 작성 당근마켓에서 발표한 내용을 기반으로 사내에서 BigQuery 강의함 지역 사업팀도 계셔서 온라인으로 라이브 방송을 진행함 쏘카엔 데이터 그룹 제외하고도 80명이 넘는 분들이 이미 SQL을 날리고 있는데, 수준의 편차가 너무 심해서 발표를 아주 쉬운 단계(거의 엑셀 기초부터..?) vs 고급 단계로 나눠서 해야될 것 같다고 느낌 =&gt; 이 부분은.. 그 이후 바빠서 못했지만 내년엔 더 진행을..! 머신러닝 엔지니어링엔 TDD가 자주 사용되지 않지만, Test Code를 작성하도록 방식을 만듬 팀내 코드 리뷰를 도입해 코드도 나름 빡세게 리뷰함 BigQuery GIS에 좌표 데이터를 넣는데 자꾸 오류가 나서 끝없이 디버깅해보니 “좌표계”에 대한 개념이 중요한 것을 깨달음 정서적 안정감 딱히 문제 없음 기타 한빛미디어가 주최한 데브그라운드에서 Mobility X Data : 모빌리티 산업의 도전 과제에 대해 발표함 사람들이 모빌리티 데이터에 흥미를 가질 수 있도록, 카쉐어링과 라이드헤일링에서 겪는 문제에 대해 말함 글또 3기 모집을 시작했는데, 지원자가 거의 80명이였음 7월 학습 MLOps에 매우 관심이 커져서 공부함. 일단 실험을 위한 플랫폼인 Sacred에 대해 깊게 공부하고 글 작성함 회사 업무에 몰입해서 진짜 정신없이 7월 삭제됨 상반기 회고 결과 업무를 약간 천천히 한 것 같기도..? 란 생각에 7월엔 셀프 관리를 더 잘 진행함 6월에 한빛미디어에서 발표하고, 끝나니 바로 DSTS에서 발표해야 해서 시간이 적었지만 업무와 발표 준비 시간을 잘 분배했음(하지만 마감에 벼락치기가 최고) 정서적 안정감 딱히 문제 없음 기타 Edwith PySpark를 활용한 데이터분석 퍼블리싱 글또 3기 시작(3기 멤버 : 약 40명) 8월 학습 개발 업무를 했고, 실서비스에 적용되는 것들을 만들었음 장애가 전혀 나지 않도록 설계하고, 혹시 최악의 경우를 방지할 부분도 미리 대비함 서비큐라님의 쿠버네티스 교육을 듣고 왔는데, 쿠버네티스를 더 잘 이해할 수 있게됨 채용 행사에 데이터 직군 상담하기 위해 서울대, 고려대 등을 돌아다님. 많은 분들과 이야기했음(참고로 쏘카 전문연 가능한 기업!) 신입/인턴 면담 그로스해커스와 쏘카가 프로젝트를 같이 하게되서 풀타임 인턴과 프로젝트 하시는 분들이 계셨는데, 이분들을 위해 어떤 것을 해볼까 고민하다 모두 만나뵙고 커리어에 대한 이야기를 나눔 정서적 안정감 딱히 문제 없음 기타 DSTS2019에서 TF에서 팀 빌딩까지 9개월의 기록 : 성장하는 조직을 만드는 여정에 대해 발표함 기술적인 발표가 아닌 성장하는 조직에 대해 발표했는데, 생각보다 많은 분들이 좋아해주셨음. 이 발표 자료를 보시고 IT회사들의 리더분들이 연락옴 9월 학습 카이스트에 채용 행사 참여. 카이스트 공부하기 정말 좋아보임 첫 미국 출장. Oreilly Strata New York에 다녀옴 차량 하드웨어는 어떤 방식으로 통신하는지 알게 됨 쏘카 테크 블로그를 만들고, 이 블로그를 운영하는 역할을 CTO님에게 받음. 아직 초반기지만 작성한 글을 리뷰하는 시스템을 구축함 정서적 안정감 작성할 순 없지만 요청 업무가 매우 많아져서(=비즈니스가 엄청 성장하고 있기 때문에) 그 부분을 쳐내는데 시간을 많이 사용함. 팀원들은 본래 집중할 Task을 할 수 있도록 잡무를 거의 제가 처리함 정서적 안정감이 낮아짐 기타 데이터 그룹과 좋은 관계를 맺은 그로스해커스분들을 위해 성장을 좋아하는 사람이, 성장하고 싶은 사람에게에 대해 발표함 어떤 방향성을 가지고 성장해야 하는지에 대해 이야기함 10월 학습 Full Stack Deep Learning Bootcamp 정리함. 이 강의는 정말 여러 내용을 담고 있는데, 프러덕션 부분까지 담아서 매우 좋음 데이터야놀자 행사에 직접 발표하는게 아닌, 팀원분들이 발표할 수 있도록 도움. 팀원 성장시키기 : 발표 컨설팅 참고 미국에서 스트라타 행사를 듣고 최적화쪽은 Julia를 많이 사용하는 것을 보고 심심할 때 Julia로 최적화 문제를 풀어봄. 언어는 언어일 뿐인데 왜 Julia에 최적화 레퍼런스가 많을까?에 대해 고민함 정서적 안정감 뭔가 바쁘게 들어온 일을 처리하고, 미팅도 많이 했는데 되돌아보면 한 일이 없는거 아닌가?란 생각에 약간의 현타. 집중할 일에 집중해야 하는게 아닐까?란 생각이 들어 업무 방식을 변경함 패스트캠퍼스 머신러닝 온라인 강의에서 프로젝트 부분 촬영하느라 정서적으로 여유가 없었음 11월 학습 데이터 그룹에서 신입/인턴 비율이 높아지는데 그분들을 위한 맞춤 교육이 따로 없어서 카일 스쿨을 만들어 교육함 일은 왜 하는가? 번아웃을 어떻게 대처할 것인가? 올바른 자세(허리 펴!), 파이썬 시각화 등까지 진행했고 앞으론 개발적인 부분을 알려줄 예정 저는 남을 돕기 위해(성장시키기, 유저 입장에서 좋은 서비스를 경험하도록) 일을 하고 있는 것을 발표 자료 준비하며 다시 깨달음 Uber에서 만든 Deck.gl을 Python에서 활용할 수 있는 pydeck+ipywidget에 대해 공부하고 사내 전파함 정서적 안정감 시 간 순 삭! (블로그에 글 2개밖에 작성하지 않았던 시기) 닌텐도 스위치를 구입해 포켓몬 실드를 즐김. 게임하다보니 여유가 생기고 마음이 평온해지는 것을 느끼며, 아 사람은 가끔 좀 쉬어야지.. 라는 생각이 듬 기타 유동현님이 제안주셔서 Career Talk에서 데이터 관련 세부 직군 소개 및 방향 제시에 대해 발표하고 옴 Google Cloud Summit Seoul 2019 - 고객 패널과 함께 알아보는 Google Cloud Platform에 발표함 코엑스에서 600명 넘는 분들 앞에서 발표할 수 있는 기회였습니다! 제가 여태까지 발표한 장소 중 제일 거대했습니다 GDG Pangyo에서 진행한 ML Pipeliners 멘토링 진행 12월 학습 cs224w를 공부하기 시작. Graph 관점으로 문제를 풀어보기 위한 도전 Tensorflow 2.0 다시 공부 중 Google BigQuery : The Definitive Guide 책을 번역하는 중. 책 내용은 한번 봤는데, 번역하는 일은 또 빡세다는 것을 느끼는 중 쏘카 데이터그룹 테크 밋업 준비 및 홍보함. 직접 발표하진 않았지만 사람들이 좋아할 소재를 찾아 제안함 Airflow를 더 강력하게 쓰는 중. 소스코드를 까보며 사용 정서적 안정감 Task 외에 추가된 업무가 너무 많아 요새 고민하는 중. 극강의 매니저 역할을 해야하는 것일까? Task를 직접 진행해야 할까? 아직도 정답은 내리지 못했지만 이 부분에 대해 계속 고민하다보니 정서적 안정감이 종종 변하고 있음. 앞으로 미래에 어떤 사람이 될지 고민하는 중 기타 글또 3기가 12월부로 종료되었고, 1월 중 ~ 2월부터 글또 4기를 시작할 예정 키워드 정리 올해 몇 가지 키워드를 추려보면 다음과 같습니다 팀장의 역할 최적화(Optimization) MLOps 딥놀 유튜브 Notion 팀장의 역할 생각보다 낮은 연차에 팀장을 맡으며 고민이 많아졌습니다. “어떤 팀장이 되야할까?” 제가 예전에 같이 있었던 상사, 팀장님들은 어떤 사람이었는지 생각해보며 저는 어떤 상사를 원했는지 고민했습니다 저는 “팀원 개인의 욕구를 잘 충족해주고, 잘 성장할 수 있도록 돕고 필요한 경우 문제 해결을 잘 도와주는 팀장”을 하기로 결정했고, 매달 정기 면담을 통해 팀원들의 성격 + 욕구를 파악해 팀원들의 욕구를 충족시킬 수 있도록 노력했습니다 아마 팀원들의 동기 부여를 잘 주고, 팀원들이 만족하고 있다고 생각합니다 앞으론 더 잘 성장하도록 돕고, 저도 업무를 주도적으로 하며 그 지식을 공유하는게 중요하지 않을까? 생각하고 있습니다 최적화 데이터 사이언스쪽엔 보통 머신러닝/딥러닝 위주로 많은 분들이 공부하고 있습니다 저 또한 그랬는데, 최적화 방법론이 현실의 문제를 효율적으로 푸는 경우도 있는 것을 깨달았습니다 머신러닝과 최적화 방법을 혼합하면 비즈니스 문제를 효율적으로 풀 수 있습니다. 예를 들면 제품의 수요가 예측될 경우, 그 수요값에 기반해 생산 계획을 수립할 수 있습니다 이런 문제들은 예측 모델링의 결과값이 최적화의 Input으로 들어가게 됩니다. 그 후, 시나리오에 따라 전략이 결정됩니다 MLOps 머신러닝/딥러닝이 인기를 끌며 점점 많은 분들이 공부하고 있습니다. 실제 모델링 코드는 몇줄 안되지만 Production화하기 위해선 여러 개발 작업이 필요합니다 이런 부분을 자동화하기 위해 다양한 도구들이 나오고 있고, 대표적으로 Serving, Auto ML, Experiment 관리 등이 있습니다 이 부분은 데이터 엔지니어링, 개발에 관심갖던 제 성향에 잘 맞아 꾸준히 공부하고 있습니다 아마 Google에서 많은 Application을 계속 출시할 것으로 예상됩니다 딥놀 유튜브 블로그나 인스타그램은 많이 하고 있어서, 이제 도전한다면 유튜브를 해야하지 않을까? 생각했습니다 혼자서 유튜브 시작하는건 매우 부담스러워서, 지인분들과 데이터 관련 이야기하는 유튜브를 만들었습니다 혼자 진행하는 것보다 여러명이 함께하니 훨씬 규칙적으로 진행할 수 있는 것 같습니다 편집자가 따로 없어서 직접 편집해야 하지만.. Vrew 등을 활용해 더 빠르게 영상을 만들고 있습니다! 내년엔 더 많은 영상으로 찾아뵙길..! Notion 노션을 사용한지 이제 1년이 넘었습니다 예전엔 메모를 중구난방으로 했는데, 이젠 모두 노션에 저장되어 있습니다 회사에서 작성한 회의록 정리, 기술에 대한 디버깅, Project 진행 상황 템플릿, 링크 모음 등 1년 이상 사용하니 어느정도 저만의 방식이 생긴 것 같습니다 내년엔 메모를 어떻게 해야할까?에 대한 내용으로 한번 발표하는 자리를 만들어보려고 합니다 :) 링크를 아래와 같이 정리하면 다시 찾기 쉽고 보기도 이쁩니다! 마치며 2019년은 정말! 열심히 지냈고, 빠르게 지나갔습니다. 많은 것을 했지만 동시에 못했던 것들도 있으니 그 부분을 잘 채워서 2020년은 더 발전하는 해가 되도록 노력하려고 합니다-! 2019년에 블로그 글은 총 56개 작성했으니 평균으로 치면 최소한 1주에 글 1개는 작성한 것 같네요 :) 외부 발표는 총 5번 진행했고, 사내 발표는 꽤 많이..! 2020년엔 발표를 많이 할지에 대한 구체적인 생각은 없지만, 조금 더 고민해봐야 겠네요 :)",
    "tags": "diary",
    "url": "/diary/2019/12/25/2019-retrospect/"
  },{
    "title": "Rules of Machine Learning: Best Practices for ML Engineering 정리",
    "text": "Google의 Research Scientist인 Martin Zinkevich가 작성하신 Rules of Machine Learning: Best Practices for ML Engineering 나름대로 번역하고 정리한 글입니다 Machine Learning Guides에 언어를 한국어로 조정하면 번역본이 나오지만, 개인 학습 목적으로 정리했습니다(4부까지만 번역하고 그 이후는 번역하지 않았으니 원본이 궁금하시면 꼭 링크를 참고해주세요) 정리하며 제가 이해한 상태로 정리했고, 추가적으로 제가 아는 내용을 덧붙였습니다 Best Practices for ML Engineering 이 문서는 머신러닝에 대한 기본 지식을 가진 사람들이 Google의 머신러닝 best practice의 장점을 얻을 수 있도록 돕기 위한 문서 Google C++ 스타일 가이드처럼 머신러닝 관련한 실용적인 내용을 전달함 머신러닝 모델을 개발하거나 다뤄본 경험이 있다면 이 문서를 읽기 위한 배경 지식을 갖춘 것 Terminology(용어) 반복적으로 사용될 용어 정의 Instance : 예측하려는 대상. 웹페이지를 “고양이와 관련”, “고양이와 무관”으로 분류하려는 경우 웹페이지가 인스턴스임 Label : 예측 작업에 관한 답으로, 머신러닝 시스템이 도출하거나 학습 데이터에서 제공된 정답 Feature : 예측 작업에 사용되는 인스턴스의 속성. ‘웹페이지에 고양이란 단어가 나온 횟수’ 등을 예로 들 수 있음 Feature Column : 관련된 feature의 집합. 예를 들어 사용자가 거주할 수 있는 모든 국가의 집합. Feature column은 Google에서만 사용되는 용어고, Yahoo/Microsoft에선 namespace라고 함 Example : Instance(Feature 포함) 및 Label Model : 예측 작업의 통계적 표현(statistical representation) example을 사용해 모델을 학습한 후, 그 모델을 사용해 예측함 Metric : 관심이 있는 수치. 직접 최적화될 수 있고, 아닐수도 있음 Objective : 알고리즘에서 최적화하려는 Metric Pipeline : 머신러닝 알고리즘을 둘러싼 인프라. 프론트엔드는 데이터를 수집하고, Train 데이터 저장, 모델 Train, 모델 Production으로 내보내는 것을 포함함 Click-through Rate : 광고에서 링크를 클릭하는 비율 Overview 멋진 제품을 만들기 위해, 머신러닝 전문가 흉내를 내지 말고 훌륭한 엔지니어처럼 머신러닝을 활용해야 함 실제로 직면하는 문제는 대부분 엔지니어링 문제임. 결국 좋은 Feature를 갖는 것이 중요함 기본 접근 방법 1) 파이프라인이 end to end로 견고하게 되어있는지 확인 2) 합리적인 objective로 시작 3) 간단한 방법으로 상식적인 feature 추가 4) 파이프라인이 견고하게 되어있는지 확인 이 접근법은 오래 사용할 수 있고, 혹시 기본 접근 방법으로 더 이상 진행할 수 없는 경우 다른 방식을 찾아야 함 복잡성을 증가시키면 개발은 느려짐 기본 접근 방법으로 부족하면 최첨단 머신러닝 기법을 도전할 때고, 3단계 섹션을 참고하면 됨 이 문서의 구성 1부 : 머신러닝 시스템을 구축하기에 적절한 시점에 대한 이야기 2부 : 첫 파이프라인을 구축하는 방법 3부 : 파이프라인에 새 Feature를 추가하며 계속 출시, 반복하는 과정에 대한 이야기 + 모델 training-serving의 skew를 평가하는 방법 4부 : 개선이 한계에 부딪힌 경우 대처하는 방법 부록 : 자주 사용되는 시스템에 관한 배경 지식 1부. Before Machine Learning Rule #1: 머신러닝 없이 제품을 출시하는 것을 두려워하지 말기 머신러닝은 매우 cool하지만, 데이터가 필요함 다른 문제로 데이터를 가져와서 모델을 살짝 수정해 적용하는 방법은 이론적으론 가능하지만 휴리스틱보다 성능이 떨어질 가능성이 높음 머신러닝의 효과를 100% 기대한다면 휴리스틱을 사용해도 50%의 효과는 볼 수 있음 예 : 앱 마켓플레이스에서 앱 순위를 매길 때 설치율, 설치 횟수를 휴리스틱으로 사용할 수 있음, 스팸을 감지할 때 전에 스팸을 보낸 적이 있는 발신자를 걸러내면 됨. 연락처의 순위를 매길시 최근에 자주 사용한 연락처 순으로 해도 됨 머신러닝이 제품에 정말 필요한 기능이 아니면 데이터를 충분히 수집하기 전엔 사용하지 말기 Rule #2: 가장 먼저 측정 항목(Metric)을 설계하고 구현하기 머신러닝 시스템을 구축하기 전에 현재 시스템을 최대한 많이 알고 있어야 함. 그 이유는 다음과 같음 1) 시스템 사용자에게 미리 사용 권한을 받기 쉽다 2) 미래에 발생할 수 있는 문제가 있다면 지금부터 과거 데이터를 수집하는 것이 좋음 3) metric 측정을 염두에 두고 시스템을 설계하면 나중에 편함. 구체적으로 metric을 위해 로그에서 문자열을 모두 grep할 필요가 없이 설계하면 됨 4) 무엇이 바뀌고 무엇이 동일하게 유지되는지 알 수 있음. 예를 들어 1일 활성 사용자 수를 최적화하려고 할 때, 사용자 경험을 크게 바꿔도 metric에 눈에 띄는 변화가 없을 수 있음 Google Plus 팀은 read당 exapand 수, read당 reshare 수, read당 plusones 수, 댓글/읽기, 유저당 댓글 수, 유저당 재공유 횟수 등을 측정해 게시물의 품질을 계산할 때 사용함. 그리고 사용자를 그룹화해 실험할 수 있는 실험 프레임워크를 갖추는 것이 중요함. Rule #12 참고 Metric을 적극적으로 모니터링할수록 시스템을 전반적으로 파악하기 쉬워짐. 문제를 찾았다면 metric에 추가해 모니터링. 최근 릴리즈에 만족할만한 정량적 변화가 있었다면 metric에 추가하자 Rule #3: 휴리스틱이 복잡하면 머신러닝을 선택하기 단순한 휴리스틱만 갖춰도 제품을 출시할 수 있음 휴리스틱이 복잡하면 유지보수가 불가능 데이터가 확보되고 달성하려는 목표가 확실해지면 머신러닝으로 진행할 수 있음 소프트웨어 엔지니어링 작업에서 휴리스틱/머신러닝 모델인지 상관없이 계속 업데이트가 필요함 머신러닝 모델이 휴리스틱보다 업데이트 및 유지보수가 쉬움. Rule 16 참고 2부. ML 1단계: Your First Pipeline 첫 파이프라인에선 시스템 인프라에 집중하기 머신러닝의 가능성에 대해 상상하는 것도 재미있지만, 파이프라인을 먼저 믿지 않으면 어떤 일이 일어나는지 파악하기 어려움(=파이프라인이 확실해야 현재 상황을 제대로 파악할 수 있음) Rule #4: 최초 모델은 단순하게 가져가고 인프라를 제대로 만들기 첫 모델은 제품 개선에 가장 크게 기여하기 때문에 처음부터 화려한 기능을 갖추지 않아도 됨 그러나 인프라가 예상보다 더 문제를 겪을 수 있음 Facny한 새 머신러닝 시스템을 사용하기 전에 다음과 같은 내용을 결정해야 함 학습 알고리즘에 example을 제공할 방법 시스템의 good과 bad를 판단할 기준 모델을 Application에 통합할 방법. 모델을 실시간으로 적용할 수 있고, 미리 예측해 결과를 Table에 저장할 수 있음. 예를 들어 웹페이지는 미리 분류해 테이블에 결과를 저장하고, 채팅 메세지는 실시간으로 분류할 수 있음 (역자) : 실시간으로 적용하는 경우엔 Flask, TF Serving 등을 사용해 API로 제공하는 방식이 있고, 배치성으로(1시간에 1번) 진행할 경우엔 Database에 저장하는 방식이 있음 단순한 Feature를 선택하면 다음 작업을 쉽게 진행할 수 있음 Feature가 학습 알고리즘에 정확히 도달함 모델이 합리적인 weight를 학습함 Feature가 서버의 모델에 정확히 도달함 이런 3가지 과제를 안정적으로 달성할 시스템을 만들었으면, 대부분의 일을 한 것임 이제 단순한 모델에서 baseline metric과 baseline behavior를 얻어 더 복잡한 모델을 테스트할 때 활용할 수 있음 (구글의) 어떤 팀에선 “중립적”인 최초 런칭을 목표로 하는데, 이는 머신러닝으로 얻을 당장의 이익에 집착하지 않고 본질의 목표에 집중하기 위함임 Rule #5: 머신러닝과 별도로 인프라 테스트하기 인프라틑 테스트할 수 있어야하고, 시스템의 train 부분은 캡슐화해야 모든 관련 부분을 테스트할 수 있음. 특히 다음과 같은 작업이 필요함 1) 알고리즘에 데이터를 넣는 기능 테스트 생성되야 하는 Feature가 잘 채워졌는지 확인 개인정보 보호하는 범위 내에서 input 값을 직접 조사 가능하면 파이프라인의 통계를 다른 곳에서(로컬 등) 데이터 처리해 나온 통계와 비교 2) 모델을 알고리즘에서 추출하는 기능 테스트 Train 환경의 모델이 주는 점수와 Serving 환경의 모델과 동일한지 확인. Rule 37 참조 머신러닝엔 예측 불가능성이 있어서, Train 및 Serving시 Example을 생성하는 코드를 테스트할 준비하고, Serving 중 고정된 모델을 로드해 사용할 수 있는지 확인해야 함 또한 데이터를 이해하는 것이 중요함. Practical advice for analysis of large, complex data sets 참고 Rule #6: 파이프라인을 복사할 땐 데이터 누락 주의하기 기존 파이프라인을 복사해 새로운 파이프라인을 만들었는데, 새 파이프라인에 필요한 데이터가 기존 파이프라인에 누락되는 경우가 종종 있음 예를 들어 Google+ HOT 소식의 파이프라인은 최신 게시물의 순위를 매기는 것이 목적이라 과거 게시물들이 유의미한데, 복사해온 파이프라인에서 과거 게시물들을 누락시킴 이 파이프라인을 Google+ 스트림에 사용하기 위해 복사했더니 이 기능에선 과거 게시물들이 누락됨 사용자가 트정 게시물을 조회하지 않은 이유를 모델링할 경우 negative example이 모두 누락되므로 결국 쓸모없는 데이터가 됨 Play에도 비슷한 문제가 있었음. Play 앱 홈 화면을 만들며 Play 게임 방문 페이지의 Example을 포함한 파이프라인을 새로 만들었는데, 각 example의 출처를 구분짓지 않았음 Rule #7: 휴리스틱을 Feature로 변환하거나 외부에서 처리하기 머신러닝으로 해결하려는 문제들은 보통 새로 등장한 Problem은 아님. 순위 결정, 분류 등 어떤 문제든 과거에 사용하던 기존 시스템이 있음 따라서 수많은 규칙(Rule base), 휴리스틱이 이미 존재함 휴리스틱 + 머신러닝의 조합 기존 휴리스틱을 철저히 분석해야 함. 첫번째로 머신러닝 시스템으로 전환이 더 원활해짐. 둘째로 이런 규칙은 시스템에 대한 직관을 풍부하게 담고 있음. 다음 4가지 방법으로 휴리스틱을 사용할 수 있음 1) 휴리스틱을 사용해 전처리 믿을 수 없을만큼 awesome한 feature인 경우 고려할 수 있음 예를 들어 스팸 필터에서 보낸 사람이 이미 차단 목록에 들어있으면 차단 목록을 다시 학습할 필요없음. 단순히 메세지를 차단하면 됨 2) Feature 생성 휴리스틱에서 직접 Feature 생성 예를 들어 휴리스틱을 사용해 쿼리 결과의 유사도 점수를 계산할 경우 이 점수를 Feature로 넣을 수 있음 이후에 머신러닝 기법을 사용해 값을 조정할 수 있지만 처음엔 휴리스틱에서 나오는 값을 그대로 사용해도 됨 3) 휴리스틱의 input을 Feature로 사용 앱의 설치 횟수, 텍스트 문자 수, 요일을 결합하는 휴리스틱이 있다면 이런 값을 학습에 제공하는 것이 좋음. 이 때 앙상블에 적용되는 기법이 일부 적용됨. Rule 40 참조 4) Label을 수정 휴리스틱이 현재 Label에 포함되지 않는 정보를 포착하면 이 방법을 사용할 수 있음 예를 들어 다운로드 횟수를 극대화하며 컨텐츠 품질에도 중점을 두려면 앱이 받은 평균 별점 수로 label을 곱하는 것이 답일 수 있음. 정해진 방식은 없음. 첫 목표 참고 ML 시스템에서 휴리스틱을 사용하는 경우 복잡도가 추가되는 것을 주의해야 함. 새로운 머신러닝 알고리즘에 기존 휴리스틱을 사용하면 전환이 원활할 수 있지만, 더 간단한 방법으로 같은 효과를 낼 수 없는지 고민해보면 좋음 모니터링 일반적으로 알림에 실제 정보를 추가하고 모니터링할 수 있는 대시보드 페이지를 마련하곤 함 Rule #8: 시스템의 갱신(freshness) 요구사항을 파악하기 모델이 하루, 1주일, 1분기 뒤에 성능이 얼마나 떨어지는지? 이 정보는 모니터링의 우선순위를 판단할 때 도움이 됨 하루동안 모델을 업데이트하지 않았더니 제품의 품질이 떨어지는 경우 모델을 지속적으로 모니터링하는 엔지니어를 두는 것이 좋음 광고 시스템같이 매일 새로운 광고가 유입되는 경우 업데이트가 매일 진행되야 함 예를 들어 Google Play 검색의 모델이 업데이트되지 않으면 1개월 이내에 부정적인 영향을 미침 Google+의 HOT 소식에서 게시물 id를 갖지 않는 일부 모델은 자주 내보낼 필요가 없고, 게시물 id를 갖는 모델은 자주 업데이트됨 갱신 기준은 시간에 따라 변화할 수 있음(특히 모델에서 feature column이 추가되거나 삭제될 경우) Rule #9: 모델을 내보내기 전에 문제를 탐지하기 많은 머신러닝 시스템엔 모델을 서빙 환경으로 내보내는 단계가 있음. 내보낸 모델에 문제가 있는 경우 유저가 바로 알아차림 모델을 내보내기 전에 sanity check(품질 검사)를 해야 함 홀드아웃 데이터에 대해 모델의 성능이 적절한지 확인해야 함 또는 데이터의 신빙성이 의심되면 모델을 내보내지 말아야 함 계속 모델을 내보내는 팀은 대부분 AUC를 확인한 후 내보냄 모델을 제때 내보내지 못하는 것은 이메일 알림으로 해결할 수 있지만, 문제 있는 모델을 제공하면 사태가 커짐 따라서 유저에게 영향을 미치는 것보다 늦는게 나을 수 있음 Rule #10: 조용한 실패(silent failures)에 주의하기 개발 시스템보다 머신러닝 시스템에서 자주 나타나는 문제 Join 대상이 되는 특정 테이블이 더 이상 업데이트되지 않는 경우, 이 테이블 기반으로 머신러닝 시스템을 구축하면 겉보기엔 특별한 문제가 없어도 실제론 성능이 점점 떨어짐 몇 달 동안 그대로였던 테이블을 갱신하는 것으로 놀라운 성능 개선 효과를 거둘 수 있음 구현 변경으로 Feature의 포함 범위가 바뀌기도 함 Feature column이 90%에서 60%로 급락할 수 있음 데이터의 통계를 모니터링하면 이런 유형의 실패를 줄일 수 있음 Rule #11: Feature column에 소유자를 지정하고 문서화하기 시스템의 규모가 크고 Feature column이 많은 경우 각 컬럼을 누가 만들었고, 관리하는지 알아야 함 담당자가 조직을 떠날 경우 철저한 인수인계가 이루어져야 함 이름만 봐도 의미를 알 수 있는 Feature column도 많지만, 특성의 의미, 출처, 유용성을 자세히 기록해두는 습관을 들이는 것이 좋음 첫 목표(Objective) 시스템에서 중요한 metric이 아무리 많아도 머신러닝 알고리즘에 필요한 목표(objective), 알고리점에서 최적화할 수치는 일반적으로 단 하나 Objective와 Metric을 잘 구분해야 함 Metric은 시스템에서 보고하는 다양한 숫자들로 중요할 수도, 중요하지 않을 수도 있음. Rule #2 참조 Rule #12: 어떤 objective를 직접 최적화할지 너무 고민하지 말기 당신은 돈을 벌거나, 유저를 행복하게 만들거나, 세상을 더 좋게 만드는데 기여하는 것을 원할것임 중요하게 생각할 metric은 무수히 많고, 이것들을 모두 측정해야 함 그러나 머신러닝 초기엔 모든 metric이 증가하는 것을 알 수 있음 예를 들어 클릭수 및 사용 시간이 중요하다고 가정한 경우, 클릭수를 최적화하면 사용 시간도 증가할 가능성이 높음 모든 metric이 쉽게 증가할 수 있으므로, 다양한 metric 간 균형을 맞추려고 고민하지 말고 단순하게 생각하면 됨 하지만 이 규칙에도 한계가 있음. objetive와 시스템의 절대적 안전성을 혼돈해선 안됨(Rule 39 참조) 또한 직접 최적화하는 metric은 개선되지만 결국 출시에 실패하는 상황이 반복되면 objective를 수정해야 할 수 있음 Rule #13: 단순하고 관찰 가능하고 추적 가능한 metric을 첫 목표로 선택하기 궁극적인 목표를 미리 알지 못하는 경우도 많음 목표를 일단 정하고, 기존 시스템과 새로운 머신러닝 시스템 데이터를 나란히 분석하면 목표를 수정하고 싶어짐 궁극적인 목표에 대해 팀원들의 의견이 다를 수 있음 ML Objective는 측정하기 쉬우면서 진정한 목표를 반영해야 함 단순한 ML 목표를 기준으로 학습하되, 다른 로직(이왕이면 단순한)을 추가해 최종 순위를 결정할 수 있도록 “policy layer”를 상단에 두는 것이 좋음 가장 모델링하기 쉬운 대상은 직접 관찰되고 시스템 동작과 인과성을 추적할 수 있는 사용자 행동 Ranked list가 클릭되었는가? Ranked object가 다운되었는가? Ranked object가 전달, 회신, 이메일로 발성되었는가? Ranked object가 평가되었는가? 보이는 object가 스팸/음란물/불쾌감을 주는 컨텐츠로 신고되었는가? 간접 효과는 처음에 모델링하지 말기 사용자가 다음 날 방문했는가? 사용자가 사이트를 얼마나 오래 방문했는가? 일일 활성 사용자 수는 몇인가? 간접 효과도 좋은 metric으로 AB Test 및 출시 결정에 활용될 수 있음 다음과 같은 의문을 해결할 때 머신러닝을 사용하지 말기 사용자가 제품을 만족하고 있는가? 사용자 경험이 만족스러운가? 제품이 사용자의 전반적인 삶의 질을 높여주는가? 회사의 전반적인 건강함에 어떤 영향을 주는가? 모두 중요하지만 측정하기가 매우 어려움. 간접적인 기준으로 대신하자 사용자가 만족감을 느낀다면 사이트에 더 오래 머무르고, 다음 날 다시 방문할 것임 삶의 질이나 회사의 건강함과 관련되는 부분은 사람의 판단이 필수적 Rule #14: 해석 가능한 모델부터 시작하면 디버깅이 쉬움 선형 회귀, 로지스틱 회귀, 푸아송 회귀는 확률론 모델에서 나온 것들임 각 예측은 확률 또는 기대값으로 해석할 수 있음 예를 들어 Train 시스템의 확률이 병렬로 운영되거나 별도로 조사된 프러덕션 시스템의 확률과 차이가 나면 이를 통해 문제가 드러날 수 있음 단순 모델에선 feedback loop를 다루는 방법이 더 쉬움(Rule #36 참조) 이런 확률 예측을 근거로 결정내리는 경우가 많음 클릭 확률, 다운로드 확률 등의 기대값에 따라 내림차순으로 게시물의 순위를 매길 수 있음 그러나 어떤 모델을 사용할지 선택할 땐 모델에 제공된 데이터의 확률보다 Decision이 더 중요함(Rule #26 참조) Rule #15: Policy Layer에 스팸 필터링과 Quality Ranking을 분리하자 Quality Ranking이 예술이라면 스팸 필터링은 전쟁임 사용자들은 게시물의 품질을 판단하는데 사용하는 지표를 금방 알아차리고 게시물을 적당히 손질해 이런 속성을 갖게 만듬(인스타그램 생각하면 쉬울듯) 따라서 Quality Ranking에선 정상적인 의도로 게시된 컨텐츠의 순위를 매기는 데 집중해야 함 스팸의 순위를 높게 매겼다고 해서 Quality Ranking 학습 시스템을 평가절하해선 안됨 선정적인 컨텐츠도 마찬가지 이유로 Quality Ranking과 별도로 처리해야 함 때론 시스템에 규칙을 도입하기도 함 : 스팸 신고가 3회 초과하면 게시물은 제외한다 학습 모델은 최소 하루 1번 이상 업데이트되야 하고, 컨텐츠 작성자의 평판도 큰 역할을 함 이런 두 시스템의 출력을 일정 수준에서 통홥해야 함 주의 : 검색 결과의 스팸 필터링은 이메일보다 더 공격적이어야 함 정규화를 사용하지 않고 알고리즘이 수렴한다는 전제 하에 이 사실은 참임 스팸은 품질 분류용 학습 데이터에서 제외하는 것이 일반적인 관행임 3부. ML 2단계: Feature Engineering 머신러닝 시스템 lifecycle의 첫 단계에서 중요한 이슈는 Train 시스템에 데이터를 공급하고, 의미있는 metric을 측정하고 서빙 인프라를 구축하는 것 unit test와 system test를 갖춘 정상적인 end to end 시스템을 구축했다면 2단계로 넘어가자 2단계는 다양하고 알기 쉬운 Feature를 시스템에 넣으면 됨 머신러닝 2단계에서선 최대한 많은 Feature를 직관적인 방식으로 넣는 것에 관심을 가짐 이 단계는 모든 metric이 상승세를 보여야 함 출시를 반복하며 필요한 데이터를 모두 모아 train 시스템의 성능을 극대화해야 함 Rule #16: 출시와 반복을 계획하기 지금 작업 중인 모델이 마지막 출시 버전이 될 것이라거나, 반복적인 모델 출시가 언젠가 끝날거라는 기대는 버리면 좋음 이번 출시에서 추가되는 복잡성으로 인해 이후 출시가 늦춰질 가능성이 있는지 고려해야 함 많은 팀에서 지금까지 분기당 1회 이상 출시를 진행함 새 모델을 출시하는 기본적인 3가지 이유 새로운 Feature 도입 정규화 조정 및 이전 Feature를 새로운 방식으로 결합 objective 조정 모델에 관심을 기울이면 좋은 결과가 나올 수 있음. Example에 공급되는 데이터를 조사해 새로운 지표 또는 잘못된 기존 지표를 찾을 수 있음 모델을 만들며 Feature 추가, 삭제, 재결합이 쉬어야 함 파이프라인의 사본을 만들고 정확성을 검증하기가 쉬울까? 둘 이상의 사본을 동시에 실행하는 방법은 어떻게 할까? 특정 Feature가 이번 파이프라인 버전에 포함될지 고민하지 말기. 다음 출시에 포함해도 됨 Rule #17: 학습된(learned) feature이 아닌 직접 관찰하고 보고된 feature부터 시작하기 논란의 여지가 있는 주장이지만, 많은 함정을 피할 수 있음 우선 학습된 feature는 외부 시스템(예 : 비지도 클러스터링 시스템) 또는 학습 시스템 자체(예 : Factored model이나 딥러닝)에서 생성된 Feature 이런 Feature가 유용할 수 있지만 여러 문제점을 가질 수 있으므로 최초 모델에는 포함해서 안됨 외부 시스템을 사용해 Feature를 만드는 경우 외부 시스템엔 그 시스템의 목표가 있다는 것을 기억해야 함 그 외부 시스템의 목표와 나의 현재 목표와 상관이 낮을 수 있음 외부 시스템에서 스냅샷을 가져오는 경우 최신 데이터가 아닐 수 있음 외부 시스템의 특성을 업데이트하는 경우 의미가 변질될 수 있음 따라서 외부 시스템을 사용해 Feature를 제공하는 방식을 사용하려면 매우 신중한 접근법이 필요함 역자 : 날씨나 공공 데이터 API를 받아서 학습하는 경우, 데이터 제공측에서 바꾸면.. 파이프라인이 망가지는데 그 예시와 비슷한 느낌 Factored model과 딥러닝 모델은 nonconvex(볼록하지 않다)는 성질이 있음 따라서 최적 해를 구하거나 근사할 수 있단 보장이 없고, 반복할 때마다 다른 local minima가 발견될 수 있음 이런 variation이 시스템 변경에 따르는 영향인지 무작위적인지 판단하기 어렵게 만듬 deep feature 없이 모델을 만들면 탁월한 baseline 성능을 얻을 수 있고, 이 기준이 확보된 이후 특이하고 복잡한 접근법을 시도하면 좋음 Rule #18: 여러 Context로 일반화되는 컨텐츠의 feature 찾기 머신러닝 시스템은 더 거대한 시스템의 일부인 경우가 많음 예를 들어 HOT 소식에 올라갈만한 게시물은 HOT 소식에 올라가기 전에 많은 사람의 +1, 재공유, 댓글을 받음 학습 시스템에 이런 통계를 제공하면 최적화 컨텍스트와 관련해 어떤 데이터도 갖지 않는 새로운 게시물이 추천될 수 있음 유튜브의 다음 볼만한 동영상 기능에는 시청 횟수, 연계 시청 횟수를 사용할 수 있음 또한 명시적인 사용자 평가를 사용할 수도 있음 마지막으로 label로 사용 중인 사용자 행동이 있다면 다른 컨텍스트의 자료에 대해 같은 행동을 파악해 좋은 feature를 생성할 수 있음 이런 모든 feature가 새로운 컨텐츠를 가져오도록 기여함 단, 개인화는 여기에 포함되지 않음. 이 컨텍스트에서 컨텐츠를 좋아하는 사람이 있는지 알아낸 후 누가 컨텐츠를 좋아하거나 싫어하는지 알아내는 방식으로 진행함 Rule #19: 가능하면 매우 구체적인 Feature를 사용하기 소수의 복잡한 feature보다 다수의 단순한 feature를 학습하는 것이 더 간편함 검색 대상 문서의 id 및 규격화된 쿼리는 일반화에 크게 기여하지 못하지만, head query에서 순위와 label을 맞춰주는 역할을 함 따라서 feature 그룹에서 각 feature가 데이터의 매우 작은 부분에만 적용되더라도 전체 coverage가 90% 넘으면 걱정할 필요가 없음 정규화를 사용하면 작은 example에 적용되는 feature를 배제할 수 있음 Rule #20: 사람이 이해할 수 있는 방식으로 기존 feature를 결합하고 수정해 새로운 feature를 만들자 feature를 결합하고 수정하는 방법은 다양함. Tensorflow에선 Transformation을 이용해 데이터 전처리하는 방법을 제공함 가장 표준적인 방식은 이산화(discretization)와 교차(cross)임 Discretization : continuous feature를 불연속 feature로 만드는 것. 나이를 10세, 15세 보지 않고 10대, 20대 등으로 하는 것. 히스토그램의 경계를 너무 고민하지 않고 기본적인 분위 사용해도 효과를 얻을 수 있음 Cross : 둘 이상의 feature column을 결합. {남성, 여성} x {미국, 캐나다, 멕시코}의 특성으로 구성된 새로운 feature 매우 큰 feature column을 생성하는 교차는 오버피팅을 초래할 수 있음 예를 들어 검색 기능을 만들며 검색어 단어를 포함하는 feature와 문서의 단어를 포함하는 feature를 준비할 수 있음. 이걸 교차하면 매우 많음 feature가 생김(Rule #21 참고) Text를 다룰 때 두가지 대안이 있음 가장 엄격한 방법은 내적을 구함 가장 단순한 내적은 검색어와 문서가 공통적으로 갖는 단어의 수를 세는 것 그 후 이 feature를 불연속화 내적을 구하는 다른 방식은 교집합을 구하는 것 Rule #21: 선형 모델에서 학습 가능한 feature weight의 수는 데이터 보유량에 대략적으로 비례함 모델의 적절한 복잡도에 관한 훌륭한 통계 이론은 많지만, 지금은 이 규칙만 명심하면 됨 Example이 1,000개에 불과한데 학습할 수 있는지 의심하는 사람들도 있고, 예시가 100만개 정도 있으면 특정 학습 방식에 고착되므로 그 이상 필요 없다고 생각하는 사람도 있음. 비결은 데이터 사이즈에 학습 규모를 맞추는 것 1) 검색 랭킹 시스템에서 문서와 쿼리에 수백만가지 단어가 있는데 라벨이 있는 example은 1000개뿐이라면 문서 특성과 쿼리 특성의 내적, TFIDF 및 인위적으로 추출된 feature를 사용해야 함. 1000개의 example에 대략 10개 정도의 feature가 생김 2) example이 100만개라면 정규화 및 feature selection을 사용해 문서와 쿼리의 교집함을 구함. 이를 통해 수백만 개의 feature가 나오지만 정규화를 통해 feature가 감소함. 1,000만개의 example에서 대략 10만개 정도의 feature가 생김 3) example이 수십억 또는 수천억 개라면 feature selection과 정규화를 사용해 feature column을 문서 및 쿼리 토큰을 cross할 수 있음. 10억 개의 example에 1,000만개의 feature가 생김 마지막에는 Rule #28에 따라 사용할 feature를 결정함 Rule #22: 더 이상 사용되지 않는 feature를 정리하기 사용하지 않는 feature는 기술 부채가 됨 더 이상 사용되지 않고 다른 feature와 결합해도 유용하지 않다면 인프라에서 삭제하자 인프라를 깔끔하게 유지해야 가장 유망한 feature를 가장 빠르게 시험해볼 수 있음. feature가 다시 필요해지면 언제든지 다시 추가할 수 있음 추가하거나 유지할 feature를 결정할 땐 coverage를 고려하자. Feature가 얼마나 많은 example을 포괄하는지? 예를 들어 개인별 맞춤 feature가 있는데 사용자 중 이 feature를 사용하는 비율이 8%에 불과하면 높은 효율을 기대할 수 없음 어떤 Feature는 생각보다 큰 역할을 하기도 함. 예를 들어 데이터 중 1%만 포괄하는 feature가 있는데, 이 feature를 갖는 example 중 90%가 양성이라면 꼭 추가해야할 feature임 인간에 의한 시스템 분석 머신러닝의 세 번째 단계로 넘어가기 전에, 어떤 머신러닝 강의에서도 다뤄지지 않는 주제를 짚고 넘어가려고 함 바로 기준 모델을 어떻게 바라보고 개선할지에 관한 것 이것은 과학이라기보단 예술에 가깝지만, 바람직하지 않은 몇 가지 패턴을 피할 때 도움이 됨 Rule #23: 당신은 전형적인 최종 유저가 아니다 팀이 궁지에 몰리는 가장 쉬운 방법 fishfood(팀 내에서 프로토타입 사용) 및 dogfood(회사 내에서 프로토타입 사용)에는 많은 장점이 있지만, 직원들이 성능의 정확성에 대해 잘 살펴야 함 단점이 분명한 변경사항을 피하는 것도 중요하지만, 프러덕션 단계가 안정적이라고 판단할 요소를 철저히 테스트하는 것이 중요함 크라우드소싱 플랫폼에서 일반인을 대상으로 유료 설문조사를 진행하거나 실제 사용자를 대상으로 실험하는 방법이 있음 이렇게 하는 이유 1) 개발자는 코드부터 신경을 쓰기 마련. 특정 측면에만 주목하거나 지나치게 감정이 개입되어 확증 편향이 휩쓸릴 수 있음 2) 개발자의 시간은 소중합니다. 엔지니어 9명이 1시간 동안 회의하는데 사용되는 비용과 크라우드소싱 플랫폼에서 유료 설문조사를 진행해 얻을 수 있는 라벨 수를 비교해보자 사용자 의견이 꼭 필요하다면, 사용자 경험 방법론(uesr experience methodologies)을 사용해보자. 프로세스 초기에 사용자 페르소나를 만들고 이후에 사용성 테스트를 진행하자 사용자 페르소나는 가상적인 사용자를 의미함. 예를 들어 팀원이 모두 남성이면 35세 여성 사용자 페르소나를 만들어보자. 또한 사용성 테스트를 진행해 실제 사용자 반응을 조사하면 새로운 관점을 접할 수 있음 Rule #24: 모델 사이의 delta를 측정하자 유저가 새 모델을 접하기 전에 측정할 수 있는 가장 쉽고 유용한 항목으로 새로운 모델이 프러덕션의 기존 결과와 얼마나 다른지 계산하는 것을 뜻함 예를 들어 ranking 문제에서 동일한 쿼리 샘플을 두 모델에 실행한 후 결과의 symmetric 차이 크기에 순위별 가중치를 적용해 살펴볼 수 있음 차이가 매우 작다면 별도의 실험을 거치지 않아도 변화가 거의 없을 것을 짐작할 수 있음 차이가 매우 크다면 긍정적인 변화임을 확신할 수 있음 symmetric 차가 크게 나온 쿼리를 살펴보면 변화의 본질적인 측면을 이해할 때 도움이 됨 그러나 중요한 것은 시스템의 안정성임. 모델 자체를 비교할 때(이상적으로 0) 대칭 차이가 낮은지 확인하자 Rule #25: 모델을 선택할 땐 예측 파워보다 실용적인 성능을 우선시하자 모델에서 클릭률을 예측하려고 한다. 그러나 결국 중요한 질문은 그 예측으로 무엇을 할지?임 문서의 순위를 결정할 때 활용할 생각이라면 예측 자체보다 최종적인 순위의 품질이 더 중요함 문서가 스팸일 확률을 예측해 차단 기중늘 정할 계획이라면 허용할 대상의 정확성이 가장 중요함 대부분 이런 두 관점의 조화를 이루지만, 그렇지 않다면 소탐대실의 상황이 될 수 있음 따라서 어떤 변화가 log loss는 개선하지만 시스템의 성능을 떨어트린다면 다른 feature를 사용해야 함. 이런 상황이 자주 나타나기 시작하면 모델의 objective를 재검토해야 함 Rule #26: 측정된 오차에서 패턴을 찾아 새 feature 만들기 모델에서 잘못 예측한 training example을 발견했다고 가정 분류 작업의 경우 false positive나 false negative가 여기에 해당함 Ranking task에선 positive와 negative로 이루어진 쌍에서 positive가 negative보다 순위가 낮게 매겨진 경우일 수 있음 중요한 점은 해당 예시는 머신러닝 시스템에서 예측이 잘못된 것을 스스로 알고 있으므로 기회가 있으면 수정이 가능하다는 점 오류를 수정할 수 있는 feature를 모델에 제공하면 모델은 이 feature를 사용하려고 함 반면 시스템에서 실수를 깨닫지 못한 example을 사용한 feature는 무시됨 예를 들어 Play 앱 검색에서 사용자가 무료 게임을 검색했는데 최상위 결과 중 하나에 관련성이 떨어지는 개그 앱이 포함되었음. 따라서 개그 앱에 관한 특성을 만들었음 설치 횟수를 극대화하는 것이 목표인 경우 무료 게임을 검색하는 사용자들이 개그 앱을 많이 설치한다면 개그 앱 feature는 의도한 효과를 낼 수 없음 모델에서 잘못 예측한 example을 확보했으면 현재 feature set을 벗어나는 추세를 찾자 예를 들어 시스템에서 긴 게시물의 순위를 낮추는 경향이 발견되면 게시물 길이를 추가하자. 추가할 feature를 너무 구체적으로 고민하지 말자. 단순히 10개의 feature를 추가한 후 모델이 알아서 판단하도록 놔두자(Rule 21 참조) 이 방식이 원하는 효과를 얻는 가장 쉬운 방법임 Rule #27: 부적절한 동작이 관찰되면 정량화를 시도하자 시스템에 바람직하지 않은 속성이 있는데 기존 loss 함수로는 포착되지 않아서 이슈가 되는 경우가 있음 이러한 경우 무슨 수를 써서라도 불만족스러운 포인트를 구체적인 숫자로 바꿔야함 예를 들어 Play 검색에 ‘개그 앱’이 너무 많이 표시된다고 생각되면 평가 전문가에게 개그 앱을 판별하도록 의뢰할 수 있음 이 경우 사람이 라벨링한 데이터를 사용해도 무리가 없음 이렇게 측정 가능한 문제라면 이제 feature, objective, metric으로 사용할 수 있음 일반적인 규칙은 우선 측정하고 최적화 Rule #28: 단기적인 동작이 같더라도 장기적인 동작은 다를 수 있음 모든 doc_id와 exact_query를 조사해 모든 문서, 모든 쿼리에 관한 클릭 확률을 계산하는 시스템을 새로 구축했다고 가정 현재 시스템보다 단순하고 AB 테스트 결과가 현재 시스템과 거의 일치하는 것으로 나타나서 출시를 결정함 그런데 새 앱이 표시되지 않는 문제를 발견 왜 그럴까? 이 시스템은 자체 기록을 기반으로 해당 쿼리에 관한 문서만을 보여주므로 새 문서를 표시해야 한다는 사실을 학습할 방법이 없음 이런 시스템이 장기적으로 어떻게 작동할지 알아내는 유일한 방법은 모델이 실제로 운영될 때 획득한 데이터로만 학습하는 것인데, 이는 매우 어려운 일임 Training-Serving Skew Training-Serving Skew란 Train 성능과 Serving 성능 간의 차이 차이가 나타나는 이유 Train 파이프라인과 Serving 파이프라인에서 데이터를 처리하는 방법의 차이 학습시 데이터와 제공 시 데이터 간의 변화 모델과 알고리즘 간의 피드백 루프 Google의 프로덕션 머신러닝 시스템에도 Training-Serving Skew로 인해 성능이 저하된 경우가 있었음 가장 좋은 해법은 시스템과 데이터의 변화로 인해 예기치 않은 격차가 생기지 않도록 직접 모니터링하는 것 Rule #29: Training 환경을 Serving 환경과 일치시키는 최고의 방법은 Serving할 때 사용된 feature set을 저장하고 그 데이터를 기반으로 Training시 사용하는 것 모든 example에 대해서 불가능하다면 일부 example에 대해서라도 실천하여 서빙과 학습의 일관성을 검증할 방법을 강구해야 함(Rule #37 참조) 역자 : Serving은 보통 실시간 데이터로 적재가 되고 Training 셋은 ETL 파이프라인으로 실시간 데이터가 아닐 수 있음. 이럴 경우 Serving용 실시간 데이터를 바로 적재해 한번에 쓰자는 이야기 Google의 여러 팀에서 이런 측정을 통해 의외의 결과를 얻은 적 있음 YouTube 홈페이지는 Serving시 특성 로그 기록을 도입하여 품질을 크게 높이고 코드의 복잡성을 낮추었으며, 지금 이 순간에도 여러 팀에서 인프라를 전환하고 있음 Rule #30: 표본 추출된 데이터를 임의로 무시하지 말고 중요도에 따라 가중치를 매기기 데이터가 너무 많으면 파일 1~12만 사용하고, 나머지 파일 13~99는 무시하고 싶을 수도 있음 하지만 잘못된 생각 사용자에게 한 번도 표시되지 않은 데이터는 삭제해도 무방하지만, 나머지 데이터에는 중요도 가중치를 적용하는 것이 가장 좋음 중요도 가중치(importance weight)란 예시 X를 샘플링할 확률이 30%라면 10/3의 가중치를 준다는 의미 중요도 가중치를 사용하는 경우에도 규칙 #14에서 설명한 calibration 속성이 모두 적용됨 Rule #31: Training 및 Serving시 (DB) 테이블 데이터를 조인하는 경우 테이블 데이터는 달라질 수 있음을 명심하자 문서 ID를 해당 문서의 댓글수 또는 클릭수 등의 특성을 담은 테이블과 조인한다고 가정 학습 시점과 서빙 시점 사이에 테이블의 특성이 달라질 수 있음 역자 : 특히 시계열성 데이터에서 많이 실수함(Lag 변수) 이런 경우 학습과 서빙 간에 같은 문서에 관한 모델의 예측이 서로 달라짐 이런 문제를 피하는 가장 쉬운 방법은 서빙 시에 특성을 기록하는 것(Rule #32 참조) 테이블의 변화가 비교적 느리다면 1시간 또는 하루마다 테이블의 스냅샷을 만들어 적당히 근접한 데이터를 얻을 수 있음 그러나 문제가 완벽하게 해결되는 것은 아님 Rule #32: 가능하면 학습 파이프라인과 서빙 파이프라인 간에 코드를 재사용하자 Batch 처리는 Online 처리와 다름 온라인 처리는 도착하는 각 요청을 실시간으로 처리해야 하므로 각 쿼리에 대해 별도의 조회를 수행하는 반면, 배치 처리에서는 여러 작업을 조인 등의 방법으로 결합할 수 있음 서빙 시에는 온라인 처리를 수행하는 반면, 학습은 일괄 처리 작업 그러나 코드를 재사용할 수 있는 방법이 몇 가지 있음 예를 들어 모든 쿼리 또는 조인의 결과를 사람이 읽을 수 있는 방식으로 저장하면 오류를 쉽게 테스트할 수 있음 그런 다음 모든 정보가 수집되었으면 서빙 또는 학습 중에 공통 메소드를 실행하여 사람이 읽을 수 있는 객체와 머신러닝 시스템에 사용되는 형식을 연결하자 이렇게 하면 학습-서빙 격차가 근본적으로 방지됨 이렇게 하려면 우선 학습 코드와 서빙 코드에 동일한 프로그래밍 언어를 사용해야 합니다 그렇지 않으면 코드를 공유하기가 거의 불가능함 역자 : 우버는 Spark로 Training / Serving을 모두 통합함 Rule #33: 1월 5일까지 수집된 데이터를 기준으로 모델을 생성하는 경우 1월 6일 이후의 데이터로 모델을 테스트하자 일반적인 규칙은 모델 학습에 사용된 데이터보다 이후에 수집된 데이터로 모델의 성능을 측정하는 것 이렇게 하면 시스템의 프로덕션 성능을 더 정확히 예상할 수 있음 1월 5일까지 수집된 데이터를 기준으로 모델을 생성하는 경우 1월 6일 이후의 데이터로 모델을 테스트하자 새 데이터에 관한 성능은 기존 데이터보다 다소 저하되는 것이 정상이지만 크게 나빠져서는 안됨 우연히 daily로 변동이 생길 수 있으므로 평균적인 클릭률 또는 전환율이 변경되지 않을 수 있지만, 양성 예시가 음성 예시보다 1점 높게 나올 가능성을 나타내는 AUC는 합리적인 유사도를 보여야 함 Rule #34: 스팸 감지, 관심 이메일 판단 등 필터링을 위한 이진 분류에서는 단기적으로 다소의 성능 저하를 감수하더라도 데이터를 철저히 정제하자 필터링 작업에서는 음성으로 판정된 예시를 사용자로부터 숨긴다 서빙 시 음성 예시의 75%를 차단하는 필터가 있다고 가정하자 사용자에게 표시된 instance로 추가적인 학습 데이터를 추출하려는 생각을 할 수 있음 예를 들어 필터를 통과했지만 사용자가 스팸으로 신고한 이메일은 학습 데이터로 활용할 수 있음 그러나 이 방식은 샘플링 편향을 유발함 더 정제된 데이터를 얻는 방법은 서빙 시 전체 트래픽 중 1%를 ‘홀드아웃’으로 라벨링하고 모든 홀드아웃 예시를 사용자에게 보내는 것 이제 필터는 음성 예시 중에서 최소 74%를 차단함 이러한 홀드아웃 예시는 학습 데이터가 될 수 있음 필터가 음성 예시의 95% 이상을 차단한다면 이 접근법은 현실성이 낮음 그렇더라도 서빙 성능을 측정하려는 경우 소량의 샘플(0.1% 또는 0.001%)을 추출할 수 있음 1만 개 정도의 예시가 있으면 성능을 비교적 정확히 추정할 수 있음 Rule #35: Ranking 문제에선 특유의 왜곡이 나타날 수 있음 표시되는 결과가 바뀔 정도로 ranking 알고리즘을 급격히 변경하면 알고리즘에서 이후에 접하게 될 데이터 자체가 변화함 이러한 유형의 왜곡이 나타날 것을 대비하여 모델을 설계해야 함 여기에는 여러 가지 접근법이 있으며, 공통점은 모델에서 기존에 접한 데이터를 우선시함 1) 쿼리 하나에만 해당하는 특성 보다 여러 쿼리를 포괄하는 특성에 더 높은 정규화를 적용 이렇게 하면 모델에서 모든 쿼리로 일반화되는 특성보다 하나 또는 소수의 쿼리에 국한되는 특성이 우선시됨 이 방식은 자주 나타나는 결과가 이와 무관한 쿼리에까지 영향을 주지 않도록 차단할 때 도움이 됨 unique 값이 많은 feature에 더 높은 정규화를 적용하라는 기존의 권장사항과는 정반대임 2) featue에 positive weight만 허용 따라서 양호한 모든 특성이 ‘미지의’ 특성보다 우선시됨 3) 문서에만 국한된 feature를 배제 이는 #1의 극단적인 경우 예를 들어 특정 앱이 쿼리와 무관하게 많은 다운로드를 기록했더라도 무조건 항상 표시할 수는 없음 문서에만 국한된 특성을 배제하면 문제가 단순해짐 특정한 인기 앱을 무조건 표시하지 않으려는 이유는 모든 추천 앱을 골고루 제공하는 것이 중요하기 때문 예를 들어 ‘조류 관찰 앱’을 검색한 사용자가 ‘앵그리 버드’를 다운로드할 수는 있지만 기존 의도에 분명히 어긋난 결과 이러한 앱을 표시하면 다운로드율은 올라가지만 사용자의 궁극적인 요구사항이 해결되지는 않음 Rule #36: positional feature를 사용해 피드백 루프를 방지하자 콘텐츠의 위치는 사용자와 상호작용에 막대한 영향을 줌 앱을 1번 위치에 표시하면 실제로 클릭수가 올라가며, 앞으로도 그러할 것으로 확신할 수 있음 이 문제를 다루는 방법 중 하나는 positional feature, 즉 페이지에서 콘텐츠가 차지하는 위치에 관한 특성을 추가하는 것 모델 학습에 위치 특성을 사용하면 ‘1st­position’과 같은 특성에 높은 가중치를 부여하도록 모델이 학습됨 따라서 ‘1st­position=true’를 갖는 예시에서 다른 요소에 적은 가중치가 부여됨 Serving 시에는 후보의 점수를 매긴 후에 표시 순서를 결정하게 되므로 모든 인스턴스에 위치 특성을 지정하지 않거나 동일한 기본 특성을 지정함 위치 특성은 이와 같이 Training과 Testing 간에 비대칭성을 가지므로 모델의 나머지 부분과 별도로 유지하는 것이 중요함 모델을 positional feature의 함수와 나머지 특성의 함수를 더한 합으로 만드는 것이 가장 좋음(앙상블) 예를 들어 위치 특성과 문서 특성을 교차해선 안됨 Rule #37: Training/Serinvg Skew를 측정하자 격차가 발생할 수 있는 원인은 보통 몇 가지로 정리되며, 다음과 같이 나눌 수 있음 학습 데이터와 홀드아웃 데이터의 성능 차이 일반적으로 이 차이는 불가피하며 반드시 나쁜 것은 아님 홀드아웃 데이터와 ‘다음날’ 데이터 간의 성능 차이 이 차이도 불가피함 다음날 성능을 극대화하는 방향으로 정규화를 조정해야 함 홀드아웃 데이터와 다음날 데이터 간에 상당한 격차가 있다면 일부 feature에 시간 민감성이 있어 모델의 성능을 저하한다는 증거일 수 있음 ‘다음날’ 데이터와 실시간 데이터 간의 성능 차이 학습 데이터의 example에 모델을 적용할 때와 serving시 동일한 example에 모델을 적용할 때 완전히 같은 결과가 나와야 함(Rule #5 참조) 따라서 이 차이는 엔지니어링 오류를 시사할 가능성이 높음 4부. ML 3단계: Slowed Growth, Optimization Refinement, and Complex Models 2단계가 마무리되고 있음을 나타내는 구체적인 징후 가장 먼저, 월별 개선 폭이 둔화하기 시작 측정항목 간에 절충 관계가 나타나기 시작 즉, 몇몇 실험에서 상승하는 측정항목과 하락하는 측정항목이 동시에 나타남 여기서부터 문제가 복잡해짐 개선을 이루기가 어려워졌기 때문에 머신러닝 시스템을 정교화해야 함 이 섹션에는 이전 섹션보다 다소 비현실적인 규칙이 포함될 수 있으므로 주의! 머신러닝 1단계와 2단계는 일사천리로 진행할 수 있지만 3단계부터는 스스로 길을 찾아 나가야 함 Rule #38: unaligned된 objective가 문제가 된다면 새로운 Feature에 시간 낭비하지 말자 Metric 개선이 한계에 다다르면 현재 머신러닝 시스템의 목표에서 벗어난 문제점을 찾기 시작할 때 앞에서도 설명했듯, 기존의 알고리즘 목표로는 제품의 목표를 해결할 수 없다면 알고리즘 목표와 제품 목표 중 하나를 변경해야함 예를 들어 클릭수, +1 또는 다운로드 횟수를 최적화할 수 있지만 출시 결정을 내릴 때는 인간 평가자의 의견도 참고할 수 있음 Rule #39: 출시 결정은 제품의 장기적인 목표를 반영해야함 설치 횟수 예측의 logistic loss를 줄일 수 있는 아이디어를 생각했다 해당 feature를 추가했더니 로지스틱 손실이 감소했다 실시간 실험 결과 설치율 상승이 관찰됨 그런데 출시 검토 회의에서 일일 활성 사용자 수가 5% 하락했다는 지적이 나옴 따라서 모델을 출시하지 않기로 결정 실망스러운 결과지만 출시 결정에는 여러 가지 기준이 작용하며 ML을 통해 최적화할 수 있는 것은 그중 일부에 불과하다는 점을 알게 됨 현실 세상은 게임과 달라서, 제품의 상태를 일률적으로 판단할 수 있는 ‘체력 수치’ 같은 개념이 없음 팀에서는 수집 가능한 통계를 총 동원하여 시스템의 미래 성능을 효과적으로 예측하기 위해 노력해야함 Engagement, 1일 활성 사용자(DAU), 30일 DAU, 광고주의 투자수익 등을 고려해야 함 이런 metric은 AB 테스트로 측정할 수 있지만 사용자 만족도, 사용자 수 증가, 파트너 만족도, 수익 등의 더욱 장기적인 목표를 대변하는 역할을 함 제품의 유용성과 품질 향상 및 5년 후의 회사 발전과 같은 목표에 대해서도 이를 대변하는 metric을 생각할 수 있음 출시 결정을 내리기 쉬운 유일한 경우는 모든 측정항목이 개선되거나 적어도 악화되지 않을 때 팀에서 정교한 머신러닝 알고리즘과 단순한 휴리스틱 사이에서 선택할 수 있으며 단순한 휴리스틱이 모든 측정항목에서 더 나은 결과를 보인다면 휴리스틱을 선택해야함 또한 가능한 모든 metric 값 사이에 명백하게 우열이 가려지지도 않음 구체적으로 다음과 같은 두 가지 시나리오를 살펴보자 실험 DAU Revenue/Day A 1 million $4 million B 2 million $2 million 현재 시스템이 A라면 B로 전환할 가능성은 낮음 현재 시스템이 B라면 A로 전환할 가능성은 낮음 이러한 상황은 모순적으로 보이지만, 측정항목에 관한 예측은 적중한다는 보장이 없으므로 어떠한 변화에도 상당한 위험이 뒤따름 두 metric 모두 팀에서 우려하는 위험을 수반함 뿐만 아니라 어떠한 측정항목도 팀의 궁극적인 관심사인 ‘지금부터 5년 후에 제품이 어떠한 위치에 있을까?’라는 의문을 해결해 주지 못함 사람들은 자신이 직접 최적화할 수 있는 측정항목 하나를 중시하는 경향이 있음 대부분의 머신러닝 도구는 이러한 환경에 적합함 이러한 환경에서 새로운 feature를 개발하는 엔지니어가 끊임없이 계속되는 출시에 대응해야함 머신러닝 유형 중 이 문제를 다루기 시작하는 유형이 multi-objective learning임 예를 들어 각 측정항목에 관한 하한선을 갖는 제약조건 충족 문제를 작성하고 측정항목의 특정한 선형 조합을 최적화 그러나 이렇게 하더라도 모든 측정항목을 머신러닝 목표로 손쉽게 규격화할 수 있는 것은 아님 문서가 클릭되거나 앱이 설치되는 이유는 콘텐츠가 표시되었기 때문 그러나 사용자가 사이트를 방문한 계기를 알아내기는 훨씬 어려움 사이트의 미래 실적을 전반적으로 예측하는 문제는 AI-Complete 문제로 컴퓨터 시각인식 또는 자연어 처리만큼이나 어려움 Rule #40: 앙상블을 단순하게 유지하기 Raw feature를 사용해 컨텐츠의 순위를 바로 결정하는 통합 모델은 디버깅 및 파악이 가장 쉬운 모델임 그러나 모델의 앙상블(다른 모델의 점수를 종합하여 만든 단일 ‘모델’)은 더 우수한 성능을 발휘할 수 있음 단순성을 유지하려면 각 모델은 다른 모델의 입력만을 취하는 앙상블이거나 여러 특성을 취하는 기본 모델이어야 하며, 두 가지 입력을 모두 취해서는 안 됨 별도로 학습되는 다른 모델을 기반으로 하는 여러 모델이 있는 경우 이러한 모델을 결합하면 부적합한 동작이 나타날 수 있음 앙상블에는 ‘기본’ 모델의 출력만을 입력으로 취하는 단순 모델을 사용하자 그리고 앙상블 모델의 속성을 직접 규정할 필요가 있음 예를 들어 기본 모델이 산출하는 점수가 상승하는 경우 앙상블의 점수가 하락해서는 안됨 또한 가급적이면 입력 모델이 semantically적으로으로 해석 가능하도록 보정 등의 작업을 거쳐야함 그래야 underlying(기반) 모델의 변화가 앙상블 모델에 혼선을 주지 않음 또한 underlying(기반) 분류자가 예측한 확률이 상승할 때 앙상블이 예측한 확률이 하락하지 않도록 강제해야함 Rule #41: 성능 개선이 한계에 다다르면 기존 신호를 다듬기보다는 본질적으로 새로운 정보를 추가하자 사용자의 인구통계 정보를 추가함 문서에 포함된 단어에 관한 정보도 추가함 템플릿 탐색을 수행하여 정규화를 조정함 그런데 핵심 측정항목이 1% 이상 개선된 출시가 몇 분기 동안 단 한 번도 없었습니다. 이제 어떻게 해야 할까요? 이제 완전히 다른 feature를 위한 인프라 구축을 시작할 때 예를 들면 사용자가 어제, 지난주, 작년에 액세스한 문서 내역, 다른 출처에서 가져온 데이터 등 위키데이터 항목 또는 사내 보유 데이터(예: Google의 지식 정보)를 사용하자 딥러닝을 활용하자 ROI를 조정하고 새로운 feature를 위한 업무량을 늘려야함 다른 엔지니어링 프로젝트와 마찬가지로, 새 feature를 추가하는 데 따르는 편익과 복잡성이 올라가는 데 따르는 비용을 저울질해야함 Rule #42: Diversity, Personalization, Relevance는 polularity와 상관관계가 의외로 낮을 수 있음 컨텐츠 집합의 Diversity은 여러 가지 의미를 가질 수 있는데, 가장 흔한 것은 컨텐츠 출처의 다양성을 의미함 Personalization란 각 사용자에게 자신만의 결과를 제공하는 것 Relevance이란 특정 쿼리의 결과가 다른 어떠한 결과보다도 해당 쿼리에 적합하다는 의미 따라서 이러한 세 가지 속성은 특별한 속성으로 규정됨 그러나 평범함이 최선인 경우도 많다는 것이 문제 시스템에서 클릭수, 사용 시간, 시청 횟수, +1, 재공유 등을 측정한다면 결과적으로 컨텐츠의 인기도를 측정하는 것 어떤 팀에선 다양성을 갖춘 개인별 모델을 학습시키려고 함 이를 위해 시스템의 personalize(사용자의 관심사를 나타내는 특성) 또는 diversify(이 문서가 다른 반환 문서와 저자, 콘텐츠 등의 특성을 공통적으로 갖는지를 나타내는 특성)에 기여하는 특성을 추가하지만, 가중치가 생각보다 낮거나 부호가 반대라는 사실을 알게됨 Diversity, Personalize, Relevance가 중요하지 않다는 의미는 아님 이전 규칙에서 설명했듯이 후처리를 통해 Diversity 또는 Relevance을 강화할 수 있음 더 장기적인 목표가 개선되는 것으로 나타난다면 popularity와 별개로 Diversity/Relevance이 중요하다고 판단할 수 있음 후처리를 계속 사용할 수도 있고, Diversity 또는 Relevance 기준으로 목표를 직접 수정할 수도 있음 Rule #43: 당신의 친구들은 다른 제품에 같은 경향이 있지만 당신의 관심사는 그렇지 않은 경향이 있음(해석이 어려워서 원문을 남깁니다 : Your friends tend to be the same across different products. Your interests tend not to be) Google의 여러 팀에서는 한 제품에서 관계의 긴밀함을 예측하는 모델을 취하여 다른 제품에 성공적으로 적용함으로써 큰 성과를 거둠 반면, 여러 제품 분야를 넘나드는 맞춤화 특성으로 인해 고생하는 팀도 있음 이론적으로는 성공해야 할 것 같은데, 실제로는 잘 되지 않음 한 부문의 원시 데이터를 사용하여 다른 부문의 사용자 행동을 예측하는 방법은 성공을 거두기도 함 또한 사용자가 다른 부문에서 활동한 적이 있다는 사실만 알아도 도움이 될 수 있음 예를 들어 사용자가 두 제품을 사용했다는 사실 자체가 큰 의미를 가질 수 있음 Reference Rules of Machine Learning: Best Practices for ML Engineering",
    "tags": "ml data",
    "url": "/data/2019/12/15/rules-of-ml/"
  },{
    "title": "CS224W - Machine Learning with Graphs 1강 정리",
    "text": "Stanford CS224W : Machine Learning with Graphs 1강을 듣고 정리한 글입니다 CS224W : Machine Learning with Graphs 예전 강의에선 Analytics, Social Network, Analytics graph에 집중했으나, 최근엔 ML에 집중 Why Networks? 네트워크는 상호 작용하는 엔티티의 복잡한 시스템을 설명하기 위한 일반적인 언어임 Two Types of Networks/Graphs Network(Natural Graphs) 사회는 70억개 이상의 개인의 모음 전자 장치를 연결하는 통신 시스템 유전자, 단백질의 상호 작용으로 생명 조절 우리의 생각은 뇌의 수십억 개의 뉴런으로 이루어짐 Information Graphs 정보, 지식이 구성되고 연결됨 Scene graphs : 장면의 객체와 관계 유사성 네트워크 : 데이터 수집, 유사한 지점을 연결함 종종 크게 구별이 가지 않음 많은 데이터가 네트워크임 이 시스템은 어떻게 구성되어 있는가? 이것들의 design property는 무엇인가?- 많은 시스템 뒤엔 구성 요소간의 상호 작용을 적용하는 다이어그램, 네트워크가 있음 네트워크를 이해하지 않으면 이런 시스템을 모델링하고 예측할 수 없음 더 나은 예측을 위해 Relational Structure를 어떻게 이용할까? Graphs : Machine Learning 복잡한 도메인(텍스트, 이미지 등)은 관계형 그래프로 표현할 수 있는 구조를 가지고 있음 관계를 명시적으로 모델링해 더 나은 성능을 달성함 네트워크에 관심가져야 하는 이유 복잡한 데이터를 설명하기 위한 범용 언어 과학, 자연, 기술 네트워크는 예상보다 비슷함 분야간 단어 공유 컴퓨터 과학, 사회 과학, 물리학, 경제학, 통계학, 생물학 데이터 가용성, 계산 이슈 웹/모바일, 바이오, 건강, 의료 Impact 소셜 네트워크, 약물 설계, AI 추론 등 Networks and Applications Node classification 주어진 노드의 type, color 예측 Link prediction 두 노드가 연결되어 있는지 예측 Community detection 밀도있게 연결된 노드 클러스터 구분 Network similarity 두 노드, 네트워크의 유사도 측정 여러 예시 네트워크 구조가 시스템의 robustness에 어떤 영향을 미치는지 이해해야 함 네트워크 구조와 네트워크의 동적 프로세스 간의 상호 작용 및 장애에 대한 영향을 평가하기 위한 정량적 도구 개발 현실에서 실패가 재현할 수 있는 법칙을 따르고, 네트워크 도구를 사용해 정량화하고 예측할 수 있음을 배움 오 추천에도 이렇게 link prediction 사용할 수 있구나 노드를 d차원 임베딩에 매핑해서 유사한 네트워크 환경을 가진 노드가 서로 가까이에 되도록 설정 오 사이드이펙트 예측! 약의 pari가 주어지고 사이드 이펙트 예측하기 신기함 A와 B를 같이 사용할 때 근육을 분해할 가능성은? Network Analysis Tools snap.py snap c++ NetworkX, graph-tool (facebook의 big-graph도 있음) Structure of Graphs 네트워크의 구조 Network는 object 쌍이 링크로 연결된 object 모음 네트워크 구조는 무엇일까? Network의 구성 요소 Object : nodes, vertics, entity N Interactions : links, edges E System : network, graph G(N, E) Networks와 Graph의 차이 Network는 실제 시스템을 의미함 Web, Social network Language : Network, node, link Graph는 network의 수학적 표현 Web graph, Social graph Language : Graph, vertex, edge 필요할 땐 구별하겠지만, 대부분 두 용어를 번갈아 사용할 예정 Choosing Proper Representations 협력하는 개인을 연결하면 professional network sexual 관계를 연결하면 sexual network 인용하는 과학 논문을 연결하면 citation network Network를 정의하는 방법 Graph 작성 Node란 무엇인가? Edge란 무엇인가? 적절한 네트워크 representation 선택 주어진 도메인, 문제에서 네트워크를 성공적으로 사용하기 위해 중요한 결정 어떤 경우엔 독특하고 명확한 표현이 있음 다른 경우엔 표현이 unique하지 않음 링크를 할당하는 방법에 따라 공부할 내용이 다양함 Network 표현 방식 Undirected 방향성이 없는 그래프 Links : undirected, Symmetrical, reciprocal 예시 : Collaborations, 페이스북 친구 관계 Directed 방향성이 있는 그래프 Links : directed, arcs 예시 : 트위터의 팔로잉 노드의 차수(Node Degrees) Undirected Node degree, k_{i} : 노드 i에 인접한 edge의 수 Average degree =&gt; 어디에 쓰지? Directed in-degree : 해당 노드로 향하는, out-degree : 해당 노드가 향하는 source : k^{in}=0 sink : k^{out}=0 완전 그래프(Complete Graph) 서로 다른 두 개의 꼭짓점이 반드시 하나의 변으로 연결된 그래프 서로 다른 두 개의 vertex가 반드시 하나의 edge로 연결 E = E_{max}일 경우 complete graph average degree = N-1 이분 그래프(Bipartite Graph) 핀터레스트에서 사용 이분 그래프는 모든 링크가 U의 노드를 V의 노드에 연결하도록 노드를 U,V로 나눌 수 있는 그래프, U와 V는 독립 U끼리는 연결되지 않고, V끼리도 연결되지 않음 인접한 정점끼리 다른 색으로 칠하고 모든 정점을 두 가지 색으로만 칠할 수 있는 그래프 예시 Authors to Papers Actors to Movies Users to Movies Recipes to Ingredients “Folded” networks Author collaborations networks Movie co-rating networks BFS, DFS 탐색 이용하면 확인할 수 있음 이분 그래프(Bipartite Graph)란 참고 B와 C는 5와 공통적으로 연결됨 Representing Graphs : 인접 행렬(Adjacency matrix) node i와 j로 가는 링크가 있다면 1, 아니면 0 인접행렬 : 그래프에서 어느 꼭짓점들이 변으로 연결되었는지 나타내는 행렬 undirected graph는 대칭 행렬이지만, directed graph(오른쪽)의 경우 대칭 행렬이 아님 Representing Graphs : Edge list 엣지 리스트 : 연결된 엣지들의 집합 Representing Graphs : 인접 리스트(Adjancency list) 그래프의 한 꼭짓점에서 연결되어 있는 꼭짓점들을 하나의 연결 리스트로 표현 인접 행렬에 비해 edge가 희소한 그래프에서 효과적임 Networks are Sparse Graphs 현실 네트워크는 Sparse 따라서 인접 행렬은 0으로 채워짐! Edge Attribute 가능한 옵션 Weight (예 : 커뮤니케이션의 빈도) Ranking (예 : Best friend, Second best freind) Type (Frient, relactive, co-worker) Sign : Friend vs Foe, Trust vs Distrust Propeties depending on the structure of the reset of the graph : 공통 친구 수 More Types of Graphs Unweighted, Weighted(undirected) Self-edges(self-loops), Multigraph Connectivity of Undirected Graphs 두 정점은 경로로 연결될 수 있음 연결이 끊어진 그래프는 둘 이상의 연결된 구성 요소로 구성됨 여러 컴포너튼가 있는 네트워크의 인접 행렬은 block-diagonal 형태로 작성할 수 있어서 0이 아닌 요소는 사각형으로 제한되고 아니면 모두 0임 Connectivity of directed Graphs strongly connected(강하게 연결된) directed graph 각 노드에서 다른 모든 노드로 경로가 있고, 반대도 마찬가지 SCC를 식별할 수 있지만 모든 노드가 SCC는 아님 weakly connected(약하게 연결된) directed graph 가장 자리 방향을 무시하면 연결됨 Network Representations Email network =&gt; directed multigraph with self-edges Facebook friendship =&gt; undirected, unweighted Citation networks =&gt; unweighted, directed, acyclic Collaboration networks =&gt; undirected multigraph of weighted graph Mobile phone calss =&gt; directed, (weighted?) multigraph Protein Interactions =&gt; undirected, unweighted with self interactions Readings P. Erdos, A. Renyi. On Random Graphs I. Publ. Math. Debrecen,1959. P. Erdos, A. Renyi. On the evolution of random graphs. Magyar Tud. Akad. Mat. Kutato Int. Koezl., 1960. B. Bollobas. Random Graphs. Cambridge University Press. M.E.J. Newman, S. H. Strogatz and D.J. Watts. Random graphs with arbitrary degree distributions and their applications. Phys. Rev. E 64, 026118, 2001. R. Milo, N. Kashtan, S. Itzkovitz, M.E.J. Newman, U. Alon. On the uniform generation of random graphs with prescribed degree sequences. Arxiv, 2004. D. Ellis. The expansion of random regular graphs. Lecture notes from Algebraic methods in combinatorics, Cambridge University, 2011. S. Arora, S. Rao and U. Vazirani. Expander Flows, Geometric Embeddings and Graph Partitioning. In proc. STOC ‘04, 2004.",
    "tags": "ml graph data",
    "url": "/data/2019/12/03/cs224w-ml-with-graph/"
  },{
    "title": "지도 데이터 시각화 : Uber의 pydeck 사용하기",
    "text": "Uber의 대규모 WebGL 기반 데이터 시각화 도구인 Deck.gl를 파이썬에서 사용할 수 있도록 만든 pydeck 사용 방법에 대해 작성한 글입니다 제가 연습하며 사용한 코드는 Nbviewer로 보실 수 있습니다 :) Deck.gl Homepage Uber에서 만든 WebGL 기반 대용량 데이터 시각화 도구 WebGL : 웹 기반의 그래픽 라이브러리로 웹 브라우저에서 3D 그래픽을 사용할 수 있도록 해줌. 참고 : 위키백과 주요 특징 데이터 시각화에 Layer적 접근(계층적 접근) Deck.gl을 사용하면 존재하는 레이어를 활용해 복잡한 시각화를 구성할 수 있으며, 재사용 가능한 레이어로 쉽개 패키징하고 공유할 수 있음 이미 입증된 레이어 종류를 제공하고 있음 GPU의 고정밀 계산 Deck.gl은 GPU에서 64비트 부동 소수점 계산을 emulating해서 비교할 수 없는 정확성과 성능으로 데이터 세트를 렌더링함 React와 Mapbox GL 통합 Deck.gl은 React와 잘 맞고, 리액트 프로그래밍 패러다임에서 효율적인 WebGL 렌더링을 지원함 Maxbox GL과 함께 사용하면 mapbox 카메라 시스템에 매핑되서 맵박스 지도에서 2D, 3D 시각화를 할 수 있게 됨 Introduction deck.gl은 대규모 데이터 시각화를 단순히 할 수 있도록 설계됨 사용자는 기존 레이어 구성을 통해 적은 노력으로 인상적인 시각화 결과를 얻을 수 있고, 고급 WebGL 기반으로 만든 자바스크립트 레이어로 패키징할 수 있는 아키텍처를 제공함 pydeck pydeck Github pydeck을 deck.gl을 파이썬에서 사용할 수 있도록 만든 라이브러리 pydeck의 목표 : Python 사용자가 많은 Javascript를 몰라도 deck.gl 맵을 만들 수 있도록 하는 것 Jupyter Notebook에서 가장 잘 작동하고, 노트북에서 결과를 보거나 HTML로 추출할 수 있음 pydeck의 고유한 기능 Folium, Ipyleaflet 등의 지도 라이브러리와 대비한 기능 Python에서 deck.gl의 전체 레이어 사용 가능 대규모 데이터에서 색상 변경, 데이터 수정 지원 시각화에서 선택한 데이터를 Jupyter Notebook의 커널로 다시 전달할 수 있는 양방향 통신 Python API를 통해 수십만 개의 데이터를 2D / 3D로 매핑하는 기능 단, 아직 공식 Release는 아니고 Beta임 글 작성 기준 최신 버전은 pydeck-0.1.dev5 Install 추후에 바뀔 수 있으니 pydeck Github 참고 설치 pip3 install pydeck Extension 설정 jupyter nbextension install --sys-prefix --symlink --overwrite --py pydeck jupyter nbextension enable --sys-prefix --py pydeck Mapbox API 등록 deck.gl처럼 pydeck 라이브러리도 Mapbox의 베이스맵 타일이 필요함 Mapbox API는 특정 사용량까진 무료이지만, 그 이상을 사용하고 싶을 경우 비용을 내야함. API pricing 참고 Mapbox API access token으로 이동해 회원 가입 Access tokens의 Token을 복사 노트북에서 매번 정의하지 않고 환경 변수로 추가. 터미널에서 vi ~/.zshrc(bash를 사용하면 vi ~/.bashrc)을 한 후 아래 내용 추가 export MAPBOX_API_KEY=\"pk로 시작하는 여러분들의 Token 값\" # ESC :wq 로 저장하고 빠져나오기 터미널에서 수정 사항 적용 source ~/.bashrc # 만약 zsh을 사용하면 source ~/.zshrc 터미널에서 MAPBOX_API_KEY가 제대로 나오는지 확인 echo $MAPBOX_API_KEY 만약 노트북에서 Mapbox API key is not set 에러가 발생하면 터미널을 아예 끄고, Jupyter notebook 재실행 Example Code 공식 홈페이지에서 제공하는 예시 단 r.to_html()을 r.show()로 바꿈(노트북에서 바로 보이도록 하기 위해) import pydeck # 2014 locations of car accidents in the UK UK_ACCIDENTS_DATA = ('https://raw.githubusercontent.com/uber-common/' 'deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv') # Define a layer to display on a map layer = pydeck.Layer( 'HexagonLayer', UK_ACCIDENTS_DATA, get_position='[lng, lat]', auto_highlight=True, elevation_scale=50, pickable=True, elevation_range=[0, 3000], extruded=True, coverage=1) # Set the viewport location view_state = pydeck.ViewState( longitude=-1.415, latitude=52.2323, zoom=6, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36) # Render r = pydeck.Deck(layers=[layer], initial_view_state=view_state) # r.to_html('demo.html') r.show() # html 저장하지 않고 바로 보고싶은 경우 사용 Layer 종류 deck.gl에서 지원하는 Layer를 잘 파악해둬야 좋음 기본적으로 Layer나 CompositeLayer 클래스를 상속함 Core Layers : 일반적인 용도의 레이어 ArcLayer 위도 / 경도 좌표로 지정된 소스와 대상을 연결 BitmapLayer 지정된 경계에 비트맵 이미지를 렌더링 ColumnLayer HexagonLayer에 의해 렌더링되는 기본 레이어 GeoJsonLayer GeoJson 형식의 데이터를 가져와 다각형, 선, 점으로 렌더링 GridCellLayer 집계 후 CPUGridLayer에 렌더링됨 ColumnLayer의 변형 IconLayer 지정된 좌표에서 래스터 아이콘을 렌더링 LineLayer 위도 / 경도 좌표로 지정된 소스와 타겟점을 flat line으로 렌더링 PathLayer 좌표 점 리스트를 가져와 돌출 선으로 렌더링 PointCloudLayer 3D 위치, 색 등을 가져와 특정 반지름을 가진 구를 렌더링 PolygonLayer 채워졌거나 스트로크된 다각형을 렌더링 PolygnLayer는 CompsiteLayer ScatterplotLayer 위도 경도 쌍으로 이루어진 점을 가져와 특정 반지름의 원으로 렌더링 SolidPolygonLayer 채워진 다각형을 렌더링 TextLayer 텍스처 레이블을 맵에 렌더링함 IconLayer의 확장판 Aggregation Layers : 입력 데이터를 집계 및 육각형, 컨투어 히트맵 등으로 시각화하는 레이어 ContourLayer 주어진 임계값과 셀 크기에 대해 Isoline, Isband를 시각화 Isoband : 주어진 임계값 범위 값을 포함하는 다각형 Isoline : 등치선 GridLayer point의 배열에 기반해 렌더링 일정한 셀의 크기를 취하고 입력 포인트를 셀로 집계함 GPUGridLayer GridLayer가 GPU에서 렌더링(WebGL2가 지원되는 브라우저 한정) CPUGridLayer GridLayer가 CPU에서 렌더링 HexagonLayer pint의 배열에 기반으로 육각형 히트맵 렌더링 육각형의 반지름을 사용해 그림 ScreenGridLayer 위도 및 경도 좌표 포인트의 배열을 가져와 히스토그램 빈으로 집계해 그리드로 렌더링 셀 사이즈를 조정하면 다시 집계해 렌더링함 HeatmapLayer 데이터의 Spatial 분포를 시각화할 때 사용 내부적으로 Gaussian Kernel Density Estimation을 구현해 히트맵 렌더링 Geo Layers : 지도 타일, 지리 공간 색인 시스템, GIS 형식을 지원하는 지리 공간 레이어 GreatCircleLayer ArcLayer의 변형 Great Circle(대원) H3ClusterLayer H3으로 나타난 Cluster 렌더링 H3HexagonLayer H3으로 렌더링 S2Layer S2 토큰(지리 공간 인덱스)을 기반으로 지오메트리를 계산해 채워진 다각형 렌더링 TileLayer getTileData로 타입을 가져와 GeJsonLayer에서 렌더링 TripsLayer 차량 Trip을 나타내는 path를 렌더링 currentTime이 바뀌며 이동하는 모습을 시각화할 수 있음 Mesh Layers : glTF 형식의 그래프에 대한 실험 지원. 3D 모델 지원 SimpleMeshLayer ScenegraphLayer pydeck 사용 방법 pydeck은 geojson, Pandas Dataframe을 Input으로 사용 가능(URL도 사용 가능) 큰 흐름 1) 데이터 선택 2) Layer 선택 3) ViewState 정의 4) 렌더링 1) 데이터 선택 import pandas as pd UK_ACCIDENTS_DATA = 'https://raw.githubusercontent.com/uber-common/deck.gl-data/master/examples/3d-heatmap/heatmap-data.csv' pd.read_csv(UK_ACCIDENTS_DATA).head() 2) Layer 선택 pdk.Layer()에서 첫 인자에 레이어 이름을 String으로 작성 참고 : pdk.Layer(type, data, id=None, get_position=’[lng, lat]’, **kwargs) 그 후 데이터와 각종 인자를 넣어줌 세부 인자는 Layer Document에서 찾아서 넣으면 됨 import pydeck as pdk layer = pdk.Layer( 'HexagonLayer', UK_ACCIDENTS_DATA, get_position='[lng,lat]', elevation_scale=50, pickable=True, auto_highlight=True, elevation_range=[0, 3000], extruded=True, coverage=1) 3) ViewState 정의 ViewState는 지도 데이터를 기준으로 카메라 각도를 지정함 기본적으로 지도를 잡고 드래그, 회전할 수 있음 view_state = pdk.ViewState( longitude=-1.415, latitude=52.2323, zoom=6, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36) 4) 렌더링 pdk.Deck으로 레이어와 view state를 통합 layers 인자에 여러 레이어를 추가할 수 있음 r.show()로 시각화하고, 만약 저장하고 싶다면 r.to_html()사용 r = pdk.Deck(layers=[layer], initial_view_state=view_state) r.show() pdk.Deck Class 참고 pdk.Deck( layers=[], views=[{\"controller\": true, \"type\": \"MapView\"}], map_style='mapbox://styles/mapbox/dark-v9', mapbox_key=None, initial_view_state={\"bearing\": 0, \"latitude\": 0.0, \"longitude\": 0.0, \"maxZoom\": 20, \"minZoom\": 0, \"pitch\": 0, \"zoom\": 1}, width='100%', height=500, tooltip=True, ) 렌더링 후 업데이트 layer의 속성을 수정한 후, r.update()를 사용해 업데이트 가능 layer.elevation_range = [0, 10000] r.update() 시간이 지나며 업데이트하기 import time r.show() for i in range(0, 10000, 1000): layer.elevation_range = [0, i] r.update() time.sleep(0.1) Tooltip 추가하기 pdk.Deck()으로 객체를 생성할 때 tooltip을 설정하면 툴팁을 넣을 수 있음 1) tooltip=True을 주면 가진 모든 내용을 툴팁으로 보여줌 2) 특정 값만 HTML으로 스타일을 입힐 수도 있음 import pydeck as pdk layer = pdk.Layer( 'HexagonLayer', UK_ACCIDENTS_DATA, get_position='[lng, lat]', auto_highlight=True, elevation_scale=50, pickable=True, elevation_range=[0, 3000], extruded=True, coverage=1) # Set the viewport location view_state = pdk.ViewState( longitude=-1.415, latitude=52.2323, zoom=6, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36) # Combined all of it and render a viewport r = pdk.Deck( layers=[layer], initial_view_state=view_state, tooltip={ 'html': '&lt;b&gt;Elevation Value:&lt;/b&gt; {elevationValue}', 'style': { 'color': 'white' } } ) r.show() 3) 그냥 Text로 표현할 수도 있음 import pydeck as pdk layer = pdk.Layer( 'HexagonLayer', UK_ACCIDENTS_DATA, get_position='[lng, lat]', auto_highlight=True, elevation_scale=50, pickable=True, elevation_range=[0, 3000], extruded=True, coverage=1) # Set the viewport location view_state = pdk.ViewState( longitude=-1.415, latitude=52.2323, zoom=6, min_zoom=5, max_zoom=15, pitch=40.5, bearing=-27.36) # Combined all of it and render a viewport r = pdk.Deck( layers=[layer], initial_view_state=view_state, tooltip={ \"text\": \"Elevation: {elevationValue}\" } ) r.show() ipywidgets을 사용해 Interactive 시각화 사용 방식 1) 우선 베이스가 되는 Deck을 만들어서 r.show()로 보여줌 2) ipywidget 슬라이더 생성 3) ipywidget에서 사용할 함수 정의 =&gt; 마지막에 r.update() 사용 4) slider.observe로 Deck과 슬라이더를 연결 예제 코드 import pandas as pd import pydeck as pdk LIGHTS_URL = 'https://raw.githubusercontent.com/ajduberstein/lights_at_night/master/chengdu_lights_at_night.csv' df = pd.read_csv(LIGHTS_URL) df['color'] = df['brightness'].apply(lambda val: [255, val * 4, 255, 255]) plottable = df[df['year'] == 1993].to_dict(orient='records') view_state = pdk.ViewState( latitude=31.0, longitude=104.5, zoom=8, max_zoom=8, min_zoom=8) scatterplot = pdk.Layer( 'HeatmapLayer', data=plottable, get_position='[lng, lat]', get_weight='brightness', opacity=0.5, pickable=False, get_radius=800) r = pdk.Deck( layers=[scatterplot], initial_view_state=view_state, views=[pdk.View(type='MapView', controller=None)]) r.show() # Widget 슬라이더 생성 import ipywidgets as widgets from IPython.display import display slider = widgets.IntSlider(1992, min=1993, max=2013, step=2) # Widget에서 사용할 함수 정의 def on_change(v): results = df[df['year'] == slider.value].to_dict(orient='records') scatterplot.data = results r.update() # Deck과 슬라이더 연결 slider.observe(on_change, names='value') display(slider) 뉴욕 택시 데이터 시각화 pickup ~ dropoff arc layer agg_query = \"\"\" WITH base_data AS ( SELECT nyc_taxi.*, pickup.zip_code as pickup_zip_code, pickup.internal_point_lat as pickup_zip_code_lat, pickup.internal_point_lon as pickup_zip_code_lon, dropoff.zip_code as dropoff_zip_code, dropoff.internal_point_lat as dropoff_zip_code_lat, dropoff.internal_point_lon as dropoff_zip_code_lon FROM ( SELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` WHERE EXTRACT(MONTH from pickup_datetime) = 1 and pickup_latitude &lt;= 90 and pickup_latitude &gt;= -90 and dropoff_latitude &lt;= 90 and dropoff_latitude &gt;= -90 ) AS nyc_taxi JOIN ( SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon FROM `bigquery-public-data.geo_us_boundaries.zip_codes` WHERE state_code='NY' ) AS pickup ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude)) JOIN ( SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon FROM `bigquery-public-data.geo_us_boundaries.zip_codes` WHERE state_code='NY' ) AS dropoff ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude)) ) SELECT pickup_zip_code, pickup_zip_code_lat, pickup_zip_code_lon, dropoff_zip_code, dropoff_zip_code_lat, dropoff_zip_code_lon, COUNT(*) AS cnt FROM base_data GROUP BY 1,2,3,4,5,6 limit 10000 \"\"\" agg_df = pd.read_gbq(query=agg_query, dialect='standard', project_id='{여러분들의 프로젝트 id}') # 100개만 agg_df = agg_df.sort_values('cnt', ascending=False) agg_df = agg_df[:100] arc_layer = pdk.Layer( 'ArcLayer', agg_df, get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]', get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]', get_source_color='[255, 255, 120]', get_target_color='[255, 0, 0]', width_units='meters', get_width=\"cnt/50\", pickable=True, auto_highlight=True, ) nyc_center = [-73.9808, 40.7648] view_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9) r = pdk.Deck(layers=[arc_layer], initial_view_state=view_state, tooltip={ 'html': '&lt;b&gt;count:&lt;/b&gt; {cnt}', 'style': { 'color': 'white' } } ) r.show() 여기서 width_units와 get_width 쪽이 문서나 참고 자료가 거의 없어서 이것 저것 시도함 ipywidgets을 사용한 요일별 Arc Layer agg_query2 = \"\"\" WITH base_data AS ( SELECT nyc_taxi.*, pickup.zip_code as pickup_zip_code, pickup.internal_point_lat as pickup_zip_code_lat, pickup.internal_point_lon as pickup_zip_code_lon, dropoff.zip_code as dropoff_zip_code, dropoff.internal_point_lat as dropoff_zip_code_lat, dropoff.internal_point_lon as dropoff_zip_code_lon FROM ( SELECT * FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` WHERE EXTRACT(MONTH from pickup_datetime) = 1 and pickup_latitude &lt;= 90 and pickup_latitude &gt;= -90 and dropoff_latitude &lt;= 90 and dropoff_latitude &gt;= -90 LIMIT 100000 ) AS nyc_taxi JOIN ( SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon FROM `bigquery-public-data.geo_us_boundaries.zip_codes` WHERE state_code='NY' ) AS pickup ON ST_CONTAINS(pickup.zip_code_geom, st_geogpoint(pickup_longitude, pickup_latitude)) JOIN ( SELECT zip_code, state_code, state_name, city, county, zip_code_geom, internal_point_lat, internal_point_lon FROM `bigquery-public-data.geo_us_boundaries.zip_codes` WHERE state_code='NY' ) AS dropoff ON ST_CONTAINS(dropoff.zip_code_geom, st_geogpoint(dropoff_longitude, dropoff_latitude)) ) SELECT CAST(format_datetime('%u', pickup_datetime) AS INT64) -1 AS weekday, pickup_zip_code, pickup_zip_code_lat, pickup_zip_code_lon, dropoff_zip_code, dropoff_zip_code_lat, dropoff_zip_code_lon, COUNT(*) AS cnt FROM base_data GROUP BY 1,2,3,4,5,6,7 \"\"\" agg_df2 = pd.read_gbq(query=agg_query2, dialect='standard', project_id='geultto') default_data = agg_df2[agg_df2['weekday'] == 0].to_dict(orient='records') arc_layer = pdk.Layer( 'ArcLayer', default_data, get_source_position='[pickup_zip_code_lon, pickup_zip_code_lat]', get_target_position='[dropoff_zip_code_lon, dropoff_zip_code_lat]', get_source_color='[255, 255, 120]', get_target_color='[255, 0, 0]', width_units='meters', get_width=\"cnt/50\", pickable=True, auto_highlight=True, ) nyc_center = [-73.9808, 40.7648] view_state = pdk.ViewState(longitude=nyc_center[0], latitude=nyc_center[1], zoom=9) r = pdk.Deck(layers=[arc_layer], initial_view_state=view_state, tooltip={ 'html': '&lt;b&gt;count:&lt;/b&gt; {cnt}', 'style': { 'color': 'white' } } ) r.show() # Widget 슬라이더 생성 import ipywidgets as widgets from IPython.display import display slider = widgets.IntSlider(0, min=0, max=6, step=1) # Widget에서 사용할 함수 정의 def on_change(v): results = agg_df2[agg_df2['weekday'] == slider.value].to_dict(orient='records') arc_layer.data = results r.update() # Deck과 슬라이더 연결 slider.observe(on_change, names='value') display(slider) Kepler.gl vs Deck.gl Kepler.gl은 블로그에 써둔 것처럼 매우 사용하기 쉬움 단, 대용량 데이터 시각화 하면 크롬이 refresh 결국 deck.gl 기반으로 만들어진 도구 사용은 쉽지만 쿼리를 날려서 csv 저장하고 웹에 올려야되는 불편함 Deck.gl은 약간의 코딩이 필요(그래도 Python으로 가능하면 양호한 편이라 생각) 대용량 데이터도 나름 잘 되는 편 노트북에서 한번에 다 뽑아낼 수 있는 장점 하지만 적응하기 까지 약간의 시간이 필요하고, 개발 진행중이라 계속 바뀔 가능성이 존재 결국 목적에 맞도록 적절하게 사용하면 좋을 것 같아요 :) Reference Deck.gl Homepage Layer Document pydeck: Unlocking deck.gl for use in Python pydeck document pydeck example 전시흠님 블로그",
    "tags": "mobility data",
    "url": "/data/2019/11/24/pydeck/"
  },{
    "title": "Gaining Insights in a Simulated Marketplace with Machine Learning at Uber 번역",
    "text": "Uber Tech blog의 Gaining Insights in a Simulated Marketplace with Machine Learning at Uber 번역 &amp; 정리입니다 개인 학습 목적으로 번역했으며, 오역이나 의역이 있을 수 있습니다 :) 글을 보다 내용과 관련된 링크를 찾아 추가한 부분도 있습니다 Uber에선 Marketplace 알고리즘을 사용해 드라이버와 라이더를 연결함 알고리즘이 전 세계적으로 출시되기 전에 Uber는 알고리즘을 완벽하게 테스트하고 평가해 핵심 Marketplace 원칙에 맞는 최적의 사용자 경험을 만듬 Uber Marketplace Simulation팀은 제품 테스트를 더욱 안전하고 쉽게 실행할 수 있도록 현실 세계에 있을 상황을 모방한 driver와 rider을 시뮬레이션할 수 있는 플랫폼을 구축 Uber marketplace 엔지니어와 데이터 과학자는 에이전트 기반의 discrete event 시뮬레이터를 활용해 리스크가 없는 환경에서 새로운 기능과 가설을 신속하게 프로토 타이핑하고 테스트할 수 있음 시뮬레이터는 이력(historical) 데이터를 활용해 marketplace 서비스 및 사용자 행동 모델(user behavior model)을 만듬 Marketplace 서비스는 이런 인사이트를 활용해 Dispatch 결정을 내림 사용자 행동 모델은 simulation context를 사용해 trip 취소나 navigation 선택 같은 rider와 driver의 행동을 결정함 Uber Marketplace Simulation팀은 모델 배포와 학습을 더 쉽고 정확하게 할 수 있도록 시뮬레이션 플랫폼에서 모델을 완벽하게 빌드하고, 학습, serving할 수 있는 머신러닝 프레임워크를 구축함 ML은 시뮬레이션 정확도와 현실성을 향상시키기 때문에 시뮬레이션 플랫폼에서 사용자 행동을 예측할 때 중요한 역할을 함 실제로 플랫폼에서 가장 시뮬레이션된 사용자 행동은 모델 기반이며 집계된 과거의 익명 데이터로 훈련됨 시뮬레이션은 ML 모델 개발 및 개선에도 도움이 됨 새로운 모델은 시뮬레이션 플랫폼에 탑재되어 실제 서비스에 적용되기 전에 신속하게 반복할 수 있음 ML 모델 학습 프레임워크를 설계해 사용자가 시뮬레이션 플랫폼에서 모델을 신속하게 빌드, 배포하고 시뮬레이션을 통해 ML 모델을 개선할 수 있도록 함 Simulation platform machine learning framework ML은 시뮬레이션 플랫폼에서 점점 중요한 기능의 핵심에 있음 ML 프레임워크를 시뮬레이션 플랫폼에 도입하기 전에 ML 개발, 학습, serving은 주로 재사용할 수 없는 임시 솔루션(ad-hoc)으로 구성됨 예를 들어 ML 개발자는 시뮬레이터에서 ML 모델을 직접 구현하고 시뮬레이터가 실행될 때 모델을 학습함 이런 일회성 솔루션은 개발자가 여러 모델을 구현하고 모델 버전을 추가하며 시뮬레이터 내에서 복잡성을 누적시킴 시뮬레이터를 유지 관리하기 어려워졌고, 모델을 학습하기 위해 많은 RAM과 CPU를 사용해 시뮬레이터 성능이 저하됨 이런 유형의 시나리오에선 custom top-to-bottom 학습과 serving 코드를 커스텀한 top-to-bottom에 가질 수 없음 시뮬레이션 팀은 시뮬레이션 플랫폼에서 실행되는 대부분의 ML 학습, workload를 효율적으로 재사용 가능한 단일 프레임워크를 만듬. 프레임워크는 오픈소스와 우버에서 만든 컴포넌트들을 섞어 만들었음. 4개의 레이어에서 작동함 Layer 시뮬레이션 플랫폼에서 ML 모델을 구성하는 API 및 module을 구성하는 ML 라이브러리 모델을 학습하고 binary 파일로 저장하는 자동 학습 파이프라인 모든 모델의 메타데이터를 관리하는 백엔드 시스템 선택한 모델을 시뮬레이션에 load하는 시뮬레이터의 중앙 모듈 시뮬레이션 ML 프레임워크 학습 workflow에서 학습 파이프라인은 ML 라이브러리를 가져오고 Apaceh Spark를 사용해 Apache Hive의 데이터를 가져오고 처리하고 과거 시계열 데이터를 기반으로 모델을 자동으로 학습함 파이프라인이 학습이 끝나면, 시뮬레이션 데이터베이스 및 checkpoint model instance가 생성됨. 그 후, Uber의 storage service에 binary file로 저장됨 시뮬레이션 백엔드 서비스는 시뮬레이션 생성 요청을 받으면 유저의 셋팅을 기반으로 관련 ML 모델의 메타 데이터를 사용해 모델 checkpoint를 local disk로 저장함 그 후, 시뮬레이션 백엔드 서비스는 model factory가 있는 시뮬레이터를 실행함 그 후, 모델 팩토리는 Python ML 라이브러리를 가져와 로컬 디스크에서 checkpoint를 스캔해 관련 모델을 인스턴스화하고, 인스턴스를 시뮬레이션 core flow로 출력함 ML 프레임워크의 Train / Serving Workflow 그림 1. Automatic Training Pieplein(가운데)은 Spark(위)를 사용해 Hive에서 raw 데이터를 가져오고 simulation ML 모델(왼쪽)을 사용해 Storage 서비스(오른쪽) 및 SImulation Database(아래)에 데이터를 저장함 그림 2. 시뮬레이션을 생성하기 위해 먼저 시뮬레이션 request를 보냄(왼쪽 상단) 그 후 백엔드 서비스 모델 메타 데이터를 가져와 데이터베이스에 입력함. 그 후, 데이터베이스는 메타 데이터를 기반으로 checkpoint를 저장소에서 가져오고 디스크로 다운로드함. 그 후, checkpoint는 시뮬레이텨에서 Simulation ML Model(상단)과 결합해 Model Factory가 모델을 인스턴스화함 이 프레임워크은 Training과 Serving workflow를 분리함 이 변화는 개발자가 지속적으로 모델을 개선시킬 때 필요한 유연한 모델을 제공함 이 시스템에서 개발자가 사용자 행동 모델을 변경하려면 Python ML 라이브러리에서 구현물을 업데이트하면 됨 그 다음 자동으로 train 파이프라인을 트리거하고 최신 모델을 모든 시뮬레이터에 추가함 시뮬레이터에서 Training workflow를 제거해 시뮬레이터의 복잡성을 단순화해 RAM, CPU 사용량을 크게 개선함 이 변경은 또한 시뮬레이터의 처리량을 증가시킴 이런 간소화가 하드웨어 리소스를 보존하고 시스템의 효율성을 향상시킴 Marketplace Simulation 팀은 이 프레임워크를 사용해 표준화된 방식으로 여러 사용자 행동 및 기타 ML 모델을 시뮬레이션 플랫폼에 통합함 How supply movement models improve marketplace simulation 최근 연구에서 제안한 것처럼 ride-sharing 시물레이션에서 운전자의 움직임은 시뮬레이션의 정확도와 리얼리즘에 중요한 factor임 시뮬레이션 플랫폼에서 실험적인 marketplace 알고리즘은 실제 결과와 다른 결과를 가져옴 예를 들어 시뮬레이션의 드라이버는 현실과 다른 rider와 매칭될 수 있음 이 경우에 시뮬레이터는 경로, 주행 시간 및 하차 위치를 시뮬레이션하기 위해 historical driver 이동 데이터를 재사용할 수 없음 이 문제를 해결하기 위해 운전자의 움직임을 정확하게 시뮬레이션하는 모델을 구축해야 했음 적절한 movement 모델을 사용하면 라이더 행동 모델(예 : 라이더 취소 모델)을 향상시키고 매칭 및 가격 알고리즘에서 노이즈를 줄일 수 있음 시뮬레이션 된 세계는 제한된 과거 데이터에서 실행되고 교통 정보, 날씨, 지리적 데이터와 같이 사용자 행동에 영향을 줄 수 있는 많은 요소가 없기 때문에 운전자의 움직임을 정확하게 시뮬레이션하는 것은 매우 어려운 작업임 다행히 대부분의 marketplace 알고리즘에서 단일 운전자의 움직임은 알고리즘 결과에 큰 영향을 미치지 않음 이런 알고리즘들은 특히 rider-driver 매칭이나 가격 알고리즘들은 개별 운전자의 움직임보다 집계된(aggregated) 드라이버 분포를 활용함 이런 이유로 모든 운전자의 움직임을 완벽하게 시뮬레이션하지 않고 운전자 분포를 정확하게 시뮬레이션하는 모델을 구축함 시뮬레이션된 세계에서 목표를 달성하기 위해 online driver movement 동작을 on-trip과 off-trip으로 나눔 2가지 state에서 다른 드라이버 행동을 예측하는 하이브리드 모델을 만들어 uber의 marketplace 수요를 더 잘 예측함 그림 3. Driver movement 하이브리드 모델은 시뮬레이션하는 운전자 요청으로 시작해 예/아니오 분기를 통해 결론에 도달하고, 운전자 정보를 라우팅 엔진에 보내고, route를 통해 speed와 이동을 측정함 On-trip 드라이버를 위한 룰 베이스 모델을 적용함 시뮬레이션된 driver과 trip을 수락하면 모델은 라우팅 엔진을 사용해 경로를 따라 운전자를 미리 정해진 목적지로 안내함 Off-trip 드라이버 분포를 시뮬레이션하는 것은 매우 어려움 전에 언급한 것처럼, 시뮬레이션된 세상이 historical driver movement를 재현할 수 없음 이런 이유로 ML 모델을 사용해 집계된 off-trip 드라이버의 이동을 예측함 생산적인 알고리즘을 사용해 이 모델은 드라이버 분포를 과거 분포에 가깝게 시뮬레이션함 Off-trip 드라이버 분포를 정확하게 시뮬레이션하는 목표를 달성하기 위해 트리 기반 확률 모델을 훈련함 그림 4. off-trip 드라이버 분포 시뮬레이션을 위한 stochastic 모델의 decision tree 중 하나 시뮬레이션은 off-trip 드라이버의 목적지를 예측하려고 할 때, 그림 4와 같이 드라이버 정보(위치, 타임스탬프 등)를 사용해 Tree 모델에서 관련된 리프 노드를 가져옴 리프 노드에는 전이 행렬(transition)이 포함됨 시뮬레이션된 운전자의 미래 움직임을 예측하는 매트릭스 시뮬레이션 플랫폼에서 육각형 계층적 공간 인덱스인 H3을 사용해 지구의 영역을 식별 가능한 그리드 셀로 분할함 이런 식별 가능한 그리드 셀을 사용해 아래 그림 5와 같이 전이 매트릭스를 정의함 그림 5. 이 표는 지구상의 위치를 나타내는 다양한 그리드 셀의 확률값을 보여줌 Transition 행렬의 값은 드라이버가 현재 그리드 셀 X에 있는 경우 드라이버가 그리드 셀 Y로 이동할 확률을 나타냄 이런 transition matrix를 사용해 시뮬레이션은 open 드라이버가 다음 셀로 이동할 가능성이 있는 셀을 예측할 수 있음 그런 다음 시뮬레이션은 해당 셀 내부의 위치를 임의로 선택해 open 드라이버의 목적지로 할당함. 아래 그림 6은 이 프로세스를 보여줌 그림 6. 이 Flow 차트는 리프 노드에서 시작하고 맵을 사용해 식별 가능한 그리드 셀을 가져와 드라이버를 찾음. 시뮬레이션이 전이 행렬로 드라이버 이동 확률을 계산하는 방법을 보여줌(좌측 밑) 시뮬레이션에서 운전자의 목적지 위치를 선택하는 방법을 우측 지도에서 보여줌. 운전자 대상을 예측하기 위한 트리 기반 확률 모델임 시뮬레이터가 open driver의 목적지를 예측하면 운전자의 경로와 속도를 추정한 다음 운전자를 목적지로 옮김 on-trip 및 off-trip 드라이버의 주요 차이점은 open 드라이버가 목적지로 운전하는 동안 상태를 변경할 수 있음(offline/dispatchied) Marketplace Simualtion Platform은 하이브리드 driver movement 모델을 사용해 실제 운전자 분포를 정확하게 근사함 동일한 알고리즘을 사용해 분포를 실제 분포와 비교했음. 아래 그림 7은 피크 시간 드라이버의 분포를 보여줌 그림 7. 왼쪽 지도는 실제 세계에서 운전자가 분포하는 영역이고, 오른쪽 지도는 시뮬레이션 결과. 근접함 How simulation helps us experiment with matching algorithms Uber 플랫폼은 ETA, Routing, 사용자 위치, trip 선호도 및 가격을 포함한 다양한 요소에 최적화된 매칭 알고리즘을 사용해 rider와 driver를 실시간으로 연결함 최근 연구와 강의 자료에 따르면 ride-sharing 프로그램은 종종 large-scale 조합 알고리즘을 사용해 사용자 매칭을 최적화함 그러나 사업을 다양화하며, 이 알고리즘은 더 많은 요소를 고려해야 함 점점 더 복잡해지는 매칭을 수용하기 위해 머신러닝 모델을 최적화 알고리즘에 통합해 시뮬레이션 처리량을 높임 추천 시스템과 maximum bipartite matching을 조합함 아래 그림 8에 전체 workflow가 나옴 그림 8. 상단 절반은 추천 시스템을 나타내며 몇 단계를 거쳐 드라이버를 좁히고 순위를 매김. 추천 시스템의 순서는 각 라이더에 대해 모든 드라이버를 찾고, 드라이버 옵션을 수백으로 좁히고, 링크 생성, 드라이버 옵션을 수십으로 좁히고 10보다 적을 경우 순위를 매김. 이 프로세스가 완료되면 화살표는 권장 사항이 일치 단계로 이동함. 처음에 이 알고리즘은 여러 라이더를 단일 드라이버에 연결할 수 있지만 maximum bipartite 매칭 알고리즘을 사용해 개별 드라이버와 단일 라이더를 페어링함 매칭 솔루션에서 추천 시스템은 각 pari의 점수를 가진 라이더당 몇 명의 드라이버를 제안함 추천 결과를 기반으로 가중치가 있는 bipartite graph를 만듬 이 그래프는 한 명의 rider와 한 명의 driver를 매칭시킴 이 문제를 효율적으로 해결하기 위해 maximum bipartite matching algorithm을 사용함 이분 매칭(Bipartite Matching) rider와 driver 위치만 사용해 추천 시스템을 개발하는 것은 어려운 일임 다른 맥락이 없으면 feature의 부족으로 협업 필터링, Factorization 머신, 딥 뉴럴넷을 사용하기 어려움 이 문제를 해결하기 위해 대규모 소셜 네트워크 추천을 살펴봄 Representation Learning on Graphs: Methods and Applications Parallel Clustered Low-Rank Approximation of Graphs and Its Application to Link Prediction Recommender Systems for Large-Scale Social Networks: A review of challenges and solutions 종합적인 연구를 한 후, 소셜 네트워크에서 구현된 솔루션을 사용해 추천 시스템을 만들기로 결정함 Representation Learning on Graphs: Methods and Applications The link prediction problem for social networks 그림 8에 표시된 것처럼 rider와 driver 위치를 기준으로 꼭지점을 갖는 그래프를 구성함 운전자 ETA나 rider와 driver 사이의 거리 같은 규칙 기반 제약 조건을 사용해 그래프의 Edge를 생성함 시뮬레이터가 그래프를 구성한 후, historical dispatch 결과를 기반으로 각 edge의 점수를 예측할 수 있음 그러나 각 node와 edg에 대한 정보는 위치와 경로만 포함하기 때문에 매우 제한적인 정보를 가지고 있음 GraphceSAGE와 같은 graph representation learning 방법에 영감을 받아, 아래 그림 9와 같이 이웃에서 정보를 집계해 각 node에 충분한 feature를 추출함 그림 9. rider node의 외부 원은 이웃의 집계된 feature 정보를 사용해 더 작은 원 내의 rider node를 driver node로 이동시킴. 그런 다음 다시 집계되어 하나의 rider node로 좁아짐 이런 Feature는 각 노드에 대한 근처 네트워크 구조 정보를 반영함 또한 continuouse feature를 normalized하고 모델을 일반화하기 위해 feature vector에 임베딩함 Deep Neural Networks for YouTube Recommendations를 모방함 결국 각 쌍의 점수를 예측하기 위해 정착한 모델은 DART : Gradient Boosting Tree with dropouts 학습 데이터의 상당 부분에서 음수 쌍(driver와 rider가 일치하지 않음)이 발생하기 때문에 모델을 훈련하기 전에 음수 상을 다운 샘플링하고 양수 쌍(driver와 rider가 일치한)을 샘플링함 모델을 학습한 후, 모델을 사용해 각 쌍의 점수를 기준으로 각 rider의 top-k potential driver를 예측함 이 경우 예측 결과에 따르면, 각 rider는 여러 driver와 연결되며, driver는 여러 rider와 연결될 수 있음 그런 다음 링크를 기반으로 bipartite graph를 구성하고 maximum bipartite matching algorithm을 적용해 bipartite matching 문제를 해결하고 최소 평균 드라이버 ETA를 달성함 Moving forward ML 프레임워크를 사용해 ML 모델을 시뮬레이션 플랫폼에 점진적으로 통합해 시뮬레이션에서 더 많은 사용자 행동과 실험적인 marketplace 알고리즘, 기능을 테스트할 수 있음 이 시스템은 Uber가 개발 프로세스를 가속화하고 안전하고 안정적인 transportation product를 제공할 수 있도록 도와줌 앞으로는 다음 기능을 통해 시뮬레이션 ML 프레임워크를 향상시킬 계획 Hyperparameter automatic tuning ML 모델에는 learning rate, tree depth 같은 많은 하이퍼 파라미터가 포함됨 이런 하이퍼 파라미터를 조정하면 시간이 많이 걸리고 오류가 발생하기 쉬움 하이퍼 파라미터 튜닝 툴을 모델 학습 파이프라인에 통합해 모델 개발자의 생산성을 높이고 엔지니어의 작업을 더 쉽게 만들 계획 Online model performance monitoring 시뮬레이션 플랫폼의 대부분 ML 모델은 시간 및 위치와 같은 환경적 요인에 직접 영향을 받음 시간이 지나며 시뮬레이션의 환경(예 : 교통 패턴)이 변경되므로 모델 성능이 저하됨 이런 변화에도 모델을 정확하게 유지하려면 새로운 데이터로 모델을 재학습해야 함 온라인 모델 성능 모니터링 시스템은 model serving 퍼포먼스를 모니터링하고 성능이 특정 임계값보다 낮아지면 학습 파이프라인을 트리거할 수 있음 Distributed machine learning serving Uber의 엔지니어는 더 많은 ML 모델을 시뮬레이션 플랫폼에 통합하고 있음 현재 프레임워크에는 컴퓨팅 리소스 부족이 발생해 예측 대기 시간(latency for predictions)이 크게 증가함 따라서 시뮬레이션 처리량이 악화됨 고성능 분산 ML 서비스가 이 문제를 해결할 때 도움이 될 수 있음 Configurable machine learning model development 시뮬레이션 플랫폼의 대부분 ML 모델은 Data processing logic 및 기본 머신러닝 알고리즘과 같은 공통 component를 공유함 대부분의 사용자를 위해 모델 개발 프로세스를 단순화하기 위해 configurations driven model 개발 프레임워크를 만들 예정 Input config를 기반으로 모델을 구성할 수 있음",
    "tags": "mobility simulation data",
    "url": "/data/2019/11/01/simulated_marketplace_uber/"
  },{
    "title": "DEVIEW 2019 1일차 후기",
    "text": "네이버 DEVIEW 2019 1일차 후기입니다 :) 키노트 문재인 대통령님이 오셨음 외산 클라우드 없이 AI 플랫폼 제공하기: features, training, serving, and AI Suite. 어떻게 설계했는지 위주를 말씀드릴 예정 발표 자료 자체 AI 플랫폼이 필요한 이유 Security Cost 데이터가 적거나 연산량이 적게 필요하면 클라우드 사용이 유리할 수 있음 대용량 데이터 처리를 지속적으로 수행하면 비용 무시하기 어려움 Demand 머신러닝 수요가 엄청 늘어남 이미 존재하는 플랫폼 분산 저장 플랫폼, Cuve 분산 처리 플랫폼 : C3 피쳐 엔지니어링 플랫폼은 없음 모델 학습 플랫폼 : AiTraining NSML 모델 서빙 플랫폼 : Ai Serving Platform AI Suite : End-to-End Platform 구글클라우드와 비슷 머신러닝 하나 적용하기 위해 데이터 처리 모델 학습 서빙 : 성능 평가와 용량 산정 후 서비스로 제공 데이터를 가져와서 인프라 만들고, 비즈니스에 적용하는 시간이 오래 걸림 AiFeatures 웹브라우저로 카테고리처럼 어떤 데이터가 있는지 볼 수 있음 DUMP : 데이터를 어딘가로 가져와서 ANALYZE : 잘못된, biased 데이터 없나 보고, 간단한 가시화로 데이터에 대한 인사이트를 얻고 BATCH : 잘못된 데이터는 버리고, 나머지는 가공해서 피쳐벡터를 만듬 AiFeatures 아키텍쳐 Facets 씀 참고 : https://nbviewer.jupyter.org/github/zzsza/TIL/blob/master/Tensorflow-Extended/TFDV(data validation) example.ipynb Facets 사용할 때 샘플링, HDFS를 위해 일정 부분 읽어서 전체 추론 NLP 라이브러리를 API화시킴. 웹 서버가 Rest API로 요청하고, nginx로 쓰로틀링하고 pyspark는 쓰로틀링 프록시로 다 보냄 - pyspark로 딕셔너리 물고 하는 것들을 할 수 있음 - UDF 만들어서 해결 머신러닝 모델 학습 모델 연구시 빨리 학습하고 평가하는 과정을 반복 모델과 파라미터를 빨리 얻는 것이 목적 데이터는 고정되므로 캐싱하면 이득 제품화 시 학습에 걸리는 시간을 이미 알고있음. 다음 갱신 전까지만 완료되면 되기 됨. 일정 시간 내에 최소 자원을 쓰면 좋음 배포 전에 이전 버전의 모델과 품질을 검증할 필요가 있음 켄진타우? 헤밍 디스턴스 알고리즘처럼 얼마나 서로 다른가 매번 새로운 떼이터로 학습하는 경우가 많아 데이터 캐싱 이득이 없음 학습 자동화 서빙 아키텍쳐 ONNX 형태로 받음 =&gt; Pytorch/Caffe 모두 가능 비즈니스 코드 =&gt; 사용자의 코드 =&gt; 뒷단 서버에 요청 Yarn 위에서 돌리고 있음 For ML Engine : 내 모델은 여기있고, 이 설정으로 띄워주세요 모든 단계 자동화하기 AI-suite Google Dataflow랑 비슷한 느낌? 드래그도 되는 편의성 Batch라는 주석을 하면 프러덕션 레벨의 셀 =&gt; 확장 프로그램을 Run하면 Batch만 파이썬 스크립트를 뽑아서 할 수 있음 Problem 3. 사용자 작업이 돌고 있는데 AI Suite는 어떻게 업데이트하나? 전체 컴포넌트는 컨테이너라이징 분산 작업 관리는 Celery(Airflow 왜 안썼는지는 부스로..!)",
    "tags": "lecture etc",
    "url": "/etc/2019/10/28/deview2019-review/"
  },{
    "title": "MLOps NYC19 Conference 정리",
    "text": "MLOps NYC19 영상을 정리한 글입니다 시청한 영상 MLOps in the Newsroom Netflix Presents: A Human-Friendly Approach to MLOps The Architecture That Powers Twitter’s Feature Store Serverless for ML Pipelines from A to Z Deep Learning on Business Data at Uber The Growth and Future of Kubeflow for ML Stateless ML Pipelines: Achieve reproducibility and automation while simplifying the pipeline Training session은 영상이 없지만, Review를 통해 간접적으로 내용을 볼 수 있습니다 MLOps in the Newsroom Information Platforms and the Rise of the Data Scientist(2009)라는 책에선 R / Hadoop을 사용했다고 하는데 NYTimes에선 TF / GCP를 사용함 NYTimes의 DS Software Stack Deployment at The New York Times 뉴스 20세기엔 church + state였다면 21세기엔 church + state + data(데이터가 앞 2가지 요소에 영향을 미침) 모델링 Descriptive modeling Readerscope 데이터를 설명하는 패턴을 찾음 현재 발생하는 상황에 대해 더 정교하고 실시간 통찰력이 필요했음 누가 무엇을 읽고 있는지? 그리고 어디에 있는가? 마케터가 drill down으로 LA에서 무슨 일이 있는지 등을 알 수 있음 Contextual bandit Predictive modeling 누가 구독할 것인가, 누가 떠날 것인가를 예측(퍼널 단계 단계를 예측) 리스크를 위해 성능과 해석 가능성을 중시 광고주가 광고할 때 어떤 사람에게 효과가 있는지? 관련있는 context를 제시 =&gt; 우리 광고는 Travel 부분과 digital 세대에 영향을 많이 미칠 것이다 등 텍스트 기반해서 Labeling(Inspired, Happiness, Sadness, Love 등) Prescriptive modeling 추천을 어떻게 할 것인가 톰슨 샘플링 &amp; 밴딧 사용 Modeling Social Data IDEA best AI is AI + IRL recpect for craft recpect for collaborators Netflix Presents: A Human-Friendly Approach to MLOps 넷플릭스는 출시 전에 매일 프로그램 시청자의 예상 크기(estimated size)를 알고 싶어함 The Life of a Model EDA 프로젝트를 잡고, 노트북에서 잡음 correlation를 찾고, scatter plot을 그림 2주 정도 진행 Prototyping 다양한 실험을 하고, Feature 추가, 모델링 등등을 진행 6~8주 Productionalize Ship Model to Production(v1) Scale &amp; Deploy ETL / Feature Engineering / Model Traing / Model Hoasting / Batch Scoring / Live Scoring / Audits / Scheduling etc Metaflow 빠른 프로토 타이핑을 위해 만듬 코드의 구조는 next로 다음 행동을 지정할 수 있음 프로토타입 할 때 특정 부분만 안되면 resume 명령어로 다시 실행 컴퓨팅 파워는 titus에게 말하면 됨 분산 처리 Production 배포시 Meson 사용, meson create 넷플릭스 테크 블로그 : Meson: Workflow Orchestration for Netflix Recommendations Real time Scoring 몇달이 지난 후 The Life of a Model Maintenance 모델 유지하고 version 2 빌드 v1를 안전하게 복사 새로운 feature를 추가해 진행 inspect &amp; debug Pick up &amp; iterate 다른 사람의 실험도 돌릴 수 있음 태그도 가능 Metaflow at scale 도입 후 정말 많은 프로젝트가 생기고 있음 The Architecture That Powers Twitter’s Feature Store 많은 사람들이 모델링하면 겹치는 Feature Engineering이 있음 이미 많이 진행한 팀, 이제 막 머신러닝을 도입하려는 팀 등 이미 진행한 팀꺼를 fork하거나 밑바닥부터 만들 수 있음 전사에서 사용할 라이브러리 생성 Featre Store는 library 오.. 트위터껀 아니지만 Gojek이 만든 feast가 있음 Share Feature Catalog Succinct, declarative Definitions Datasets 온라인/오프라인 접근이 가능 동영상에서 화질이 너무 낮아 알아보기 힘듬 ㅠ 추가할 Feature를 정의 Offline Integration 스칼라 사용 joinFeatures Online Integration Feature Store Client Strato FeatureStore Client에서 Strato로 데이터 보내고, 캐싱하거나 DB에 넣거나 서비스에 쓰거나 하는듯 영상은 10분만에 끝남. 딱히 인상 깊진 않음 Serverless for ML Pipelines from A to Z Code / Model Development is Just the First Step 파이프라인 예시 Weather 정보도 추가 여기서도 Feature Store란 단어가 나옴 Nuclio를 사용해 ETL과 Streaming을 가속 Nuclio Github Nuclio를 사용해 Serving Nuclio Jupyter Github Buidling ML Pipelines From (Serverless) Functions Feature Store가 있군! Demo KFServing을 쓰는듯 Deep Learning on Business Data at Uber 왜 딥러닝인가? 기존에 사용하던 알고리즘보다 딥러닝이 더 좋은 성능을 보이고 있음 특정 도메인에선 압도적인 성능(vision) 기존에 사용하던 트리 모델과 결합해 하이브리드 모델을 만듬 딥러닝 In Uber 이런 시스템을 어떻게 구현할까? Option A : TFX Option B : Apache Spark (이걸 사용) Powerful ETL Easy integration with XGBoost 이미 스파크를 사용하는 시스템이 있었음 1) Feature Store Real time과 Batch를 통합 2) Model Training Apache Spark에서 딥러닝을 어떻게 합칠까? Preprocessing SparkML Pipelines Estimator, Trnasformer, Pipeline Distributed Training Petastorm : 딥러닝 학습을 위한 데이터 접근 Parquet End-to-end Training Architecture Petastorm Blog 3) Prediction Service 자바와 딥러닝 프레임워크를 같이 실행시켜야 함 4) Authoring 데이터 사이언티스트들은 쥬피터 노트북을 좋아함 하나의 노트북에서 아이데이션, 학습, 평가, 딥러닝 모델 배포 등을 할 수 있을까? Data Access Data Preparation Model Construction Train the Model Deploy the Model 5) Don’t know Deep Learning? 딥러닝을 몰라도 Ludwig 활용 Ludwig 홈페이지, Ludwig Github Recap(요약) 거대한 데이터셋을 가진 회사에서 딥러닝을 사용하면 powerful한 모델을 만들 수 있음 Uber의 딥러닝 시스템 아키텍쳐와 E2E DL 파이프라인을 정의하기 위해 노트북 친화적으로 만든 API를 떠올리기 Apache Spark, Horovod, Petastorm을 사용함 Ludwig에서 코드 없이 딥러닝 모델을 만듬 The Growth and Future of Kubeflow for ML ML 구성은 매우 복잡함(이거 진짜 모든 MLOps 세미나에서 나오는듯…ㅋㅋㅋ) MLOps Team이 당면한 문제 10배 넘게 생산성을 가지도록 하는 방법은? Kubeflow Vibrant(활기찬) Ecosystem of Kubeflow 엄청 활발하게 발전되고 있는 에코시스템 Deploy &amp; Manage Composable, Scalable, Portable 쿠버네티스가 MLOps에 좋은 이유 Kubeflow 0.6 Metadata 인공물을 저장하고 스키마 정의 가능 Deployment Kustomize가 ksonnet을 대체함 Multi user support Pipelines API와 UI 개선 Anthos가 MLOps에 좋은 이유 Google Developer의 Anthos 소개 글 흠 Anthos는 규모에 따라 이득일지 아닐지가 나뉠듯..? Stateless ML Pipelines: Achieve reproducibility and automation while simplifying the pipeline 나이키 데이터 사이언티스트와 팀은 모델 파이프라인부터 프러덕션까지 할 수 있어야 하고, 모델의 전체 lifecycle을 알아야 함 What Stateless Pipelines Changed Airflow 사용 -&gt; 실패하면 알람 이제 모델 파이프라인을 몇분만에 만듬 Lifecycle of an ML Project CI/CD 어떻게 하는지 궁금 파이프라인 기존 에어플로 설정을 더 간소화함 아래는 뭐로 한건지 모르겠음. dsp? 모델 실행 파이프라인 예시 Providing Paths Dev / Test / Prod가 저장되는 폴더가 다름 Metrics Standard CI/CD pipeline 젠킨스파일을 가짐 Result Reference MLOps NYC 2019 Youtube",
    "tags": "basic mlops",
    "url": "/mlops/2019/10/27/mlops-nyc19-review/"
  },{
    "title": "Tensorflow KR 3차 오프라인 모임 후기",
    "text": "Tensorflow 3차 오프라인 모임 후기입니다 On-device ML with Tensorflow Lite Jason Zaman twitter.com/perfinion GDE Machine Learning On-device ML Constrained environment Small battery Limited memory Bad connectivity Why do on-device ML? 더 많은 데이터에 접근하기 위해 interaction을 더 빠르게 하기 위해 Privacy preserving(클라우드에 올라가면 이슈) Model Conversion 모델을 실행시키는 방법들 Demo App Pretrained models Retrained models Build from scratch TF Lite까지 가는 Conversion flow TensorFlow -&gt; Saved Model -&gt; TF Lite Converter -&gt; TF Lite Model Model optimization loadmap link TFLite conversion flow Sample Code Inference with TFLite Interpreter 생각보다 간단 import numpy as np import tensorflow as tf # Load TFLite model and allocate tensors. interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\") interpreter.allocate_tensors() # Get input and output tensors. input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() # Test model on random input data. input_shape = input_details[0]['shape'] input_data = np.array(np.random.ran dom_sample(input_shape), dtype=np.float32) interpreter.set_tensor(input_details[0]['index'], input_data) interpreter.invoke() # The function `get_tensor()` returns a copy of the tensor data. # Use `tensor()` in order to get a pointer to the tensor. output_data = interpreter.get_tensor(output_details[0]['index']) print(output_data) C++에서도 쉽게 가능 Incredible inference performancee를 보임 Interpreter Core는 Operation Kernels와 Accelerator Delegate, Operation Kernels로 이루어져 있음 Document Android Neural Network API delegate도 있음. 안드로이드에서도 딥러닝? Optimization Quantization Fixed point vs Floating point Post training quantization vs Quantization-aware training 진행하면 모델 사이즈가 4배 작아지고, 속도 개선됨 TensorFlow =&gt; Saved Model + Calibration Data =&gt; TF Lite Converter =&gt; TF Lite Model EdgeTPU 4 Trillion Operations per Second(TOPS) 어마어마한 친구 ㄷㄷ Fixed point only Must fully quantize Compile .tflite file for the EdgeTPU Quantize 해야하는 이유 에너지 절약 Auto Scalable한 Deep Learning Production을 위한 AI Service Infra 구성 및 AI DevOps Cycle Feat. Docker로 1만 TPS inference 구축해보기 SKT AI Center, 김훈동님 박찬엽님 Production AI Serving Infra 구성 및 방법론 Pain Point 1 Tensorflow Serving : 멀티 딥러닝 프레임워크 환경이라 커버리지가 높은 방법을 찾음 PaaS : 매우 비쌈 Flask : Python이 느리고, 험난한 Engineering Art가 필요 TensorRT Cloud serverless + NoSQL : 커버리지는 낮을 수 있음. 케바케 다양한 프레임워크 지원을 위해 다른 방법으로 접근 머신러닝 모델들 =&gt; PMML 파일로 컨버전해서 상대적으로 성능이 좋은 자바를 통해 inference 진행 진행한 방법 Flask, TensorRT 무거운 모델이면 TensorRT 3번 Flask 초 저렴 vCPU Docker 많은 인스턴스(월 4만원) vs 어벤져스급 GPU Docker(월 1000만원) 저글링 vs 시즈탱크 =&gt; 일반인과 프로게이머의 차이 DL Serving에 대한 고려 사항 배치사이즈 큰 경우, 작은 사이즈 Python Thread 처리는 최악 처리량과 정확도의 Trade Off 모델 압축 GPU의 오해 항상 왜 성능 그래프를 ResNet가지고 비교하지? Real World에서 많이 쓰는 모델들 중 MLP, LSTM은 GPU가 안좋은 경우도 있음 실제로 실험해본 결과 다 다름 CPU로 할 경우 250 TPS =&gt; 데이터 다 전역에 올리고 로그 못찍고 하면 가능했음(BiLSTM) GPU로 할 경우 16TPS TensorRT 사용하는 방법 TensorRT를 이용하는 Restful API 사용하기 Flask로 TensorRT Engine 호출하기 =&gt; 여전히 싱글 프로세스라 여전히 병목이 생길 수 있음(플라스크 앞단에서) Pain Point 2 - Poor Python Performance Python -&gt; node로 바꿔도 성능이 개선됨 자바 버텍스 쓰니 성능 80배 올랐음 Pain Point 2 - 해결 팁 Pandas를 빼고 순수 파이썬으로 쓴다 함수형 언어처럼 가상 함수를 사용(Data copy 병목 줄이기) =&gt; Python에서 Map, Lambda 사용! functionstools.partial Python Thread를 쓰지 않음 =&gt; 고루틴처럼 쓸거면 멀티프로세스가 낫고.. Go의 Goroutin 쓰듯 하려면 cotyledon MemoryView 활용 Data 핸들시 pointer 접근하듯 대용량 Data Memory 복사 방지 C언어로 데이터 핸들링할 때 병목이 생기는 부분은 메모리카피 =&gt; 줄이기 위한 포인터 Microservice로 잘게 쪼개고 모든 것은 비동기로 PMML + Java 컨버전 훈동님이 마소에서 발표한 사례를 실제 프러덕션에 적용하진 못함. gunicorn을 사용해 안정성을 잡음 WSGI -&gt; ASGI로 큰 도움이 됨 Pooling, streaming, web hook 등의 비동기 방법이 있는데 어떤 라이브러리 쓸지 정의해서 컨버전함 싱글톤을 데코레이터로 감싸기 CPU는 메모리가 더 많아서 멀티프로세스에 강함 어디서 병목이 생기는지 잘 봐야하고, Low Level Debug 필요 : CProfile, kCacheGrind 등을 사용함 Production AI Open Source Eco System Pandas UDF 스파크를 깔고 pandas로 배치에 최적화 Horovod 분산, 스파크로 메모리 처리 가능 Petastorm 엄청 큰 데이터로 할 경우 장비에 로딩이 안되면 하둡에 수백기가 접근할 경우 사용 Horizon 강화학습시 파이토치로 하고 onnx ML WorkFlow Airflow, Kubeflow TFX TensorFlow 모델을 쓰면 좋음 mlflow MLOps 솔루션으로 스파크쪽으로 만드는 것 Serving at SCale : Seldon Rapids.ai : GPU를 cuda 사용 Clipper.ai 딥러닝 쿼리 캐싱 ONNX : 압축 관련쪽에서 사용 Production AI DevOps 지속적 통합과 배포 도커 개발환경 깔끔하게 하기 / 다른 곳에서 재현 가능 마이크로서비스 Stateless하게 사용 딥러닝은 state가 없음 멀티톤 고려하면 지옥이.. 커뮤니케이션 많은 팀의 사람들이 각자 모델 만듬 코드형 인프라스트럭쳐 테라폼 =&gt; 클라우드 서비스를 코드 레벨로 관리 인프라 생성이 아닌 쿠버네티스로 한번에 다 처리함 쿠버네티스 Pod을 관리하는 상위 개념들 컨테이너일수도 있고, 여러 개를 관리하기 위한 개념 단위 Replicaset, Deployment 정도 집중 TargetCPUUtilizationPercentage를 지정하면 그 기준으로 처리함 배포는 Azure Devops의 pipelines의 기능을 활용 Audit log : 누가 뭘 했다 새로운 이미지 배포를 위해 latest 태그 만드는 것을 지양..! 환경/설정이 변경된 것이 아니라 배포가 되지 않음 Kubectl 명령을 devops 내에서만 실행 Rollback 배포 기록 관리해주고, 기록 기반 롤백이 됨 Azure에서다 해준다 코드 내에서 env 호출로 사용하지만 왠만하면 환경 변수 안쓰는게 좋음 db에 접근하는 서비스를 만들고 진행해야 함 모델 서빙을 위한 패턴 One fat image 600k면 도커 이미지 올리는게 좋을숟도 10기가면… 이미지에 먼저 올린다? 빌드하는데 30분 넘음. 불가능한 이야기~ 빌드할 떄 모델을 넣음 Model puller sidecar Pod에 sidecar 개념이 있음. 다른 컨테이너를 붙여줌 볼륨을 공유하고 있는 웹 서버 + 스토리지 체크하는 친구만들어서 나눔 자연어는 전처리 후처리 코드를 만들까 사이드카를 넣을까? 고민 중 Attached volumn 클라우드 기능 사용 자신 컴퓨터의 하드디스크처럼 mount 모델 경로 여기에 넣어주세요~ 이런 느낌 프러덕션 환경에서 연구하기 하이퍼커넥트 AI lab 하성주님 프러덕션(서비스 중인/될 제품)과 연구(불확실한 기술 개발)의 이야기 AI Lab 모바일 환경에서 실시간으로 이미지 다루는 것에 고민함 기존 팀의 포커스는 모바일 환경에서 실시간으로 이미지 다루기였음(MMNet) 이제 뭐할까? 워크샵 10개 학회 =&gt; 3700편 논문 유저 니즈/비즈니스 트렌드 기반 아이디어 브레인스토밍 300가지 정도의 잠재적인 활용처 고민 1년간 로드맵 구성 프로젝트 선정 실현 가능성 임팩트 기술적 중요도 트렌드 Keyword Spotting 특정 핵심어가 발성되었는지 검출하는 문제 고려 사항 도메인 확장 CV 외 도메인으로 확장 난이도 분류 문제라 상대적으로 쉬움 기존 전문성 경량 모델에서 모바일 배포를 많이 해서 좋은 성과를 금방 거둘 수 있다 판단 Literature Survey 수백편을 목록으로 뽑고, 관력있을 것 같은 것들을 뽑음 데이터 오픈, 코드 정리 유무 등 스프레드시트 진행 Baseline 비교 대상이 되는 모델이 있어야 개선이 의미 있음 점진적으로 비교할 수 있는 모델을 늘려가며 다양한 컴포넌트를 확보 프로덕션에는 이미 연구된 모델을 구현하는 것이 충분할 수도 있음 Baseline Selection 합리적 성능이 나오는지? (SotA와 비교) 구현 난이도 및 공식 코드 공개 여부 재현이 까다로운 경우가 자주 있음 프러덕션에서 적용될 제약 조건을 얼마나 만족하는지? 모바일 CPU에서 실시간 수행 가능한가? Data 공개 데이터셋 오픈된 데이터를 바탕으로 리포팅 학계에 없으면 줄 수 없다고 많이 말함 비공개 데이터셋 내가 관심있는 도메인의 모델 성능은 다를 수 있음 데이터 수집에 대한 고민 어노테이션, 정합성 확인 데이터 탐색이 필수 PoC 일단 처음엔 큰 모델을 만들어서 우리가 원하는 정확도가 나오는지 파악함 중간 산출물이 프러덕션 활용될 수 있도록 고민함 Process 일단 한바퀴 돌면 어디가 문제인지 알 수 있어서 어느정도 휴리스틱도 괜찮음 오디오 처음이라 컴포넌트 만듬 Evaluation 모델 리포팅 결과가 재현 안되는 경우가 있음 모델 최적화하는 사람의 역량에 따라 다를 수 있음 Flops 연산량을 뽑아줌 =&gt; 모바일에선 약한 상관관계가 있음 제품을 어떻게 사용할지에 대해 고민 Research 유망한 모델을 재현하며 아이디어 얻기 다른 도메인의 아이디어 훔쳐오기 팀원과 토론 KWS Research Progress 오리지널 REsNet과 다른 구조 최대한 맞추니 결과가 좀 좋아짐. 하지만 속도가 빠르지 않아 가속할 방법 고민 기존 모바일 컴퓨터 비전 전문성을 지렛대로 활용 Audio Processing CNN Based KWS 네트워크가 깊어져야 함 Temporal Convolution 모바일에서 만들면 1d가 매우 빠름(캐시 친화적인 이슈) TC-ResNet 2D -&gt; 1D로 변환 Result 정확도 11% 개선, 모델 대비 385배 빠름 Publishing Ablation 테스트를 통해 불필요한 컴포넌트를 이해 Ablation Test 프러덕션땐 잘되면 그냥 냅두지만 논문 쓰다보면 제거할 수 있음 =&gt; 결국 다시 프러덕션때도 제거 Retrospection 3개월만에 진행 기계학습 전문성 &gt; 도메인 전문성이었떤 예 기존 전문성을 잘 활용함 Production + Research 제작을 중심으로 하는 회사에서 성공적인 기계학습 조직 운영하려면 서로 기대를 맞추고 서로 윈윈할 수 있어야 함 Expectation Management Positive Sum Game Ownership 당근마켓 추천 시스템 당근마켓, 전무익님 초기 타겟은 여성 육아 맘이였는데 요샌 10대~60대 다양한 사람들이 생김 첫 화면, 홈 피드에서 추천을 제공하고 있음 큰 목적이 없어도 피드를 사용함 동네 사람들의 제품 구경, 득템하는 재미 첫 단계 처음부터 완전한 피드 개인화 불가 소규모 스타트업 최신 글의 중요성 고려 추천 글을 피드 사이에 노출 유튜브 추천 시스템 논문을 활용 추천 시스템 구조 사용자 정보(봤던 글, 검색 키워드) =&gt; 추천 모델 =&gt; 다음 볼 글 예측 실시간 추천 Two Stage 후보 모델 + 랭킹 모델 후보 모델만 구현함 학습 데이터 학습 네트워크 후보 모델의 역할 전체 글 중 사용자가 좋아할만한 200개 찾기 추천 후보 pool을 빠르게 준비 단일 모델론 수많은 글 중 단 하나를 맞추는건 쉽지 않아 이런 구조로 진행 측정 결과 실시간 처리 Articel vectors를 학습해 인덱스 구축 user vector와 모든 article vectors에 대한 dot product 연산 대체 매우 빠른 유사 벡터 검색이 가능하고, faiss 라이브러리 활용함 Nearest neighbor index 준비 Inference CPU 0.01s 이하 추천 시스템 구축 데이터 수집 학습 Examples 이전에 본 글을 BigQuery ARRAY_AGG 사용함 CSV 추출 데이터 전처리 병렬 분산 처리 데이터가 많을수록 데이터가 선형적으로 시간 소요 Tensorflow Transform 활용 TF Transform Training-serving skew Cloud Dataflow 사용 모델 학습 지속적으로 안정적 학습이 중요 인프라 관리가 부담, 클라우드 관리형 서비스 사용 Cloud AI Platform Tensorflow Estimator로 모델 작성 모델 서빙 시간이 지나며 새로운 글을 추천하기 위해 지속적 학습, 업데이트 파이프라인 시스템 워크플로우 작업 실행 실행 로그/결과 확인 정해진 일정에 맞춰 실행 직접 구현하기보다 오픈소스 활용 Kubeflow pipelines 컨테이너 기반 워브플로우 작업 정의 편리 개발과 실행 환경을 동일하게 유지할 수 있음 파이프라인 작성 파이프라인 배포 파이프라인 배포 코드 Output Viewer 예측 결과를 쉽게 볼 수 있음 홈 피드 목록 중간에 넣고 있음 피드 개인화 추천 효과 글 보기 +6% 방문 시간 +5% 라이트닝 토크 Machina Black 기술로 법률 시장을 혁신 NLP + Legal 인공지능 변호사 : 계약서 자동 검토, 계약서 데이터 활용 인공지능 판사 : 판결 예측, 송무 데이터 활용 Legal Translation 법률 번역 =&gt; 영어도 잘하고 법도 잘 알아야 함 연말에 베타 서비스 Art LAB AI &amp; Robotics + Technology Transformation : ART, 변혁으로 삶에 뿌리내림 Team : 친구이자 동료가 됨 Beauty AI 아름다움, 관리, 힐링 =&gt; 나를 아끼고 존중하며 소중히 여김 기획 AI : 소비자 니즈 센싱, 맞춤형 추천, 신제품 기획 개발 AI : 원료-제품 관계, 재고 관리, 업무 효율화 Lomin 컴퓨터 비전 집중 DeepFake 검출 무상에서 개인 정보 비식별화 문자 인식 솔루션 개발 중 Visual Identity Protector : 개인 정보 Nota 경량화 기술 연구 클라우드를 없애고 다른 Edge device에 모델이 돌아가도록 진행 Medical Device 회사와 의료 영상, 생채 진행 Security Company와 같이 CCTV 모델 연구 에스아이에이 RNN을 이용한 전력 가격 예측 Github 위성 데이터로 판별 위성 영상으로 지구에 뭐가 있는지 판단 다양한 논문 연구하고 제출함 경진대회 장려 중ㅋㅋㅋ Object Detection, Ship Detection, Explainable AI, Vehicle Detection 등 집중 Hutom 수술 중 발생하는 문제를 데이터 기반으로 해결 암 수술은 기구를 통해서 진행하고 있음 로봇 팔이 들어감 데이터 SEE : Stereo Camera DO : Controller 키네마틱 데이터 DACON 경진 대회 교육에 활용 국방 + 쎄트렉아이와 함께해서 곧 대회 열림 위성 이미지 객체 추출 비즈니스 데이터 Fit, 비즈니스 대회 Fit, 운영 자동화(치팅, 자동 리포팅 등), 고도화 및 적용 PR12 매주 일요일 밤 10시에 매주 2편씩 논문 읽고 발표 영상 녹화해 공유 여태 누적 200편 컴퓨터 비전 위주로 진행 ㅋㅋㅋㅋㅋㅋㅋㅋ 아침에 영상 보는 모임 나옴 총 View : 431,766 Like : 3,863 Unlike : 238 최다 조회수 3만회 =&gt; Faster RCNN 외국인들이 영어 번역해달라고 하기도 함 진원님 최초 기수부터 마지막까지 계속 하고 계심! 파이팅!! Kafka 스트림을 위한 멀티프로세스 딥러닝 추론 네이버 쇼핑플랫폼 이성철님 쇼핑 카테고리 분류 과거엔 관리자가 쇼핑몰 카테고리를 직접 매핑했으나, 요새는 딥러닝 기반 카테고리 분류를 하고 있음 Kafka 스트림을 위한 추론 시스템 개발 메세지 기반, 썸네일 만들고 카테고리 분류 절차 등을 카프카에 태움 Raw Data 텍스트 전처리 =&gt; Numpy Array 만듬 이미지 전처리 디스크 IO가 없어야 함(메모리에서 올림) 나누기는 GPU에서 처리 나누기는 CPU consuming한 연산 Multi-process inference Consumer : 토픽에서 메세지를 받아오는 역할 Preprocessor : 받아온 메세지를 모델에 입력할 수 있도록 데이터 변환 Classifier : GPU 모델을 로딩해서 들어오는 데이터를 계속 inference GPU를 최대로 쓰기 위해 Classifier보다 더 많은 Preprocessor가 필요(이렇게 하면 분류기 앞에 배치 큐에 많은 것들이 담김) Inference performance Multi process Class(Consumer의 예) Multiprocessing.Process 상속하고 잘 종료하기 위해 stop() 함수 작성하고 run() 함수 필수 작성 결과 quere가 full이면 대기 Multiprocessing의 Array가 있구나 main.py Process Start / Stop 이미지와 텍스트 학습 데이터 준비 Training 데이터셋 만들기 이미지와 텍스트를 같이 담을 수 있어야 하고 Spark Tensorflow 사이에서 자유롭게 사용 가능해야 함 TFRecords가 가장 적합 이미지 URL만 아닌 바이너리 파일 100 ~ 200 MB가 적절 파티션 개수 조절!! Load data using tf.data 런타임 shuffle은 미리 하고 저장 Graph Neural Networks 류성옥님 분자가 그래프로 표현됨 Inductive Biases in Neural networks 파라미터라이즈를 할 수 있음 뉴럴넷은 연속함수를 근사할 수 있단 사실 때문에 사용 MLP에서 더 나아가서 도메인에 맞게 학습 이미지 : CNN Autoregressive : RNN 의도적으로 bias를 걸어주는 것을 inductive bias라 표현함 Social Graph, 3D Mesh, Molecular Graph는 illegular함. 물분자와 에탄올의 그래프가 모두 다름 주어진 그래프에 다르게 inductive bias를 걸 수 있을까?에 적합한 Graph neural network Node feature updates in GNNs Graph Convolution networks Feature들이 벡터로 표현 노드가 어떻게 업데이트가 되는지를 표현하는 기본적인 식 주변 사람과 connection을 의미하는 adjacency matrix Graph Attention Networks 관계를 더 고려해 feature update self-attention, positonal encoding을 주로 사용함 =&gt; 그래프 어텐션은 주변의 관계를 묘사할 때 어텐션을 사용 Message Passing Neural Networks GRU Edge passing이 있을 때 사용 가능 Learning tasks with GNN Node-level prediction 노드 단위 예측 Edge-level predictions 유저, 아이템 노드가 있을 경우 주로 본 사람은 누구인가? =&gt; 누가 뭘 봤다라는 connection 레이어를 예측하면 아이템의 관계를 inference 할 수 있음 Relational inference Graph-level prediction 이 그래프 구조가 이 병을 해결할 수 있는가? Graph generations 노드를 추가할 것인지? 엣지를 추가할 것인지? Learning physical dynamics 각자 다른 공들이 파티클이고 움직임(무슨 법칙이 있음) 움직임을 예측 더 궁금하시면 Graph Neural Networks Github 나만의 코퍼스는 없다? 자연어처리 연구 데이터의 구축, 검증 및 정제에 관하여 조원익님 무엇을 위해 데이터를 만드는가? 코퍼스의 종류 Annotation의 종류 통사(syntax)에서 의미(semantics)를 넘어 화용(pragmatics)가지! 이모션 태그, 다른 문장을 표현할 수 있는지? 말장난인지 아닌지 등 사실 영역의 경계가 점점 흐려지고 있음 MATTER cycle MAMA portion 어노테이션은 2명 이상, 많을수록 좋음 주석자간 의견 일치도도 고려 Cohen’s Kappa 주석자 주석자들이 얼마나 언어학적 훈련을 받았는지 가늠을 해서 배정해야 함 언어 배경, L2 이상 케이스 스터디 1 호출어 없이 알아서 반응하는 음성 대화 서비스 가이드라인 작성 케이스 스터디 2 Keyphrase extraction 데이터 정리 행사 후기 기술적으로 깊은 이야기를 많이 진행해서 만족스러웠습니다 회사에서 사용할 수 있는 키워드도 꽤 얻었습니다! 오랜만에 만났던 분들도 모두 반가웠습니다 :) 100분 넘는 분들이 일요일까지 오셔서 열정을 보여주셔서 자극받고 갑니다..! 그리고 Art Lab에서 이벤트하셨는데 제가 당첨되서 엄태웅님의 책을 받았습니다!! 대학원생 때 알았더라면 좋았을 것들 책 너무 좋으니 관심있으시면 꼭 보셔요 :) 다음엔 타임 테이블을 미리 공지해주셔도 좋을 것 같아요 /ㅁ/",
    "tags": "lecture etc",
    "url": "/etc/2019/10/20/tensorflow-kr-3th-meeting/"
  },{
    "title": "팀원 성장시키기 : 발표 컨설팅",
    "text": "팀원을 성장시키는 다양한 방법 중, 개발 발표 컨설팅한 내용에 대해 작성한 글입니다 팀원들의 발표를 어떻게 동기부여하고 어떻게 발표 준비를 도왔는지에 대한 글입니다 데이터야놀자 2019에서 다음 발표를 도왔습니다 정민정님의 GAN을 활용한, 내 손글씨를 따라쓰는 인공지능 이창현님의 타다(TADA) 서비스의 데이터 웨어하우스 : 태초부터 현재까지 권윤환님의 모빌리티 데이터팀 신입 분석가의 1년 회고 팀원 성장시키기 회사에서 팀을 이끄는 역할을 하다보면, 팀원을 어떻게 더 빠르게 성장할 수 있도록 도울 수 있을까?를 고민하게 됩니다 우선 팀원이 성장할 의지를 가지고 있어야 하고 팀장이 팀원의 욕구를(바라는 커리어 등) 수시로 잘 파악해야 하고 욕구(바라는 커리어 등)와 회사에서 진행되는 일의 교집합을 잘 찾아 Task로 만들고, 적절하게 분배하면 좋다고 생각합니다 이 부분은 [TF에서 팀 빌딩까지 9개월의 기록 : 성장하는 조직을 만드는 여정]에서 발표한 내용처럼, 팀원들과 계속 고민하고 실천하고 있습니다 요즘 또 든 생각은 팀원들의 욕구는 계속 듣고 발전하며, 평소에 접하지 못했던 것을 하면 또 새롭게 성장하지 않을까?란 생각을 했습니다 단, 팀원들이 평소에 접하지 못했던 것 중 제가 경험했던 일이면 더 잘 도울 수 있을 것 같다고 생각했습니다 고민한 결과, 저는 과거에 광고 동아리에서 발표를 많이 했고, 경영학과에서 한학기에 6 전공 6 팀플 6 발표라는 경험, 개발 컨퍼런스 등에서 다양한 발표를 했기 때문에 발표에 대해 자신이 있었습니다 이번엔 팀원들을 발표하도록 권장하고, 발표의 모든 과정을 돕기로 결정했습니다 발표 컨설팅을 위해 제가 진행한 활동은 다음과 같습니다 발표가 좋은 이유 설명 발표할 의지가 있는 팀원 파악 발표할 컨퍼런스 찾기 발표 신청하기 발표 스토리 초안 작성 돕기 발표 자료 만드는 가이드라인 제시 발표 자료 초안 피드백 발표 자료 최종 피드백 발표 전 리허설 발표 후 자료 공유 발표가 좋은 이유 개발자에게 (개발자가 아니여도 모두에게 좋지만) 발표가 좋은 이유를 설명하면 좋습니다 단순히 “우리 회사 홍보할 겸 회사에서 일한거 발표 좀 해봐” 라는 수직적인 말과 함께 발표하라는 분도 있다고 들었습니다. 이런 방식은 노노.. 자발적인 의지가 중요합니다 (제가 생각하는) 발표가 좋은 이유 다음과 같습니다 1) 진행한 업무를 정리하는 시간 보통 회사에서 진행한 업무를 어딘가에 잘 메모해두지만, 발표용으로 이해하기 쉽게 정리하는 분들은 적은 것 같습니다(저도 그래요..!) 프로젝트가 끝난 후, 혹은 진행 과정에 한번 업무를 정리하면 자신이 했던 일을 정리하며 좋고, 추후에 다른 분들이 입사하셔도 발표 자료를 전달하며 이런 흐름으로 진행했다고 알릴 수 있습니다 2) 공식적으로 자신이 진행한 일을 외부에 알릴 수 있는 수단 보통 회사에서 진행한 일은 대부분 “대외비” 취급하는 경우가 있습니다. 이럴 경우 자신이 진행한 일에 대해 공식적으로 말하기 꺼려집니다 단, 발표를 하면 (필요할 경우 내부 피드백을 거치며) 공식적으로 자신이 진행한 일을 어느정도 가이드라인 하에 알릴 수 있기 때문에 커리어에 도움이 된다 생각합니다 3) 지식 나눔 자신이 가진 지식을 누군가에게 나누는 동시에 다른 분들도 자신의 지식을 공유하는 경우가 있습니다. 공유하는 문화가 더 아름다운 개발 생태계를 만든다고 생각합니다 4) 본인 커리어에 도움 2)의 맥락과 유사한데, 외부 발표를 통해 자신을 알릴 수 있는 계기가 됩니다. 학생이라면 추후 첫 취업에 영향을 미칠 수 있고, 이미 회사를 다니시는 분도 추후 먼 훗날의 커리어에 도움이 될 수 있습니다 5) 회사 PR 회사 업무에 대해 이야기하는 경우엔 회사의 기술력을 마음껏 뽐낼 수 있습니다! 회사에 더 좋은 분이 올 수 있는 계기가 될 수 있습니다 발표할 의지가 있는 팀원 파악하기 발표가 좋은 이유를 팀원 개인에게도 말하고, 팀 전체에 말하고, 같이 일하는 동료에게도 말하고, 제가 운영하는 커뮤니티에도 말하며 발표할 의사가 있는 분들을 찾았습니다 꼭 팀원이 아니어도, 같이 일하는 동료와 커뮤니티에서 만난 인연, 대학생 분들 등 다양한 분을 돕는 것이 좋다고 생각해 총 3분의 의지를 확인했습니다 의지를 확인하기 위해 3번 정도 말한 것 같습니다..! 회사에서 저희 팀 소속이신 팀원 회사에서 같이 일하는 동료 개발자 글쓰기 모임 글또에서 활동하시는 분 모두 “데이터” 관련 일을 하거나 데이터 업을 희망하기 때문에 포괄적인 “데이터야놀자 2019”에 발표하면 좋을 것 같다 말씀드렸습니다 발표 신청하기 개발 컨퍼런스에 발표하고 싶은 의지만 있다고 모두 발표할 수 있는 것은 아닙니다 컨퍼런스의 퀄리티 혹은 목적에 맞도록 사전에 발표 내용을 제출하고 그 내용을 토대로 최종 발표자가 선정됩니다 이 발표자 신청 과정부터 잘 통과하지 못하면 결국 발표를 못하기 때문에, 이 부분부터 같이 진행했습니다 신청하기 위해 파악한 내용은 다음과 같습니다 1) 개발 컨퍼런스의 슬로건과 유사한 발표 고민하기 개발 컨퍼런스에서 슬로건이 없는 경우도 있지만 보통은 작은 가이드라인이라도 있는 경우가 많습니다 예를 들면 파이콘 한국 2019의 슬로건은 “Connect the Pythonistas” 입니다. 다양한 분야의 파이써니스타를 연결한다는 의도입니다 데이터야놀자 2019는 “현업에서 얻은 데이터 경험”, “현장 또는 나와 함께하는 데이터” 를 큰 테마로 가졌습니다 2) 발표의 스토리라인 잡기 각자 발표할 내용과 무엇을 전달하고 싶은지 잘 고민해야 합니다 ㄱ) 발표를 통해 어떤 내용을 전달할 것인가? ㄴ) 발표의 예상 청중은? ㄷ) 발표의 예상 난이도는? 난이도가 있다면 어느 정도 알아야 수월하게 들을 수 있는가? ㄹ) 희망하는 예상 시간대는? 3분과 대화를 한 결과, 모두 각자가 진행한 업무 또는 프로젝트가 있었습니다. 그 내용을 최대한 잘 반영해 발표 신청서에 담았습니다 발표 신청서를 정성스럽게 작성하고, 의도에 맞는다면 잘 될거라 믿었습니다 발표 신청서에 위에 말한 ㄱ ~ ㄹ + 발표의 서론/본론/결론을 작성하도록 했습니다(이 때 작성한 내용과 실제 발표의 흐름이 달라도 괜찮습니다) 서론/본론/결론의 스토리라인 초안 작성을 도울 때 아래와 같이 가이드 드렸습니다 발표를 위한 말랑말랑한 Intro(서론)이 있는가? 흥미 유발할 수 있는가? 본론 : 진행한 업무 / 프로젝트 흐름대로 일단 작성 결론 : 그래서 어떤 내용을 전달하고 싶은지 다시 정리 발표 자료 초안 피드백 발표 자료 초안 피드백은 발표일 -7일 쯤에 했습니다 보통 마감일에 자료를 만드는 경우가 있는데, 이럴 경우 발표를 많이 해보지 못한 분들은 발표 연습을 충분히 하지 못하는 경우가 있습니다 따라서 초안 피드백을 발표일 1주 전에 하고, 그 사이에 최종 자료를 만드는 방식을 제안했습니다 초안 피드백에 보는 내용은 다음과 같습니다 1) 전달하고 싶은 내용을 한줄로 설명하면 무엇인가요? 이 부분은 발표 전체를 관통할 수 있는 내용이 있는지, 발표의 무게 중심을 잡을 수 있는 용도로 여쭤봤습니다 이 부분을 통해 자신이 어느정도 이야기할지(가끔 이야기가 산으로 가거나 끝없이 아무말 대잔치하는 분들이 있어서..) 고민하셨습니다 2) 내용을 단순 “나열”한 것이 아닌 적절한 “스토리”가 같이 있는지? 발표 시간에 따라 다르지만 25분 혹은 40분 발표는 구성에 따라 짧게 느껴질 수도, 길게 느껴질 수도 있습니다 짧게 느껴지는 경우엔 발표자분의 발표력이 좋거나, 흥미로운 내용을 전달했거나, 스토리가 적절하게 있는 경우라고 생각합니다 발표 경험이 적으면 발표력이 좋지 않을 수 있기 때문에, 최대한 흥미로운 내용 + 스토리를 적절히 조합하도록 권장하고 있습니다 스토리라고 하면 “왜 이 일을 하게 되었는가? 어떤 생각을 하다 이렇게 했는가?”로 시작해 의식의 흐름을 나열하곤 합니다 “이 일을 하며 힘들었던 상황은? 그걸 어떻게 극복했는가?” 더 쉽게 이해할 수 있도록 도식화를 할 수 있지만 일단 초안 피드백에선 스킵합니다. 일단 전체적인 틀을 채우고 시각화나 도식화는 발표 전 최종 피드백에서 진행합니다 3) 발표를 보는 분들이 유용하게 얻어갈 수 있는 내용이 있는지? 발표를 통해 정보를 얻는 경우가 많습니다. 이 부분은 발표를 듣는 청자분들에 따라 달라질 수 있지만, 저는 발표를 보는 분들이 카메라를 들고 사진을 찍고 싶어하는 내용이 있는지? 정도로 생각하고 있습니다 또는 자료 조사를 충분히 했던 내용 정리해서 공유하거나, 트러블 슈팅했던 내용을 공유하곤 합니다 이 부분이 쌓이면 핵심 포인트가 많다고 생각합니다 저는 항상 큰 이야기가 끝나면 한 장에 요약하도록 요청했습니다 이 부분에선 디테일을 보기 보다 발표의 큰 흐름, 주제, 발표의 핵심 포인트 위주로 피드백 드리고 시각화할 수 있는 내용은 아이디어 위주로 드렸습니다 예시로 제가 발표한 Little Big Data #1. 바닥부터 시작하는 데이터 인프라를 예시로 공유드렸습니다 오프라인과 온라인에서 둘 다 피드백드렸는데, 온라인에서 드린 내용을 예시로 보여드리면 아래와 같습니다 발표 자료 최종 피드백 초안 피드백 후 3~4일 뒤, 즉 발표일 -3일 쯤에 그 순간까지 완성한 자료를 통해 최종 피드백 드렸습니다 1) 이 최종 피드백에선 내용을 크게 틀지 않고 주어진 시간 안에 발표를 마무리할 수 있는지? 발표일이 가까워지면 내용을 크게 더 넣으려고 하다가 오히려 발표를 더 못할수도 있기 때문에 내용을 크게 틀지 않는 선에서 피드백 드리는 것이 중요한 핵심입니다 발표 시간이 총 40분이고 크게 전달할 내용이 3개라면 intro 5분, 첫 이야기 8분, 두번째 이야기 8분, 세번째 이야기 8분, 마무리 5분 정도로 말씀드렸습니다(강조하고 싶은 부분 위주로 조정 가능하고, 타이트하게 시간을 계산하도록 했습니다) 2) 시각적 표현에서 더 부드럽고 재미있게 할 수 있는지? 시각적 표현을 하느냐에 따라 발표력이 달라진다고 생각하고 있습니다 이미지를 구글에서 어떻게 검색할지, 어떻게 추상화하면 좋을지 등을 같이 고민합니다 Icon 이미지의 경우 Flaticon을 사용하면 깔끔할 수 있다는 점을 말씀드렸습니다 단, 저작권 표시는 꼭-! 3) 단순 나열 같은 흐름이 있는지? 발표의 구성을 효과적으로 어떻게 바꿀까? 초안 - 최종 피드백의 경계가 사실 오묘하긴 하지만, 시기에 따라 집중하는 부분이 달랐습니다 피드백 예시입니다 발표 전 리허설 발표일 -1일에 제가 컨설팅 해드리는 3분을 모두 회사 사무실로 불러 사전 리허설을 진행했습니다 이왕이면 프레젠테이션 포인터를 구해서 서서 실전처럼 발표할 수 있으면 좋습니다(없다면 마우스라도 사용) 이 때 주로 보는 것은 다음과 같습니다 1) 주어진 시간 안에 잘 발표하는지? 주어진 시간 기준으로 너무 빨리 끝내면 유용한 내용이 적을수도 있고, 혹은 말을 너무 빨리하는 습관이 있을 수 있습니다 주어진 시간 기준으로 너무 늦게 끝내면 행사에 차질이 생길 수 있고, 발표하다가 급한 마음에 발표의 뒷 부분을 빠르게 끝내느라 정신없을 수 있습니다 주어진 발표 시간에서 질문 시간 5분을 제외하고 적당하게 끝내는지를 파악했고, 만약 발표가 길어 질문 시간이 없는 경우도 있었습니다 스탑워치를 통해 1장당 몇 초 사용했는지를 기록해 전달했습니다 2) 발표자의 특징 파악해서 전달 실제 발표 시간과 리허설의 시간이 비슷할 수 있지만 사람에 따라 달라질 수 있습니다 발표하는 톤과 평소에 알던 성격 기반으로 실제 발표에선 말이 빨라지는 스타일일지, 말이 느려지는 스타일일지에 대한 제 의견을 말씀드렸습니다 또한 리허설을 하다가 목이 약한 것 같은 분이 계시면 발표 전에 꿀물이나 도라지차 마시고, 발표 전에 꼭 물을 챙겨가도록 말씀드렸습니다(목 관리 중요..) 3) 실전 발표시 유용한 Tip 발표에서 갑자기 예상치 않은 TMI를 할 수도 있는데(혹은 갑자기 예상이 없던 드립 등..) 그럴 분들은 없었지만 그냥 딱 지금까지 한 것만 하는 것이 좋을 것 같다 정도로 제안했습니다 발표가 너무 일찍 끝나면 질문을 많이 받거나 중간 중간에 시간을 체크해서 발표 시간을 조절하는 방법 발표에서 죄송합니다 &lt;- 같은 표현을 굳이 쓰지 말고 당당하게 진행하면 좋다 등을 이야기했습니다(굳이 쓰지 않아도 될 표현) 최대한 잘할 수 있다는 응원과 격려를 가득! 했습니다 발표 실제 발표하는 모습을 잘 지켜보고 모두 사진을 찍어드렸습니다 발표 전에 떨려하시는 분 계시면 가서 대화 해드리고 멘탈 관리(?)를 해드렸습니다 발표 후 자료 공유 발표만 했다고 끝이 아닌, 자료 공유도 적절하게 잘 하면 좋습니다 슬라이드쉐어 또는 스피커덱 추천드리고 있습니다 슬라이드쉐어는 조금 더 생태계가 크고, 한번 올리면 수정 불가능, 키노트에서 작성한 내용은 수정이 필요 등의 특징 스피커덱은 수정은 가능하지만 생태계가 (상대적으로) 작음 기존에 슬라이드쉐어를 많이 사용하셨다면 아예 다 스피커덱으로 옮기는 방법도 추천드리고 있습니다 키노트 자료를 슬라이드쉐어에 올리실 경우 터미널에서 아래 내용을 입력해주셔야 한국어가 잘 보입니다. 관련 링크 LANG=C LC_ALL=C sed -i '' s'|/Registry (Adobe) /Ordering (Korea1) /Supplement [0-9]|/Registry(Adobe) /Ordering(Identity) /Supplement 0|g' 파일.pdf 발표 자료를 업로드한 후, 필요하다면 페이스북 그룹에 공유해 자신이 진행한 내용을 컨퍼런스에 오지 않은 분들도 알 수 있도록 공유하도록 권장하고 있습니다 진행하며 느낀 점 사실 처음엔 “컨설팅”은 생각보다 수월하겠지? 란 생각을 했는데 사실 제가 발표 준비하는 것보다 더 많은 시간을 투자했습니다 하지만 주변 분들(팀원)이 성장하는 모습을 볼 수 있었고, 앞으로도 시간이 생긴다면 이런 부분을 같이 고민하고 나누고 싶습니다 :) 다만.. 무상으로 하기엔 시간을 너무 많이 투자해서 고민이 되네요..! 제 빡센(?) 피드백 받으며 발표를 준비하신 정민정님, 이창현님, 권윤환님 고생하셨습니다!",
    "tags": "diary",
    "url": "/diary/2019/10/20/helping-presentation/"
  },{
    "title": "Full Stack Deep Learning Bootcamp 정리",
    "text": "Spring 2019 Full Stack Deep Learning Bootcamp의 영상을 보고 정리한 내용입니다 Lab(실습), Guest Lecture는 정리하지 않았습니다 부트캠프 후기 오프라인에서 참석한 것은 아니지만, 강의를 듣고 후기를 남김 일단 강의가 매우 고퀄!!!!!!(감동) 국내에선 이런 내공을 담은 강의를 거의 보지 못했음 프로젝트를 어떻게 해야되는가, 프로젝트 우선순위를 선정하는 부분도 알려줘서 매우 좋았음. 실제 현업에서도 매번 고민하는 이슈들(Lecture 2) 데이터를 어떻게 다룰까에 대해 말해주는 점이 좋았음. 단순 로컬/클라우드만 하는게 아니라 Database도 말해줌(lecture 6) 팀 구성이 어떻게 되는지, 직군이 무엇이 있는지 알려준 점이 좋았음(lecture 7) 딥러닝 트러블 슈팅에 대해 알려줘서 좋았음. 언더피팅, 오버피팅일 때의 전략을 제공함(lecture 10) 모델의 성능 개선할 때 결과를 어떻게 봐야하는지에 대한 직관을 얻게 해줌(lecture 10) 이 부트캠프는 단순히 강의만 듣는게 끝이 아니고, 꼭 Lab(실습)을 해야함!! 코드를 보면 현업에서 사용할 소재들이 꽤 많음(Lambda 배포라던가) =&gt; Github 참고 앞으로 어떤 분야를 연구할 것인가에 대해 흥미로운 주제를 던져주는 부분도 좋았음(lecture 13) 단, Serving하는 부분에 대해 엄청 깊게 알려주는건 아니고, 부트캠프답게 큰 그림을 그려주고 실습을 같이 하며 하나를 구현해봄 =&gt; 이 부분은 결국 키워드를 캐치해서 스스로 해보는게 좋을듯..! MLOps는 Awesome Production Machine Learning 자료가 매우 풍부하며, Facebook MLOps KR 그룹도 있습니다 :) 어떤 사람에게 이 강의가 좋을까요? 어느정도 머신러닝/딥러닝 공부를 한 후, 실제 서비스 구현에 흥미있는 분 개발 베이스에서 머신러닝/딥러닝 공부를 하셨던 분 프로젝트를 어떻게 하면 좋을지 고민되는 분 딥러닝 프로젝트 트러블슈팅에 대해 고민되는 분 Production 과정 전반에 대해 관심있는 분 Bootcamp의 목적 개발자가 딥러닝에 친숙해지기 위한 Hands-on 프로그램 모델 학습은 딥러닝 Production의 일부분이고, 이 코스에선 Production화하기 위한 모든 것들을 가르칠 예정 Problem을 명확히하고 프로젝트의 cost를 측정 Data를 찾고, 전처리하고, 라벨링 적절한 Framework와 Infra를 선정 학습의 reproducibility 관련 트러블슈팅 대규모 모델 Deploy 컴퓨터 비전과 자연어 처리 시스템을 프러덕션 환경에 배포하는 프로젝트를 진행 선택적 필기 시험으로 지식을 테스트 프로젝트 Lecture 1 : Introduction 발표 자료 Organizer Pieter Abbeel : 버클리 교수님, Covariant.AI Scientist Sergey Karayev : STEM AI Head Josh Tobin : Research Scientist at OpenAI Object Detection in Computer Vision 2012년까지의 대세 Image =&gt; Hand-engineered features(SIFT, HOG, DAISY) =&gt; SVM =&gt; 분류 딥러닝 이후 Image =&gt; 8 레이어 뉴럴넷 =&gt; 분류 딥러닝이 발전한 예시를 보여줌(다양한 업계) Google, OpenAI, Facebook, Uber 등 다양한 딥러닝 엔지니어와 리더들과 이야기해서 컨텐츠를 만듬 Lecture 2 : Machine Learning Projects 목표 머신러닝 프로젝트를 이해하기 위한 프레임워크 도입 머신러닝 프로젝트 Best Practices 머신러닝 프로젝트 Lifecycle 1) Planning &amp; Project setup 어떤 프로젝트를 하기로 했는지 요구사항, 목표 설정 리소스 할당 등 2) Data Collection &amp; Labeling 데이터 수집 센서 설치 annotation 노가다 등 그러나 데이터를 얻기 너무 어려움 3) Training &amp; debugging OpenCV로 베이스라인 구현 SoTA 모델 찾고 구현 구현체 디버깅 Task에 맞도록 모델 개선 잘 안되면 데이터를 더 수집 labeling이 신뢰할 수 없음을 깨달음 Task가 어려운 것을 깨닫고 각각의 trade off를 비교해 어떤 것이 제일 중요한가 고민 4) Deploying &amp; testing 프로토타입 테스트하고 프러덕션화 그러나 프러덕션에서 작동 안될 수 있음 training data와 deployment의 데이터의 mismatching을 고치고 데이터를 더 수집 Metric &amp; Performance 선택한 Metric이 사용자 행동을 유지하지 않음 =&gt; 다시 고민 프러덕션시 Performance가 좋지 않음(더 빠르거나 정확해야 할까? 더 알아야 하는 것 도메인에서 SOTA가 뭔지 확인 가능하면 이해하고 다음에 무엇을 시도할지 알아야 함 강의의 나머지 개요 프로젝트 우선 순위 지정 및 목표 선택 모델을 평가할 지표 선택 비교의 기준이 될 베이스라인 선택 프로젝트 우선 순위를 정하는 방법 1) 영향력이 큰 ML 문제 파이프라인의 복잡한 부분이 있는가 간단한 예측이 가치가 있는가 Feasibility(실행 가능성) 데이터 사용 유무, 정확도 요구사항, 문제의 어려움이 모여 Cost를 만듬 2) ML 프로젝트의 Cost는 데이터의 가용성에 의해 결정되지만 정확성 요구도 큰 역할을 함(예 : 95%의 정확도는 가져야 해!) Accuracy requirements가 중요한 이유 높은 성능까지 가려면 cost가 지수함수처럼 증가함 제품 설계로 이런 Accuracy 요구를 줄일 수 있음(=목적이나 활용 방법에 따라 다르게 할 수 있다는 뜻) 정리 1) 영향력이 큰 ML 문제를 찾으려면, 파이프라인의 복잡한 부분과 cheap prediction이 가치있는 곳을 찾아보기 2) ML 프로젝트이 cost는 데이터의 가용성 + 정확도 요구사항에 의해 결정됨 지표 선택의 핵심 1) 현실 세계는 지저분하고, 일반적으로 많은 측정 항목에 관심있음 2) 그러나 머신러닝 시스템은 단일 숫자를 최적화할 때 가장 잘 작동함 3) 결과적으로 metric 결합해서 공식을 만들어야 함 4) 공식은 변경될 수 있음 어떤 Metric이 좋은가? Metric Combine Simple average, weighted average threshold n-1 metric, n번째 평가 등 복잡하고 도메인 특화된 공식 Average precision(AP) = curve의 아래 지역 mAP = mean AP Pose estimation Metric 고르는 예시 Evaluate current performance Train a few models 현재 모델로 어떤 문제가 있는지 파악할 수 있음 베이스라인 선택의 핵심 1) 베이스라인은 모델 성능에 대한 하한선을 제공(최소한 이거 이상은 해야한다) 2) 하한선이 엄격할수록 더 유용함(출판된 내용, 사람보다 좋다는 기준 등) 베이스라인이 중요한 이유 같은 모델이 다른 베이스라인을 가지면 다음 행동이 달라짐 베이스라인을 찾을 수 있는 곳 Scripted baseline의 예시 : OpenCV 스크립트, Rule based 방법 등 Simple ML baseline의 예시 : 일반적인 feature based model(bag-of-words classifier), linear classifier, Basic neural network model(vgg without batch norm, weight norm) 등 사람 관련 베이스라인을 어떻게 만들 수 있을까 질문 머신러닝 모델이 잘못되었다는 것을 어떻게 방지할 수 있을까? 좋은 질문. 모델의 confidence를 사용 그러나 머신러닝에서 여전히 어려운 문제. 모델은 자주 잘못되었는지 모름 매우 적대적인 사례를 넣어서 어떻게 나오는지 보는 방법도 있고, 고민이 필요 Lecture 3 : Intro to the Text Reecognizer Project 발표 자료 프로젝트 Architecture POST request =&gt; 이미지 Decode =&gt; Line Detector =&gt; Line Text Recognizer =&gt; Encode Response의 흐름 LineDetector와 LineTextRecognizer가 필요 실습 개요 Lab 1: Codebase walkthrough Lab 2: Single-character prediction. Before predicting full lines, try a simpler problem Lab 3: LineTextRecognizer. Build a synthetic dataset. LSTM + CTC loss model Lab 4: Tools for experimentation. Weights &amp; Biases, parallel experiment running Lab 5. Experimentation. Try some things and run some things overnight Lab 6. LineDetector. Train the line detection model Lab 7. Data. Managing data, including label and versioning Lab 8. Continuous integration. Testing your model Lab 9. Deployment. Put the model into production Project Structure Web backend : Serving predictions, provisioning api/ # Code for serving predictions as a REST API. tests/test_app.py # Test that predictions are working Dockerfile # Specificies Docker image that runs the web server. __init__.py app.py # Flask web server that serves predictions. serverless.yml # Specifies AWS Lambda deployment of the REST API. Data data/ # Training data lives here raw/ emnist/metadata.toml # Specifications for downloading data Experimentation : 평가나 여러 노트북 파일 evaluation/ # Scripts for evaluating model on eval set. evaluate_character_predictor.py notebooks/ # For snapshots of initial exploration, before solidfying code as proper Python files. 01-look-at-emnist.ipynb Convenience scripts tasks/ # Deployment build_api_docker.sh deploy_api_to_lambda.sh # Code quality lint.sh # Tests run_prediction_tests.sh run_validation_tests.sh test_api.sh # Training train_character_predictor.sh Main model and training code text_recognizer/ # Package that can be deployed as a self-contained prediction system __init__.py character_predictor.py # Takes a raw image and obtains a prediction line_predictor.py datasets/ # Code for loading datasets __init__.py dataset.py # Base class for datasets - logic for downloading data emnist_dataset.py emnist_essentials.json dataset_sequence.py models/ # Code for instantiating models, including data preprocessing and loss functions __init__.py base.py # Base class for models character_model.py networks/ # Code for building neural networks (i.e., 'dumb' input-&gt;output mappings) used by models __init__.py mlp.py tests/ support/ # Raw data used by tests test_character_predictor.py # Test model on a few key examples weights/ # Weights for production model CharacterModel_EmnistDataset_mlp_weights.h5 util.py training/ # Code for running training experiments and selecting the best model. gpu_util_sampler.py run_experiment.py # Parse experiment config and launch training. util.py # Logic for training a model with a given config LineTextRecognizer Model architecture Lecture 4 : Infrastructure and Tooling 발표 자료 이상과 현실 Hidden Technical Debt in Machine Learning Systems 이야기함 논문 리뷰 링크 참고 다양한 도구들 Deep Learning Frameworks Tensorflow가 Production에 사용하기 편함 특별한 이유가 없으면 Tensorflw/Keras or PyTorch 사용 둘 다 동일한 포인트로 수렴 define by run으로 쉽게 개발 execution graph를 다양한 플랫폼에서 최적화 일화로, 사람들이 파이토치로 바꿀때 행복해함 Development, Training/Evaluation 하드웨어 GPU Basics NVIDIA가 유일한 선택이였던 시절이 있음 Google TPU가 현재 제일 빠름 Intel, AMD가 곧 시작될 예정 NVIDIA가 매년 새로운 아키텍쳐를 공개함 Cloud Providers AWS, GCP, Azure의 3파전 AWS가 제일 비쌈 Spot 요금을 사용하면 많이 할인되지만 갑자기 꺼질 수 있음 하이퍼 파라미터 서치 실험엔 적합하지만, 실패를 처리하려면 인프라가 필요 GCP는 TPU도 사용할 수 있음 GCP도 Spot이 있음, 거의 AWS보다 저렴함 Azure AWS와 꽤 비슷 Spot 없음 On-prem Options 4개까진 꽤 쉬움 사전 구축된 것을 구매 Lambda Labs, NVIDIA, Supermicro, Cirrascale etc Cost Analysis도 했음 다른 고려 사항 Data location 클라우드에 데이터가 1TB 이상 있따면 compute하기 수월 Data privacy 데이터를 클라우드에 올릴 수 없다면 on-prem Security 추천 Resource Management 많은 사람들이 여러 GPU를 사용하고, 격리된 환경을 가져야 함 바라는 것 실험하기 쉽고, 적절한 의존성과 리소스 할당 Solutions 스프레드시트 파이썬 스크립트 SLURM Docker + Kubernetes Kubernetes-GPU-Guide 참고 최근 Kubeflow도 열심히 만들고 있음 Software specialized for ML use cases Distributed Training 모델을 학습할 때 여러 GPU를 사용해야할 경우가 있음 단순히 병렬로 실행하는 것은 덜 유용했으나, 데이터가 커지며 프레임워크의 병렬화가 좋아지고 있음 Data vs Model Parallelism Iteration time이 너무 길어지면, 데이터를 parallel하게 학습해야 함 모델 병렬처리는 복잡성을 증가시키며 거의 가치가 없음 Tensorflow Distributed 코드를 크게 재구성해야 함 점점 좋아지고 있음 Horovod Tensorflow, Keras, PyTorch 지원 MPI를 사용해 Tensorflow Distributed보다 덜 복잡함 Experiment Management 실험할 때 코드, 매개변수, 데이터셋을 추적하기 어려움 여러 실험을 하면 더 어려움 모델을 자동으로 저장하는 것도 좋음 Tensorboard 단일 실험에서 좋은 솔루션 다양한 실험을 커버하진 못함 Losswise, Comet.ml 등 Weights &amp; Biases : 실습때 활용할 예정, Document Sacred도 좋아요 Hyperparameter Optimization 하이퍼파라미터 서치할 때 유용 간단하게 정의해서 사용할 수 있음 keras와 잘 맞는 것들 talos Hyperas Hyperopt 머신러닝의 일반적 패키지와 다 맞음 Non-bayesian 알고리즘 SIGOPT : Hyperparameter 서비스 제공 Weights &amp; Biases도 제공 Microsoft의 nni도 좋음 All-in-one Solutions 최근 트렌드 : 하나의 시스템에 모두 넣음 개발(hosted notebook) AutoML을 사용해 실험을 여러 컴퓨터로 스케일링 버전 관리 및 결과 review 모델 배포 성능 모니터링 FBLearner Flow Michelangelo Cloud ML Engine(AI Platform) AI Platform의 일부가 됨 TFX Kubeflow Amazon SageMaker Neptune FLOYD Paperspace Lecture 5: Tooling and Experimentation Labs 코드로 실습함 Weights &amp; Biases Lecture 6 : Data Management 발표 자료 Introduction 대부분 딥러닝은 레이블된 많은 데이터가 필요함 스스로 플레이하는 RL이나 GAns은 제외하고 =&gt; 아직 실용적이지 않음 공공 데이터셋은 경쟁 우위를 가질 수 없음 그러나 스타트 포인트로는 괜찮음 Roadmap 1) Data Labeling 2) Data Storage Data(images, sound) and metadata(labels, user activity) 3) Data versioning 데이터셋은 사용자 활동, 추가 레이블을 통해 업데이트되며 모델에 영향을 미침 4) Data workflow 원본 데이터와 메타 데이터를 학습 가능한 데이터로 집계 및 변환 Data Labeling User Interfaces HIVE 사이트를 통해 bounding box, segmentation, keypoints, cuboids 등을 뽑음 Annotator는 매우 중요하고, 퀄리티 보장이 되야함 Sources of labor 사람을 고용함, 퀄리티 품질을 관리할 수 있음 그러나 비싸고 스케일업이 느림 크라우드 소싱 Service companies 기간 선정 좋은 사례(Godl standard)를 직접 라벨링 여러 경쟁 업체와 샘플 요청 Software 풀 서비스 데이터 라벨링은 비쌈 일부 회사는 소프트웨어 제공, 오픈소스도 있음(Prodigy) Data Storage Filesystem Storage의 기초 layer 기본 단위는 텍스트나 binary가 가능한 “file”이고 버전이 지정되지 않고 쉽게 덮어씀 네트워크로 연결 가능(ex: NFS) : 여러 머신이 네트워크를 통해 액세스 분산 가능(ex: HDFS) : 여러 곳에 저장하고 액세스 가능 Access 패턴이 주의! 빠르지만 병렬을 아님 Objest Storage S3, Google Storage API로 파일에 접근 가능, 직접 데이터 저장하지 않아도 됨 기본 단위는 “object”, 보통 binary, image, sound versioning이나 redundancy가 서비스에서 가능함 parallel이 가능하지만 빠르지 않음 Database 구조적인 데이터에 지속적이고, 빠르고, 확장 가능함 실제로 모든 것이 RAM에 있지만, software가 디스크에 모든 것을 기록하고 유실되지 않도록 함 기본 단위는 “row”, unique id, rows, columns binary 데이터가 아님 Postgres가 거의 90% 이상 좋을 수 있음, 비정형 JSON도 지원 Data Lake 다양한 소스에서 데이터를 가공해 저장 Redshift, BigQuery 등 정리하면 Data versioning DVC Data versioning에 특화된 솔루션 Data workflows Train시 아래와 같은 데이터를 가져와야 함 metadata : posting time, title, location log 그리고 classifiers를 돌려서 결과 도출 Task Dependencies A가 끝나고 B 작업을 해야하고, B 작업 이후에 C, D, E를 하고 C, D, E가 완료되면 F 진행하는 이런 의존 관계가 있음 Makefiles Makefile에서 파이프라인 연산 가능 Luigi and Airflow Distributing work Lecture 7 : ML Teams 발표 자료 Job Role ML DevOps 주로 소프트웨어 개발자 Role이고, SWE 파이프라인을 구축 Data Engineer ML Team과 적극적으로 SWE ML Engineer(머신러닝 엔지니어) ML 기술 + SWE 기술의 Mix 보통 SWE로 일하시던 분들이나 공부하고 발전 ML Researcher ML 전문가 ML team structures 대부분 팀은 SWE + ML 스킬이 혼합됨 팀의 모든 사람들이 어느정도 SWE 기술을 가져야 함 ML Researcher의 다른 견해 SWE와 통합하기 어려움 빠르게 움직이기 위해 알아야 한다 Data Engineer의 다른 견해 일부 조직은 ML팀과 같이함 별도의 팀이어야 함 Managing ML Teams Task가 쉬울지 어려울지 미리 말하는건 매우 어려움 ML progress는 nonlinear 무엇이 효과있을지 불명확해 계획이 어렵긴 함 프로젝트 타임라인 추정이 매우 어려움 Production ML은 research와 engineering 사이에 있음 research와 engineering의 문화 gap이 존재 면접 질문의 큰 틀 Why does the Residual Block in the ResNet architecture help with the vanishing gradient problem? Lecture 10 : Troubleshooting 발표 자료 딥러닝에서 디버깅은 매우 어렵지만, 중요한 주제 왜 어려울까? 당신의 퍼포먼스가 좋지 않은 이유 흔한 Dataset 구조 이슈 충분하지 않은 데이터 Class imbalances Noisy labels Train / Test 데이터셋의 분포가 다름 Key mindset for DL troubleshooting Pessimism(비관적으로 보자) 트러블슈팅 전략 Start simple 1) 간단한 아키텍쳐 선택 2) 일반적으로 좋은 default 사용 3) Input 정규화 4) 문제를 간단히하기 Implement &amp; debug 자주 발생하는 실수 Tensor의 shape가 안맞는 경우 전처리 실수(정규화를 잊거나 너무 많이하거나) loss function의 부정확함 train/eval 모드 설정을 까먹음 수치적 불안정 : inf, NaN 모델 구현을 위한 일반적인 조언 가벼운 구현 v1은 가능하면 간소하게 200줄 미만 이미 구현된 component 사용 tf.nn.relu(tf/matmul(W, x)) 대신 tf.layers.dense! tf.losses.cross_entropy 복잡한 데이터 파이프라인은 나중에! 일반 메모리에 올려서 진행 1) 모델 Run Debuggers for DL code Pytorch : 쉬움, ipdb 사용 tensorflow : trick(허나 2.0이 나왔으니 이 방법은 안해도?) graph session 만들고, training loop 갖고, tfdb 사용 Shape mismatch 와 대박 꿀팁 Casting issue OOM Tensor가 너무 크거나, 데이터가 많거나, operation에 중복이 있거나 등 Other common errors 2) single batch에 오버피팅 3) 알고있던 결과와 비교 Evaluate Test error = irreducible error + bias + variance + val overfitting 이건 train, val, test가 모두 같은 분포를 가진다는 가정을 가짐. 진짜 그럴까? 2개의 Validation 구성해보기 하나는 Train에서 샘플링, 하나는 Test에서 샘플링 결과 분석 정리하면, Test error = irreducible error + bias + variance + distribution shift + val overfitting Improve model/data 1) 언더피팅인 경우 ConvNet, ResNet, 파라미터 튜닝 등을 함 2) 오버피팅인 경우 H ~ J는 추천하지 않음 데이터 추가하고, weight decay 추가, data augmentation도 했음 3) 분포 이동 Error analysis Domain adaptation 레이블이 없는 데이터 또는 제한된 레이블이 있는 데이터가 있는 상황에서 source 분포를 학습해 또다른 target 데이터를 일반화하는 기술 레이블된 test 데이터가 적을 경우, 유사 데이터가 존재할 경우 유용 4) Re-balance datasets test set보다 val이 더 좋아보이면, 오버피팅일 수 있음 보통 작은 val set 또는 많은 하이퍼 파라미터 튜닝에서 발생함 그렇다면, val data를 다시 수집해야 함 Tune hyper-parameters 어떤 파라미터가 민감한지 작성해보고 조정 Method 1: manual 하이퍼파라미터 최적화 Method 2: grid search Method 3: random search Method 4: coarse-to-fine Method 5: Bayesian hyperparam opt Lecture 12 : Testing and Deployment 발표 자료 ML Test Score Testing / CI Unit / Integration Tests 각각의 모듈과 전체 시스템을 테스트 Continuous Integration 테스트는 항상 모델이 deploy되기 전에 새로운 코드로 repository에 푸시되야 함 Saas for CI CircleCI, Travis, Jenkins Containerization(via docker) 테스트를 위한 격리된 환경 CircleCI / Travis Saas for continuous integration repository와 통합되서 테스트 docker container의 command로 job을 정의할 수 있고, 리뷰 결과를 저장할 수 있음 No GPU CircleCI has a free plan Jenkins / Buidkite 개인 하드웨어, 클라우드 등에서 CI하기에 좋은 옵션 GPU 사용하기 좋음 Very flexible Docker 도커에 이해하면 좋음 Docker vs VM Dockerfile and layer-based images Dockerhub and the ecosystem Kubernetes and other orchestrators Web Deployment REST API HTTP requests - response를 사용해 serving 웹서버가 띄워져 있고, prediction system을 call Option VM에 deploy Container에 deploy Serverless function에 deploy Cloud instance에 배포 단점 프로비저닝이 힘듬 인스턴스는 사용하지 않는 경우에도 비용 지불 Container에 배포 단점 컴퓨팅 시간이 아닌 자체 서버 가동 시간에 비용 지불 Serverless function으로 배포 단점 전체 배포 패키지는 500MB, 5분 이내 실행, 3기가 메모리 이하 등을 만족해야 함(AWS Lambda) CPU 연산만 가능 Model Serving 머신러닝 모델에 특화된 Web development options 종류 Tensorflow Serving(Google) Model Server for MXNet(Amazon) Clipper(Berkely RISE lab) Saas solutions like ALgorithmia Tensorflow Serving MXNet Model Server Clipper Algorithmia 핵심 정리 CPU inference를 수행할 경우, 더 많은 서버를 사용하거나 서버리스로 가는 방법이 있음 Dream : Lambda처럼 Docker를 쉽게 배포 모델의 요구 사항에 따라 Lambda 또는 Docker가 제일 좋음 GPU inference를 수행할 경우, TF Serving 및 Clipper 같은 adaptive batching 방법이 유용함 Monitoring 무언가 잘못되면 알람해줘야 함 Cloud provider는 최근에 모니터링 솔루션을 가짐 기록할 수 있는 모든 것을 모니터링할 수 있어야 함(예 : Data Skew) 실습에서 보여줌 Interchange ONNX Hardware / Mobile Problems 모바일 기기는 메모리가 작아서 연산하기 느림 network size를 줄이거나, trick, quantize weight해야 함 모바일 딥러닝 프레임워크는 full version보다 덜 발전함 네트워크 구조를 바꿔야할 수 있음 Tensorflow Options Tensorflow Lite 최근 솔루션으로 사용됨 operator가 제한되긴 함 Tensorflow Mobile 더 많은(그러나 전체는 아닌) operator가 있음 Mobile NVIDIA for Embedded Lecture 13 : Research Directions 발표 자료 연구 방향성에 대해 알려주려는 시간 다양한 연구 분야에 대해 쉽게 설명해주고, 참고하면 좋을 논문들을 제공했음 이 부분은 정리하기보다, 키워드를 캐치하고 추후에 따로 정리하는게 더 좋을듯 Reference Full Stack Deep Learning Homepage Full Stack Deep Learning Labs Github",
    "tags": "basic mlops",
    "url": "/mlops/2019/10/06/fullstack-deeplearning-bootcamp/"
  },{
    "title": "BigQuery UDF 사용하기",
    "text": "Google Cloud Platform의 BigQuery에서 udf 사용하는 방법에 대해 작성한 글입니다 이 글은 2019년 9월에 작성되었습니다 BigQuery temp udf, BigQuery persistent udf에 대해 다룹니다 아직 persistent UDF는 pre-release이므로 추후에 기능이나 방식이 변경될 가능성이 있습니다 BigQuery UDF UDF UDF는 User Define Function의 약자로 사용자가 정의한 함수 Python 같은 프로그래밍 언어에서 함수를 만드는 것처럼, SQL에서 함수를 만드는 것을 UDF라고 표현하기도 함 UDF의 종류 Temp UDF : 쿼리문 위에 정의해서 사용하는 방식 Persistent UDF : BigQuery의 Dataset에 저장해서 사용하는 방식으로 여러 사람과 사용할 경우 유용 UDF로 사용할 수 있는 언어 SQL 표현식 자바스크립트 Useful UDF 모음집 bigquery-utils bigquery-jslibs Temp UDF(SQL) 일반 UDF는 Temp udf로 쿼리문을 작성할 때 함께 작성해야 함 CREATE { TEMPORARY | TEMP } FUNCTION function_name ([named_parameter[, ...]]) [RETURNS data_type] AS (function_definition); RETURNS 뒤엔 리턴될 데이터의 타입(생략 가능하지만 명시적으로 적는 것을 추천) AS 뒤엔 함수 정의 정의 후 ;를 꼭 붙여야 함 SQL 표현식을 사용한 예시 CREATE TEMP FUNCTION add_three_and_divide(x INT64, y INT64) RETURNS FLOAT64 AS ((x + 3) / y); # 밑에는 Sample Data WITH numbers AS (SELECT 1 as val UNION ALL SELECT 4 as val) SELECT val, add_three_and_divide(val, 2) AS result FROM numbers; Temp UDF(Javascript) Javascript UDF 외부 javascript 파일을 참고하거나 직접 javascript 문법을 사용할 수 있음 심플한 문법을 사용하면 “ 하나만 사용해도 되고, snippet이거나 여러 줄이면 triple-quoted(“””) 사용 Best 사례나 한계점은 링크 참고 자바스크립트 문법을 사용할 경우 CREATE { TEMPORARY | TEMP } FUNCTION function_name ([named_parameter[, ...]]) RETURNS data_type LANGUAGE js AS \"\"\"body\"\"\"]; 예시 CREATE TEMP FUNCTION customGreeting(a STRING) RETURNS STRING LANGUAGE js AS \"\"\" var d = new Date(); if (d.getHours() &lt; 12) { return 'Good Morning, ' + a + '!'; } else { return 'Good Evening, ' + a + '!'; } \"\"\"; SELECT customGreeting(names) as everyone FROM UNNEST([\"Hannah\", \"Max\", \"Jakob\"]) AS names; 외부 자바스크립트 파일을 참고할 경우 Google Storage에 라이브러리 파일을 넣은 후 사용 해당 라이브러리에 있는 함수를 AS 뒤에 넣어서 사용 CREATE { TEMPORARY | TEMP } FUNCTION function_name ([named_parameter[, ...]]) RETURNS data_type LANGUAGE js OPTIONS ( library=['gs://bucket/library.js'] ) AS \"\"\"librarys_method()\"\"\"]; Persistent UDF Temp UDF는 쿼리를 작성할 때마다 위에서 정의해야 하는 불편함이 있음 따라서 이런 정의를 저장해두고, 활용하고 싶을 경우 Persistent UDF 사용 Dataset 아래에 UDF로 저장해서 활용함 CREATE 문 뒤에 TEMP 없이 바로 FUNCTION을 작성 Persistent UDF 생성하기(SQL) BigQuery udf 데이터셋 생성 multiply_inputs 함수 생성 CREATE OR REPLACE FUNCTION udf.multiply_inputs(x FLOAT64, y FLOAT64) RETURNS FLOAT64 AS (x * y); udf 데이터셋 확인 언어, 반환 유형, 정의, 인수 쿼리에서 UDF 사용하기 Dataset.function_name으로 사용 =&gt; 여기선 udf.multiply_inputs WITH numbers AS (SELECT 1 AS x, 5 as y UNION ALL SELECT 2 AS x, 10 as y UNION ALL SELECT 3 as x, 15 as y) SELECT x, y, udf.multiply_inputs(x, y) as product FROM numbers Persistent UDF 생성하기(Javascript) SQL에서 하는 방식과 동일 BigQuery udf 데이터셋 생성(이미 존재하면 생략) custom_greeting 함수 생성 CREATE OR REPLACE FUNCTION udf.custom_greeting(a STRING) RETURNS STRING LANGUAGE js AS \"\"\" var d = new Date(); if (d.getHours() &lt; 12) { return 'Good Morning, ' + a + '!'; } else { return 'Good Evening, ' + a + '!'; } \"\"\"; udf 데이터셋 확인 쿼리에서 UDF 사용하기 SELECT udf.custom_greeting(\"BYEON\") AS result 외부 자바스크립트 파일을 활용할 경우 temp UDF처럼 Google Storage에 저장하고 사용 Uber의 H3 함수를 사용해보는 예시 Github에서 자바스크립트 파일 다운로드 Raw 클릭 후 다른 이름으로 저장 Google Storage에 해당 파일 업로드 저는 geultto-udf라는 곳에 저장 BigQuery에서 함수 정의 CREATE OR REPLACE FUNCTION udf.geo_to_h3(lat FLOAT64, lng FLOAT64, resolution INT64) RETURNS STRING LANGUAGE js AS '''return h3.geoToH3(lat, lng, resolution)''' OPTIONS (library=[\"gs://geultto-udf/h3-js.umd.3.5.0.js\"]); udf 데이터셋 확인 쿼리 예시 SELECT udf.geo_to_h3(36.123123, 128.123123, 8) as h3 정리 CREATE TEMP FUNCTION을 사용하면 TEMP UDF 생성 CREATE FUNCTION을 사용하면 Persistent UDF 생성 Dataset 아래에 저장 CREATE OR REPLACE FUNCTION는 기존에 함수가 저장할 경우 수정함 UDF는 SQL, Javascript로 생성 가능 Javascript 활용시 단순 문법으로 생성 가능하고, 자바스크립트 라이브러리 파일을 사용해 생성 가능 공식 문서 : 권장 사항, 한도, 제한사항이 있으니 꼭 확인! Reference Standard SQL User-Defined Functions New in BigQuery: Persistent UDFs Persistent UDFs + BQ GIS = ♥ bigquery-utils bigquery-jslibs",
    "tags": "BigQuery gcp",
    "url": "/gcp/2019/09/20/bigquery-udf/"
  },{
    "title": "Python SimPy 사용법 - 파이썬으로 시뮬레이션 만들기",
    "text": "Python SimPy 프레임워크를 사용해 파이썬으로 시뮬레이션 만드는 방법에 대해 작성한 글입니다 SimPy SimPy는 process 기반 discreate-event 시뮬레이션 프레임워크 코루틴을 잘 사용한 예시 SimPy의 process는 Python 제네레이터 함수로 정의되고, 고객, 차량 에이전트 같은 active component를 모델링할 때 사용될 수 있음 제한된 용량을 모델링하기 위해 shared resource도 제공함(서버나 체크아웃 카운터, 터널 등) 이론적으로 SimPy로 continuous simulation을 할 수 있지만, 도와주는 기능은 없음 반면 SimPy는 공유된 자원끼리 상호작용이 없는 고정된 크기의 시뮬레이션엔 오버 스펙임 SimPy Document 영상 자료 소스코드 : Bitbucket 짧은 예제 2개의 시계가 다른 time interval로 움직이는 예제 import simpy def clock(env, name, tick): while True: print(name, env.now) yield env.timeout(tick) env = simpy.Environment() env.process(clock(env, 'fast', 0.5)) env.process(clock(env, 'slow', 1)) env.run(until=2) 설치 pip3 install simpy Introduction 사실 SimPy는 단순한 비동기 이벤트 dispatcher임 주어진 시뮬레이션 시간에 이벤트를 생성하고 예약함 이벤트는 우선 순위, 시뮬레이션 시간, event id에 의해 정렬됨 이벤트는 콜백 list를 가지고 있으며, event loop에 의해 실행됨 이벤트는 return value를 가질 수 있음 구성 요소 1) Process 2) Resource 2-1) Resources Resource PriorityResource PreemptiveResource 2-2) Containers 2-3) Stores Store FilterStore PriorityStore 3) Environment 1) Process customer, vehicles 등 일반적인 agent를 모델링 순서가 있는 형태의 프로세스 Event 인스턴스를 yield하는 제네레이터 함수를 만듬 Generator 형태로 생성 제네레이터 내부에서 특정 activity가 수행될 때 resource에서 request를 날려 resource를 일정 시간동안 사용 process의 이벤트 시간이 모두 지나면, 이벤트의 value를 받을 수 있음 중요한 이벤트 유형 : Timeout 이 유형은 일정 시간이 지난 후 트리거됨. 주어진 시간동안 휴면 상태를 유지함 Environment.timeout()으로 사용할 수 있음 import simpy def car(env): \"\"\" 자동차 프로세스 주차하고 여행을 떠남 parking과 driving 상태를 스위칭함 \"\"\" while True: print('Start parking at %d' % env.now) parking_duration = 5 # 환경에서 timeout 이벤트를 발생시킴(parking_duration동안 휴면) yield env.timeout(parking_duration) print('Start driving at %d' % env.now) trip_duration = 2 yield env.timeout(trip_duration) env = simpy.Environment() env.process(car(env)) env.run(until=15) 2) Resource 용량이 제한된 일종의 컨테이너 프로세스는 자원에 무언가를 넣거나 얻으려고 시도함 리소스가 꽉 찼거나 비어있으면 대기열에서 대기해야 함 모든 리소스엔 최대 용량과 두 개의 대기열이 있음 대기열 1개는 리소스를 넣기 위한 프로세스용 : put ⇒ request 다른 하나는 꺼내기를 위한 프로세스용 : get ⇒ release 프로세스가 resource에 request하고, 사용한 후 release함 예를 들어 주유소에 차량이 도착하고, fule pump가 되면 떠남 리소스 구조 BaseResource(capacity): put_queue get_queue put(): event get(): event 총 3가지로 구현됨 Resources Resource : 양수여야 하고 기본값은 1 현재 사용자 또는 대기중인 사용자 목록, 리소스 용량을 검색할 수 있음 import simpy res = simpy.Resource(env, capacity=1) def print_stats(res): print('%d of %d slots are allocated.' % (res.count, res.capacity)) print(' Users:', res.users) print(' Queued events:', res.queue) def user(res): print_stats(res) with res.request() as req: yield req print_stats(res) print_stats(res) procs = [env.process(user(res)), env.process(user(res))] env.run() PriorityResource 프로세스가 각 요청에 우선 순위를 제공할 수 있음 더 중요한 요청 먼저 리소스에 액세스함 숫자가 작을수록 우선 순위가 높음 import simpy def resource_user(name, env, resource, wait, prio): yield env.timeout(wait) with resource.request(priority=prio) as req: print('%s requesting at %s with priority=%s' % (name, env.now, prio)) yield req print('%s got resource at %s' % (name, env.now)) yield env.timeout(3) env = simpy.Environment() res = simpy.PriorityResource(env, capacity=1) p1 = env.process(resource_user(1, env, res, wait=0, prio=0)) p2 = env.process(resource_user(2, env, res, wait=1, prio=0)) p3 = env.process(resource_user(3, env, res, wait=2, prio=-1)) env.run() # p3이 p2보다 늦게 리소스 요청했지만 우선 순이가 높아 더 일찍 리소스 사용함 PreemptiveResource 종종 새로운 요청이 기존 자원을 내쫓고 점유해야할 수 있음 PriorityResource에서 상속하고 preempt flag를 추가 request함(True로) PreemptiveResource는 preemption보다 priorities가 더 우선순위가 높음 low priority, preemptive가 high priority를 뛰어넘을 수 없음 import simpy def resource_user(name, env, resource, wait, prio): yield env.timeout(wait) with resource.request(priority=prio) as req: print('%s requesting at %s with priority=%s' % (name, env.now, prio)) yield req print('%s got resource at %s' % (name, env.now)) try: yield env.timeout(3) except simpy.Interrupt as interrupt: by = interrupt.cause.by usage = env.now - interrupt.cause.usage_since print('%s got preempted by %s at %s after %s' % (name, by, env.now, usage)) env = simpy.Environment() res = simpy.PreemptiveResource(env, capacity=1) p1 = env.process(resource_user(1, env, res, wait=0, prio=0)) p2 = env.process(resource_user(2, env, res, wait=1, prio=0)) p3 = env.process(resource_user(3, env, res, wait=2, prio=-1)) env.run() Containers 미분화된 대량 생산, 소비를 모델링할 수 있음 물 같은 연속적인 것과 사과와 같은 불연속적인 것이 사용 가능 주유소의 가스 / 휘발유 탱크를 모델링할 때 사용할 수 있음 현재 level을 검색할 수 있음. capacity(GasStation.monitor_tank() import simpy class GasStation: def __init__(self, env): self.fuel_dispensers = simpy.Resource(env, capacity=2) self.gas_tank = simpy.Container(env, init=100, capacity=1000) self.mon_proc = env.process(self.monitor_tank(env)) def monitor_tank(self, env): while True: if self.gas_tank.level &lt; 100: print('Calling tanker at %s' % env.now) env.process(tanker(env, self)) yield env.timeout(15) def tanker(env, gas_station): yield env.timeout(10) # Need 10 Minutes to arrive print('Tanker arriving at %s' % env.now) amount = gas_station.gas_tank.capacity - gas_station.gas_tank.level yield gas_station.gas_tank.put(amount) def car(name, env, gas_station): print('Car %s arriving at %s' % (name, env.now)) with gas_station.fuel_dispensers.request() as req: yield req print('Car %s starts refueling at %s' % (name, env.now)) yield gas_station.gas_tank.get(40) yield env.timeout(5) print('Car %s done refueling at %s' % (name, env.now)) def car_generator(env, gas_station): \"\"\" 차량을 생성하는 함수 \"\"\" for i in range(4): env.process(car(i, env, gas_station)) yield env.timeout(5) env = simpy.Environment() gas_station = GasStation(env) car_gen = env.process(car_generator(env, gas_station)) env.run(35) Stores : object의 생산과 소비를 모델링 할 수 있음 일반적인 Store import simpy def producer(env, store): for i in range(100): yield env.timeout(2) yield store.put('spam %s' % i) print('Produced spam at', env.now) def consumer(name, env, store): while True: yield env.timeout(1) print(name, 'requesting spam at', env.now) item = yield store.get() print(name, 'got', item, 'at', env.now) env = simpy.Environment() store = simpy.Store(env, capacity=2) prod = env.process(producer(env, store)) consumers = [env.process(consumer(i, env, store)) for i in range(2)] env.run(until=5) FilterStore : store에서 object를 가져갈 때 커스텀 함수를 사용하는 store 다양한 속성이 있는 기계 공장을 모델링함 FilterStore의 items에 값을 저장함!! import simpy from collections import namedtuple Machine = namedtuple('Machine', 'size, duration') m1 = Machine(1, 2) # Small and slow m2 = Machine(2, 1) # Big and fast env = simpy.Environment() machine_shop = simpy.FilterStore(env, capacity=2) machine_shop.items = [m1, m2] # Pre-populate the machine shop def user(name, env, ms, size): machine = yield ms.get(lambda machine: machine.size == size) print(name, 'got', machine, 'at', env.now) yield env.timeout(machine.duration) yield ms.put(machine) print(name, 'released', machine, 'at', env.now) users = [env.process(user(i, env, machine_shop, (i % 2) + 1)) for i in range(3)] env.run() PriorityStore : 우선 순위에 따라 필터링할 수 있는 store inspector 프로세스가 maintainer 프로세스가 우선 순위에 따라 복구되도록 기록함 import simpy env = simpy.Environment() issues = simpy.PriorityStore(env) def inspector(env, issues): for issue in [simpy.PriorityItem('P2', '#0000'), simpy.PriorityItem('P0', '#0001'), simpy.PriorityItem('P3', '#0002'), simpy.PriorityItem('P1', '#0003')]: yield env.timeout(1) print(env.now, 'log', issue) yield issues.put(issue) def maintainer(env, issues): while True: yield env.timeout(3) issue = yield issues.get() print(env.now, 'repair', issue) _ = env.process(inspector(env, issues)) _ = env.process(maintainer(env, issues)) env.run() 3) Environment simulation 하려는 환경 환경은 이벤트 목록에 이벤트들을 저장하고 현재 시뮬레이션 시각을 추적함 예시 차량 인스턴스 예시 import simpy class Car: def __init__(self, env): self.env = env # Start the run process everytime an instance is created. self.action = env.process(self.run()) def run(self): while True: print('Start parking and charging at %d' % self.env.now) charge_duration = 5 # We yield the process that process() returns # to wait for it to finish yield self.env.process(self.charge(charge_duration)) # The charge process has finished and # we can start driving again. print('Start driving at %d' % self.env.now) trip_duration = 2 yield self.env.timeout(trip_duration) def charge(self, duration): yield self.env.timeout(duration) env = simpy.Environment() car = Car(env) env.run(until=15) 자동차가 완전히 충전될 때까지 기다리지 않고 도중에 충전 중단하고 운전할 경우 interrupt()를 호출해 실행 중 프로세스를 중단할 수 있음 import simpy def driver(env, car): yield env.timeout(3) car.action.interrupt() class Car: def __init__(self, env): self.env = env self.action = env.process(self.run()) def run(self): while True: print('Start parking and charging at %d' % self.env.now) charge_duration = 5 # We may get interrupted while charging the battery try: yield self.env.process(self.charge(charge_duration)) except simpy.Interrupt: # When we received an interrupt, we stop charging and # switch to the \"driving\" state print('Was interrupted. Hope, the battery is full enough ...') print('Start driving at %d' % self.env.now) trip_duration = 2 yield self.env.timeout(trip_duration) def charge(self, duration): yield self.env.timeout(duration) env = simpy.Environment() car = Car(env) env.process(driver(env, car)) env.run(until=15) 다음 글에선 같이 시뮬레이션 예제를 구현해보는 글을 작성할 예정입니다 :)",
    "tags": "simulation data",
    "url": "/data/2019/09/02/simpy-intro/"
  },{
    "title": "Constraint Programming의 이해",
    "text": "Constraint Programming에 대해 작성한 글입니다 Constraint Programming 2가지 흥미로운 포인트 Computational paradigm 제약 조건을 사용해 각각의 변수가 취할 수 있는 값들의 집합을 줄임(=Search Space를 줄임) 어떤 솔루션에도 나타나지 않을 값을 제거 Modeling methodology 문제의 구조를 가능한한 명확하게 전달 문제의 세부구조를 표현 가능하면 solver에게 많은 정보를 제공 구체적 예시 체스판에서 8개의 퀸이 있고 서로 공격함 같은 column, row, diagonal에 있으면 서로 공격 upper and lowd diaagonal로 이동 가능 여왕이 점점 배치되며 feasible solution이 없는 것을 볼 수 있음, 다른 곳에 두었어야 함 Choice란? 엄청 많은 선택이 있음 일단 처음에 우리가 다루는 선택은 특정 값을 할당하는 것 선택이 잘못될 수 있음 최적화에서 자주 잘못되곤 함 solver가 역추적해서 다른 값을 선택해야 함 Coloring a Map 인접한 두 지역에 같은 색을 받지 않도록 지도에 색을 칠하기 4개의 Color 모든 map은 4가지 색으로 칠해져야 함 컴퓨터로 입증된 최초의 정리 constraint programming을 어떻게 적용할까? Decision variable를 선택 Decision variable의 constraints를 표현 Decision variables : 각각의 나라에 색칠된 색 Decision variables의 도메인 4 color Constraint 인접한 나라에 같은 색을 사용할 수 없음 Branch and prune pruning 가능한만큼 search space를 줄임 branching 문제를 하위 문제로 분해하고 하위 문제를 탐색 Constraint의 역할 feasibility checking pruning The propagation engine constraint programming의 core 간단한 알고리즘 너 feasible이니? =&gt; T/F 서치 스페이스 줄임 Feasibility checking 변수의 값이 주어지면 제약 조건을 충족시킬 수 있는지 확인 Pruning 만족할 경우 도메인의 값을 결정 알고리즘은 각 제약조건마다 전용 알고리즘이 존재 Send More Money Problem 복면산은 문자나 그림으로 표현된 숫자를 맞히는 것 숫자는 겹치지 않고, 맨 앞자리 숫자는 0이 아님 Decision Variables 글자 값을 나타내는 각 글자들을 변수로 지정 carry마다 변수 존재 Propagation engine constraint programming solver의 핵심 간단한 고정된 알고리즘 Global Constraint Constraint programming의 중요한 특징 다양한 어플리케이션에서 발생하는 하위 조합을 표현 Global constraint도 마찬가지로 하위 조합 표현 Global Constraint가 존재하는 이유 constraint c(x1, …xn) x1 in D1, … xn in Dn 1) Feasibility testing 제약 조건이 유지되도록 변수 도메인에서 값을 찾을 수 있는지 테스트 2) Pruning D_{i}의 v_{i}가 주어졌을 때, x_{i}=v_{i}를 만족하는 솔루션이 있는가? constraint를 만족하는 variable domain을 찾을 수 있는가? 이런 질문과 함께 search space 정리함 Global Constraint의 종류 1) AllDifferent 모든 값이 다른 제약 조건 2) Table Constraints 가능한 집합을 Table로 만들고, 특정 조건을 만족하는 경우를 찾음 모든 변수에 대한 카테시안 곱의 하위 집합을 명시 패러다임 도식화 여러 Constraint Global Constraint 스도쿠 Constraint Programming을 활용해 스도쿠를 풀 수 있음 Graph Coloring Constraint Programming에서 집중하는 것 Feasibility Optimize 하는 방법 Solve a sequence of satisfaction problems Find a solution impose a constraint that the next solution must be better 최적의 솔루션을 찾도록 보장하는 경우 이론적으론 알지만 실제로 찾는 시간이 너무 오래 걸림 새로운 제약 조건이 search space를 감소할 때 효과적(스케쥴링 문제가 그 예시) Constraint Programming modeling techniques Symmetry breaking 많은 문제들이 대칭적임 검색 공간의 대칭 부분을 탐색하는 것은 쓸모없음 symmetries의 종류 variable symmetries value symmetries Balanced Incomplete Block Designs(BIBDs) Input : (v,b,r,k,l) Output : 0/1로 구성된 행렬 Constraint : row의 수는 r, 컬럼의 수는 k, 스칼라곱은 I Why BIBDs? combinatorial design 예제 variable symmetries 예제 행과 열을 swap할 수 있음 =&gt; 대칭 variable symmetries를 해결하는 방법 변수에 순서를 정함 row symmetries를 고려 사전(lexicographic) 제약을 추가함 lexicographic ordering 첫 값을 비교해서 어떤 것이 큰지? 같으면 그 다음값 a: 0 1 1 0 0 1 0 / b: 1 0 1 0 1 0 0 =&gt; a&lt;=b a: 1 1 1 0 0 1 0 / b: 1 0 1 0 1 0 0 =&gt; a&gt;=b Scene Allocation 영화의 scene을 찍음 배우들은 scene에서 연기하며, 하루에 k개의 scnes을 찍을 수 있음 배우들 고용시 하루마다 비용이 나감 Objective minimize the total cost Value symmetries day는 교환 가능 1일차랑 2일차 장면을 모두 바꿔도 됨 s가 solution일 때, p(s)는 s의 날짜가 순열 p에 의해 순서가 배열된 솔루션임 이 symmetries를 없애려면? 하루에 1개식만 생각 월요일 화요일 등 모두 같음(단, 급여가 달라지면 다른 방법..) Redundant constraints(중복 제약) Motivation Semantically redundant do not exclude any solution computationally significant reduce the search space redundant constraint를 어떻게 찾을까? 모델에 포착되지 않는 솔루션의 속성을 나타냄 모델에 constraint을 더 많이 넣을수록 컴파일 속도가 더 빨라짐 Constraint Programming의 중요한 측면 Magic Series Decision variables는 발생 횟수를 나타내고 발생 횟수가 제한됨 중간 값이 얼마인지 알면 더 강한 제약조건을 나타낼 수 있음 Redundant constraints의 역할 1) First solution의 property 표현 다른 제약조건의 전파를 강화 2) Second gloval view 제공 존재하는 제약조건 결합 커뮤니케이션 증가 Market Split Problems Surrogate constraint 존재하는 제약조건 결합 Car Sequencing redundant constraint이 존재 조립 공정에서 일정 순서가 있고, 특정 옵션이 있음 생산 유닛당 capacity constraint가 존재 : 5대 중 최대 2대는 moonroof가 필요할 수 있음 capacity constraint이 충족되도록 자동차 생산 차를 몇대 생산할 수 있는지 추론가능하고, 10대에서 2대 생산 =&gt; 이제 8대 =&gt; 이렇게 계속 실제 자동차수를 제한할 수 있음. 본질적으로 알고있는 제약임 Dual Modeling 문제를 모델링할 때 다양한 방법이 존재할 수 있음 같은 decision variable을 사용하지 않아도 됨 두 모델은 서로 보완하는 강점을 가질 수 있음 그들 사이에 선택하는게 어려움 어떤 모델은 특정 제약조건을 잘 표현하고, 다른 모델은 다른 제약조건을 잘 표현 따라서 dual modeling은 문제의 여러 모델을 설명하고 제약 조건과 연결하는 아이디어에서 시작 CP 8 Reference Coursera Discrete Optimization",
    "tags": "optimization data",
    "url": "/data/2019/09/01/constraint_programming/"
  },{
    "title": "if kakao 2019 2일차 후기",
    "text": "if kakao 2019 2일차 세션을 들으며 메모한 글입니다 금융사기 잡는 카카오뱅크의 데이터 사이언스 전상현님 금융사기를 어떻게 잡는지, 시도했던 것들을 소개 카카오뱅크의 데이터 사이언스 소개 Anomaly Detection, Topological, Network Analysis 등 7월에 고객 1000만명 돌파 2030세대 전체의 45%가 카카오뱅크 이용 카카오택시 내역을 신용평가에 반영하고 있음 은행 업무 중 발생하는 데이터 문제를 해결 데이터에서 인상이트 찾기 예 : 모임통장 이후 카뱅의 변화 분석 기계학습/딥러닝 모형 개발 보이스피싱 탐지 모형 개발 이상거래 탐지 모형 개발 금융사기와 FDS 보이스피싱은 점점 늘고 있고, 금융사기도 지능화되고 있음 스미싱, 보이스피싱, 파밍, 대포통장 등의 방법 FDS(이상금융거래 탐지 시스템) 앱 원격 조종? 기기 정보 / 앱 실행 정보 / 악성 앱 설치 여부 거래가 없던 계좌에 큰 금액 이체? 거래 내역 카카오뱅크는 금융사기 문제는 어떻게 접근하고 있나? 카카오뱅크는 전자금융 FDS와 체크카드 FDS가 존재 전자금융 자체 개발 인터넷 은행, 카뱅만의 상품, 활동적인 고객이 존재 빠른 사기 탐지, 더 정확한 탐지, 신종 사기 대응해야 함 더 맞는 규칙 기반 모형 + 머신러닝 모형 머신러닝 지도학습 극단적인 데이터 불균형, 정상 99.9% 사기 0.1% 데이터 불균형이 심하면 사기 예측 모형이 동작하지 않음 고객 정보, 금융 활동, 앱 활동을 토대로 DNN =&gt; 모든게 정상이라 예측함. 사기를 잡을 수 없음 따라서 언더샘플링과 오버샘플링을 통한 정상/사기 비율 개선 정상은 언더, 사기는 오버 딥러닝 사용시 정상과 사기의 Loss 비율을 조절, 정상 loss : 사기 loss = 1:10 ANN을 통해 정상 근처의 데이터만 샘플링 사기 데이터 생성을 위해 VAE 사용(그냥 늘리거나 통계적으로 할 수 있지만 VAE 사용) 시계열 이벤트 데이터 처리 : 앱 활동 =&gt; 로그인 =&gt; 간편이체 =&gt; 대출금조회 =&gt; 마이너스대출 =&gt; 로그아웃 앱 활동은 FastText로 벡터화 계좌이체, 간편이체, 계자조회(잔고&lt;0), 계자조회(잔고&gt;0), 해외송금, 계좌개설, 마이너스대출, 신용대출 등 벡터화한 이벤트는 1D CNN을 통과시킴, 속도 때문에 1D CNN씀 =&gt; 정상과 사기 비율 맞춤 비지도학습 Auto-Encoder를 통한 이상거래 탐지 원본 데이터를 넣으면 z로 압축하고, 그 압축된 데이터를 복원하는 모형 정상 데이터로 학습한 뒤, 이상 데이터가 들어오면 X와 X’의 차이가 커지는 것으로 이상 거래를 탐지 z를 같이 고려한 Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection 보다 세밀하게 이상거래를 탐지할 수 있지만 충분하진 않음 실제 정상적인 행동이지만 사기도 많음 성능 개선 정도 데이터 샘플링, Loss 조절, 이벤트 벡터화, 1D CNN, Auto Encoder를 통해 Recall 50% 상승, Precision 20% 상승 Recall은 쉽지만 Precision 올리는게 어려워서 더 개선할 예정 정리 탐지 모형 개발은 금융 사기 탐지의 극히 일부분 복잡한 딥러닝 모형이 답을 주진 않음 모형도 중요하지만 변수 선택 및 전처리가 더 주용 서비스 반영은 또 다른 큰 산 요새 실험중인 기법들 TDA(Topoglocial Data Analysis) 이상탐지, 기계학습 모형 설명, 시각화 Graph Deep Learning 그래프 데이터, 노드와 엣지의 특성 반영 Label Propagation 준지도학습, 라벨의 일부만 알 때 TDA 2007년 mapper 알고리즘 발표 후 주목됨 이상탐지에 많이 사용되고, 의학/금융 분야에서 활용 AYASDI라는 회사가 많은 특허 보유함 Google에 TDA 치면 한국분 1분 나오는데, 카뱅분임 핵심 Topological - Data - Analysis 데이터가 갖는 모양으로 모양의 의미를 분석 테이블 데이터가 가운데 이미지처럼 그래프로 나오고, 어느쪽은 정상 1형 당뇨 등등이 구분됨 원본 데이터를 가로축 따라 데이터 나누고(데이터 영역이 겹침), 그 데이터를 kmeans로 클러스터링 =&gt; Topology 구성 노드 : 각 영역의 데이터 군집 연결 : 공통 데이터가 존재하는 인접한 군집 k-means와 topological 비교 데이터 특성을 잘 표현하는 군집과 관계를 찾아냄 Outlier 또한 군집으로 잘 찾아냄 TDA for Credit Card Fraud Detection 신용카드 데이터에 대해 TDA 해봄 색이 진할수록 군집에 사기가 많다는 것 뉴럴넷은 사기가 있다 없다만 알 수 있는데, 몇가지의 사기냐는 어려움. TDA는 어떤 형태의 사기가 있다는 것을 파악할 수 있음 풀리커넥티드 들어가기 전 데이터로 TDA를 해봤는데, 색이 진할수록 사기가 많음 이제 더 분석해서 결과를 얻을 예정 질문 새로운 사기 대응은 어떻게 하는가? 비지도로 탐지할 수도 있고, 평상시에 데이터 분석하며 할 수 있고, 새로운 사기가 올라오면 금융권끼리 사례 공유가 됨 Autoencoder의 쓰레솔드 어떻게 줬는지? 실제로 오토인코더로 탐지하지 않음. 정상과 사기가 잘 분리가 안됨. 탐지는 안되고 feature 정도로만 사용 쓰레솔드를 다 조절해봤는데, 오탐이 너무 높았음 이걸 통해 정상이나 아니다 나누진 않음 후기 Anoamly Detection하는 방법을 깔끔하게 말씀해주셔서 만족스러운 발표! TDA란 것도 알게되서 유익했음 Buffalo: Open Source Project for Recommender System 김광섭님 버팔로의 특징 지원 알고리즘 뛰어난 생싼성과 성능 편의 기능 실험할 때 필요한 기능들 추천시스템과 버팔로의 관계 왜 오픈소스를 개발했는가? 추천시스템을 보통 협업 필터링, 무비렌즈 데이터에 대해 주로 이야기함 사용자에게 콘텐츠 혹은 정보를 소비하는 경험을 제공하는 기술 카카오 추천시스템 토로스의 일부 기능 개발 배경 2010년 초반에 오픈 소스 프로젝트가 거의 없었음 특히, 원하는 크기의 데이터를 적절한 시간에 효과적인 자원으로 분석할 수 있는 프로젝트가 없었음 초기엔 Matrix Factorization 모듈 쓰기 편하고 실용적이고 스케일러블, 만족스러운 품질의 결과를 줄 라이브러리 6년정도 된 프로젝트인데 이제 공개한 것 버팔로 알고리즘 Alternating Least Square Bayesian Personalized ranking matrxi factorization Word2vec CoFactors Matrix Factorization 행렬로 표현된 데이터를 더 작은 차원의 행렬로 분해하는 방법 장점 데이터의 압축 숨겨진 특성의 활용 4개의 MF 알고리즘을 제공하는데 왜 4개나 필요한가? 행렬 분해할 때 목적 함수의 정의, 데이터의 구성과 변형에 따라 서로 다른 성질의 은닉 벡터를 구할 수 있음 여러 행렬 분해 알고리즘을 사용해 다양한 특질을 확보할 수 있음 버팔로는 원하는 크기의 데이터를 적절한 시간에 효과적인 자원으로 수행할 수 있는 알고리즘 위주로 선택하며, 품질보다 확장성이 더 중요함 그러나 scalabiltiy &gt; quality가 항상 성립하는 것은 아님. 대부분 문제는 Scalability는 타협 가능하지 않고 Quality는 타협 가능함 Scalability가 중요한 이유 NNZ 20M : 0이 아닌 값이 이만큼 있다 품질은 어디까지 타협해야 할까? 다양한 알고리즘을 실험한 결과 체감상 차이가 미미하고 시스템 성능에 미치는 영향도 차이가 크지 않음 초반엔 조금만해도 성능이 오르지만 점점 하다보면.. 시간대비 효율이 안나옴 투자 대비 효율로 전환 긴 파이프라인으로 구성된 시스템 Ceiling Analysis 다른 알고리즘과 비교 - implicit, gmf, pyspark랑 비교 - 대비해서 빠름 - 자원을 효과적으로 쓰고 있는지도 테스트해봄 =&gt; latent vector 크기 고정하고 cpu core 하나씩 늘리며 얼마나 빨라지는지 봄) batch learning으로 잘라서 사용 가능 높은 생산성 유사 콘텐츠 추천과 개인화 추천에 대한 병렬처리 기능 제공함 from buffalo.algo.ali import ALS from buffalo.parallel.base import ParALS als = ALS.new('./ml20m.bin') als.most_similar('Starwars') #parallel par = ParALS(als) par.num_worker = 8 par.most_similar(['Starwars', 'Startrek']) 진행한 노력들 Python/C++ 코드 베이스와 병렬처리를 효과적으로 구현 쓰레드 작업할 때 job으로 쓰레드를 나누는데 job의 크기가 다 다름 병목이 누적되며 차이를 만듬 데이터 사이즈를 고려해 job을 배분하면 전체적 속도가 월등히 빨라짐 청크화된 데이터베이스를 사용해서, SSD에 내려둠 - 자세한 것은 Github repo 편의 기능 Validation Stream 데이터에서 Matrix Market 데이터로 변환 가능 데이터 순서 기록을 고려해 최근 데이터만 추가하는 등 Evaluation 어쩧게 할지 지정할 수 있음 Tensorboard 텐서보드 옵션을 지정하면 버팔로가 넘김 Hyper-parameter optimization 연구 목적 + 서비스 개발에도 중요한 부분 모든 머신러닝 알고리즘에서 하이퍼파라미터가 가장 중요하다고 해도 과언이 아닌데, 이런 기능을 내장하고 있음 hyperopt를 wrapping min, max가 이슈가 있어서 조금 수정한 것은 있지만 거의 hyperopt와 유사 모델 deploy도 됨 요약 C++/Python CPU 최적화, GPU 지원 효과적인 시스템 자원 활용 연구/개발 목적의 편의 기능 제공 벤치마크 표준화 예측, 유사도 맥락 고려 다양한 데이터 양질의 데이터 등 지속적인 오픈소스 활동 신규 작업: 실시간 추천 어플리케이션 서버 기존 성과물 개선 작업 : n2, buffalo 후기 추천할 때 한번 써봐야겠음 오픈소스에 기여하는 카카오 추천팀 멋짐..! 루카스님 리스펙 TensorRT를 이용한 카카오 OCR 모델 Inference 성능 최적화 이현수님 딥러닝 모델의 Trade off 연산량 증가하면 정확도 향상됨 정확도와 속도는 반비례 Inference 성능 향상을 위한 방법들 네트워크 구조 개선 정확도도 좋고, 연산도 효율적인 모델 학습 MobileNet, EfficientNet, Parallel decoding 학습된 모델의 inference 과정 최적화 compression quantization + a TensorRT Framework 활용 OCR Model 구조 Detection Model + Recognition Model로 구성 Text Detection Model Convolution + Unpool layer, unet Text Recognition Model CNN Feature Extraction + Self Attention(transformer) Connextionist Temporal Classification TensorRT 딥러닝 inference 최적화를 위한 NVIDIA 플랫폼 학습된 모델을 TensorRT로 가져와 Inference할 엔진 생성 최적화 방법 Precision Quantization 35 bit floating point -&gt; 16 bit / 8 bit inference 효율성 증가 데이터 전송 효율 증가 GPU 가속을 통한 연산 속도 향상 infenrece 정확도 감소 Quantization-aware Training Network Calibration(INT8) Layer &amp; Tensor Fusion 공유된 메모리로 연산 =&gt; 하나로 합침 Dependency가 없는 연산도 합쳐서 수행 Kernel Auto-Tuning GPU 종류, 배치 사이즈, Tensor shape 등을 기반으로 가장 효율적인 GPU 커널 선택 직접 Operation 구현해서 최적화 가능 TensorRT 사용 방법 학습된 모델 가져오기 Createing a network definition from scratch weight 파일 매핑 importing a model 모델을 파싱, 파싱할 수 있는 형태로 네트워크 구조 수정해야 함 이 부분 위주로 설명 TensorFlow 모델을 Import TensorFlow GRaph -&gt; UFF Network -&gt; TensorRT Engine -&gt; Engine을 이용한 Inference TensorFlow Model을 inference하기 모델 parsing을 위해서 확인할 사항들 TensorFlow 모델 그래프 확인 TensorRT에서 사용할 수 있는지 확인 shape 연산의 파라미터를 상수로 고정 즉, 가변 길이 Input이 아니라 고정 크기의 input 연산의 format 및 shape 확인 channel first format 사용 Dense / Conv1D 사용 힘듬 Plugin Layer를 이용한 연산 구현 TensorRT에서 사용 가능한 연산 구현해 대체 후기 중간에 잠시 일이 있어서 나갔다 왔지만, 발표의 흐름이나 정보 획득 관점에서 유익했음! 나중에 직접 사용할 때 발표 자료 보면 더 도움이 될 듯 비주얼 컴퓨팅을 활용한 카카오맵 박영욱님 후기 맥북 배터리 문제로 메모하진 못했지만, 친절한 목소리톤이라 듣기 좋았음 카카오맵에 이런 다양한 기술이 있구나! 싶었던 세션 카카오 대리 시공간 데이터를 이용한 강화학습 및 최적화 조창민님 어떻게 문제해결을 했는지 그 과정을 공유하려고 함 카카오모빌리티 AI 파트 소개 엔지니어링 스킬이 중요 실서비스, ML, 통계 등 실 서비스 장애 0건, 0.1초 미만 Latency 서비스 고도화는 곧 성과 매출 얼마나 높이고, 전환율을 높여달라 딥러닝은 하나의 방법이고 항상 최고의 방법은 아님 실제 서비스에 모델을 적용하고 불가능한 KPI를 달성하는게 목표. 모델링의 정확도 뿐만 아니라 안정적인 서비스가 목표 카카오 T 대리 서포터즈 카카오 T에서 대리 서비스 승객의 호출이 자동 배정 되는, 기사님을 위한 서비스 승객에겐 보다 빠른 연결 이동 요청 어느 위치에서 어디까지 가달라 이 서비스가 좋은 이유 초보기사분들에겐 판단이 어려움. 계속 콜을 봐야되고.. 집으로 가는 것도 고려해서 콜을 받아야 함 해결해야 될 문제 어떻게 이동 추천을 하지? 대리 기사님당 콜 수행 n건 이상 수익은 얼마 이상! 집 방향으로 이동을 어떻게 해야하지? 수요 예측 모델 대리 데이터 EDA 연도 상관없이 비슷하고, 월에 따라 분포 차이. 요일에 따라 콜 차이 심함, 시간의 영향도 받음 오늘 휴일인지 아닌지는 큰 차이가 없음 다음날이 휴일이냐에 따라 콜이 확 달라짐 지역 Kmeans로 클러스터링 콜의 분포에 따라 지역을 나눔 Auto correlation을 보니 1시간 30분 이전까지 사용하면 예측률이 오르겠구나!를 얻음 일단 다양한 모델 다 돌림 10분 단위 과거 콜 데이터로 예측 갑자기 서비스 담당자분이 다른 쪽으로 이동… 어쩌다보니 이걸 다 함 시뮬레이터 RL 환경 만들기 예측은 그냥 있는 데이터 쓰면 됨 하지만 누군가 어디로 이동시킬 때 더 잘나올까?는 예측할 수 없음 우리 지도를 싹 그리고 대리 시장 그리고..! 대리 기사님처럼 뿅! (타다에도 이렇게 진행하고 있음) 행정구역단위의 시뮬레이터 읍면동 같은 것.. 행정 구역으로 기사님 보내기엔 너무.. 작고 등등 택시 또는 지하철이 끊길땐? 바로 옆에 조금만 가면 되는데 굳이 강남으로? Geohash 기반 시뮬레이터 사각형으로 자를 수 있음 =&gt; 모두 일정함 대리 수요에 따라 서비스지역 한정 Geohash 기반 시뮬레이터 각 거리가 좌우는 같음, 대각선은 기존보다 더 이동함 H3 기반 시뮬레이터 어느 방향으로 가나 길이가 동일함 현재 이거로 만드는 중 랜덤하게 대리 기사를 출근. 어느 위치에 이동을 시킴. 여기로 가세요! 반경 검색! (몽고DB 사용) 그 후 배정 승객에게 가면 자동차를 몰고 목적지로 이동 가상 환경에선 과거 30분 수요 데이터, 미래 30분안에 도착 예정 Agent Agent의 현재 위치 등등을 모두 가지고 있음 모델링보다 시뮬레이터가 100배 더 어려움 Monte Carlo Tree Search 대리가 콜 받고 도착해서 또 콜받고.. 핵심은 콜 받는거 아닌가? 강릉까지 가서 내리면 집에 어케오지..? 택시타면 마이너스 계속 이어지도록 하는게 중요함 Tree구조 Tic Tac Toe 게임 만들었는데 비겼으면 비겼지 절대 지지 않음 문제는 경우의 수가 많아지면 학습 속도가 너무 느려짐 실제 적용까지 너무 힘듬.. 차원이 너무 많음 Tabular Learning State가 있고 Action 존재 random이랑 비교함 Q Learning 결론 : H3 위치만 넣었는데 의외로 잘됨 단, 많은 기사님들이 들어올 떄 argmax를 취하기 떄문에 몰려버림 이걸 해결해야 함 Reinforce Method 강화학습이 아닌 다른 알고리즘 softmax를 넣어 확률값으로 방향마다 나옴 Policy learning에서 q value의 구현에 따라 알고리즘이 바뀜 10.59 gradient가 클수록 학습을 많이 해야함. 상황은 좋은데 왼쪽으로 갈 확률은 적음 인싸이트는 얻었지만 모델의 업데이타 너무 느림! 게임이 끝날 때까지 기다려야 함 그리고 Blackbox Reinforcement with convex optimization 문제점 서비스 초기엔 딥러닝이 애매함 서비스 중후반엔 딥러닝 쓰고 AB Test 가능함 딥러닝이 디버깅이 힘듬 서비스 초기엔 더 명확한 것을 가지고, 빠르게 튜닝 가능한 알고리즘이 필요함 현실은 알 수 없음. 기사님들의 행동은? 시급제라 기사님들이 존버함 (ㅋㅋㅋㅋㅋㅋ아..진짜 똑같네…) RL에서 사용하는 Future Discounted Reward의 개념을 사용하고 싶지만 딥러닝은 사용하고 싶지 않음 리워드는 그대로 사용하되 방법만 변경 Convex Optimization! 수리적 모델 컨벡스는 마주치는 부분이 없어야 함 직선이면 리니어 프로그래밍 Simplex Optimization 인접한 근단점들의 목적함수의 값들을 계속 검토하며 최적해 구하는 방법 그냥 단순하게 최적점으로 이동하며 optimize 목표는 q value, 하루 끝났을 때 최대 수익을 maximize. 그걸 convex optimization 최종 공식 특정 지역의 콜 평균 - 해당 지역의 Agent 수 - 이동 거리 =&gt; 기사님들의 하루 수입을 최대화 beta들을 다 곱하고 beta를 최적화 콜이 나오는 지역으로 유도하고 다른 기사님들과 겹치지 않도록 최적화 시뮬레이션 결과 2배 이상, 실제로도 늘었다고 함 N-Step A3C 앞으로 방향은? N-Step 바로 앞단의 이익이 어떻게 되는가? Temporal Difference n번째까지 진행 이 알고리즘으로 슈퍼마리오 구현 N-step 64로, 64 프레임 이후에 떨어져 죽느냐? 등등에 따라 값이 변함 이제 A3C, Curiosity 등을 적용할 예정 질문 시뮬레이터 생성시 노드 단위까지 가는걸 했는지, geohash간 이동 거리 등으로 했는지 =&gt; geohash간 후기 제일 만족스러웠고, 재미있었던 세션 제가 회사에서 진행하고 있는 업무들이랑 너무 비슷하고, 문제 푸는 방식도 유사해서 잘 하고 있구나 생각했음. 따로 여쭤보니 시뮬레이터 만들대 SimPy 쓰셨다고 하셔서 동질감도.. (여기도 시급제라 이슈가..) 명함 주고받았는데 나중에 대화 한번 해봐도 좋을 것 같음",
    "tags": "lecture etc",
    "url": "/etc/2019/08/30/ifkakao-2019-review/"
  },{
    "title": "최적화로 바라본 Knapsack Problem",
    "text": "최적화(Optimization) 관점에서 본 Knapsack 문제를 정리한 글입니다 Cousera Discrete Optimization 2주차 강의 내용입니다 Greedy Algorithms 가장 귀중한 아이템을 먼저 가져가고 다음 배낭에 들어갈 수 있는 다음 것을 가져가는 방식 Idea : Take the most valuable items first 다른 Greedy 알고리즘과 비교 디자인하는 방식에 따라 매우 복잡할 수 있고, 품질이 달라질 수 있음 실현 가능한(feasible) 솔루션을 쉽게 구축할 수 있다고 가정 한번에 하나의 항목을 선택 여기서 Greedy의 의미를 다르게 할 수 있음 1) 무게에 기반해, 작은 물건부터 가져감 2) 가치가 있는 것부터 가져감 3) 가치의 밀도를 측정. dollars per kilogram 하나의 문제에 다양한 greedy 알고리즘이 존재할 수 있음 어떤 것이 다른 것보다 나을 수 있음 input에 의존적 장점 빠르게 구현 가능 매우 빠를 수 있음 단점 보통 퀄리티를 보장할 수 없음 퀄리티가 input에 따라 크게 다를 수 있음 문제의 feasibility는 쉬워야 함 항상 greedy로 시작할 수 있음 그 후 Constraint Programming Local Search mixed Integer Programming 방법 feasible solution을 확실하게 찾기 high quality 솔루션을 확실하게 만들기 다른 input에 로버스트한(이상치에 영향을 덜 받는) 솔루션 이상적으로 해당 솔루션이 최고임을 입증 Knapsack Modeling Knapsack 문제를 수학적 모델로 공식화하는 방법 item 조합 I이 있을 때 weight w_{i} value v_{i} capacity K for a Knapsack 다음 조건을 만족하는 부분 집합 찾기 maximum value를 갖는 그러나 Knapsack의 capacity를 초과하지 않아야 함 모델링하는 방법 Decision Variable(결정 변수) 선택 의사 결정의 대상 목적함수 식이나 제약조건 식에서 미지수로 나타나는 변수 이 변수들의 constraint(제약 조건)을 표현 문제에 구체적으로 정의되어 있음 Objective function(목적 함수)을 표현 각 해답의 퀄리티가 결정됨 Optimization model의 결과 declarative formulation how가 아니라 what을 명시 최적화엔 많은 모델링 방법이 가능함(오픈 마인드!) Decision Variables x_{i} item i가 선택되었는지 아닌지(1이 선택, 0은 미선택) Problem constraint 선택된 item은 knapsack의 capacity를 초과할 수 없음 \\sum_{i\\in I}w_{i}x_{i} \\le K Objective function 선택된 것들의 total value 추적 \\sum_{i\\in I}v_{i}x_{i} Exponential Growth 가능한 솔루션이 얼마나 있는지? Search space(Solution space) 모든 것이 feasible하진 않음(실현가능하진 않음) Knapsack의 capacity를 초과할 수 없음 How many are they? 2^∣I∣ 탐색하는데 드는 시간은? ∣I∣=50일 때 1,285,273,866 centuries 너무 오래걸림 Dynamic Programming 어떻게 최고의 Knapsack 솔루션을 찾을 수 있을까 다이나믹 프로그래밍을 이용해서! 최적화 테크닉으로 많이 사용됨 다양한 종류의 문제에서 잘 작동하며 특히 생물학에 계산에서 잘 작동 기본 원칙 분할하고 정복한다 bottom부터 계산한다 Convetions and notations I = {1,2,…n} O(k,j)는 capacity k and items [1..j]의 optimal solution을 뜻함 Recurrence Relations (Bellman 방정식) 우리가 해결하는 방법을 안다고 가정 O(k,j)를 풀고 싶음 만약 w_{j} \\le k일 때, 2가지 케이스가 있음 j를 선택하지 않으면 O(k, j-1)가 best solution j를 선택하면 best solution은 v_{j} + O(k-w_{j}, j-1) summary O(k,j) = max(O(k, j-1), v_{j} + O(k-w_{j}, j-1)) if w_{j} &lt;= k O(k,j) = O(k,j-1) otherwise 당연히 O(k,0) = O for all k Simple program recursive 피보나치 수를 찾기 위해 간단한 프로그램을 짤 수 있음 fib(n-1)은 fib(n-2)가 이미 풀려있어야 함 Dynamic Programming recursive 식으로 bottom up으로 연산함 start with zero items continue with one item then two tiems then all items 이 알고리즈의 복잡도 테이블을 모두 채워야 함 O(K n) polynomial? how many bits does K need to be represented on a computer? Relaxation, branch and bound Introduce branch and bound The value of relaxation Exhausitive Search(완전 탐색) 가능한 방법을 전부 만들어보는 알고리즘 Brute Force Search라고도 함 총 몇가지 나오는지 파악 =&gt; 너무 많으면 탐색 시간이 부족 경우의 세를 세어보고 완전 탐색해야 함 Branch and Bound(분기 한정법) 참고 자료 Iterative two steps branching 문제를 여러 하위 문제로 쪼갬 여러개의 작은 feasible subregion로 구성하고 그 과정이 subregion 각각에 대해 재귀 반복을 거쳐 tree 구조를 형성 bounding 하위 문제의 best solution을 찾고 optimistic estimate함 feasible subregion 내에서 최적해를 찾기 위해 upper and low bound를 빠르게 찾는 방법 maximization : upper bound minimization : lower bound Relaxation optimistic estimate를 찾는 방법 Optimization is the art of relaxation capacity constraint를 relax할 수 있음 어려운 문제를 해결하기 쉬운 문제로 근사한 것 Wikipedia : Linear programming relaxation Depth-First branch and Bound 왼쪽 테이블에서 하나씩 선택하고 안하고를 분기해서 답을 찾아감 A Knapsack Model 초콜렛을 조각낼 수 있음 특정 결정 변수의 fractional value를 취할 수 있음 Linear relaxation 나중 수업에서 배울 예정 Integrality 요구 조건을 완화함 모든 value를 정수(integer number)로 바꾸고 그것들의 분수로 이완 푸는 방법 용량이 소진되지 않은 상태에서 아이템 선택 마지막 아이템의 분수 선택 linear relaxation이 문제의 해답은 아님 relaxation으로 9 / 6 / 11.7 Search Strategies Branch and bound를 위한 탐색 전략 Search Strategies 트리를 탐색하기 위한 다양한 전략 Depth-first, best-first, least-discrepancy many others Depth-first 노드 추정이 best found solution보다 나쁠때 처리 Go deep when does is prune? 여태 찾은 best solution보다 나쁠 경우 is it memory efficient? exaggerate(과장됨) Best-First best 추정일 때 node를 선택 go for the best when does is prune? 여태 찾은 solution보다 모든 노드들이 나쁠 경우 is it memory efficient? exaggerate(과장됨) 모든 Tree를 탐색 =&gt; eponential time, space worst 사례 Least-Discrepancy Greedy heuristic을 신뢰 Good heuristic을 추정하면 실수가 적은 search tree가 binary 휴리스틱할 경우 branching이 왼쪽으로 분기 휴리스틱이 잘못되면 branching이 오른쪽으로 분기 Limited Discrepancy Search(LDS) 실수를 줄인다 실수로 증가하는 search space 탐색 휴리스틱을 더 신뢰 Explores the search space in waves no mistake one mistake two mistakes.. 이럴 때마다 search space가 줄어듬 trust the greedy heuristics when does it prune? best-first와 동일 is it memory efficient? depth-first와 best-first 대비 Relaxation and Search Discrete optimization heaven이 만든 match relaxation을 어떻게 하냐에 따라 속도가 개선될지 결정 Assignments Getting started 최적화에 silver bullet은 없음 다양하게 접근 High quality : 10*4+3*2=46 Scalable, lower quality solution 7*6=42 Reference Coursera Discrete Optimization",
    "tags": "optimization data",
    "url": "/data/2019/08/21/knapsack_problem/"
  },{
    "title": "파이콘(PyCon) 2019 세션 정리",
    "text": "파이콘 2019에서 세션을 들으며 메모한 글입니다 파이썬과 커뮤니티와 한국어 오픈데이터 박은정님 키노트 프로그래밍은 왜 하는가? 취미 먹고사니즘 개인의 성장 나의 커뮤니티에 기여하기 위해 팀포퐁 2011년 문제 의식 : 누구를 뽑을 것인가? 발견한 문제점 1) 객관적 자료 부족 2) 뽑은 국회의원을 잊고 지냄 국회정보 시스템에 정보는 다 있지만.. 어렵고 재미없음 어떤 일을 하는지, 누가 누군지 모르겠고.. 접근성 떨어지는 문서 검색 엔진이 접근할 수 없는 정책 국민 모두가 접근할 수 있어야 하지 않을까? 팀포퐁은 정치적 중립성, 자동화, 개방성의 가치를 가지고 결정이 됨 기술로 대한민국 정치를 뒤흔들자 직접 서비스를 만들며 성장하자 프로토타입, 의원 네트워크 분석 등 대한민국 정치의 모든 것이란 웹서비스가 생김 타임라인을 위해 D3 NLP PDF parsing 선거구 시각화를 위해 지도 데이터도 만듬 메르카토르 같은 투영법도 배움 국회의원/의안 데이터 REST API =&gt; 데이터를 퍼블릭에 공개 좋은 개발 문화 문서화 : 팀 내외와 소통하는 방ㅇ법 Git의 좋은 프랙티스 : 매너있게 소통 새 프로젝트에 새 도구 도입 탁상공론을 벗어나 working prototype으로 보여주기 느낀점 나와 내 주변을 바꿈 =&gt; 결국 세상을 바꾸게 되지 않을까 시간이 좀 걸려도 괜찮다. 모르는 건 배우면 된다 데이터를 가공해 오픈 데이터로 내놓는 것도 의미가 있다 KoNLPy 한국어 분석을 편리하게 하기 위해 시작 오픈 소스는 있나? 성능은 어떻게 다르지? 처음부터 커뮤니티 기여하겠단 생각은 아니었음 파이콘이 한국에서 처음 열린대! =&gt; 잘 패키징해서 공개! 사람들은 왜 KoNLPy를 사용했을까? 초보자 : 사용법이 쉬워서? 학생 : 보고 따라할 예시가 있어서? 실무자 : 다양한 구현체 간 성능 비교가 편해서? 외국인 : 문서가 영어로 쓰여 있어서? 환경적 요인 : 이 당시 파이썬이 한창 인기몰이 내가 필요한 도구는 내가 만들어 공유한다 생각지도 못한 도움을 주고 받을 수 있고, 가치있는 일이다 두번째 PyCon KR representation learning이 굉장히 핫해짐 word2vec, doc2vec을 파이썬 커뮤니티에 소개해볼까? KoNLPy를 공개했지만 토이데이터가 별로 없다 nsmc라는 한국어 영화평 데이터 주어진 영화평을 긍정 또는 부정으로 분류하는 데이터셋 IMDB 데이터를 벤치마크 내가 가진 기술이 대단하지 않아도 커뮤니티에 기여할 수 있다! 한국어 오픈 데이터 꼭 하고 싶었던 이야기 알파벳은 영어가 아니다 한글은 한국어가 아니다! 한글은 문자, 한국어가 언어 한국어 NLP, Korean NLP 용어 정의를 명확히 한국어 오픈데이터 생각보다 굉장히 많음 좋은 사례 1: KorQUAD QnA 데이터셋 충분한 양의 데이터 공개 리더보드까지 공유 라이센스 부분은 공유되지 않음 좋은 사례 2: KSS Single Speark Speech Dataset 박규병님 국내 최초의 음성 오픈 데이터 제법 많은 분량 라이센스가 뚜렷하게 명시되어 있어서 무엇을 할 수 있고 없는지가 명확함 왜 오픈 데이터가 중요한가요? 1) 벤치마크가 될 수 있음 머신러닝 모델들은 데이터와 지표가 같아야 모델간 서로 비교 가능 비교가 가능해지면 기술 발전이 옴 2) 누구나 바로 분석이나 모델링을 시작할 수 있음 프로그래밍에서 reinventing the wheel이 경계의 대상이 되듯 모두가 데이터 취득과 정제를 할 필요는 없음 데이터를 공개할 때 확인하면 좋은 점 사용자가 바로 다운로드해서 사용할 수 있는가? 원문에 갱인정보/저작권 문제는 없는가? 오픈데이터는 기술 발전을 위해 매우 중요하지만 개인정보와 저작권도 존중받고 지켜져야 함 가급적이면 라이센스를 꼭 명시 오픈 데이터를 사용할 때 확인하면 좋은 점 데이터가 충분히 큰가? 라이센스가 무엇인가? 재배포가 가능한가? 상업용으로 이용해도 되는가? 한국어 같이! 공유 :) 파이썬으로 구현하는 최적화 알고리즘 차지원님 발표 자료 Github 에너지 스타트업 개발자로 취업하며 적응할 때 어려움이 있었음 백그라운드 넓고 넓은 최적화의 세계 수학적 계획법/최적화 기법 최신/비전통 최적화 기법 확률론적 과정 기법 통계적 방법 최적화 문제 구성 목적 함수 제약 함수 변수 계수 상수 변수, 목적 함수, 제약조건에 따라 문제 유형이 다름 최적화 유형의 난이도 랭킹 LP : 선형 계획법 목적함수, 제약함수가 모두 1차식이고 결정변수가 모두 실수 현실 문제는 비선형 NLP : 비선형 계획법 목적함수, 제약함수 중 비선형 표현. n차 이건 오픈소스가 거의 없음 오래 걸림 모델링 라이브러리 유저 - 솔버의 통역사 솔버 연산 엔진 솔버 성능에 따라 최적 결과값이 다름 기술적 해결법으로 문제 유형을 변겅하는게 좋음 모델링 라이브러리의 역할 상황에 맞는 솔버 선택 가능 쿼드라틱을 잘 푸는 솔버는 이거 등 모델링 구현 방법 Matrix form (CVXOPT, lv_solve, scipy.solve) 계수를 뽑아서 c로, x로 쿼드라틱은 더 복잡해짐 Symbolic form PuLP 좀 더 직관적으로 표현 에러가 나면 어떤 제약식에서 나왔는지 말해줌 하지만 대부분 오픈소스 라이브러리가 LP만 지원.. symbolic은 거의 없음 cvxpy 수식 표현방법이 다름 정리된 목적함수가 작으면 심볼릭, 크면 매트릭스 Matrix 효율적 메모리 관리 다양한 개발환경 Symbolic 유지보수의 편리성 낮은 개발 난이도 상황 1) 상업용 솔버를 사용하면 해당 인터페이스 자체 모델링 라이브러리 사용 상황 2) 오픈소스 잘 모르겠다면 CVXPY 추천 솔버 선택 팁 사용 용도에 따른 SW 라이센스 문제 성능, 연산 신뢰성 문제 내 최적화 문제 유형을 지원하는지 : LP, MILP, QP, NLP 등 SW 라이센스 종류 GNU GPL 사용하면 코드 공개해야함 GNU LGPL 소스코드 공개 안해도 됨 실사용 리뷰 GLPK : GNU라 기업에서 쓰기엔 좀.. CLP/CBC OSQP인 ADMM 기법을 쓴다면 이걸 사용하는게 좋음! 예산의 제한이 없으면 CLPEX Industry Standard로 압도적 속도 및 성능 대학생이라면 Gurobi 아카데믹 지원이 됨 미래를 길게 보면 구로비가 안좋을수도.. 스타트업이라면 외부 배포할 일이 없고 LP만 풀면 GLPK 성능만 기똥찬 CLP/CBC OSQP 최적화 모델 구현 전력시장을 예시 전력은 발전과 소비가 늘 동시에 전체의 총 수요와 발전을 잘 매치해야함 언밸런스라면 주파수가 안맞아서 큰일날 수 있음 각 발전기 최적 발전량 결정 3개의 발전기의 발전량 스케쥴링 조건 1 : 발전기 설비 상황 조건 2 : 수급 균형(발전량=수요량) 목적 함수 : 비용 최소화 발전단가 최소화 제약 조건 최소 용량 &lt;= 발전량 &lt;= 최대 용량 총 발전량 = 전력수요 변수가 너무 많아 일일히 하기 어렵 CVXPY와 numpy 호환이 좋음 문제가 더 복잡하고 커질수록 솔버마다 차이가 커짐 심화 예제 : 하지만 현실은 QP 사실 발전비용은 2차곡선 목적함수 변경 (QP) 제곱식을 추가 수십만개의 변수, 수많은 비선형 제약조건 발전기를 켤지/끌지도 결정해야 (IP) 선로제약, 전압안정도 고려해야함 (NLP) 참고 자료 Comparison of Open-Source Linear Programming Solvers : 62페이지 A comparative analysis of optimization solvers 최적화에 대해 이미 알고있어서 꽤 아는 내용이었음. 그래도 오픈소스를 한번 싹 정리해주셔서 매우 좋았음! 뚱뚱하고 굼뜬 판다(Pandas)를 위한 효과적인 다이어트 전략 오성우님 발표 자료 Pandas는 관계형 데이터를 빠르고 유연하게 개발됨 흔히 맞닥뜨리는 문제 데이터를 읽지도 못하고 시간도 오래 걸리고 작성하는 코드 방식도 제각각 원인 Pandas Memory mapping issue Python Auto Garvage Collectiong Data processing in memory at once using wrong syntax using one CPU Pandas way! 다이어트 전략 전략 1 : 식사량 조절, Memory Optimization 전략 2: 식이요법, Enhancing Performance 전략 3 : 습관, Adopting convension 국민건강보험공단(NHIS)에 제공하는 데이터 4억 중 일부를 전처리 1) 식사량 조절 큰 사이즈의 데이터 메모리 사용을 최적화 문자열로 된 데이터를 숫자/영어로 변환 데이터 형식 변환 판다스는 고정된 바이트 유지 데이터 형식을 알면 dtype에 형식 지정 데이터를 불러왔지만 크기를 줄이고 싶으면 astype check_dtypes() 메모리 사용량이 줄어듬 Category 데이터 형식 사용 R에서 factor가 python category Object를 Category로 변환하면 메모리 사용량 크게 감소 범주가 무수히 많은 경우엔 object보다 비효율적일 수 있음 Object는 2.3초, Category는 0.3초 파일 저장 포맷 변경 CSV는 string 기반이라 IO 효율이 없음 Meta data가 없어 연속성 가지고 사용 불가 hdf5, parquet, pickle, feather 등을 사용함 parquet가 압축 형태로 제공되서 좋음 개인적으로 feather, pickle 프로젝트나 협업 목적은 parquet, hdf5 요약 데이터 타입 형식 지정 2) 식이습관 2-1. Vectorization 벡터화 연산을 사용하면 반복문을 사용하지 않고도 모든 원소에 대해 반복연산이 가능 len(df) .iterrows() .apply Pandas Series Vectorization Numpy Array Vectorization 넘파이가 더 빠름 @np.vectorize를 사용하면 Custom 함수를 vectorization해줌 2-2. 효율적인 알고리즘 Pandas의 수백개 메소드를 어떻게 조합하냐에 따라 시간이 다름 .sort_values(ascending=False).head(5) vs .nlargest(5) : 후자가 속도 10개 넘게 빠름 2-3. df.apply()는 만능이 아님 Custom 함수를 만들어 apply를 사용해 데이터 처리, 분석. lambda와 사용 List Comprehension pd.Series.apply() pd.DataFrame.isin pd.DataFrame.query np.isin pd.DataFrame.merge 아래로 갈수록 더 빠름 merge는 Database에 익숙 요약 반복문 사용은 피하고 Vectorization 사용 수행 시간을 더 빠르게 하고 싶으면 좀 더 효율적 알고리즘을 고려 Custom 함수는 우선 apply, 오래 걸리면 Pandas built-in 함수를 찾아 조합해 사용 3) 생활 습관 Method Chaining 가독성이 좋고, 성능이 좋음 한계 DataFrame의 중간 체크가 어려움 진짜 성능이 좋아지는진 물음표 데이터를 줄일 경우 사용 inplace parameter inplace 선호나느 사용자 속도가 더 빠르고, 메모리를 더 효율적으로 사용한다 pandas Core 개발자는 inplace를 없앰. 실행 이후에 메모리에 데이터에 남음 데이터를 일부먼 수정할 경우 사용 기타 유의사항 Case by Case Garbage collection delete를 주기적으로 하고 gc.collect Not designed for Big Data? 스케일링 cuDF 실질적으로 바로 활용할 수 있는 꿀팁이 가능했던 세션, in보다 merge해서 보는게 더 빠르다는건 (SQL에서 자주 보던 방식이었지만) 신기했음 파이썬 3.7 어찌 그렇게 빨라졌나 정겨울님 발표 자료 사실 과거 버전에도 계속 성능 개선은 있었음 3.7은 릴리즈 노트에 Optimization을 꼭 집어서 말함 목표 성능 개선을 위한 아이디어 더 알고싶으면 어디를 볼지 그래도 완전 어렵지 않은 C 코드 기본 메소드 호출 표준 라이브러리에 속한 클래스에 있는 여러 메서드들을 최적화함 METH_FASTCALL 컨벤션을 따르도록 바꿈 생긴건 obcode처럼 생겼지만 사실 C 함수 선언 컨벤션 파이썬 tuple이 PyObject의 C 배열로 들어옴 더 알고싶으면 bpo-27810 python 3.8에 positional arguments 개념이 동입됨 Argument Clinic 함수 변환과 생성을 도와주는 도구 CPython 내장 함수 인수 처리를 돕는 DSL PEP-436 참고 파이썬 시작 시간 인터프리터를 실행하는 시간이 감소함 맥 30%, Linux 10% abc 모듈을 C로 다시 짬 site.py가 사용하는 sysconfig.py의 특정 함수를 import하지 않고 복사해서 사용하니 빨라짐 인터프리터를 실행할 때 자동으로 import, site-package 경로를 찾고 추가 sysconfig의 함수를 site.py로 가져오는데 이게 무거웠던 상황 인스턴스 메서드 호출 positional argumnets 함수 호출에 대한 새로운 opcode 2개 추가 asyncio.sleep asyncio.sleep()이 2배 빨라짐 =&gt; zero나 음수일 떄 빨라짐 기존엔 delay가 0이면 바로 리턴인데, delay가 음수면 future 만들고 루프에 스케쥴링 했는데 안하도록 바뀜 이 부분 생략하게 됨 sleep0은 그냥 yield만 있는 형태 이벤트 루프 만들기 asyncio.get_event_loop, get_running_loop 함수를 C로 짜서 4배 빨라짐 캐싱 사용은 미미하고 os.getpid()대신 getpid()를 사용(C쪽) =&gt; 성능 향상 80% 동시에 실행하기 asynci.gather()가 15% 빨라짐 마이크로서비스에서 하나를 받아 다른걸 처리 =&gt; 한번에 가져오도록 돌리게 해줌 코루틴 하나하나를 functools.partial을 제거 다 끝났을 때만 로직을 실행 asyncio.ensure_future 1.17배 빨라짐 if문 순서를 바꿔서 성능이 개선됨 typing 모듈 가져오기 제네릭 형식을 더 잘 지원하기 위해 특수 메서드 추가 던더메소드는 경고없이 깨질 수 있다고 함 리스트 정렬 40~75% 개선 리스트를 정렬할 때 대부분 값들이 서로 동일한 타입이라 가정 매번 값을 비교할 때마다 타입 검사하지 말고 한번만 해두자가 아이디어 여러 가정을 세우고 pre-check stage를 만듬 아주 최악의 경우 pre-check 때문에 15% 느려짐(safe_object_compare로 넘어가기까지) 딕셔너리 복사 dict.copy()가 5.5배 빨라짐 정규표현식 case-insensitive matching이 20배 빨라짐 파이썬 그 자체에 관심을 더 가지면 좋겠다고 생각하도록 만들어준 세션 머신러닝 및 데이터 과학 연구자를 위한 python 기반 컨테이너 분산처리 플랫폼 설계 및 개발 신정규님 발표 자료 2015년 리뷰 연구자 및 교육자를 위한 계산 및 분석 플랫폼 설계 현대 과학 연구와 개선 학계와 업계, 사회의 간극이 존재한다 생각 재현 가능한 데이터 연구 플랫폼 만듬 논문용 코드가 쉽게 스케일업되는 서비스! 컨테이너 기반의 고밀도 분산처리 플랫폼 및 사용자 편의 기능 구현!!하자고 했음(프로토도 있으니..) Backend AI Github 용어 설명 Baremetal 가상 머신. 안에 있는 VM은 격리된 환경에서 삼 Shared : OS, Container Layer, Container 컨테이너 : 호스트 운영체제 커널을 공유 구현 Container layer : LXC, runC, Jail 컨트롤 그룹을 사용해 구현 클라우드 매니저 클러스터에 VCM 또는 스토리지 생성 IaaS 구축을 위한 소프트웨어 컨테이너 관리 솔루션 컨테이너를 생성, 삭제, 배치, 관리하는 역할 담당 권불 10주 이 바닥은 10주마다 바뀐다 Rootless 컨테이너 및 특화 컨테이너 솔루션의 등장 마이크로서비스의 대두 어플리케이션을 작은 단위로 쪼갬 서비스 모듈 + 통신 빠른 버전업 및 지속적 통합 기반 언어의 변화, 프로토콜도 확장 비동기 시대 Asynchronous IO의 도래 마이크로서비스 : 지연 시간 예측 난도 많은 마이크로서비스들이 비동기로 동작 우공이산 기본 설계하기 오픈소스화 하다보니 다양한 사람들의 환경을 지원해야 함 머신러닝 모델 대비! 허나 Tensorflow 등장하고 난리남 하위호환성 안줘 ㅠ nvidia-docker도 바뀜 PyTorch의 등장 버전마다 구현체가 다름 설상가상 문제를 해결하면 새로운 것이 또 따라옴 병렬화 GPU가 계속 놀아요 ㅠ_ㅠ 데이터 병렬화나 모델 병렬화 데이터 파이프라인도 싱글 노드 - 멀티 GPU, 멀티 노드 - 멀티 GPU에 맞춰 처리 연구분야 세상의 속도가 다르게 흐르는 곳 아직도 Python2… 2를 지원해야하네 TensorFlow Extended 이런거 최근까지 python2까지만 지원됨 Python 2/3 징원 인터페이스를 어떻게 하지? ZeroMQ 거의 모든 언어로 구현체가 존재! 지원 언어가 15개 이상 되니 대환장 Kernel Runner 컨테이너의 운영체제만 영향을 줌 장점 : 백엔드 AI 에이전트는 3.6으로 하고 안에는 다른거도 가도 됨 배포할 떄 계속 문제.. 그래서 Embeded Kernel Runner로 씀 화룡정점 언어별 SDK 구현. 기존 구현체들은 너무 방대 오프라인 환경 pip는 항상 온라인이.. Builder 만듬 Ansible로 배포 자동화 GUI 코어 Webcomponent 기술 기반 메인 앱 장고 기반 파이프라인 앱 별도 개발해서 통합 예정 앱 Electron 기반 JavaScript ES6 코드 서빙 웹 콘솔 모드 Consoel-server와 결합 Manager 프록시 웹소켓 프록시 허브 전체 클러스터 관리용 고가용성 매니저가 왜 3개씩 존재하지 않나? 로드 밸런서 구현 Fleet 운영시 Voting 알고리즘으로 대장 선정 웹소켓 프록시 터널링을 사용해서 ssh 터널링 쓰는데.. 자꾸 사용자 환경에 깔기 싫다! 해서 웹에서 아답터를 위해 프록시 터널링 설계해서 구현함 홍익인간 GPU 가상화 fGPU 구현 및 테스트 하나를 가상화해서 여러개처럼 사용해보니 이게 더 빠르네? 우리의 코드는.. GPU를 괴롭히지 못함 나눠서 멀티로 하는게 더 빠름 어마어마하다..ㅋㅋㅋㅋㅋㅋㅋ 고생하신게 엄청 보였고, 그 기간에 다 하셨다는 것도 대단.. 추천시스템, 이제는 돈이 되어야 한다 최규민님 발표 자료 앞에 좀 못들음 ㅠ 어떤 MAB 알고리즘이 가장 좋은 성능을 내는가? 대부분 추천 시스템에서 TS-MAB가 가장 좋은 성ㅇ능을 발휘함 엡실론 그리디 MAB 기본적 Bandit 알고리즘 작품별 CTR 예측 Best Arm 선택 탐색과 활용이 적절한가? CTR이 낮은 작품의 impressions별 CTR 신뢰 구간을 보자 Impression이 많아질수록 CTR이 수렴함이 보임 10M Impressions 추천될 수 없는 작품, 즉 Optimal Arm이 아님에도 계속 Impressions을 소비함(regret:손실 증가) 톰슨 샘플링 엡실론 방법은 작품별 impresson 양을 동일하게 부여했지만, 이건 작품별(Arm)별 베타 분포로 샘플링 a=click, b=unclick 1000개부터는 점점 CTR 수렴 추천될 수 있는 최소 CTR이 15%라 가정하면 작품 1번은 100 impressons까지만 trial되고 그 이후엔 서택되지 않음 적절히 신뢰할만한 impression이 모이면 탐색 -&gt; 활용으로 전환 CTR이 25%이상 작품은 계속 탐색하고 수렴 빠르게 수렴 및 빠르게 engage 실제 웹툰 작품들의 베타 분포 변화 계속 탐색함 신뢰 구간이 점점 좁아지며 수렴하는 작품들이 생김 해석 CTR이 낮은 작품(Arm)들은 적당히 탐색하다가 추천을 하지 않고, CTR이 좋을 것 같으면 계속 탐색 TS MAB는 탐색&amp;활용 Trade-off에서 적절힌 Reget(손실)을 최소화 User Clustering 작품의 CTR은 유저 성향별로 다름 유저 A : 25% (30대 액션물 좋아함) 유저 B : 2.1% (순정 만화를 좋아하는 여성) 유저 C : 7.1% (모든 만화를 좋아하는 남자) 유저별로 CTR이 높을 작품을 추천하면! 모든 유저별 X 모든 작품에 대해 CTR 측정이 필요함 그런데 모든 경우에 대해 CTR 측정은 불가능 그래서 유저 클러스터링함 CTR이 비슷할 것 같은 유저끼리 모으고(8개) 그 내에서 CTR을 예측 어떻게 클러스터링하는가? 웹툰을 열람한 성향이 있다고 가정 유저가 최근 읽은 작품들 읽은 작품으로 CB(image, Text)를 Feature로 User Feature 생성 =&gt; 유사한 유저끼리 클러스터링 이 8개의 클러스터가 유저의 성향을 적절히 반영하는가? 알려진 스코어가 잘 measure 안되서 정성적 탐색을 함 유저클러스터 결과 성별 분포 각 클러스터별 성별 편향성이 뚜렷함 여성 중심 / 남성 중심 / 여성 &amp; 남성 혼합 각 클러스터별 어떤 장르가 많이 소비되는지 파악 클러스터별 소비만화 Tag들 파악 개인화 홈 추천 클러스터 할당 추천할만한 작품 Targeter MAB를 통해 후보 작품을 Ranking 광고 시스템과 비슷함 유저클러스터 + MAB 적용 연관추천 - ViewerEnd 추천 목적 : 현재 읽고 있는 작품과 유사한 작품 중에서 열람수를 늘리기 위해 CTR(%)이 가장 높을 작품 추천 사용되는 추천 모델들 item feature MAB 비슷한 작품 Targeter =&gt; 작품 Ranker 현재 본 작품과 설명이 유사한, 썸네일이 유사한 것을 추천 item feature 대표 이미지, 작품 제목, 설명, 유저 Feedback으로 작품간 상관 특징 추출 Image로 Style 특징점을 추출해 스타일이 유사한 작품 추출 Image로 Object Detection Task를 통해 특징점을 추출해 Image Object가 유사한 작품을 뽑음 Word2Vec으로 작품의 제목, 설명으로 특징점을 추천 Matrix Factorization(ALS) with implicit feedback 사용 유사 작품 + MAB 구매 전환을 최대화하기 위한 실험들 열람 중심 =&gt; 구매 중심 이제는 돈을 벌고 싶어요! 열람 중심은 전체 열람(Clicks) 합이 최대화하도록 구현 구매 중심은 유료 열람(Use Coin)합이 최대화 실험 1. 매출 중심 MAB Rewardd : Use Coin 열람 전환율 구매 전환율 결과 폭망 ㅠ 실험군(Beta)는 대조군(Alpha) 대비 -20% 떨어짐 =&gt; 실험 종료 왜 그럴까? 실험 2. Conditional Bandit 실험 사건의 종속성 반영 첫번째 MAB : Reward=Click(무료 열람) 두번째 MAB : Rewardd=Use Coin 결과 실험군의 대조군 대비 유료 열람수의 변화는 없음 두번째 MAB의 수렴성이 보장되지 않음 실험 3. Retention Model 리텐션이 높은 작품을 추천해보자 작품마다 열람지속 스코어 측정 열람지속 스코어가 높은 작품을 연관추천 추가해 MAB 수행 결과 열람 전환율(CTR)은 지표 상승 구매 전환율은 변화 없음 실험 4. Seen decay 유저에게 노출했는데 클릭하지 않는다면 해당 작품에 대한 Negative Feedback 안보면 패널티를 주자 실험 결과 대조군 대비 실험군의 CTR은 상승했지만 구매 전환율은 변화 없음 그물외 다양한 실험들 많이 해봄 다시 고민 왜 실험의 구매 지표 개선이 없지? 컨텐츠 사용자의 구매 패턴부터 다시 분석 독자들은 어떻게 유료 작품을 열람할까? 추천을 보고 - 클릭 - 무료 열람 - 지속 열람 - 유료 열람 엄청 긴 퍼널을 가지고 있음 노출 대비 유료 전환율은 0.1% 이하로 매우 낮음 무료에서 열람까지 걸리는 시간은 평균 4.56일.. 좀 길다 구매 전환율은 매우 낮고 구매 Feedback은 매우 지연됨 구매 최적화를 위한 AB 실험의 지표 추적과 측정이 어려움 지금 개인화/연관 추천은 읽지 않은 작품을 추천 최초 열람 유도가 지속열람, 유료 열람에 상관관계 또는 인과관계가 있는가? 또 해봄 연속적 열람 전환율이 어떻게 되나? 추천과 비추천 로직의 변화가 있을까? 베이스(에디터픽) 대비 CTR 242% 4회 전환율은 42% 구매 전환율은 추천에 인과관계가 있지만 작품의 품질과 유료 결제 경험에 의존적임 목적 개인화/연관 추천은? 열람 작품 늘리기 초급 타켓팅 푸시? 지속 열람장의 유료 전환 중급 현재 소프트 클러스터 방법 쓰고 있음 내가 드라마 좋아할 확률 30%, 액션 20% 여러 토픽을 가진 클러스터로 구성함 정성적으로 하고 AB Test용으로 추천 안되는건 어떻게? 작품의 다이버시티를 이용하고, 여러 풀을 섞음 implicit als 앞 극초반부는 못들었지만 이미 알던 내용들이라 다행, 전반적으로 매우 좋았음. 국내에서 추천, MAB를 잘하는 팀은 이 팀이 아닐까 생각된다..! 따로 시간내서 규민님이랑 오랜만에 담소했는데, 역시 즐거운 시간이었음 꼬마 모차르트가 되어보자 김준우님, 한상곤님 Github 인간과 기계가 차이가 없는 느낌 재미있는 프로젝트를 하고 싶었음 TensorFlow의 Magenta 이것도 알리고 예술적인 것들을 해봅시다! 구글에서 바흐 탄생 기념일에 뭔가를 만듬 소프라노 악보를 만들면 코드를 배치해줌 미디 파일을 찾아보자! www.vgmusic.ocm 8bit 음악이 많음. 학습용 이외엔 쓰면 안되요..! 8bit midi 파일을 만듬 상용 불가 미디파일을 합법적으로 구하는 방법을 시작하겠습니다 발표 톤 너무 웃겨욬ㅋㅋㅋㅋㅋ magenta colab latent loops 멜로디 4개를 학습, 새로운 패턴이 나옴 피아노징 8개의 버튼으로 그럴듯한 연주를 하게 해줌 Neural Drum 4박자만 패턴 입력하면 나머지 기계까 완성 PIANO scribe 피아노 들려주면 midi로 바꿔줌 마젠타는 파이썬으로 해볼 수 있겠다! 윈도우에선 잘 안되서 WSL 선택 단점 토이 프로젝트엔 괜찮지만 WSL1은 커널이 위에 올라가서 느림. WSL2는 성능이 좋아질거라 보임 Onsets and Frames 모델을 이용해 추론 마주친 문제와 해결 파이썬 2.7 ㅋ 소리내기 위한 pyfluidsynth가 2점대만 지원 Demo 보여줌 마젠타 라이브러리가 신기했고, 새로운 분야에 대해 알 수 있어서 좋았음 Advanced Python testing techniques 안재만님 발표 자료 테스트 코드 내가 쓴 코드가 잘 자동하는지 확인하는 코드 프로젝트의 규모가 커지고, 참여하는 사람이 늘어날수록 반드시 필요 마음의 안정감을 가지고 프로젝트 유지보수가 가능 pytest 조금 더 powerful assert문 사용 가능 더 자유로운 fixtur e사용 지원 테스트 실패할 때 uniitest보다 유용한 정보 제공 315+개의 플러그인 존재 function 베이스로 테스트 Fixture 사용하기 다른 곳에서 정의하면 테스트 코드에서 사용할 수 있음 기본적인 API 테스트 구현 유저가 가입하고 실제로 성공했는지 확인 복잡한 API response가 200이고 유저가 있고 아이디가 있고 등등… 테스트 코드가 읽기 어려워짐 어려워서 주석을 달았지만.. 도움은 안됨 관리자가 ban시키고 되는지 확인하는 테스트는? 아까보다 더 복잡함 유저가 가입한 후 관리자가 승인하고 밴 시키고 로그인 안되는데 탈퇴 시도 성공하는데 재가입시 24시간동안 가입이 되지 않는것 테스트 Sure를 이용해 더 직관적으로 테스트코드 구현 Assertion을 쓰기 쉽게 해주는 라이브러리 CPython에서만 사용 가능 Number, String, Collection, Callable 등에 대한 assertion 지원 가입한 후 관리자가 승인해야만 제대로 로그인되는지 테스트하는걸 더 직관적으로 가능 should.have.key which.should BDD를 이용해 더 간단하고 재미있게 테스트 코드 구현하기 Behavior Driven Development TDD와 유사 테스트 코드보다 비즈니스 요구사항에 집중하자 지원되는 Library behave, lettuce, pytest-bdd 먼저 자연어로 테스트 코드를 작성 Feature : 테스트할 대상 기능 시나리오 : 테스트 상황 설명 Given : 테스트에서 사용할 데이터 컨텍스트 Backgoround : 공통적으로 사용되는 데이터, 컨텍스트 When : 수행할 조건 Then : 결과 login.feature를 정의하고 시나리오 작성 Parametrized testing in BDD 어떤 name의 약은 색이 뭐고, 어떤 결과엔 True, False 등 BDD 사용시 유의점 지금 발표 보면 오 BDD 짱! 이지만 단점이 있음 자연어로 define된 시나리오와 python 테스트 코드가 분리되어 있어 찾기 어려움 IDE 도움을 받자 최대한 재사용 가능한 문장으로 테스트 구성 예 : 가입에 성공하고 로그인을 한 유저가 있다 =&gt; 정상 가입을 한 유저가 있다. 유저가 로그인을 한다 When에서 동작을 수행한 return 값은 Then에서 가져올 수 없음 Given에서 context 만들어놓고, When에서 수행한 동작을 context에 저장하고 Then에서 확인 오늘부터 인증 서버를 분리해 새로 구현합니다 로직 변경 테스트 코드의 존재로 리팩토링을 안정감 가지고 진행할 수 있음 1) 테스트용 인증 서버를 올려야겠다 2) 항상 떠있는 개발 서버? HTTP Mocking Mocking : 가장의 HTTP Mocking : 발생하는 HTTP Call을 interrupt하여 실제로 HTTP Call을 보내지 않고, 정해진 동작을 수행해 response를 반환하도록 함 Python : HTTPretty Monkey patching 런탕임에 정ㅇ의되어 있는 클래스나 모듈의 attribute를 바꿈 테스트 코드에서 많이 활용 A 모듈을 테스트, B 모듈에서 나온 결과는 항상 같은 값으로 고정 HTTPretty도 Monkey patching을 이용해 구현되어 있음 B는 import하지 않고 고정된 값을 가져오고 싶은 경우 사용 데이터를 받아 예측하는 코드 예시 모든 경우 수동으로 모든 경우를 테스트할 수 없음 랜덤한 input에 대해 function이 리턴해야할 값이나 수행할 동작을 테스트 어떤 input이 들어와도 error가 나면 안됨 QuickCheck : Randomized testing(하스켈) python은 hypothesis library given으로 랜덤한 input을 받도록 만들어주는 데코레이터 strategies는 랜덤하게 주어질 input의 규칙 머신러닝 모델 나이 예측 ML 모델에서 나온 나이 값이 0에서 100 사이인지, contribution이 0에서 1사이인지 너무 오래 걸리는 것 같아요 Benchmark test 함수를 수행하는데 걸린 시간 측정 1초에 몇개의 요청을 처리할 수 있는지 근사 다수아 사용하는 실 서비스 환경에서 반드시 필요 많이 노출될 API Basic 패턴 : start.time 사용하곤 함 benchmark로 감싸주면 더 편하게 가능 그 외 유용한 툴 벤치마크가 부족하면 Profiling 툴 사용(pytest-profiling) 함수별 execution graph와 실행시간 보여줌 N대의 서버들이 부하를 얼마나 버티는지 알고싶어요 http://locust.io 간단한 pytest plugin 만들기 goconvey 로직을 바꾸면 실시간으로 테스트 성공 실패를 웹으로 알려주는 툴 python으로 구현하려고 하는데 아직 못만듬! 내년에 꼭 소개드리겠습니다 pytest-convey pytest-watch로 감지, pytest-json-report로 포맷팅 정리 테스트 코드를 작성하기 쉽게 만들어 두는 것이 굉장히 중요 테스트코드 리팩토링 Top-down 방식으로 일단 코드 작성 테스트에 유용한 라이브러리 상용 Sure - assertion 알아보기 쉽게 작성 가능 BDD 기타 방법 HTTPPretty, Randomized, Monkey Patch 꿀팁을 많이 얻을 수 있어서 매우 좋았음!! gRPC와 python을 활용한 Microservice 개발기 송지형님 MSA, gRPC 등 발표 자료 클라우드 엔지니어로 시작 메가존 클라우드의 고민 고객들에게 좀 더 가치를 전해줄 수 있는 것이 없을까? 안정적이고 확장 가능한 플랫폼 MSA 새로운 Feature가 느슨하게 연결되어야 함 MSA : 하나의 어플리케이션을 작은 서비스로 나누고, 프로세스를 가지고 가벼운 메커니즘을 사용해 HTTP resource API 서비스간 독립적 형태로 존재 유연한 대처가 가능 서비스별 별도 기술 적용 가능 자바, 파이썬, 노드 모두 가능 대부분 RESTful API를 통한 서비스 통신 단, Service간 Network를 통한 통신 통신 Overhead 증가 서비스별 다른 언어 사용 가능이 진짜일까? 생각보다 쉽지 않음 gRPC High performance, open-source universal RPC framework 통신 오버헤드를 최대한 줄이려는게 목적 pluggable support : 여러 플러그인을 사용 가능 구글이 개발한 RPC 기반 Framework MSA와 같은 분산 구조에 적합한 효율적인 통신 프로토콜에 대한 고민 Remote Procedure Call 원격지의 프로그램을 로컬에서 실행하는 것처럼! Code의 간결함 RESTful의 HTTP 규격 불필요 다시 돌아온 특징 프로토콜 버퍼 구조화된 데이터를 시리얼라이즈하기 위한 flexible, efficient, automated 메커니즘 메세지 처리 기술인데 더 작고 빠르고 심플 서버 클라이언트간 메시지를 정의하는 역할 실제 통신시 byte stream으로 데이터를 인코딩해 통신 XML보다 3~10배 작고 20~100배 빠름 HTTP/2 커뮤니케이션, 양방향 response도 정의 가능 Server Implementation MSA에 특화된 Service간 Message 성능 향상에 집중한 RPC Framework Protobuf 관리가 쉽지 않음 gRPC를 잘 좀 써보자 API Pkg -&gt; proto -&gt; Build -&gt; Dist -&gt; Template Protobuf 관리 API source 받아서 /proto 디렉토리에 작성 makefile 작성 비즈니스 로직에 맞도록 진행 마이크로서비스, gRPC에 대한 깔끔한 설명을 해주셔서 좋았음 온라인 뉴스 댓글은 정말 사람들의 목소리일까? - PART2 이준범님 발표 자료 데이터 크롤링 : 10분 단위 AWS 클라우드 위에서 처리 직접 데이터 수집 - 처리 - 가설 - 라벨링 등에 대해 잘 나와있음 역시 준범님! 더 읽은 자료 100억건의 카카오톡 데이터로 똑똑한 일상대화 인공지능 만들기 : 연구 시작 ~ 진행 과정을 잘 알려줌. NLP에 큰 흥미는 없지만 재미있게 읽을 수 있었음 파이썬 웹서버 REST API 문서 쉽고 빠르게 작성하기 : 언젠가 API 문서화를 고민할 때 다시 읽어보면 좋을 것 같은 문서 from banksalad import python : 뱅크샐러드가 파이썬을 활용해 어떻게 발전해왔는지 알 수 있는 문서 Pickle &amp; Custom Binary Serializer : Serialize에 대해 알 수 있는 문서 튜토리얼 자료 pytest로 파이썬 코드 테스트하기 슬라이드쉐어 Tutorial 자료, pytest를 사용한 기본에 대해 잘 나와있음 Python으로 지리공간데이터 다루기 아직 발표자료 미공개인듯. 추후 확인해보기 행사 부스 프로그래머스 구구단 구현 이벤트 올해는 프로그래머스 구구단 구현 이벤트에 참여함 코드 초반엔 1등했으나 마감일에 갑자기 다크호스분의 등장으로 2등 1등분은 파이썬 로고를 만드셔서 리스펙.. 저도 좋아요 눌렀음 그리고 에어팟 받아서 만족 :) 오랜만에 본 광고동아리 선배님도 계셔서 반가웠음-! 스캐터랩 연애 시뮬레이션 쿼리와 센텐스, 감정 결과가 주어지면 어떤 답을 해야하는지 문제가 있었음 사실 약간 시도하다가 막혔지만, 영상이 매우 고퀄이라 재미있게 시도해봄 확실히 자연어쪽을 세련되게 잘 한다는 느낌을 받았음",
    "tags": "lecture etc",
    "url": "/etc/2019/08/18/pycon2019/"
  },{
    "title": "Discrete Optimization 입문",
    "text": "Discrete Optimization 입문에 대해 작성한 글입니다 Discrete Optimization 냅색 문제 NP hard, 다루기 어려운 문제 대신 근사 알고리즘(Approximate)이 존재함 Optimization 컴퓨터 사이언스에서 어려운 문제중 하나 Supply chains, sport scheduling, logistics, eletrical power system, manufacturing 등에서 사용됨 NP-Complete 문제의 2가지 속성 답을 매우 빠르게 확인할 수 있음 NP-Complete 문제 하나를 빠르게 푼다면, 모든 것을 풀 수 있음 Exponential time이 가장 worst case라고 믿음 항구 항구의 각각 요소가 계속 움직임 =&gt; 최적화 문제 일반적으로 매일 해결 Energy 에너지는 load와 demand가 계속 만남 이걸 잘 매칭시키고 싶어함 Sport Scheduling 스포츠 스케쥴링 누가 누구랑 대결할 것인가 여러 제약조건 같은 경기장을 사용할 수 없음 베이스볼, 축구 등 시즌도 맞춰야 함 Exponential Runtime 지수로 상승함 문제 사이즈가 커지면서 점점 풀리지 않음 Optimization Problems 어디에나 있음 엄청나게 풀기 어려움 그러나 풀어야 함 재미있음 푸는 것이 중요함 예시 Kidney(신장) Exchanges Basic Facts 우리는 1개의 신장이 필요 80,000명의 환자 미국에서 매년 4,000명이 죽음 호환 이슈가 존재 주는 사람, 받는 사람의 그래프가 생김 사이클이 생김 이건 매우 작은 문제지만, 가지수가 많아지면 풀기 어려워짐 Disaster Management 허리케인의 Damages 금액을 산정 어떤 지역에 문제가 있고, 그 이후 빠르게 복원(예를 들면 전기) 이럴 경우 정전의 크기를 줄이려고 노력하고, 빠르게 복구하고 싶음 Routing Aspect 관점(어디부터 복구할 것인가)과 Power Flow Aspect 관점(blackout size를 줄임) Reference Coursera Discrete Optimization",
    "tags": "optimization data",
    "url": "/data/2019/08/13/discrete-optimization-intro/"
  },{
    "title": "Python에서 Auth를 사용해 Google Spreadsheets 연동하기",
    "text": "Python에서 Google Spreadsheets 연동하는 방법을 작성한 글입니다 구유림님의 gspread를 이용하여 Python에서 구글 시트 연동하기 에 gspread를 사용하는 방법에 대해 자세히 작성되어 있습니다 :) 이번 글은 Service Account Key 없이 Web Auth를 이용해 구글 시트를 연동하는 방법을 다룹니다 Google Sheets 사용할 라이브러리 gspread pydata_google_auth pydata_google_auth Github 이 라이브러리는 Google API 인증을 위한 헬퍼 패키지 BigQuery를 사용할 경우에도 사용함 : [pandas-gbq에서 인증(Authentication) 설정하기] 참고 원리 원리는 생각보다 간단 pydata_google_auth를 통해 credentials를 생성 거기에 존재하는 access_token을 통해 gspread에 인증 단, pydata의 access_token은 token에 저장되고 gspread는 access_token이 필요함 이 부분을 덮어쓰는 코드 추가 코드 import pydata_google_auth import gspread SCOPES = [ 'https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/spreadsheets' ] credentials = pydata_google_auth.get_user_credentials(SCOPES, auth_local_webserver=True) credentials.access_token = credentials.token gc = gspread.authorize(credentials) gc.list_spreadsheet_files() 참고 한번만 인증하면 그 다음은 웹이 뜨지 않아서 왜 그런가? 하고 소스를 뜯어봄 (Mac 기준) ~/.config/pydata/pydata_google_credentials.json에 전에 사용한 credentials를 저장하기 때문 공식 문서의 credentials_cache를 참고하면 저장함 만약 다른 계정으로 로그인하고 싶으면 $HOME/.config/pydata/pydata_google_credentials.json(Mac) 또는 $APPDATA/.config/pydata/pydata_google_credentials.json(윈도우)를 삭제 Reference pydata-google-auth 공식 문서 gspread를 이용하여 Python에서 구글 시트 연동하기 pandas-gbq에서 인증(Authentication) 설정하기",
    "tags": "python development",
    "url": "/development/2019/08/09/spreadsheet-with-auth/"
  },{
    "title": "Sacred와 Omniboard를 활용한 실험 및 로그 모니터링",
    "text": "Python Sacred를 더 잘 활용하기 위해 Omniboard를 붙이는 과정을 작성한 글입니다 Sacred는 머신러닝 실험을 도와줄 Python Sacred 소개를 참고하시면 좋을 것 같습니다 Omniboard Omniboard는 머신러닝 실험 관리 도구인 Sacred를 위한 웹 대시보드 MongoDB에 연결해 Sacred의 실험, Metric, 로그를 시각화해줌 React, Node.js, Express와 Bootstrap을 사용해 만들어짐 특징 Experiment management 모든 실험을 Tabular 형태로 저장 특정 컬럼을 show / hide 가능 Rolled up metric value를 컬럼으로 추가 가능(minimum validation loss, maximum training accuracy 등) 각종 실험에 Tag나 Note를 작성할 수 있음 Metric column을 제외한 모든 컬럼으로 필터 가능 Experiment Drill Down Metric Graph를 볼 수 있음 Console Output을 보여줌 모든 런타임 라이브러리 dependencies를 보여줌 소스 파일을 보거나 다운로드 가능 Artifacts(인공 산물? 파일물?)을 보거나 다운로드 가능 호스트의 하드웨어 스펙을 볼 수 있음 Git hash/version control 정보를 볼 수 있음 스크린샷 Tabular 형태의 데이터 Tags, 메모, 각종 파라미터 저장됨 Metric Plot Metric Column 관리 Output Log Experiment Details Host Info 설치 MongoDB 설치 Mac brew install mongodb Linux(Ubuntu 16.04) sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 9DA31620334BD75D9DCB49F368818C72E52529D4 echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/4.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list sudo apt-get update sudo apt-get install -y mongodb-org echo \"mongodb-org hold\" | sudo dpkg --set-selections echo \"mongodb-org-server hold\" | sudo dpkg --set-selections echo \"mongodb-org-shell hold\" | sudo dpkg --set-selections echo \"mongodb-org-mongos hold\" | sudo dpkg --set-selections echo \"mongodb-org-tools hold\" | sudo dpkg --set-selections MongoDB 실행 Mac brew services start mongodb Linux(Ubuntu 16.04) sudo service mongod start pymongo 설치 pip3 install pymongo omniboard 설치 npm이 설치되어 있어야 함(Node.js v8 이후 버전) npm install -g omniboard Docker나 Docker Composer를 사용하는 방법은 공식 문서 참조! Omniboard 실행 omniboard -m hostname:port:database -u username:password:secret 로 실행 omniboard -m localhost:27017:sacred Advanced connection properties를 셋팅하려면 --mu 옵션을 사용하면 됨 MongoDB connection URI omniboard --mu \"mongodb://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;/&lt;database&gt;[?options]\" Omniboard 대시보드 실행 Omniboard is listening on port 9000! 라는 멘트가 나오면 웹브라우저에서 localhost:9000을 입력하면 대시보드가 실행됨 처음엔 실험을 하지 않아서 아무 창이 뜨지 않으나, 실험을 하면 하나씩 추가됨 Sacred Experiment 실행 머신러닝 실험을 도와줄 Python Sacred 소개의 예제에 있던 rf, svc 실험을 FileStorageObserver가 아닌 MongoObserver로 변경해 다시 실행함 from sacred.observers import MongoObserver ex = Experiment('svc') ex.observers.append(MongoObserver.create(url='localhost:27017', db_name='sacred')) 그 후 Metric Plot을 그리기 위해 Sacred Example Code Pytorch를 Clone 후 실행함 git clone https://github.com/maartjeth/sacred-example-pytorch.git cd sacred-example-pytorch # train_nn.py의 DATABASE_NAME 변수를 sacred로 수정한 후 실행 python3 train_nn.py 대시보드로 이동(localhost:9000) 실험 이름, Tags, 메모 등이 보임 Tags나 Notes는 그냥 입력하면 됨 우측엔 Result나 각종 파라미터가 저장됨 빈칸인 공간은 해당 실험이 그 파라미터를 사용하지 않아서! 맨 좌측의 파란색 삼각형 클릭하면 자세한 정보가 나옴 Metrics Plot을 보면 Steps이 지나며 loss가 감소하는 것을 볼 수 있음 Captured Out Print한 로그가 나타나는 공간 Experiment Details 실험 디테일 정보가 저장되는 곳 Host Info OS, CPU, Python version 등을 볼 수 있음 Config Config 설정값을 볼 수 있음 활용 방안 사용하고 있는 머신과 MongoDB를 연결해, 다양한 실험을 돌리고 Omniboard로 결과를 모니터링할 수 있음 이 문서는 local에 설치했지만, Docker를 사용해 설치하면 더 편리할 것 같음 Reference Sacred 공식 Github Sacred-example-pytorch Omniboard Document",
    "tags": "version mlops",
    "url": "/mlops/2019/07/22/sacred-with-omniboard/"
  },{
    "title": "머신러닝 실험을 도와줄 Python Sacred 소개",
    "text": "머신러닝 실험에서 사용되는 Config, Parameter 등을 더 손쉽게 저장할 수 있도록 도와주는 Python Library Sacred에 대한 글입니다 Sacred 대시보드 관련 내용은 Sacred와 Omniboard를 활용한 로그 모니터링에 작성했습니다! Python Sacred 머신러닝을 하다보면 각종 파라미터에 따라 결과가 어떻게 나오는지 기록해야함 처음엔 .log 파일에 작성해두다가, 점점 방대해지는 모습을 보게됨 Sacred는 환경 설정을 다시 reproduce할 수 있도록 도와주는 도구 실험의 모든 파라미터 추적 여러 설정에 대해 쉽게 실험을 할 수 있음 DB에 각 실행의 설정을 저장 결과를 reproduce Omniboard 등을 통해 저장된 값을 시각화할 수 있음 Omniboard란? Sacred에서 저장된 파라미터를 더 쉽게 볼 수 없을까?에 대한 고민으로 시작된 프로젝트 Sacred에서 저장된 파라미터를 Table로 보여주고, 성능도 알려줌 Sacred + Omniboard 조합을 사용하는 것을 매우 추천! Omniboard 설치 및 실행은 Sacred와 Omniboard를 활용한 로그 모니터링 참고 :) 공식 문서에 작성된 내용 Every experiment is sacred Every experiment is great If an experiment is wasted God gets quite irate Sacred의 Main mechanisms ConfigScopes : 함수의 local 변수를 편리하게 다룰 수 있음.@ex.config 데코레이터로 사용 Config Injection : 모든 함수에 있는 설정을 접근할 수 있음 Command-line interface : 커맨드 라인으로 파라미터를 바꿔서 실행할 수 있음 Observers : 실험의 모든 정보를 Observers에게 제공해 저장함. 이것들은 MongoDB에 저장 Automatic seeding : 실험의 무작위를 컨트롤할 때 도와줌 예시 설치 pip3 install sacred Dependency 설치 pip3 install numpy, pymongo First Experiment 샘플 코드를 통해 Sacred에 대해 알아보기 first_config.py 파일 생성 from sacred import Experiment ex = Experiment('hello_config') # Notebook일 경우 # ex = Experiment('hello_config', interactive=True) @ex.config def my_config(): recipient = \"world\" message = \"Hello %s!\" % recipient @ex.automain def my_main(message): print(message) ex = Experiment(‘실험 이름’) @ex.config로 파라미터를 저장함 @ex.automain 을 사용해 command line으로 실행할 수 있도록 설정 Jupyter Notebook에선 @ex.main을 사용하고, 실험을 만들 때 interactive=True로 설정 실행 python3 first_config.py 터미널에서 config 출력 python3 first_config.py print_config 여러번 실행하면 seed값이 달라지는 것을 알 수 있음 Command Line에서 파라미터 injection with [파라미터]=”value” 로 넣을 수 있음 python3 first_config.py with recipient=\"that is cool\" Experiment를 import해서 실행하는 방법 from first_config import ex r = ex.run() Python에서 파라미터 injection run할 때 config_updates 를 설정 from first_config import ex r = ex.run(config_updates={'recipient': 'awesome'}) Capture Functions from sacred import Experiment ex = Experiment('my_experiment') @ex.config def my_config(): foo = 42 bar = 'baz' @ex.capture def some_function(a, foo, bar=10): print(a, foo, bar) @ex.automain def my_main(): some_function(1, 2, 3) # 1 2 3 some_function(1) # 1 42 'baz' some_function(1, bar=12) # 1 42 12 some_function() # TypeError : missing value for 'a' Experiment 관찰하기 Experiments는 실행할 때 다양한 정보를 수집함 시작 시간, 중단 시간 사용된 환경 설정 발생한 에러나 결과물 머신의 기본 information 그 버전과 의존적인 패키지 import한 모든 local source files ex.open_resource로 file을 오픈하고 ex.add_artifact로 파일을 추가함 Observer는 MongoObserver, File Storage Observer, TinyDB Observer, SQL observer 등이 있습니다 공식 문서 MongoObserver에 연결하기 from sacred import Experiment from sacred.observers import MongoObserver ex = Experiment('hello_config') ex.observers.append(MongoObserver.create(url='your_server:27017', db_name='My_DB')) @ex.config def my_config(): recipient = \"world\" message = \"Hello %s!\" % recipient @ex.automain def my_main(message): print(message) FileStorageObserver 연결하기 from sacred import Experiment from sacred.observers import FileStorageObserver ex = Experiment('hello_config') ex.observers.append(FileStorageObserver.create('my_runs')) @ex.config def my_config(): recipient = \"world\" message = \"Hello %s!\" % recipient @ex.automain def my_main(message): print(message) Auth를 붙이고 싶은 경우 authentication protocol을 참고 Docker로 설정하고 싶은 경우 Docker Setup 참고 Seed 관리 Automatic Seed python3 first_config.py with seed=123 Global Seed 글로벌 시드 random, np.random이 설정되있는 경우 그걸 자동으로 설정 main 함수 안에서 수동으로 seed를 설정하면 모두 적용됨 Special Arguments _rnd 와 _seed 인자를 통해 random number를 생성할 수 있음 @ex.capture def do_random_stuff(_rnd, _seed): print(_seed) print(_rnd.randint(1, 100)) Logging Sacred는 Python logging 모듈을 사용해 정보를 저장함 Command Line에서 실행할 때 -l ERROR 를 추가해 실행하면 ERROR 아래 로그를 출력하지 않음 CRITICAL, ERROR, WARNING, INFO, DEBUG 등이 있음 Experiment에 로그 추가하기 실험에 로그를 추가하고 싶을 경우 _log 인자를 사용해 로그를 추가할 수 있음 from sacred import Experiment from sacred.observers import FileStorageObserver ex = Experiment('hello_config') ex.observers.append(FileStorageObserver.create('my_runs')) @ex.config def my_config(): recipient = \"world\" message = \"Hello %s!\" % recipient @ex.capture def some_function(_log): _log.info('Custom Info message!') @ex.automain def my_main(message): some_function() print(message) Logger 커스텀하기 Logger를 커스텀하고 싶은 경우, experiment 정의한 후, logger를 재정의하면 됨 # custom_logger.py import logging from sacred import Experiment ex = Experiment('log_example') # set up a custom logger logger = logging.getLogger('mylogger') logger.handlers = [] ch = logging.StreamHandler() formatter = logging.Formatter('[%(levelname).1s] %(name)s &gt;&gt; \"%(message)s\"') ch.setFormatter(formatter) logger.addHandler(ch) logger.setLevel('INFO') ex.logger = logger @ex.config def my_config(): recipient = \"world\" message = \"Hello %s!\" % recipient @ex.capture def some_function(_log): _log.info('Custom Info message!') @ex.automain def my_main(message): some_function() print(message) 실행 python3 custom_logger.py Ingredients 어떤 Task는 다양한 실험을 실행해야 할 수 있음 코드 중복을 피하는 방법은 함수를 추출해 가져오는 것 이런 작업을 구성해야 하면 구성 값을 모든 실험에 복사해야 함 Ingredients는 연관된 함수의 설정을 정의하고 다른 실험에 재사용할 수 있는 방법 특정 hook을 사용해 실험을 실행시킬 수도 있음 예제 from sacred import Ingredient, Experiment # ================== Dataset Ingredient ======================================= # could be in a separate file data_ingredient = Ingredient('dataset') @data_ingredient.config def cfg1(): filename = 'my_dataset.npy' normalize = True @data_ingredient.capture def load_data(filename, normalize): print(\"loading dataset from '{}'\".format(filename)) if normalize: print(\"normalizing dataset\") return 1 return 42 @data_ingredient.command def stats(filename, foo=12): print('Statistics for dataset \"{}\":'.format(filename)) print('mean = 42.23') print('foo=', foo) # ================== Experiment =============================================== @data_ingredient.config def cfg2(): filename = 'foo.npy' # 이 config가 덮어쓰기됨 # add the Ingredient while creating the experiment ex = Experiment('my_experiment', ingredients=[data_ingredient]) @ex.config def cfg3(): a = 12 b = 42 @ex.named_config def fbb(): a = 22 dataset = {\"filename\": \"AwwwJiss.py\"} @ex.automain def run(): data = load_data() # foo.npy print('data={}'.format(data)) Custom command Command 실행에 따라 다른 함수가 실행되도록 할 수 있음 @ex.command 로 함수를 감싸주면 됨 # my_command.py from sacred import Experiment ex = Experiment('my_commands') @ex.config def cfg(): name = 'kyle' @ex.command def greet(name): print('Hello {}! Nice to greet you!'.format(name)) @ex.command def shout(): print('WHAZZZUUUUUUUUUUP!!!????') @ex.automain def main(): print('This is just the main command. Try greet or shout.') 실행 python3 my_command.py greet # Hello kyle! Nice to greet you! python3 my_command.py shout # WHAZZZUUUUUUUUUUP!!!???? 실전 적용 위에서 나온 Sacred 예제는 모두 공식 홈페이지를 참고했는데, 이번엔 직접 머신러닝 예제를 구현 코드는 Github에 존재 Folder Structure ├── experiments : 실험 스크립트가 존재하는 폴더 │   ├── random_forest.py │   └── svc.py └── main.py : 실험을 실행하는 main.py random_forest.py from sklearn import datasets, model_selection from sklearn.ensemble import RandomForestClassifier from sacred import Experiment from sacred.observers import FileStorageObserver from sklearn import metrics ex = Experiment('rf') ex.observers.append(FileStorageObserver.create('my_runs')) @ex.config def cfg(): n_estimators=100 seed = 42 @ex.capture def get_model(n_estimators): return RandomForestClassifier(n_estimators) @ex.automain def run(_log): X, y = datasets.load_breast_cancer(return_X_y=True) _log.info(\"[INFO] Now split dataset in RF\") X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2) clf = get_model() clf.fit(X_train, y_train) y_pred = clf.predict(X_test) _log.info(\"[INFO] End Predict!\") return metrics.accuracy_score(y_test, y_pred) svc.py from sklearn import svm, datasets, model_selection from sacred import Experiment from sacred.observers import FileStorageObserver ex = Experiment('svc') ex.observers.append(FileStorageObserver.create('my_runs')) @ex.config def cfg(): C = 1.0 gamma = 0.7 kernel = \"rbf\" seed = 42 @ex.capture def get_model(C, gamma, kernel): return svm.SVC(C=C, kernel=kernel, gamma=gamma) @ex.automain def run(_log): X, y = datasets.load_breast_cancer(return_X_y=True) _log.info(\"[INFO] Now split dataset\") X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2) clf = get_model() clf.fit(X_train, y_train) _log.info(\"[INFO] End Fit!\") return clf.score(X_test, y_test) main.py from experiments.random_forest import ex as rf_ex from experiments.svc import ex as svc_ex rf_run = rf_ex.run() print(rf_run.config) print(rf_run.result) print(rf_run.info.keys()) print(\"New experiment run!\") rf_run2 = rf_ex.run(config_updates={\"n_estimators\": 10}) print(rf_run2.config) print(rf_run2.result) svc_run = svc_ex.run() print(svc_run.config) print(svc_run.result) 실행 python3 main.py Metric _run.log_scalar(metric_name, value, step) 이런식으로 metric을 생성해 추가할 수 있음 자세한 내용은 Metrics API 참고 다음 글은 Omniboard와 연결해 설정 및 파라미터 시각화 하는 방법에 대해 소개하겠습니다 :) Reference Sacred 공식 Github 많은 실험을 저장해 사용한 예제는 Kaggle telstra Github 참고 Pytorch에서 사용한 예제는 Github 참고 Sacred_Deep_Learning",
    "tags": "version mlops",
    "url": "/mlops/2019/07/21/python-sacred/"
  },{
    "title": "Google Spreadsheets(Sheets)에서 BigQuery 사용하기",
    "text": "스프레드시트에서 BigQuery를 사용해 데이터를 추출하는 방법에 대해 작성한 글입니다 OWOX를 사용하는 방법 Data Connectors를 사용하는 방법 BigQuery BigQuery는 Google Cloud Platform의 Data Warehouse 제품 관련된 내용은 BigQuery의 모든 것 발표 자료에 많이 나와있음 Spreadsheet 스프레드시트는 구글의 제품으로 우리가 친숙한 엑셀과 비슷한 구조를 가지고 있음 스프레드시트에서 간단한 시각화, 피벗 테이블 등 데이터를 가공할 수 있음 BigQuery + Spreadsheet 매일 특정 쿼리를 날려 데이터를 추출하고 싶을 경우 BigQuery의 스케쥴 쿼리를 사용하거나, Python를 사용해 데이터를 추출할 수 있음 개발에 친화적인 사람이라면 위 방법도 편리하지만, 개발에 친화적이지 않은 사람들은 스프레드시트를 사용하는 방법이 더 친숙 2가지 방법 OWOX를 사용하는 방법 (부가기능, Add-ons) Data Connectors를 사용하는 방법 G Suite Business 고객만 사용할 수 있음 결과를 10,000행까지 받을 수 있음 OWOX 사용 방법 1) 스프레드시트에서 부가기능(Add-ons) 추가 sheet.new로 새로운 스프레드시트로 접속한 후, 상단의 부가기능을 클릭 부가기능 검색창에 OWOX를 검색 OWOX BI BigQuery Reports의 오른쪽에 있는 추가 버튼 클릭(+ 무료) Google 로그인 후, OWOX BI BigQuery Reports이(가) 내 Google 계정에 액세스하려고 합니다 라는 부분에서 허용을 클릭 만약 BigQuery API가 활성화되어 있지 않다면 확인창이 추가로 뜨는데, 확인 누르면 됨 2) 부가기능 활성화 스프레드시트에서 부가기능 - 부가기능 관리 클릭 부가기능 - OWOX BI BigQuery Reports이 보인는지 확인 3) 쿼리 날리기(=데이터 추출하기) 부가기능 - OWOX BI BigQuery Reports - Add a report 클릭 create 클릭 이 부분에 쿼리를 작성! 단, 쿼리를 작성할 때 #stadardSQL인지 #legacySQL인지 명시해줘야 함 Save &amp; Run 클릭 결과가 스프레드시트의 새로운 시트로 저장됨 쿼리의 결과를 Report라고 부름 4) 쿼리 수정하기 Select a query에 있는 쿼리를 선택한 후 Edit 클릭해 수정할 수 있음 Revision history를 확인하면 예전에 사용한 쿼리를 볼 수 있음 5) 파라미터 설정하기 How to add parameters?를 클릭하면 파라미터를 동적으로 사용하는 방법에 대해 알려줌 Input 입력하는 방식, 날짜를 선택하는 방식, 리스트에서 선택하는 방식이 가능 6) 이미 작성된 레포트를 수정하고 싶은 경우 Edit current report 클릭 후 수정 7) 데이터를 최신화하고 싶은 경우 Refresh currernt report 클릭 (현재 레포트만 수정) Refresh all reports 클릭 (모든 레포트 수정) 8) 쿼리를 주기적으로 돌리고 싶은 경우(스케쥴링) Schedule reports 클릭 Enable reports to run automatically 클릭 Schedule reports to run every day between 4 am - 5am 에서 빈도와 시간대를 설정 이 시간대는 스프레드시트의 시간대에 따라 달라지는데, 파일 - 스프레드시트 설정을 통해 현재 스프레드시트의 시간대를 확인할 수 있음 Execute all configured reports 클릭을 해제하면 여러 Report 중 어떤 것을 스케쥴링할지 선택할 수 있음 Receive email notification을 실패할 때 메일 받거나 매번 메일을 받는 옵션을 설정할 수 있음 I understand Google BigQuery query pricing policy를 체크해야 함 BigQuery는 쿼리를 날릴 때마다 비용이 부과되기 때문에 이 점을 꼭 유의해야함 Data Connector 사용 방법 이건 일반 계정에선 보이지 않고 G Suite Business를 사용하는 계정에서만 보임(19.7.20 기준이며, 추후에 변경될 수 있습니다) 1) 데이터 연결 데이터 - 데이터 커넥터 클릭 2) 쿼리 날리기(=데이터 추출하기) 데이터 연결할 프로젝트 선택한 후, 쿼리 작성 클릭 결과 미리보기를 진행한 후, 결과 삽입을 클릭하면 데이터가 추가됨 3) 쿼리 수정 데이터 하단에 있는 새로고침이나 버튼을 클릭하면 수정할 수 있음 뭘 사용할까? 취향의 차이가 존재할 수 있으나 아직(19.07.20 기준) OWOX가 조금 더 기능이 우세(스케쥴링 기능, 메일 전송) Data Connector는 추출할 Row 수가 아직 최대 10,000개까지만 가능한 점이 아쉬움(OWOX는 메모리 상황에 따라 다르지만 10,000개 이상도 가능) 또한 Data Connector는 G Suite를 사용해야 생기는 기능 따라서 보통의 경우엔 OWOX를 사용하는 것을 추천드립니다 :) Reference Connecting BigQuery and Google Sheets to help with hefty data analysis BigQuery Data Connector for Google Sheets: how to get more than 10,000 rows",
    "tags": "BigQuery gcp",
    "url": "/gcp/2019/07/20/bigquery-with-spreadsheet/"
  },{
    "title": "Google Cloud Functions과 Cloud Scheduler를 사용해 Python 스크립트 주기적으로 실행하기",
    "text": "Google Cloud Functions과 Cloud Scheduler를 사용해 Python 스크립트를 주기적으로 실행하는 방법에 대해 설명한 글입니다 Google Cloud SDK를 설치한 후 아래 글을 읽어주세요 사용할 도구 Cloud Functions으로 함수를 클라우드에 올리고, 그 함수를 Cloud Scheduler로 주기적으로 돌릴 예정 Google Cloud Functions 클라우드에서 코드를 간단히 실행할 수 있음 AWS의 Lambda와 유사함 과거엔 자바스크립트(Node.js 런타임)만 있었으나, 최근에 Go(1.11), Python(3.7)도 추가됨 : 2019년 7월 6일 기준 HTTP 함수와 백그라운드 함수로 나뉨 전자는 HTTP를 통해 함수를 직접 호출 후자는 Google Cloud Pub/Sub의 메세지 또는 Google Cloud Storage 버킷의 변경 사항을 통해 Cloud Functions를 간접적으로 호출 가격은 호출 횟수, 코드가 실행되는 컴퓨팅 리소스 사용 시간, 네트워크 사용량에 따라 결정되며 무료 사용량은 월 200만회까지 무료, 리소스 사용은 메모리는 400,000GB-seconds, 컴퓨팅 시간은 200,000 GHz까지 무료이며 자세한 내용은 Pricing 문서 참고 Reference 코드를 보고싶은 경우엔 python-docs-samples 참고 용도 Cloud Scheduler App Engine 트리거, Pub/Sub을 통해 메세지 보내기, Compute Engine에 HTTP 엔드포인트 호출 등 다양하게 가능 crontab보다 쉽게 사용 가능 AWS의 클라우드 워치와 유사 월 3개의 Job까지 무료(횟수가 아니고 Job의 개수에 비용 부과 : Job이 20번 돌아도 상관 없음) 단, 기본적으로 1분에 최대 60개의 Cron이 가능하고 하루 최대 만건의 CronJob 실행이 가능한데 더 필요할 경우 Quota Request를 해두면 됨 작업 흐름 Cloud Functions 포맷에 맞게 Python으로 함수를 만든다 Cloud Functions에 배포한다 Cloud Scheduler에서 주기적으로 Cloud Functions의 함수를 실행시킨다 Hello World 우선 아무것도 모르는 상태에서 Hello World를 찍어보며 진행하겠습니다 main.py 작성 def hello_world(request): return 'Hello, World!\\n' 배포 gcloud beta functions deploy hello_world --runtime python37 --trigger-http 배포시 아래 메세지가 나타남 Cloud Functions Console에서 확인해보면 아래와 같이 배포된 것을 알 수 있음 최근 배포 시간 우측의 세로로 3개의 점을 클릭한 후, 함수 테스트 클릭하면 아래와 같이 나옴 함수 테스트 버튼을 통해 테스트 가능 위에서 배포를 한번 더 진행해서 버전: 2로 나옴 Python으로 Cloud Functions 만들기 간단하게 로컬에서 파이썬 스크립트를 만든 후, gcloud functions를 사용해서 올리면 끝! Python에서 함수를 만들 경우엔 아래와 같은 구조로 생성 folder/ ├── main.py : main.py 아래에 있는 함수를 Cloud Functions에서 사용 가능 └── requirements.txt : 설치할 패키지 만약 개인이 만든 패키지도 같이 올리고 싶은 경우 folder/ ├── main.py ├── requirements.txt └── mypackage/ ├── __init__.py └── myscript.py Cloud Functions의 Python 3 Runtime에 설치된 시스템 패키지는 링크를 참고하면 알 수 있음 : git, wget 등이 설치되어 있음 Cloud Functions 배포하기 배포는 다양한 방식이 존재하는데 크게 아래와 같이 나눌 수 있음 1) 로컬에서 파일을 업로드한 후, 배포하는 경우 2) Github나 Bitbucket 같은 소스 저장소의 코드를 배포하는 경우 두 방법 모두 gcloud 도구를 사용할 예정이며, 콘솔에서 진행하고 싶으면 Console 문서 참고 여기선 1) 위주로 진행 gcloud 명령어 구조 gcloud beta functions deploy &lt;NAME&gt; &lt;TRIGGER&gt; --stage-bucket &lt;STORAGE_BUCKET&gt; --entry-point &lt;FUNCTION_NAME&gt; : Cloud Functions의 이름으로 문자, 숫자, 밑줄, 하이프만 포함할 수 있음. entry-point 옵션을 지정하지 않는한 모듈에서 같은 이름의 함수를 내보내야 함 : 트리거의 종류로, 자세한 내용은 [링크](https://cloud.google.com/functions/docs/calling/?hl=ko) 참고. `--trigger-http`, `--trigger-topic`, `--trigger-bucket`, `--trigger-event`, `--trigger-resource` 등이 있음 --stage-bucket나 --entry-point는 선택적 인자! (없어도 상관 없음) –stage-bucket : 함수의 코드를 저장하는 Google Storage 버킷. 이 인자를 생략할 경우 별도의 storage에 저장해서 활용 –entry-point : 배포시 사용하는 과 다른 이름을 가지는 경우 사용 그 외에도 --allow-unauthenticated 등의 인자도 있음 더 자세한 명령어는 gcloud beta functions deploy 링크 참고 예시 위에서 처음 진행할 때 사용한 코드 gcloud beta functions deploy helloworld --trigger-http entry point를 지정한 경우 Cloud Functions의 이름은 hello고, 함수 사용시엔 helloworld라는 이름으로 사용함 gcloud beta functions deploy hello --entry-point helloworld --trigger-http Cloud Scheduler로 주기적으로 실행하기 콘솔에서 Cloud Scheduler로 이동 작업 만들기 클릭 지역 설정. Cloud Functions에서 만든 Region과 동일하게 설정 작업 만들기 빈도는 crontab 형식으로 작성 시간대는 기준 시간대를 설정. 대한민국도 존재 대상에 HTTP를 설정 Cloud Functions에서 나온 URL을 입력 예시 : https://us-central1-{project}.cloudfunctions.net/{entry_point} Body가 필요하면 작성한 후, 만들기 생성한 후, Cloud Scheduler 화면을 보면 아래와 같이 스케쥴 Job들이 나타남 gcloud로도 가능한데, gcloud beta scheduler jobs create http를 참고하면 볼 수 있음 gcloud beta scheduler jobs create http 앞으로 고민할 내용들 앞에서 진행한 Cloud Functions는 앞에 인증하는 과정이 없기 때문에 아무나 request할 수 있음. 이 점은 실제 서비스엔 치명적이기 때문에 보안을 신경써야 함 AWS에선 GATEWAY를 사용해 API_KEY 값을 같이 물고 들어가는 방식으로 사용했는데, 아직 Cloud Functions는 그런 작업이 잘 발달하지 않고, 레퍼런스도 적은 느낌(제가 잘못 알고있다면 제보 부탁드립니다!) Firebase를 사용하는 예시가 있긴한데, 굳이? 라는 생각이 들고, Cloud Endpoints 를 사용한다는 말도 있는데, Python 예제는 거의 없음 Wrapping Google Cloud Functions HTTP Triggers in Endpoints CloudSQL를 어떻게 연결할 수 있는지? 우선 공식 문서에 나와있긴 함 Terraform을 사용해 인프라를 코드로 관리 Reference Python-docs-samples Github Cloud Functions 공식 Document Serverless Architecture with Cloud Function Cloud 스케줄러로 Compute 인스턴스 예약하기 Google Cloud Functions Tutorial : Using the Cloud Scheduler to trigger your functions",
    "tags": "cloud-functions gcp",
    "url": "/gcp/2019/07/07/google-cloud-functions-python/"
  },{
    "title": "2019년 상반기 회고 및 글또 3기 시작",
    "text": "2019년 상반기 회고 및 글또 3기에 대한 생각을 담은 글입니다 2019년 + 저번 회고 리뷰 2019년이 되고 30살이다! 라고 생각하던게 엊그제 같은데, 벌써 절반이 끝났습니다. 시간이 정말 너무 빠른 것 같아요 특히 상반기는 더 더 더 빠르게 지나간 것 같습니다 회고에 앞서 저번 회고엔 어떤 내용을 작성했는지 2018년 회고, 2019년 다짐을 다시 보고 왔습니다. 2019년 다짐은 아래와 같네요 독서 개발 서적 외에도 다양하게 읽자 =&gt; 개발 서적이 아닌 책은 리디북스에서 월 1권정도 읽고 있네요..! 돌이켜보니 리디셀렉트를 적극적으로 이용하고 있지 않은 느낌? 더 독서해야겠네요 개발 서적은.. 일단 보이면 다 지르고 천천히 읽고 있습니다. 개발 서적은 월 3권 정도는 보는 것 같네요 요리 1도 안하네요ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 아 음… 내 다짐은 어디로… 요리를 꼭 해야되나? 라고 생각이 변하고 있네요(그 시간에 코딩을 하지..!) 음.. 이건 좀 더 지켜보고 어떻게 할지 고민해야겠습니다 책 집필 상반기에 데이터 사이언스를 공부하고 싶은 분들을 위한 책을 내자! 가격도 그냥 거의 만원? 이렇게 생각했습니다. 관련 내용은 Github에 이미 꽤 있어서 쉬울거라 판단했습니다 하지만 글을 작성하려니 생각보다 어려웠는데, 그 이유는 제가 모든 분야를 자세히 알진 못한다. 얕게만 알고 있음 =&gt; 그 사이에 비전, 자연어 등 계속 발전하고 있는데 제가 팔로업을 하지 못해서 과연 쓸 수 있을까? 고민했습니다 블로그 글 작성과 책 쓰는 것은 완전 다른 일. 글 자주 쓴다고 책을 잘 쓰는 것은 아님 팔리는 책을 만들기 위해선 생각보다 더 디테일을 잡아야 한다 이런 생각 + 사내 SQL 강의, 외부 발표(한빛미디어 데브그라운드), Edwith 컨텐츠 작업, 스타트업 머신러닝 플레이어 모임 등을 하다보니 시간이 지났고, 아마 이 내용은 함께 쓸까? 라는 생각이 듭니다 생각보다 제가 BigQuery 책을 더 쓰고 싶어한다는 점을 깨달았습니다. 약 300쪽이 되는 BigQuery의 모든 것 자료 만들 때 즐거웠습니다. 아마 책을 쓴다면 하반기에 빅쿼리를 써볼까 고민하고 있습니다 회사에서 다짐 회사에서 시간이 정말 빨리간 이유는 3월 ~ 5월에 두 팀의 팀장을 겸직해서 정말 시간이 빨리 지나갔고, 타다가 성장하면서 할 일들도 많아져서 더 몰입하며 일했습니다 6월 부터는 두 팀의 팀장이 아닌 한 팀의 팀장만 맡기로 했고, 여유가 다시 돌아오고 있습니다 팀원의 성장, 데이터 그룹의 성장 등 어떻게 구성원들과 함께 더 성장할까?를 매번 고민하고 있습니다 데이터 그룹이 아닌 기획, 마케팅 등 다양한 부서에서 SQL를 배우고 싶다는 욕구가 있어 사내 SQL 강의도 진행했습니다. 유튜브 라이브방송으로 지방에 계신 분들도 수강하셨어요 글또 글또 3기는 밑에서 따로 이야기를! 강화학습 강화학습을 재미로 꼭 공부하려고 하는데, 현재 하는 일이 많아서 하반기 또는 내년으로 미루려고 합니다! GDE 이건 회고에만 있는데 실질적으로 아무 행동도 하지 않았네요. 클라우드 스터디잼은 종종 참여했고, 기존에 제가 자주 사용하지 않았던 내용에 대해 테스트해보고 있습니다 그리고 상반기에 작성했던 데이터 사이언티스트가 되기 위해 진행한 다양한 노력들이란 글이 구글에서 데이터 사이언티스트란 검색 결과에 상위 노출되는 것을 보고 뿌듯했었네요 :) 상반기 평가(만족스러운가?) 전반적으로 만족하고 있습니다. 상반기는 고요하고 평온하게 제가 할 일을 묵묵히 해온 시간이라고 생각합니다 제품의 성장을 더 부스팅할 수 있도록, 하반기엔 조금 더 집중해 성과를 내보려고 합니다 외부 발표도 진행했고, 제가 배운 것들을 나눌 수 있어서 만족스럽습니다 최근 회사에 조인하신 인턴분들을 보며 더 자극되고, 저도 더! 성장하려고 다짐하고 있습니다 Discrete Optimization, GIS 등 완전 새로운 내용들을 익히며 즐겁게 지내고 있습니다 글또 3기 글또는 아마 제가 꾸준히 가져갈 모임이라고 생각하는데, 이번엔 지원자가 60명이었습니다 총 합격자는 44분으로, 직전 기수인 2기 대비 2배 증가한 인원입니다 매 기수마다 약 2배씩 늘고 있는데, 그럼 4기엔 80분인가..(설마) 많은 분들이 있어 좋은 시너지가 더 나올거라 생각하고 있습니다 아이디어를 다양하게 내주셔서 글또 Github도 만들고, 기존에 수동으로 했던 작업들을 자동화할 수 있도록 진행하고 있습니다 좋은 분들과 함께인 만큼 저도 열심히 좋은 커뮤니티를 유지하고 글을 잘 쓸 수 있도록 서포트하겠습니다 (강제 상호 리뷰 시스템으로 아마 더 많은 긍정적 효과가 나오길 기대하고 있어요!) 하반기 하반기가 끝나면 또 2019년이 벌써 끝났구나.. 하면서 회고할 것 같네요. 그 때 돌이켜볼 때 제가 기대하는 것들은 아래와 같습니다 전문성을 놓지 말기 팀장 포지션이 되면 관리직이 되기 쉬운데, 아직은 관리직과 실무를 같이 겸하는 사람이 되고 싶습니다. 매우 빡빡하고 어렵지만, 그래도 노력하면!!! 가능할 것이라고 생각하고 있어요 :) 건강 이제 슬슬 건강을 매우 신경쓸 나이라고 생각하고 있어요(사실 나이 상관없이 건강은 언제나 중요) 글또처럼 꾸준히 운동하는 그런 환경을 만들어 제게 테스트해볼 생각입니다 좋은 영향력 요새 꼭 개발이나 머신러닝이 아니어도 다른 사람에게 긍정적인 영향을 끼치고 싶다는 생각을 많이 하고 있습니다. 좋은 영향을 끼칠 수 있도록 다양하게 노력해볼 예정이에요 :) 하반기엔 더 즐거운 일 가득하길 바라며 상반기 회고를 끝냅니다-!",
    "tags": "diary",
    "url": "/diary/2019/07/07/2019-half-retrospect/"
  },{
    "title": "Tensorflow Data Validation 사용하기",
    "text": "TFX에서 Data Validation을 담당하고 있는 TensorFlow Data Validation(TFDV)에 대한 글입니다 TFDV Github Repo TensorFlow Data Validation는 데이터를 더 쉽게 이해하고 점검할 수 있도록 도와주는 라이브러리 Google에서 매일 페타바이트의 데이터를 분석하고 점검할 때 사용 Train, Test Data의 요약 통계, 분포를 쉽게 비교 가능 필수 값, 데이터에 대한 기대치를 의미하는 데이터 스키마 생성 지정된 조건을 벗어나는 경우(예시 : int 타입을 예상했는데 float이 들어오는 경우) 예외를 식별 Python 2.7, 3.5만 지원 Mac OS(시에라) 이상, Ubuntu 16.04 이상 지원 윈도우는 추후 지원 예정 내부적으로 Facet, Apache Beam 사용 회사 업무시, 데이터 경진대회(캐글 등)에서 유용할 듯 이 글은 0.13.1 버전 기준으로 작성했습니다. 추후 계속 수정될 예정입니다 코드는 모두 Github에 올렸습니다. Nbviewer로 확인 부탁드려요! 기능 Statistics 생성 및 시각화 Train과 Test셋 데이터 분포 동시에 확인 스키마 추론 데이터 검증 Drift와 Skew 체크 설치 pip install tensorflow-data-validation 설치한 후, 확인 import sys, os import tempfile, urllib, zipfile import tensorflow_data_validation as tfdv print('TFDV version: {}'.format(tfdv.version.__version__)) 데이터 준비 시카고 택시 데이터를 다운로드 후, 압축 해제 BASE_DIR = tempfile.mkdtemp() DATA_DIR = os.path.join(BASE_DIR, 'data') OUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output') TRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv') EVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv') SERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv') zip, headers = urllib.urlretrieve('https://storage.googleapis.com/tfx-colab-datasets/chicago_data.zip') zipfile.ZipFile(zip).extractall(BASE_DIR) zipfile.ZipFile(zip).close() Statistics 생성 및 시각화 csv, dataframe, tfrecord을 통해 Statistics를 생성할 수 있음 [methods for methods in dir(tfdv) if \"generate\" in methods] &gt;&gt;&gt; ['generate_statistics_from_csv', 'generate_statistics_from_dataframe', 'generate_statistics_from_tfrecord'] Statistics 생성 train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA) # method : tfdv.generate_statistics_from_csv(data_location, column_names=None, delimiter=',', output_path=None, stats_options=&lt;tensorflow_data_validation.statistics.stats_options.StatsOptions object at 0x14436a110&gt;, pipeline_options=None) train_stats를 확인해보면 feature별로 다양한 통계치를 가지고 있음 시각화 tfdv.visualize_statistics(train_stats) # method : tfdv.visualize_statistics(lhs_statistics, rhs_statistics=None, lhs_name='lhs_statistics', rhs_name='rhs_statistics) Interactive하게 직접 만져보고 싶으신 분은 웹에서 제 Github nbviewer 참고! Numeric Features 맨 왼쪽에 Sort By 옵션 Non-uniformity Amount missing/zero 오른쪽에 Chart to show은 분포, Quantiles, Value list length 등으로 볼 수 있고, 클릭으로 log로 변환 가능 Categorical Feature도 아래처럼 시각화됨 SHOW RAW DATA를 누르면 카테고리 데이터 count 결과 보여줌 Train과 Eval Data 동시에 시각화 Kaggle에서 매우 유용할 듯 eval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA) tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats, lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET') 이제 파란색은 Eval data, 주황색은 Train data 우측 Charts to show에서 percentages를 클릭시 Eval과 Train의 비율을 포개서 보여줌 스키마 추론 tfdv.infer_schema로 스키마를 추론한 후, tfdv.display_shcema로 출력 infer schema 결과 스키마 Protocol buffer가 생성됨 schema = tfdv.infer_schema(statistics=train_stats) # method : tfdv.infer_schema(statistics, infer_feature_shape=True, max_string_domain_size=100) tfdv.display_schema(schema=schema) 각 Feature별 Type, Presence, Valency, Domain 출력 Feautre의 속성을 바꾸고 싶을 경우 tfdv.get_feature로 가져온 후 수정 tfdv.get_feature(schema, 'payment_type').name = \"oh\" schema # find name: 'oh' 다시 복구 tfdv.get_feature(schema, 'oh').name = \"payment_type\" 데이터 검증 Validation 기능 예를 들어 Train엔 없는 Categorical Value가 Test에 존재한다면? anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema) tfdv.display_anomalies(anomalies) company, payment type feature에 예상하지 못한 값이 있음 문제 해결을 위해 min_domain_mass 제약 조건을 추가하고, value를 수동으로 추가함 # Relax the minimum fraction of values that must come from the domain for feature company. company = tfdv.get_feature(schema, 'company') company.distribution_constraints.min_domain_mass = 0.9 # Add new value to the domain of feature payment_type. payment_type_domain = tfdv.get_domain(schema, 'payment_type') payment_type_domain.value.append('Prcard') # Validate eval stats after updating the schema updated_anomalies = tfdv.validate_statistics(eval_stats, schema) tfdv.display_anomalies(updated_anomalies) Serving시 데이터 검증 requirements를 정의해 특정 문제가 발생하면 자동으로 수정될 수 있게 보조해주는 기능도 존재 serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA) serving_anomalies = tfdv.validate_statistics(serving_stats, schema) tfdv.display_anomalies(serving_anomalies) tips 컬럼은 완벽하게 missing이고 trip_seconds는 FLOAT 타입을 예상했는데 INT 타입이 들어옴 타입 문제 해결 방법 infer_type_from_schema=True인 StatsOptions를 생성한 후, generate_statistics_from_csv에 추가 options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True) serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options) serving_anomalies = tfdv.validate_statistics(serving_stats, schema) tfdv.display_anomalies(serving_anomalies) tips feature가 serving 데이터에 없는 문제 해결 방법 스키마 환경을 TRAINING과 SERVING으로 생성한 후, SERVING시 tips 데이터가 없는 것을 명시 # All features are by default in both TRAINING and SERVING environments. schema.default_environment.append('TRAINING') schema.default_environment.append('SERVING') # Specify that 'tips' feature is not in SERVING environment. tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING') serving_anomalies_with_env = tfdv.validate_statistics( serving_stats, schema, environment='SERVING') tfdv.display_anomalies(serving_anomalies_with_env) Drift와 Skew 체크 tfdv.validate_statistics() method를 사용해 체크 Drift Drift detection은 Categorical 데이터 및 데이터의 연속 기간(N, N+1) 사이(예를 들면 서로 다른 날의 훈련 데이터 사이)에서 지원 L-infinity distnace로 Drift를 표현하고 허용값보다 높으면 경고를 받을 수 있음 정확한 거리를 설정하는 것은 도메인 지식과 실험을 필요로하는 반복 프로세스 Skew Schema Skew 같은 스키마를 가지지 않을 때 Feature Skew Feature 생성 로직이 변경될 때 Distribution Skew Train, Serving 데이터 분포가 다를 경우 # Add skew comparator for 'payment_type' feature. payment_type = tfdv.get_feature(schema, 'payment_type') payment_type.skew_comparator.infinity_norm.threshold = 0.01 # Add drift comparator for 'company' feature. company=tfdv.get_feature(schema, 'company') company.drift_comparator.infinity_norm.threshold = 0.001 skew_anomalies = tfdv.validate_statistics(train_stats, schema, previous_statistics=eval_stats, serving_statistics=serving_stats) tfdv.display_anomalies(skew_anomalies) payment_type이 train / serving시 거리가 매우 큼 company가 과거와 현재의 거리가 매우 큼 위 예시에선 의도적으로 threshold값을 낮게 설정해 오류가 발생하도록 한 것이기 때문에 따로 수정하지 않음 스키마 저장 스키마를 검토하고 큐레이션이 되었으니 “frozen”(고정된) 상태를 반영하도록 Protocol Buffer에 저장 from tensorflow.python.lib.io import file_io from google.protobuf import text_format file_io.recursive_create_dir(OUTPUT_DIR) schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt') tfdv.write_schema_text(schema, schema_file) !cat {schema_file} 데이터 형태가 다를 경우 현재 TFRecord, CSV, Dataframe만 지원하는데 그 외의 데이터 타입을 사용한다면 Apache Beam의 PTransform을 사용해 데이터를 가공할 수 있음 링크 참고하면 예시가 나와있음 구글 클라우드에서 사용하기 설치 pip download tensorflow_data_validation \\ --no-deps \\ --platform manylinux1_x86_64 \\ --only-binary=:all: 예시 코드 snippet import tensorflow_data_validation as tfdv from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions PROJECT_ID = '' JOB_NAME = '' GCS_STAGING_LOCATION = '' GCS_TMP_LOCATION = '' GCS_DATA_LOCATION = '' # GCS_STATS_OUTPUT_PATH is the file path to which to output the data statistics # result. GCS_STATS_OUTPUT_PATH = '' PATH_TO_WHL_FILE = '' # Create and set your PipelineOptions. options = PipelineOptions() # For Cloud execution, set the Cloud Platform project, job_name, # staging location, temp_location and specify DataflowRunner. google_cloud_options = options.view_as(GoogleCloudOptions) google_cloud_options.project = PROJECT_ID google_cloud_options.job_name = JOB_NAME google_cloud_options.staging_location = GCS_STAGING_LOCATION google_cloud_options.temp_location = GCS_TMP_LOCATION options.view_as(StandardOptions).runner = 'DataflowRunner' setup_options = options.view_as(SetupOptions) # PATH_TO_WHL_FILE should point to the downloaded tfdv wheel file. setup_options.extra_packages = [PATH_TO_WHL_FILE] tfdv.generate_statistics_from_tfrecord(GCS_DATA_LOCATION, output_path=GCS_STATS_OUTPUT_PATH, pipeline_options=options) 언제 TFDV를 사용해야할까? 갑자기 이상한 feature가 들어오는지 확인하고 싶을 경우 Decision surface에서 모델이 훈련 잘되었는지 확인하고 싶을 경우 feature engineering 실수 방지하고 싶을 경우 Facets 사실 TFDV의 데이터 분포 보여주는 부분은 Facets으로 이루어져 있음 Facets은 현재 2가지 시각화를 제공하는데, 1) FACETS OVERVIEW가 위에서 TFDV가 보여준 시각화고 2) FACETS DIVE로 다양한 양의 데이터를 한번에 인터랙티브하게 시각화해줌 FACETS DIVE는 아래처럼 시각화됨 데이터를 연령대별 직업별로 보여줌 Quick Draw 데이터셋을 인터렉티브하게 보려면 링크 참고 Colab에서 사용하는 예제는 Github에 나와있음 데이터를 데이터프레임으로 불러온 후, to_json을 사용해 json으로 변경 그 후 HTML Template을 사용해 노트북에서 볼 수 있도록 출력 # Load UCI census train and test data into dataframes. import pandas as pd features = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\", \"Country\", \"Target\"] train_data = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=features, sep=r'\\s*,\\s*', engine='python', na_values=\"?\") test_data = pd.read_csv( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=features, sep=r'\\s*,\\s*', skiprows=[0], engine='python', na_values=\"?\") # Display the Dive visualization for the training data. from IPython.core.display import display, HTML jsonstr = train_data.to_json(orient='records') HTML_TEMPLATE = \"\"\" &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/0.7.24/webcomponents-lite.js\"&gt;&lt;/script&gt; &lt;link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"&gt; &lt;facets-dive id=\"elem\" height=\"600\"&gt;&lt;/facets-dive&gt; &lt;script&gt; var data = {jsonstr}; document.querySelector(\"#elem\").data = data; &lt;/script&gt;\"\"\" html = HTML_TEMPLATE.format(jsonstr=jsonstr) display(HTML(html)) Refenrece TFDV Github TFDV Document Facets Github",
    "tags": "tfx mlops",
    "url": "/mlops/2019/05/12/tensorflow-data-validation-basic/"
  },{
    "title": "Mac, Linux에서 Zip 파일에 password(암호) 설정하는 방법",
    "text": "Mac OS와 Linux(Ubuntu)에서 Zip 파일에 암호를 설정하는 방법에 대해 작성한 글입니다 별도의 프로그램 설치 없이 터미널에서 진행하는 방법입니다 Mac OS에서 압축시 암호 걸기 Mac Zip 파일로 압축할 때, 암호 설정하는 방법(Terminal) zip 명령어를 날릴 때 -e 옵션을 주면 암호를 설정할 수 있음 약자의 의미는 encrypt(암호화)를 뜻함 입력하면 Enter password: 가 나오고 암호를 입력하면 됨(Verify password에서 1번 더 입력) zip -e 압축_파일.zip 압축에_사용될_파일 특정 확장자로 끝나는 모든 파일을 압축하고 싶은 경우엔 *를 사용하면 됨 zip -e 압축_파일.zip *.json 폴더째 압축하고 싶은 경우 -r 옵션 을 주면 됨(암호까지 주려면 -er) zip -er zip_file.zip ./ zip에 대해 궁금하면 zip을 치면 관련 명령어가 나옴 Mac OS에서 압축 풀기 unzip 명령어를 통해 압축을 풀 수 있음 암호가 설정되어 있으면 암호를 물어봄 unzip zip_file.zip unzip을 치면 관련 명령어가 나옴 Linux(Ubuntu)에서 압축시 암호 걸기 설치 apt-get update apt-get install zip zip 명령어 사용시 -P 옵션 사용하면 암호를 설정할 수 있음 그러나 보안에 취약하기 때문에 비추천 zip -P 암호 압축_파일.zip 압축에_사용될_파일 Mac과 동일하게 -e 옵션을 주면 암호를 설정할 수 있음 zip -e 압축_파일.zip 압축에_사용될_파일 Linux(Ubuntu)에서 압축 풀기 설치 apt-get update apt-get install unzip Mac OS와 동일하게 사용 unzip 압축_파일.zip",
    "tags": "linux development",
    "url": "/development/2019/05/08/mac-zip-password/"
  },{
    "title": "Airflow BigQuery Operator의 이해",
    "text": "Apache Airflow의 BigQuery Operator에 대한 글입니다 BigQuery BigQuery는 Google Cloud Platform에서 매우 좋은 평가를 받고 있는 Managed 데이터 웨어하우스 데이터 분석용 데이터베이스로 매우 좋고, 빠른 속도가 장점 더 자세한 내용은 공식 문서 또는 Google BigQuery Users 참고 자료 참고! Airflow Operator Apache Airflow는 AWS/GCP Operator들이 잘 구현되어 있음 굳이 따지면 GCP 쪽 Operator가 더 잘되어 있는 편 공식 문서 BigQuery Operator는 내부적으로 BigQueryHook을 사용해 Google Cloud Platform과 연결 처음 Operator 사용할 땐 “내부적으로 Hook을 통하는구나” 정도로 먼저 이해한 후, 사용해도 무방 BigQuery Operator 공식 문서 Github Link 총 11개 Operator가 존재(1.10.1 버전 기준) BigQueryCheckOperator BigQueryIntervalCheckOperator BigQueryValueCheckOperator BigQueryGetDataOperator BigQueryCreateEmptyDatasetOperator BigQueryCreateExternalTableOperator BigQueryDeleteDatasetOperator BigQueryOperator BigQueryTableDeleteOperator BigQueryToBigQueryOperator BigQueryToCloudStorageOperator 사용하는 사람마다 다르겠지만 제가 제일 많이 사용하는 Operator는 BigQueryOperator, BigQueryCreateExternalTableOperator, BigQueryTableDeleteOperator, BigQueryToBigQueryOperator BigQueryOperator 공식 문서 BigQuery SQl 쿼리를 날려주는 Operator 쿼리 결과를 Table로 저장할 수 있음 사용 예시 from airflow import models from airflow.contrib.operators.bigquery_operator import BigQueryOperator default_dag_args = { 'owner': 'zzsza', 'start_date': datetime(2019, 4, 2), 'email': ['your_email@gmail.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 0, 'project_id': 'your_proeject_name' } query = \"\"\" SELECT * FROM `project.dataset.table` LIMIT 1000 \"\"\" with models.DAG( dag_id = 'extract_feature_dag', schedule_interval= '30 0 * * *', default_args=default_dag_args) as dag: bq_query = BigQueryOperator( task_id='extract_daily_metric', bql=query, use_legacy_sql=False, destination_dataset_table='dataset.table', write_disposition='WRITE_TRUNCATE' ) bq_query 파라미터 (몇개만 설명) BigQueryOperator(bql=None, sql=None, destination_dataset_table=None, write_disposition='WRITE_EMPTY', \\ allow_large_results=False, flatten_results=None, bigquery_conn_id='bigquery_default', delegate_to=None, \\ udf_config=None, use_legacy_sql=True, maximum_billing_tier=None, maximum_bytes_billed=None, \\ create_disposition='CREATE_IF_NEEDED', schema_update_options=(), query_params=None, labels=None, \\ priority='INTERACTIVE', time_partitioning=None, api_resource_configs=None, cluster_fields=None, \\ location=None, *args, **kwargs) bql는 이제 deprecate 예정, sql 사용(.sql 파일도 사용 가능) destination_dataset_table : &lt;project&gt;.&lt;dataset&gt;.&lt;table&gt; 형태로 사용 write_disposition : WRITE_EMPTY(빈 경우만 쓰기), WRITE_TRUNCATE(덮어 쓰기), WRITE_APPEND(데이터 Append), 참고 문서 bigquery_conn_id : Connection 설정 이름 BigQueryCreateExternalTable GoogleCloudStorageToBigQueryOperator과 유사한 작업을 하는 Operator로 BigQueryCreateExternalTable은 Bigtable, Google Storage, Google Drive 등에서 데이터를 가지고 올 수 있음(더 넓은 개념) date = \"{#{ macros.ds_format(macros.ds_add(ds, -1), '%Y-%m-%d', '%Y/%m/%d') }#}\" # 코드에선 # 빼주세요 create_table = BigQueryCreateExternalTable(bucket='bucket_name', source_objects=[f'{date}/user_log.csv'], schema_fields =[{\"name\": \"user_id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"}, {\"name\":\"purchase_amount\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}], schema_object='bigquery-schema/user_log.json' # Google Storage path, source_format='CSV', bigquery_conn_id='bigquery_default', google_cloud_storage_conn_id='storage_default' BigQueryDeleteDatasetOperator 데이터셋 삭제 Operator delete_temp_data = BigQueryDeleteDatasetOperator(dataset_id = 'temp-dataset', project_id = 'temp-project', bigquery_conn_id='_my_gcp_conn_', task_id='Deletetemp', dag=dag) BigQueryToBigQueryOperator BigQuery Table A를 B로 옮길 때 사용하는 Operator 데이터 마이그레이션시 사용 move_table = BigQueryToBigQueryOperator(source_project_dataset_tables='project.dataset.table', \\ destination_project_dataset_talbe='project.dataset.tableb', write_disposition='WRITE_TRUNCATE', \\ bigquery_conn_id='bigquery_default') 참고 자료 Apache Airflow - Workflow 관리 도구(1)",
    "tags": "airflow mlops",
    "url": "/mlops/2019/04/17/airflow-bigquery-operator/"
  },{
    "title": "Interpretable Machine Learning",
    "text": "패스트캠퍼스 Alumni Special Seminar, Interpretable Machine Learning 세미나를 듣고 정리한 글입니다 총평 요새 Interpretable Machine Learning의 필요성을 느끼고 LIME이나 깃북을 보며 공부하고 있었는데, 연사님의 좋은 발표 덕에 Interpretable ML의 큰 그림을 더 잘 이해할 수 있게됨 많은 사람들이 비슷한 생각을 하고 있구나- 깨달았고, 회사 업무에 어떻게 적용할 수 있을지 고민해볼 예정 추후 참고하면 좋을 컨텐츠 https://christophm.github.io/interpretable-ml-book/ : 전반적인 내용이 잘 나와있는 Gitbook Interpretable Machine Learning, ICML 2017 : ICML 2017 Tutorial Interpretable ML Symposium : NIPS 2017 모두의 연구소 Safe AI Lab LIME Github Kaggle Machine Learning Explainability Paper Interpretable machine learning: definitions, methods, and applications Why Should I Trust You? Explaining the Prediction of Any classifier Deep Lattice Networks and Partial Monotonic functions Grad-CAM : Visual Explanations from Deep networks via Gradient-Based localization Clinically applicable deep learning for diagnosis and referral in retinal disease Boolean Decision Rules via Column Generation An Interpretable Model with Globally Consistent Explanations for Credit Risk Please Stop Explaining Black Box Models for High Stakes Decisions Co-teaching : Robust Training of Deep Neural Networks with Extremely Noisy Labels 패스트캠퍼스 Alumni Special Seminar - Interpretable Machine Learning 연사님 윤상웅님 장병탁 교수님 연구실 SNU Robotics 해줌 Intro 빌리언즈라는 드라마 2명의 트레이더 의견 중재가 필요한 상황 얼마나 확신하는지 물어봄 이 때 답변을 못하면? 트레이더는 곧 ML Model 한다면? 과거 데이터 /시뮬레이션에 대해서 철저히 검증됨 알고리즘이 기반한 아이디어가 말이 되고 납득이 됨 리스크가 적음 하지 않는다면? 과거 데이터에 대해서만 검증(=시뮬레이션에 대해서만) 알고리즘이 작동하는 원리가 말이 안됨 리스크가 큼 ML의 숙명 - 1 에러가 반드시 발생함 Bayes Error 정의 : Classification 문제에서 이론적으로 도달할 수 있는 최소한의 오분류율 각 class의 확률분포가 겹치는 만큼 발생 Bayes Error보다 오분류율을 줄일 수 없으며, 많은 경우 Bayes Error ≠ 0 새로운 정보를 가진 Feature가 추가되어야만 Bayes Error를 낮출 수 있음 어떤 에러는 아주 치명적 loss(암 환자를 정상으로 분류) &gt; loss(정상을 암환자로 분류) ML의 숙명 - 2 예측값만 제공 인간을 설득시키기엔 부족함 알고리즘 작동으로 얻는 정보가 너무 적음 일부 알고리즘은 불확실성(uncertainty) 혹은 확정도(confidence)도 함께 제공 그러나 항상 제공하진 않음 결국 신뢰의 문제 ML 알고리즘을 믿고 의존할 수 있는가? ML 알고리즘이 잘못되었을 때 인간이 바로잡을 수 있는가? ML 알고리즘이 예측하지 못한 안 좋은 효과를 일으키지 않는가? Trusthworthy Machine Learning 신뢰할만한 머신러닝, 다양한 분야랑 연결됨 1) Interpretable ML 2) Causality : 상관관계가 아닌 인과관계를 학습 코잘리티 X로부터 Y를 예측할 수 있다고 X가 Y의 원인인 것은 아님 X와 Y가 상관관계(correlation)이 있을 뿐 관측 데이터로 인과관계를 알아내는 것은 매우 어려운 일 외부 변수(Confounding variable) → 현실에선 항상 존재 당연한 것(암 걸린 사람이 항암제 처방을 받았을 것이다)을 인과관계로 정의할 수 있는데, 그럼 bias… 그러면 안됨 이론적으로 어디까지 가능한지 학회에서 연구중 3) Fairness : 알고리즘의 공정성 ML 알고리즘은 편향(bias)되기 쉬움 예시 흑인 사진을 고릴라라고 인식 인도 신부 복장을 입은 사람을 댄서로 인식 수집한 데이터의 편향 데이터셋 제작자의 부주의 4) Privacy-preserving ML : 사용자 데이터 보호 ML 모델 개발 과정 및 서비스 중 확인을 위해 로그 데이터를 까보는데, 민감한 개인 정보가 노출됨 암호화된 데이터로 학습할 수 있을까? 애초에 데이터 수집 단계에서 Privacy를 보존할 수 있을까? Differential Privacy 설문지에 에이즈 보균자냐고 물어보면 정직하게 답변하지 않을 것 동전을 던져서 앞면이 나오면 무조건 YES, 뒷면이 나오면 솔직한 답을 적음 얻은 YES 응답률에서 0.5를 빼고 2배를 하면 실제 YES 응답 비율을 구할 수 있음 5) Adversarial attack : 해킹에 대한 보호 DNN의 입력에 작은 perturbation을 출력해 원하는대로 출력을 조절 왜 발생하는지? 어떻게 방어할 것인지? 고민하고 있음 Interpretable Machine Learning 예측 결과에 더해서 사람이 이해할 수 있는 형태로 추가적인 정보를 제공할 수 있는 머신러닝 알고리즘을 연구하는 분야 목표 Interpretability란 무엇인가? Interpretability가 가능한가? 왜 필요한가? UI 기획으로도 풀 수 있음 신뢰를 얻는 방법은 상황과 상대에 따라 다름 문제 상황, 데이터셋, 알고리즘, 사용자의 상황에 따라 다르게 접근 새로운 알고리즘만이 해결책은 아님 여러 문제에 맞는 해결책을 직접 고민해야 함 시각화는 어떻게? 등 Interpretable ML의 효용 (From selvaragu, 2017) 어디에 도움이 될 것인가 (1) 연구 개발 단계의 ML 인간 수준 이하의 알고리즘의 경우(ex: VQA) 왜 못하는지 ⇒ 확인 후 개선 (2) 상용화 수준의 ML 인간 수준에 가깝거나 유용한 수준(ex: Image classification) 사용자들에게 신뢰를 얻고 활용도를 증진 (3) 인간 이상의 수준 ML 인간보다 압도적으로 뛰어난 Interpretability를 통해 인간을 가르칠 수 있음 널리 퍼져 있는 믿음 Linear model은 interpretable ( 학계의 정설) Linear Regression, Logistic Regression 1) 개별 가중치를 파악 전체적인 규칙을 알 수 있음 ⇒ Global interpretability 함정 다중 공선성 입력 feature간 상관성이 높을 경우 weight 값이 직관과 다르게 얻어질 수 있음(그래프를 뿌려보면 양의 상관관계가 보이는데 실제로 나온 계수는 음수) 상관성이 높은 feature를 제거 차원 축소 혹은 feature delete ⇒ 휴리스틱 2) 개별 Data point에 대해 예측해야되는 순간 나이, 거리, 등록기간에 따른 재등록 확률을 예측 w_{i} x_{i} : log odds-ratio에 대한 기여도 Local interpretability 3) 확률값과 신뢰구간 모델의 출력은 확률값 0 ~ 1 1에 가까우면 확실하게 재등록 0에 가까우면 확실히 이탈 0.5면? → ?? 모델의 예측이 얼마나 확실한지 제공 Predictive Uncertainty / Confidence Score Interpretable ML의 세 방향 1) Global interpretability 모델이 전체적으로 어떤 원리로 예측하는지 설명 분산 분석, 통계 테스트 2) Local interpretability 개별 data point에 대해 예측의 이유 설명 w_{i} x_{i} 항들의 값을 비교 3) Uncertainty 예측값에 얼마나 확신이 있는지? 1)과 2)는 아직 정립된 용어는 아님 Global Interpretability 1) Monotonicity(단조성) 대출 연체를 했는데 신용등급이 올라간다? 소득이 늘었는데 신용 등급이 떨어진다? 입력 변수와 예측 변수의 단조성을 강제로 넣어줌! 상식과 벗어난 거동을 방지 모델의 전체적인 작동을 설명하기 쉬움 선형모델은 단조성이 보장됨 굉장히 강력한 제한조건 소수의 모델에만 적용 가능 Linear 모델은 Monotonic함 Gradient Boosting(XGB, LGBM)은 monotonic constraint option 존재 원래 에러를 무조건 줄이라고 하면 사인 함수로 할텐데, 모노토닉 constraint 옵션을 주면 어디는 트리를 만들지 않고 단조 증가/단조 감소 하도록 예측 실용적으론 제일 유용 Neural Networks 신경망과 유사한 구조를 사용하며 단조성 조건 추가 Tensorflow 구현체 공개됨 TensorFlow Lattice https://github.com/tensorflow/lattice 1D Lattice 한 레이어나 유닛 일종의 Look-up Table 5개의 점이 찍혀있는 좌표를 알면 그에 맞는 함수 생성 Key가 아닌 값(없는 좌표)을 받으면 linear interpolation 파라미터 값은 gradient descent로 학습 1차원 함수로 다 표현 Monotonicity Key1 &lt; Key2이면 Value1 ≤ Value2가 되도록 constraint Activation function의 역할 ReLU의 선형결합과 동일 Exclusive or 문제 Linear는 풀 수 없음 1,3 / 2,4 같은 클래스 단조성 강제하면 모델이 가질 수 있는 함수의 Set이 줄어드니 함수가 약해지는 것이 맞음 단조성이 선형성은 아님 모든 변수에 대해서 모노토릭할 필요는 없음 ⇒ partial monotonic KD Lattice 2^{k}의 격자점 3개의 입력을 받음 격자점이 아닌 값이 들어오면 multilinear interpolation Monotonicity가 필요할 경우 격자점들의 값에 제한조건 부여 Deep Lattice Network 쌓아둠 Monotonic이라고 지정된 입력이 지나가는 layer는 모두 monotonic constraint constraint를 만족하는 파라미터를 찾는 최적화 과정이 non-trivial Classification/Regression loss를 최소화하도록 SGD를 이용해 학습 단조적 연산만 하도록 강제 모노토릭한 것을 하면 반쯤은 지켜야한다는 constraint가 생김 ⇒ 더 많은 연산량 2) Feature Importance Rough한 정보지만 여러 종류 모델에 적용 가능 Feature selection과 밀접한 연관 변수가 너무 많을 경우 어떤 변수를 예측모형에 사용할지 선택하는 작업 (1) Feature Ablation Feature를 제거하고 돌렸을 때 성능이 얼마나 떨어지는지 확인 성능이 많이 떨어지는 feature가 중요한 feature 장점 무식한 방법이지만 직관적으로 이해하기 쉬움 단점 연산이 많음 (2) Mutual information 정보 이론에서 나옴 두 확률변수 X, Y가 서로에게 얼마나 정보를 가지고 있는지 나타내는 값 MI(X, Y)=0이면 X와 Y가 독립 Corr과 다른 것 Corr은 선형 관계만 표시하는데, 이건 비선형도 사용 가능 MI가 큰 feature가 중요한 feature일 확률이 높음 Nearest neighbor, kernel density estimation 등을 이용한 비모수적(non-parameter) 추정 방법 가능 MI가 아니더라도 t-statistic 등 여러 지표로 측정 가능 (3) L1 constraint 가중치의 L1 nort으로 regularization 0이 많이 들어간 솔루션을 찾음 중요하지 않은 feature는 weight는 0이 됨, Sparse solution Local Interpretability 이게 연구가 많이 됨 1) Local Surrogate LIME(Local interpretable model-agnostic explanations) 복잡한 모형을 국지적(local)으로 근사하는 선형 모형을 만듬 특정 포인트 주변에 선형 모형을 fitting 전체에 대해선 모르지만 특정 부분 근처엔 왼쪽은 ~, 오른쪽은 ~ https://github.com/marcotcr/lime Interpretable Data Representation 이것에 대해 생각해볼 수 있음 이 Feature가 항상 의미가 있을까? 해석이 불가능한 것이 있을 수 있음. 이미지에서 개별 픽셀(452번째)이 중요하다고 ??? Feature Transform : 루트, 제곱 등 ⇒ 루트는 중요한데 제곱은 중요하지 않다? Feature 자체로 의미가 있는 경우와 없는 경우를 구분 LIME f(x) : black-box model based on features g(z) : local explanation based on interpretable representation \\pi_{x}(z) : 가중치 \\Omega (g) : regularization g(z)가 f(x)를 잘 모사하면서 너무 복잡하지 않도록 학습 2) Counterfactual Explanation Counterfactual = What if 만약 이랬다면 다른 결과가 얻어졌을 것이다 가상 혹은 진짜의 데이터 포인트를 제시 입력값 중 하나, 두개 여러개를 바꿈 ⇒ 예측 결과가 달라짐 Counterfactual 끝판왕 = 입력을 바꿔서 돌려볼 수 있도록 공개 Simulator 사용자가 직접 값을 바꿔보면 어떻게 되는지? 헬스장 거리를 줄여보니 예측값이 좋아짐 단점 : Adversarial attack 등 위험 classification을 모두 공개하면 attack할 수 있음 예를 들어 보험료 산정 알고리즘이면 그걸 해킹해보고.. 특정 조건을 찾아서 내 보험료를 깎을 수 있음 3) Gradient-based Explanation Grad CAM 가장 성공적이고 널리 쓰이는 방법 Class activation 에 가장 기여하는 hidden neuron을 찾음 가장 윗층의 conv layer 사용 가장 high level 정보 공간적인(Spatial) 정보를 보유하고 있음 각 Feautre map의 기여도를 계산함(Global Average Pooling in CAM) Class activation Map L을 얻음 L을 upsample(bilinear)하여 입력 image에 시각화 장점 Classification만 했는데 Detection 가능 여러 object 가능 학습된 네트워크에 적용하기만 하면 됨 재학습, 구조 변경이 없어서 예측 성능 유지 guided backpropagation과 함께 사용해 더 향상 ConvNet으로 시작하는 task는 모두 적용 가능 예측 결과에 중요한 이미지 부분을 heatmap 형태로 제공 일반적인 ConvNet에 대해 Image classification, captioning, visual QA 등 여러 종류의 task에 사용 가능 네트워크에 변형도 가하지 않고 어떤 성능 저하도 없음 Grad CAM 실험 인간 실험 실제 사람들에게 설문조사 같은 것을 뿌림 이 방법을 사용했을 때 더 신뢰를 느끼는가? 결론 : Grad-CAM visualization이 더 믿을만한 알고리즘을 고르는데 도움이 됨 모델 실패 사례 분석 vgg16이 잘못 예측하는 경우를 분석 어째서 잘못된 예측을 하는가? 관측된 사례 Class label의 모호함 헷갈릴만한 경우 잘못된 예측에 대해 엉뚱한 CAM이 나오면 모델 학습에 문제가 있다는 것 하지만 구체적 개선방안이 언제나 명확한 것은 아님 Class 레이블의 애매한 이유 레이블러의 실수 때문(노이즈 때문) 이라면 통계적 접근을 할 수 있을..(딥뉴럴넷의 특징) Co-teaching : Robust Training of Deep Neural Networks with Extremely Noisy Labels https://papers.nips.cc/paper/8072-co-teaching-robust-training-of-deep-neural-networks-with-extremely-noisy-labels.pdf 학습 과정 중간을 보면 레이블이 제대로 된 것부터 학습하고, 아닌 것에 나중에 fitting(나중에 training error가 줄어듬) 잘못달린 것 같은데? 라고 의심할 수 있고 로버스트하게.. 제안 이론적 근거가 확실하진 않지만 실험적으로 관찰되는 것 Uncertainty 설명이 아닌 예측에 대해 얼마나 확신을 가지는지 제시할 수 있는 기법에 대해 이야기함 Class 1, 2 분류하는 알고리즘 좋은 분류기라면 확실한 것은 확실하다 말하고 경계선은 약간의 불확실성이 있다고 말할 수 있을 듯 Confidence Score ML 알고리즘은 예측값만 출력하는 것이 일반적 그러나 알고리즘에 따라 내부적인 변수를 활용해 예측에 얼마나 확신을 가지고 있는지 알 수 있는 경우가 있음 “확률”의 형태를 띄고 있지 않음 → 직관적으로 이해하기 어려움 SVM Margin Ensemble Methods : 개별 tree들의 예측이 일치하면 high confidence, 불일치하면 low confidence Predictive Probability 로지스틱 회귀나 뉴럴넷은 아웃풋을 확률의 형태로 출력할 수 있음 확률값이 1에 가까우면 high confidence, 아니면 low confidence 정량적으로 (Shannon) Entropy를 기준으로 판단할 수 있음 3개의 클래스로 예측한 후 결과 확률이 많이 차이가 안나는 경우 (비등비등) 결과 확률이 차이가 크게 나는 경우 엔트로피 계산 : 데이터가 얼마나 무질서에 가까운지 측정 유니폼 분포라면 엔트로피가 높고, 극단적인 분포라면 엔트로피가 낮음 엔트로피가 높으면 확신을 많이 하고있진 않겠구나.. 응용 Active Learning 데이터셋을 수집하는 단계부터 생각 Label된 데이터가 조금 있고, 새로 취득할 때 어떤 데이터를 취득해야 모델의 학습에 도움이 될까? Labeling은 비싼 작업이므로 최대한 효율적으로 데이터를 수집하는 것이 중요 Idea : 모델이 가장 헷갈려하는 데이터를 수집 현재 데이터로 모델 학습 Unlabeled data들에 대해 예측을 수행 예측 확률의 entropy가 가장 큰 sample들을 labeling 학습 데이터셋에 추가 어떤 것에 취득해야 할까 알고리즘 학습에 도움이 될까?가 핵심 unlabeld 데이터에 모두 예측을 한 후, 엔트로피가 높은 것을 선택 함정 최근에 Report된 현상 확률값이 부정확하단 말이 있음(not calibrated) P(Y=1\\vert X)가 0.5인 데이터들을 모아놓고 보면 P(Y=1\\vert X) ≠ 0.5 이 현상을 개선하기 위해 많은 연구가 이루어짐 Bayesian Machine Learning 불확실성을 위해 많이 사용하는 기법 Bayesian 통계에선 확률=확신의 세기 모델의 parametere들을 distribution을 갖는 random variable로 취급 단점 계산이 어려움 closed-form solution이 없을 수 있어 근사가 필요 필요한 계산 양이 많음 Gaussian Process Regression 가장 깔끔하고 아름다운 머신러닝 알고리즘 중 한나 통계학에서 확률 과정(Stochastic Process) 중 하나인 Gaussian Process를 확장해 ML에 사용할 수 있도록 한 것 무한히 많은 개수일 때 어떻게 생각할 수 있을까? 무한개 중 아무렇게나 N개를 골라도 가우시안 분포를 가진다고 정의 머신러닝에선 예측에 쓸 수 있도록 조금 바꿈 학습 데이터와 예측 대상이 되는 y가 모두 하나의 Gaussian distribution을 따른다고 가정 평균이 0이고 covariance matrix가 커널 function으로 주어짐 conditional gaussian distribution의 공식에 의해 평균과 분산을 구함 Predictive uncertainty가 closed form으로 구해지는 몇 안되는 사례 Inverse 곱해서 N^{3}이라 몇천개 넘어가면 계산 속도가 급격하게 하락 응용 : Bayesian Optimization Auto ML을 시작시킨 개념 Active Learning과 유사 베이지안 뉴럴넷을 다루고 싶었는데 시간 관계나 난이도상 못다루긴 했는데, 관심이 있다면 더 찾아보기 https://arxiv.org/abs/1506.02142 제공되는 형태에 초점을 맞춤 모델 전체에 대해 interpretable을 줄 것인가? 개별에 대한 interpretable을 줄 것인가? 애초에 Interpretable ML이 가능할까? 애초에 가능하면 왜 이 사단이 났을까? 데이터엔 신호와 소음으로 나눠서 생각 데이터마다 다 다름! 어떤 데이터는 신호가 낮고 소음이 많고 어떤 데이터는 신호가 높고 소음이 적음 낮은 신호/높은 소음 : 베이즈 에러가 높다고 볼 수 있음 ⇒ 주식/금융 데이터, 사회과학 데이터 단순한 모델 소음이 높으면 오버피팅을 막아야 함 데이터의 노이즈를 잘 무시하는 것이 관건 단순한 모델이니 설명할 필요가 없음 높은 신호/낮은 소음 : 베이즈 에러가 낮다고 볼 수 있음 ⇒ 사진, 음성, 텍스트, 바둑 복잡한 모델 언더피팅이 문제 데이터가 많이 필요 복잡한 패턴을 포착하는 것이 목표 고슴도치와 여우 고슴도치 초기에 설정한 규칙을 고수, 예측을 잘 못함 근본적 아이디어가 모든 것을 결정한다고 생각 여우 사소한 생각과 증거들을 고려하고 면밀히 살핌 여러 접근 방법을 동시에 적용 한두마디로 설명할 수 없지만 예측력 상승 앙상블 알고리즘, ML 알고리즘 Human-(in/out of)-the Loop 데이터를 이용해 할 수 있는 일 1) 데이터 시각화 2) 지식의 생산 (가설 검정) 3) 예측 Human in the Loop이 interpretable이 중요한 듯 신용평가 Credit Scoring 사례 이 사람이 돈을 빌렸을 때 잘 갚을 수 있을까? 굉장히 큰 임팩트를 주고 있다는 것을 깨달음 역사 1941년 미국에서 시작 1950년대 후반 Fiar &amp; Isaac에 의해 방법론 정립 후 상용화 ( Score card &amp; Logistic regression) 현재 FICO는 세계 최대의 신용정보 회사 FICO Score가 미국인들에겐 신용등급과 동일한 단어 우리 나라는 NICE신용평가, KCB(Korea Credit Bureau)이 수행 NIPS 2018 워크샵 https://sites.google.com/view/feap-ai4fin-2018/ https://community.fico.com/s/explainable-machine-learning-challenge Winner : IBM 리서치팀 http://papers.nips.cc/paper/7716-boolean-decision-rules-via-column-generation 명시적인 룰을 뽑아내는 알고리즘을 만듬 2등 Duke 대학팀 왜 굳이 Black-Box 모델을 쓰고 그걸 설명하려고 하나? 애초부터 transparent한 white-box 모델을 쓰자 거의 유사한 성능을 낸다! http://dukedatasciencefico.cs.duke.edu/ 웹 데모로 직관적으로 표현 히든 뉴런에 이름이 달려있음 발표자님이 이걸 좋아한 이유 : 단순히 알고리즘이 중요한 것이 아닐 수 있음! 보여주는 과정도 생각을 Medical 사례 https://nips.cc/Conferences/2018/Schedule?showEvent=12346 이 부분을 보니 환자가 ~~ 병이야 영국에서 만든 어플리케이션 유방암 진단을 받은 사람이 얼마나 더 살 수 있는지? https://breast.predict.nhs.uk/index.html 결과를 보여줄 때 생존 확률을 보여주고, 아이콘 형태로 100명을 가정할 때 몇명이 살아나는 확률입니다 (심리적으로 검증) 라고 보여줌 어플리케이션 자체가 신뢰를 얻어야 함 의사, 간호사, 환자, 환자 부모님, 연구자 등 다양한 이해 관계자가 설명을 얻어가는 포인트가 다름 Communicating Uncertainty 생각보다 사람은 확률을 잘 인지하지 못한다 생존 확률이 0.92 → 와닿지 않음 100명 중 92명이 생존 → 와닿음 Heart Age 나의 심장질환 발병확률(위험)이 몇 세의 확률과 같은가? Explainable AI는 종합예술 다면적인 설명이 제공될 필요가 있음 인간은 본능적으로 거짓이 아닌 것은 여러 각도에서 보아도 참이라는 것을 알고 있음 다양한 측면에서 제공된 정보가 모두 일치할 때 신뢰를 얻을 수 있음 복합적인 요소가 작용 심리, 통계, 윤리, 디자인, 언어 사용자 스터디, 좋은 기획, 좋은 디자인이 필요 끝판왕 알고리즘 하나로 해결되는 문제가 아님! 다하지 못한 이야기 Shapley Value Bayesian Deep Learning 하나 더 있었는데 발표자료 보고 작성 질문 Self Attention과 Grad CAM의 차이는? 아이디어 공유되는 것이 있음 Attention 기법은 모델을 설계할 때 애초에 입력한 데이터를 중점적으로 볼지 말지 취사선택 하는 모듈을 중간에 넣는 것",
    "tags": "interpretable data",
    "url": "/data/2019/04/14/interpretable-ml-intro/"
  },{
    "title": "Google Cloud Next 19 정리 및 후기",
    "text": "Cloud Next 19의 영상들을 보고 정리한 글입니다 다음엔 저도 현장에 가서 이런 내용들을 듣고 싶네요!(잘 정리하고 전파할 수 있는데..!) 정리한 영상 ML Ops Best Practices on Google Cloud Accelerating Machine Learning App Development with Kubeflow Pipelines What’s New with BigQuery ML and Using it to Assess Data Quality Data Processing in Google Cloud: Hadoop, Spark, and Dataflow Rethinking Business: Data Analytics With Google Cloud AI Hub: The One Place for Everything AI 메모 영상 볼 수 있는 곳 https://cloud.withgoogle.com/next/sf/next-onair 다양한 분야 중 Analyze &amp; Learn archive를 중점적으로 봄 Google Cloud 블로그 글 All 29 AI announcements from Google Next ‘19: the smartest laundry list : AI/인공지능 기술에 대한 총 정리 글 News to build on: 122+ announcements from Google Cloud Next ‘19 : Cloud Next 19에서 나온 모든 새로운 것 정리 Cloud Next 1일차 정리 Cloud Next 2일차 정리 Cloud Next 3일차 정리 이상훈님의 2일차 정리글 총평 우선 구글 클라우드의 데이터 처리 도구들(Pub/Sub, Dataflow, Dataproc, BigQuery)는 정말 강력하고 더 쉽게 사용할 수 있게 됨 진입 장벽이 낮아지는 중 미래엔 데이터 엔지니어링 스킬이 기본적으로 있는 머신러닝 엔지니어들도 많이 나올 것 같음 AI Platform, Kubeflow, TFX, TFDV, TFMA 등을 보면 MLOps도 활발하게 진행되고 있는 듯? Kubeflow 정복할 예정! 이걸 위해 쿠버네티스 공부중.. Tensorflow보다 PyTorch가 상승하는 것 같지만 프러덕션에선 대부분 Tensorflow를 써야할 듯 지향점이 다른 느낌 그냥 토치 텐서 다 상관없이 해야할 듯? 회사에서 BigQuery쪽은 적극적으로 사용하고 있기 때문에 최신 기능을 모두 팔로업하고 있는데, 다른 도구들도 조금씩 테스트해보고 좋은 것은 바로 사용해볼 예정! 서울 리전! ML Ops Best Practices on Google Cloud https://www.youtube.com/watch?v=20h_RTHEtZI MLOps Challenges Data Validation TFDV Model Analyze TFMA Model 정확도가 시간이 지나며 점점 낮아지고 지속적 모니터링이 불가능한 상황이 자주 오는데, 이럴 때 사용하면 좋음 trip_start_hour가 특정 값일때 정확도를 알려줌 - 예시 : 현재 모델이 주간엔 정확도가 높고 야간엔 낮다 이런 결론이 나올 수 있음 Kubeflow Kubernetes-native OSS Platform to Develop, Deploy and Manage Scalable and End-to-End ML Workloads Cloud AI Platform A code-based intergrated development environment for data science and machine learning inside the GCP console 아키텍쳐 Accelerating Machine Learning App Development with Kubeflow Pipelines https://www.youtube.com/watch?v=TZ1lGrJLEZ0 Hidden Technical Debt in ML Systems를 보면 모델링이 차지하는 부분은 매우 작음 Kubeflow A Kubernetes-native open source platform to develop, deploy and manage, scalable ML Workloads ML Workflow Orchestration 코드 예제 Share, Re-use &amp; Compose zip 파일로 올림(람다에서도 이렇게 진행) AI Hub도 사용 가능 코드 예제 Rapid Reliable Experimentation MlFlow랑 비교해서 어떤 우위가 있는지 궁금 GOJEK 싱가포르, 인도네시아, 베트남, 태국에서 교통, 물류 등의 서비스를 하고 있는 기업 Airflow를 사용했는데, 아래 이유로 Kubeflow 도입 실험하기 어려움 엔지니어링 무거움 Low traceability and reproducibility Data pipeline은 Airflow를 쓰고, ML pipeline은 Kubeflow 사용 roc 커브도 그려줌 TFX Taxi 예제 TFMA를 Static HTML로 생성해줌 What’s New with BigQuery ML and Using it to Assess Data Quality https://www.youtube.com/watch?v=DnlG4frLKmw BigQuery ML의 새로운 기능에 대해 말해줌 New BigQuery UI 현기증 납니다.. 빨리 나와주세요 Matrix Factorization NCAA 농구 3점슛 예측 TensorFlow Feautre pre-processing function BUCKETIZE, POLYNOMIAL_EXPAND, FEATURE_CROSS 등 시연 QueryItSmart Github 참고 BigQuery + Cloud ML engine을 사용한 웹 데모 K means clustering Booking.com의 시연 빅쿼리로 kmenas 진행(클러스터 개수 21개)한 후 데이터 스튜디오로 시각화 Data Processing in Google Cloud: Hadoop, Spark, and Dataflow https://www.youtube.com/watch?v=GRP-cGbJSCs 이미 많이 알던 내용들인데, 새롭게 알게된 내용 위주로 캡쳐 Dataflow 1년 사이에 템플릿이 정말 많아짐 Dataproc Web interface에서 제플린 선택 가능 Rethinking Business: Data Analytics With Google Cloud https://www.youtube.com/watch?v=DpngHc31a5Y Platform Data Funsion Google Data Fusion 코드 없이 파이프라인 생성 BigQuery flat-rate with Reservations 예약제 요금제? BigQuery Storage API BI Engine 시각화하는 도구 같은데, Tableau를 대체할 수 있을지 궁금 Dataproc and Composer Composer는 Managed Airflow인데, 사용하다보면 약간 불편한 점도 있긴함 Stream 데이터 처리하는 파이프라인 Dataflow SQL 빅쿼리 콘솔에서 Dataflow Engine으로 설정한 후, 아래 쿼리 날림 TUMBLE_START는 처음 보는데 신기.. 쿼리 날리면 내부적으로 Beam이 데이터 읽고 전처리하는듯 SELECT sr.sales_region, TUMBLE_START(\"INTERVAL 5 SECOND\") AS period_start, SUM(tr.payload.amount) as amount FROM `pubsub.dataflow-sql.transactions` as tr INNER JOIN `bigquery.dataflow-sql.opsdb.us_state_salesregions` AS sr ON tr.payload.state = sr.state_code GROUP BY sr.sales_region, TUMBLE(tr.event_timestamp, \"INTERVAL 5 SECOND\") pubsub 토픽을 바로 넣어서 쓰는듯..? Data Catalog AI Hub: The One Place for Everything AI https://www.youtube.com/watch?v=QMTT2ngnj9Q Google AI Hub AI Hub에 대한 설명, 다른 세션과 겹치는 부분이 있는 느낌 Kubeflow, TFX에 대해 이야기함",
    "tags": "basic gcp",
    "url": "/gcp/2019/04/13/google-next19-review/"
  },{
    "title": "Kotlin 기본 문법 정리",
    "text": "코틀린 기본 문법에 대한 글입니다 Kotlin 코틀린은 안드로이드나 서버 개발을 할 때 사용할 수 있는 언어 kotlin server overview 참고하면 서버 사이드에서 장점을 알 수 있음 자바에서 많이 쓰이는 Spring도 사용 가능 VCNC 시스템 아키텍쳐를 보면 서버 언어를 코틀린으로 쓰는 회사가 점점 증가하는듯! 이 코드를 읽기 위해 코틀린을 공부합니다..! Basic Syntax 패키지 정의 package my.demo import java.util.* 함수 정의 fun 키워드로 정의 fun sum(a: Int, b: Int): Int { return a + b } 함수 몸체가 식인 경우 return 생략 가능, return type이 추론됨 fun sum(a: Int, b: Int) = a + b 리턴할 값이 없는 경우 Unit(Object)으로 리턴 자바의 void 역할 fun printKotlin(): Unit { println(“Hello Kotlin”) } Unit은 생략 가능 fun printKotlin() { println(\"Hellok Kotlin\") } 변수 정의 val : 읽기전용 변수 값의 할당이 1회만 가능, 자바의 final과 유사 val a: Int = 1 val b = 2 // int type 추론 val c: Int // 컴파일 오류, 초기화 필요(값 할당 안함) c = 3 // 컴파일 오류, 읽기 전용이라 추후에 할당 불가 var : Mutable 변수 var x = 5 x += 1 주석 자바와 자바스크립트와 동일 // : 한줄 추석 /* */ : 여러 줄 주석(여러개 중첩 가능) 문자열 템플릿 String Interpolation(문자열 보간법) var a = 1 val s1 = \"a is $a\" a = 2 val s2 = \"${s1.replace('is', 'was')}, but now is $a\" 조건문 fun maxOf(a: Int, b: Int): Int { if (a &gt; b) { return a } else { return b } } 조건식으로 사용 가능 fun maxOf(a: Int, b: Int) = if (a&gt;b) a else b nullable 값이 null일 수 있는 경우 타입에 nullable 마크를 명시 func parseInt(str: String): Int?{ // 정수가 아니면 null 리턴 } nullable 타입의 변수를 접근할 땐 반드시 null 체크를 해야 함 그렇지 않으면 컴파일 오류 발생 fun printProduct(arg1: String, arg2: String) { val x: Int? = parseInt(arg1) val y: Int? = parseInt(args2) if (x != null &amp;&amp; y!= null) { println(x*y) } else { println(\"eithe '$arg1' or '$arg2' is not a number\") } 자동 타입 변환 타입 체크만 해도 자동으로 타입 변환!!! obj: Any는 object의 최상위 객체 fun getStringLength(obj: Any): Int?{ if (obj is String) { // obj가 자동으로 string 타입으로 변환 return obj.length } return null } while loop val items = listOf(\"apple\", \"banana\", \"kiwi\") var index = 0 while (index &lt; items.size) { println(\"item at $index is ${items[index]}\") index ++ } when expression fun describe(obj: Any): String = when (obj) { 1 -&gt; \"One\" \"Hello\" -&gt; \"Greeting\" is Long -&gt; \"Long\" !is String -&gt; \"Not a string\" else -&gt; \"Unknown\" } ranges In 연산자를 이용해 숫자 범위 체크 가능 val x = 3 if (x in 1..10) { println(\"fits in range\") } range를 이용한 for loop for (x in 1..5) { println(x) } collections 컬렉션도 in으로 loop 가능 val items = listOf(\"apple\", \"banana\", \"kiwi\") for (item in items) { println(item) } in으로 해당 값이 collection에 포함되는지 체크 가능 val items = setOf(\"apple\", \"banan\", \"kiwi\") when { \"orange\" in items -&gt; println(\"juicy\") \"apple\" in items -&gt; println(\"apple is fine too\") } 람다식을 이용해 컬렉션에 filter, map 연산 가능 val fruits = listOf(\"banana\", \"avocado\", \"apple\", \"kiwi\") fruits .filter { it.startsWith\"a\") } .sortedBy { it } .map { it.toUpperCase() } .forEach { println(it) } Basic Types 기본 타입 코틀린은 모두 객체 모든 것에 멤버 함수나 프로퍼티를 호출 가능! 숫자 Java의 숫자와 비슷 Kotlin에서 Number는 클래스 Java에서 숫자형이던 char가 kotlin에선 숫자형이 아님 리터럴(Literal) 10진수 (Int, Short) Long Double Float 2진수 8진수는 미지원 16진수 Underscore in numeric literals 언더스코어를 사용해 표현 가능 val oneMillon = 1_000_000 val creditCardNumber = 1234_5678_9012_3456L Representation Java 플랫폼에서 숫자형은 JVM primitive type으로 저장 Nullable이나 제네릭의 경우엔 박싱 박싱된 경우엔 identity를 유지하지 않음 show bytecode → decompile하면 자바 코드로 변환 Explicit Conversions 작은 타입은 큰 타입의 하위 타입이 아님 즉, 작은 타입에서 큰 타입으로 대입이 안됨 숫자끼리 변화할 땐 명시적으로 변환해야 함 val i: Int = b.toInt() // toByte(), toShort(), toInt() … 문자 Char는 숫자로 취급되지 않음 fun check(c: Char) { if (c == ‘a’) } 배열 배열은 Array 클래스로 표현 get, set ([] 연산자 오버로딩됨) size 등 유용한 멤버 함수 포함 var array: Array&lt;String&gt; = arrayOf(\"코틀린\", \"강좌\") println(array.get(0)) println(array[0]) println(array.size) 배열 생성 Array의 팩토리 함수 사용 arrayOf() 등의 라이브러리 함수 이용 val b = Array(5, { i -&gt; i.toString() }) val a = arrayOf(\"0\", \"1\", \"2\", \"3\", \"4\") 특별한 Array 클래스 primitive 타입의 박싱 오버헤드를 없애기 위한 배열 IntArray, ShortArray, IntArray Array를 상속한 클래스는 아니지만 Array와 같은 메소드, 프로퍼티를 가짐 size 등 유용한 멤버 함수 포함 val x: IntArray = intArrayOf(1, 2, 3) x[0] = 7 println(x.get(0)) println(x[0]) println(x.size) 문자열 문자열은 String 클래스로 구현 String은 characters로 구성 s[i] 같은 방식으로 접근 가능 immutable이므로 변경은 불가 var x: String = \"Kotlin\" println(x.get(0)) println(x.length) 문자열 리터럴 escape string (“Kotlin”) 전통적인 방식으로 Java String과 비슷 Backslash를 사용해 escaping 처리 raw string (“\"”Kotlin”””) escaping 처리 필요 없음 개행이나 어떤 문자 포함 가능 val s = \"Hello, world₩n\" val s = \"\"\" '''이것은 코틀린의 raw String 입니다.''' \"\"\" Reference 코틀린 공식 문서 인프런, 코틀린 강의",
    "tags": "kotlin development",
    "url": "/development/2019/04/11/kotlin-basic/"
  },{
    "title": "Decision Tree 정리",
    "text": "Decision Tree에 대해 정리한 글입니다 Decision Tree 데이터에 있는 규칙을 자동으로 찾아내는 트리 기반의 알고리즘 분류와 회귀에 모두 사용 가능 비모수 모형의 효과적인 방법 구조 Root node Decision node : 조건 Leaf node : 클래스 값 Tree Split 정보 균일도가 높은 데이터세트를 선택할 수 있는 규칙 생성 균일도를 측정하는 대표적인 방법 2가지 1) 엔트로피를 이용한 정보 이득, Information Gain 2) 지니 계수 Information Gain 엔트로피 주어진 데이터 집합의 혼잡도 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 석여 있으면 엔트로피가 낮음 Reference",
    "tags": "ml data",
    "url": "/data/2019/04/08/decision-tree/"
  },{
    "title": "데이터 사이언티스트가 되기 위해 진행한 다양한 노력들",
    "text": "데이터 공부를 시작하고 현재까지, 제 일대기에 대해 작성한 글입니다 Intro 다양한 경로로 진로 관련 질문을 많이 받고 있습니다 특히 제가 자주 받는 질문은 “문과생”이고 대학원도 진학하지 않았는데 어떻게 공부했는지?입니다 이런 질문을 많이 받다보니 글로 남기면 좋지 않을까! 생각해서 글을 작성합니다 약 4년간 과거를 회고하려고 합니다 제가 진행한 내용이 답은 아니고 누군가는 저렇게 했구나 정도로 이해하면 좋을 것 같습니다 데이터 사이언스 관련 내용이 궁금하시면 제가 작성한 아래 자료들을 참고해주세요 :) 데이터 사이언스에 대한 상담을 많이 진행한 후, 만든 [I Want to Study Data Science] 데이터 사이언스 직군에 대해 설명한 [Data Science. Intro] [Kaggle Knowhow] 데이터 인프라가 없던 환경에서 하나씩 추가하는 과정에 대해 이야기한 [바닥부터 시작하는 데이터 인프라] TF에서 팀 빌딩까지 9개월의 기록 : 성장하는 조직을 만드는 여정 : 제가 속한 쏘카의 타다데이터팀 빌딩 과정에 대해 이야기했습니다 첫 시작 간단한 자기소개 저에 대해 간략히 설명하면 경영학을 전공했고(부전공, 복수전공 없습니다) 대학 입학하고 2년간 놀다가 군대에 갔습니다 2013년 전역 후엔 대학생 연합 광고동아리인 애드파워에서 “광고”, “디자인”을 공부했습니다 경쟁 PT나 광고 공부를 통해 자신의 생각을 어떻게 효율적으로 전달할지, 기획이란 무엇인지 등을 조금이나마 배웠습니다 광고 전반, 디자인/영상에 대해 알 수 있었고 좋은 사람들을 많이 만나서 후회하지 않는 생활이었지만, 광고를 업으로 삼을 수 없을 것 같다는 생각이 들었습니다 그나마 관심있던 직군은 미디어 플래너였습니다. 수학이나 통계학을 좋아했던 성향이 있어서 매체별 데이터를 다루는 미디어 플래너에 대해 관심이 있었습니다 실제 데이터를 보니 너무 가공된 데이터란 생각을 했고, 한단계 깊은 데이터를 보고 싶었습니다 R을 활용한 데이터 분석이 궁금해 한국 RFID/USN 융합협회에서 진행한 “R 기반 빅데이터 분석 입문 과정”을 2013년 9월에 들었습니다 하지만 이 시기에 들은 내용 중 지금 기억나는건 전혀 없네요 2014년 2013년~2014년엔 공기업에서 계약직으로 근무했습니다 3학년으로 복학했지만 학교 공부에 흥미가 없고 학점도 낮아서 그냥 회사에선 어떤 일을 하는지 알고싶어 인천도시공사에 지원했는데, 합격했습니다 간단한 사무보조부터 각종 행정처리, 건축 토목 현장 관리 등을 진행했습니다 돌이켜보니 그냥 이 시기는 멍 때리던 시기입니다 공기업에 취업할 생각도 있어서 시작했지만, 결론적으로 전 조금 더 액티브한 삶을 지향한다는 것을 알게 되었습니다 2015년 2014년 말부터 지인들과 창업했습니다 와디즈에서 500만원 펀딩을 받으며 시작했습니다 달다쓰다 - 또 하나의 작은 게양 진행했던 업무는 운영, 회계, 배송, 온라인 마케팅 등 다양한 일을 했습니다. 일하던 도중 “내가 진짜 하고 싶은 일이 지금 하는 일들일까?”라는 생각에 창업팀에서 빠지고 데이터 공부를 본격적으로 시작했습니다 2015년은 요즘처럼 인터넷에 데이터 관련 다양한 정보가 넘치진 않던 시절입니다(참고로 우리가 많이 알고있는 알파고는 2016년에 나왔습니다) 그 당시 진행했던 것은 크게 2가지입니다 1) 빅데이터 동아리 BOAZ 활동 2) 오늘 밤부터 쓰는 GA 수강 빅데이터 동아리 보아즈 활동 공모전도 활성화되지 않았고, 책도 많이 없기 때문에 비슷한 지향점을 가지는 사람들을 알고 싶어 빅데이터 동아리에 지원했습니다 기반 능력이 거의 없음에도 불구하고 다행히 합격했습니다 하지만 저는 공부보단, 그냥 동아리에 출석하고 가끔 뒷풀이 가는 삶을 살았던 기억이 나네요 요새 동아리 후배분들을 보니 정말 많은 고민을 하고, 빡세게 공부를 잘 하고 있는 것 같습니다 프로젝트 진행할 때도 R과 파이썬을 거의 못했던 기억이 납니다 단, “빅데이터”란 공통된 흥미를 가진 사람들을 만났던 것이 정말 좋았습니다 오늘 밤부터 쓰는 GA 수강 이 강의는 알렉스앤컴퍼니에서 진행한 강의입니다 Google Analytics를 통해 데이터 분석할 수 있단 소식을 듣고 수강했습니다 돌이켜 보면 이 강의를 통해 하용호님을 만날 수 있었고, 계속 데이터 공부를 할 자극을 얻었습니다 하용호님은 어려운 내용을 정말 쉽게 설명해주시는 것으로 유명합니다! 슬라이드쉐어에 가면 자료를 볼 수 있습니다 2016년 빅데이터 동아리 활동이 끝난 후, 4학년 2학기인 저는 깊은 고민에 빠졌습니다 “진짜 데이터를 더 공부하고 싶은가? 아니면 그냥 취업할까?” “대학원에 갈까?” 아마 이런 고민들은 지금도 많은 분들이 고민할 내용이라 생각합니다 깊은 고민 끝에 내린 결론은 “연구”보단 실제로 제 능력을 갖고 싶었습니다 대학원이 아닌 다른 방법을 찾아보던 도중, 패스트캠퍼스 데이터사이언스 스쿨을 찾았고 많은 고민을 한 결과, 수강을 결정했습니다 360만원의 학원비와 신사 고시텔 생활비 180만원, 금전적으론 약 500만원, 시간은 3개월 정도 할애해야 하는 과정이었습니다 하지만 투자할 가치를 느꼈고, 잘한 선택이라고 생각합니다 이 기간에 어떻게 코딩해야 하는지, 데이터 사이언스에 대한 전반적 지식을 얻을 수 있었습니다 공부한 내용을 조금씩 기록하잔 의미로 인스타그램에 공부 계정 : data.scientist을 만들고 글을 꾸준히 올렸습니다. 요새도 올리고 있습니다 패스트캠퍼스 개발을 전혀 모르던 제게 많은 지식을 얻게 해준 과정입니다 요샌 광고가 많아서 대부분 패스트캠퍼스를 알고, 고민할 것 같습니다 요샌 인터넷에 많은 자료가 공개되었기 때문에 학원을 안 다녀도 된다는 분들이 많습니다. 맞습니다. 제가 공부하던 시기보단 자료가 많아졌습니다. 따라서 굳이 학원으로 시작을 하지 않아도 되긴 합니다 아예 초심자고, 의지를 컨트롤하기 힘들 것 같은 경우엔 학원도 나쁘진 않단 생각을 합니다 많은 지식을 쌓았고, 여러 회사에 면접을 봤습니다 아직도 기억에 나는 면접은 “아프리카TV”의 데이터 분석가 면접입니다 데이터 분석가로 진행하는 거의 첫 면접인 동시에 사전 과제를 처음 겪은 면접입니다 아프리카TV의 데이터 일부분을 주고, 분석하는 문제였습니다 데이터를 보고 “왜 이런 사람이 있지? 이 데이터는 뭘까? 왜 이런 결과가 나오지?” 등을 고민하며 즐거웠습니다 사실 면접 다음 달에 1달간 어머니와 유럽 여행이 계획되어 있다고 말했고, 제 자신도 부족했기 때문에 불합격했습니다 유럽에 다녀오고, 천천히 지식을 재점검하며 2016년이 지나갔습니다 2017년 파이썬, 개발, 머신러닝에 대해 얼추 이해한 상태에서 다시 고민에 빠졌습니다 공부하면 할수록, 대학원에 가야할 것 같은데..? 공부할 것이 너무 많다 고민 후 결론은 우선 취업해서 현실의 업무를 하나씩 진행해본 후, 다시 결정하자였습니다 카메라에 관심이 많았는데, 카메라 어플을 만드는 “레트리카”에서 데이터 분석가를 채용하는 공고를 봤습니다 무언가의 끌림에 바로 지원했고, 면접을 봤습니다 면접을 보며 면접관 분이 매우 똑똑하고, 말을 잘하는 것이 인상깊었습니다 논리적인 대화로 제가 KO 완패를 받아, 오기가 생겼습니다 운이 좋게도 합격했습니다. 나중에 왜 저를 합격했는지 물어보니 다양한 지원자 중 데이터에 대해 이야기할 때 눈이 반짝거렸다고 들었습니다 레트리카 레트리카는 카메라 어플로 입사 당시 DAU(Daily Active User)가 약 1,000만명이었습니다 터키, 인도, 브라질 등 다양한 국가에서 사용하는 어플! 입사하니 데이터팀은 면접관이셨던 COO님, 그리고 저보다 2달 먼저 입사한 신입분, 저 이렇게 3명이었습니다 입사하고 데이터 분석이란 어떤 일인지 몸소 체험하며 다양한 시행착오를 겪었습니다 특히 기억나는 4가지를 말씀드리면, 1) 시니어의 부재 COO분은 매우 논리적이고 날카로운 분이셨지만, 데이터 분석에 풍부한 경험이 있는 분은 아니었습니다 (여전히 매우 좋아하는 분입니다) 입사 3달까진 시니어를 뽑아주길 바랬고, 시니어 면접에 참여했습니다 어느 순간 시니어를 바라지 않고, 제가 업무를 찾아보며 시작했습니다. 책이나 인터넷 강의를 많이 들었고, 인터넷에서 경험이 많은 분들에게 페북 메세지를 보내며 궁금한 내용들을 채웠습니다 아마 시니어가 없던 이 상황에 제가 살아남기 위해 고민을 했던 점들이 지금 제게 큰 자양분이 된 것 같습니다 1년간 거의 12시~새벽 1시에 집에 갔습니다 제가 부족하다 생각했기 때문에 남아서 더 자료를 찾아보고 공부하고 일을 했습니다 2) 데이터 엔지니어링 경험 팀 내에서 유일한 파이썬 경험자였는데(솔직히 이 당시에 잘하진 못했음) 서버 개발자분이 저를 보시더니 데이터 ETL 과정을 인수인계해주셨습니다 ^_^ 완전 급하게 배워서 체하는 느낌도 살짝 있었지만, 새로운 내용을 하는 것이 너무 재밌어서 이것도 퇴근 하고 계속 공부했습니다 덕분에 Google Cloud BigQuery에 대해선 정말 많이 이해했고, ETL이 꼭 하둡/스파크를 사용하지 않아도 되는 점 등을 알게 되었습니다 서버 개발자분도 사내에 혼자셔서 바쁘셨기 때문에 제게 이런 내용을 다 넘겨주신건데, 정말 감사하게 생각하고 있습니다 2017년 말엔 아예 서버 개발자로 직군을 바꿔, 유일하게 계신 서버 개발자분에게 아주 약간의 지도를 받았습니다 지도라고 해봤자 Error 디버깅하는 것을 바라본 정도인데, 그 과정을 보고 어떻게 트러블 슈팅할지 감을 익혔습니다 그리고 쉘스크립트, 파이썬으로 맵리듀스 짜는 것도 해보고, 다양한 경험을 했습니다 사내 대시보드, 다양한 데이터를 볼 수 있는 대시보드, Airflow 등을 찾아보고 사내에 도입했습니다 3) 오퍼레이션 업무 지원 사실 데이터 분석, 머신러닝/딥러닝을 공부하다보면 무언가 별도의 조직처럼 움직일 것 같지만, 회사에서 필요로 하는 것들 중 전혀 몰랐던 것도 있습니다 기술을 통해 오퍼레이션에서 진행하는 다양한 노동 활동을 자동화하는 것도 회사에 큰 도움이 되는 것을 깨달았습니다 야한 사진을 Block하는 것들을 만들며 새로운 지식을 또 습득했습니다 컴퓨터 비전 지식, Docker 지식 2)와 3)에 대한 내용이 더 궁금하시면 제 발표자료 : 바닥부터 시작하는 데이터 인프라를 참고하면 좋을 것 같습니다 4) 개인 업무 성향 파악 입사 후, 30일/60일/90일 피드백을 통해 제가 어떤 스타일로 업무를 하는지 알 수 있었습니다 어떤 일을 하기 전에, 다양한 고민이 많았고 뜬구름도 많이 잡았습니다. 이런 피드백을 핵직구로 받은 후 계속 개선하다보니 이젠 린한 사고 과정이 체득되었습니다 꾸준히 메모하던 습관에서, 일정 관리도 더 자세히 하게 되었습니다 캘린더에 시간 단위로 일정을 등록하고 있습니다 파란색은 회사 일정, 연보라색은 개인 일정입니다 2017년 회고, 2018년 계획 블로그 글에 자세히 작성했습니다 2018년 4월까지 회사를 다니다 퇴사했습니다 퇴사한 이유는 아래와 같습니다 1) 자신에 대해 생각할 시간이 필요해서 2) 개인 공부할 시간을 확보하기 위해서 3) 취업은 언젠가 다시 할 수 있단 긍정적 마인드 자신이 나아갈 방향을 설정하는 시간인 Gap Year를 계획적으로 보내기로 다짐했습니다 특히 앞으로 어떻게 커리어를 쌓을 것인가?에 대해 계속 고민했습니다 공부한 기록과 이직 과정은 Gap Year 및 쏘카 이직 이야기에 있으니 저 글을 참고하면 좋을 것 같습니다 이 시기에 제 진로를 다시 고민한 결과, 저는 “연구”보단 현실의 문제를 푸는 “Solver”가 되고 싶어하는 성향을 발견했습니다 문제를 해결하는 동시에 엔지니어이고 싶었기 때문에, 문제 해결에 대한 공부와 엔지니어 성향 모두를 계속 공부하고 있습니다 대학생 분들에게 고민을 나누기 위해 취업 특강을 많이 했습니다 Data Science. Intro TMI) 글쓰는 개발자 모임, 글또를 시작해서 다른 분들이 글쓰는 것을 돕고 있습니다 2018년 회고, 2019년 다짐 블로그 글에 자세히 나와있어요! 쏘카 쏘카 입사한지 6개월이 지났습니다(글 쓰는 시간 기준) 2018년 9월 ~ 현재 많은 일이 있었는데, 느낀 점 몇개 말씀드리면 1) 큰 규모의 데이터팀 저희 쏘카 데이터 그룹은 현재 약 20명입니다 리서치 회사를 제외한 데이터 관련 팀 중 제일 큰 규모라고 생각합니다 각자 도메인이 다르고, 다양한 경험이 있어 대화하는 것만으로도 충분히 성장하게 됩니다 문무를 겸비한 것처럼 데이터 분석/머신러닝/딥러닝/데이터엔지니어링 모두 경험있는 분들도 꽤 있습니다 2) 재미있는 데이터와 문제 모빌리티 데이터는 정말 재미있습니다 차량에서 나올 수 있는 데이터가 무엇인지? 생각해보면 좋을 것 같습니다 그리고 최근 나온 타다도 급성장하며 생기는 데이터와 문제들! 블로그에 말씀 드리긴 어렵지만, 현실의 문제를 빠르게 해결하고 있습니다 저는 어떻게 지낼까요? 요샌 “최적화”에 대해 관심이 많습니다. Operation Research부터 시작해 휴리스틱, 메타휴리스틱, 유전 알고리즘 등 기존에 전혀 알지 못했던 내용들을 공부하고 익히고 있습니다 지리 데이터, 좌표 데이터를 많이 가공하며 새로운 도메인에 대해 적응하고 있습니다 여전히 공부 많이하고, 책도 보고 강의도 보고 그러고 있습니다 머신러닝 부분은 “시계열 예측”과 “MLOps”에 대해 계속 공부하고 있습니다 좋은 문화, 좋은 팀을 만들기 위해 저는 제가 속한 회사가 제일 좋은 회사가 되길 원하고 있습니다. 오지랖 부리며 이것저것 하고 있는데 몇개 말씀드리면 1) 스터디 진행 최근에 나온 “파이선 머신러닝 완벽 가이드”를 스터디하고 있습니다 책이 너무 좋고, 팀 동료분들이 알면 좋을 내용이 많다 판단해서 제가 스터디 매니저를 자처해 스터디를 진행하고 있습니다 누가 발표하는 것이 아닌, 정해진 시간에 책을 같이 읽고 토론하고 있습니다 스터디 내용은 Github에 올리고 있습니다 이 스터디 말고도, 스터디잼 중급반도 시작했습니다 2) 스터디 지원 쏘카에서 SQL을 사용하는 분들이 질문하시면 정성껏 답변 하고 있습니다(스터디의 QnA 봇 같은 존재..?) 기획자분들이 직접 SQL을 하는 것을 정말! 감동적인 일이라 생각해서 열심히 답변드리고 있습니다 저 말고도 다른 데이터그룹 분들도 함께 답변을 하고 있습니다 3) Kaggle에서 홍보 좋은 분들을 모시기 위해 다양한 곳에서 회사 홍보를 해야한다고 생각합니다 저는 데이터에 관심있는 분들은 캐글을 많이 할 것이란 가정하에 캐글에서 [SOCAR]라는 타이틀을 달고 대회에 참가하고 있습니다 앞으로도 좋은 팀, 좋은 문화를 가질 수 있도록 계속 노력할 예정입니다 사내에서 기획자, 마케터 분들을 위해 BigQuery의 모든 것 입문편 강의도 하고 있습니다 미래 요새 관심사인 Operation Research, 시계열 예측, MLOps 등 다양한 내용들을 공부, 정리, 현업에서 바로 활용을 하려고 합니다 친한 분들과 유튜브 채널을 만들었습니다 : DeepNOL! 데이터와 관련된 다양한 이야기를 전해드리려고 합니다 이와 별개로 제 개인 유튜브에 책 리뷰를 남기려고 합니다 구성원들과 함께 자라기를 실천하기 위해 어떤 행동을 할까 고민하고 있습니다 TF에서 팀 빌딩까지 9개월의 기록 : 성장하는 조직을 만드는 여정 : 쏘카 데이터 그룹 타다데이터팀 빌딩하는 문화에 대해 작성한 발표 자료입니다 정리 정리하면 저는 2013년부터 데이터 분석에 관심을 가지다 방황하고, 2015년부터 다시 공부하고, 2016년부턴 아예 데이터 분야에 몰입했습니다. 2018년엔 퇴사 후 제 부족한 부분을 채웠고, 현재 쏘카에서 근무하고 있습니다 이 글을 보시는 분들도 포기하지 않고, 목표를 가진 상태로 꾸준히 공부하면 좋은 결과가 있을거라 믿습니다 FAQ 제 일대기를 작성했지만, 명쾌하게 답변하지 않은 부분을 정리하려 합니다 1) 대학원에 꼭 가야할까요? 이 고민을 하기 전에, 본인이 연구를 하고 싶어하는지, 현실의 문제를 풀고싶은지 등을 고민하면 좋을 것 같습니다 연구를 하고 싶다면 대학원에 가는 것도 좋은 선택일 수 있고, 반면 현실의 문제를 풀고 싶다면 회사에 가는 것도 좋은 선택일 수 있습니다 본인의 목적과 미래를 생각해보면 조금 더 확실한 결정을 내릴 수 있을 것 같습니다 2) 비전공자인 제가 할 수 있을까요? 각자의 전공은 충분히 의의가 있다고 생각합니다. 저는 경영학에서 배운 커뮤니케이션 능력이 업무에 큰 도움이 되고 있습니다 비전공자일 경우 코딩이나 수학에서 어려움이 있을 수는 있지만, 꾸준히 공부하다보면 어느정도 해결될 것이라 생각합니다 전공이냐 비전공이냐기 보다 “나는 할 수 있고, 나의 페이스대로 잘 가고 있다”라는 생각을 하시며 꾸준히 하시면 좋을 것 같습니다 단, 정말 많은 노력이 수반되니 꾸준히 공부해주세요 3) 공부하시며 어려운 점이 있었나요? 공부할 내용이 너무 많아 어렵지만, 이런 생각을 반대로 새로운 내용을 배우는 것은 너무 재미있고, 하나씩 정복하는 재미가 있다고 생각했습니다 어렵다고 말하면 안할 생각인지 여쭤보고 싶습니다. 정말 하고 싶다면, 어려운 점도 극복할만하다 생각합니다 4) 무엇을 공부해야 할까요? 답이 없는 질문인 것 같습니다. 사람마다 배경이 다르기 때문에 어떤 것을 공부해야 한다!고 말하긴 어려울 것 같습니다 단, 채용공고를 정리한 내용을 토대로 어떤 능력이 필요한지는 파악할 수 있을 것 같습니다. 그 전에 어떤 직군을 하고 싶은지 고민해보시는 것을 추천합니다.제가 작성한 I-want-to-study-Data-Science를 참고하면 좋을 것 같습니다 5) 학원을 다녀야 할까요? 국비지원을 가야할까요? 요샌 의지만 있다면 인터넷에 다양한 자료가 있기 때문에, 학원을 굳이 안가도 됩니다. 스터디를 꾸려도 되고, 모두의 연구소 같은 곳에서 공부해도 되고 다양한 방법이 있습니다. 어떤 곳이 있는지 찾아보시고 결정하시면 좋을 것 같습니다 단, 국비지원 중 너무 많은 범위를 다루는 수업(예를 들어 자바, 하둡, 스파크, 텐서플로, 딥러닝 등을 3~6개월에 다루는 수업)이 얼마나 좋을지는 모르겠습니다(제가 경험을 안했기 때문에 판단하기 어렵지만, 과한 느낌이 있습니다) 6) 자격증을 따려고 하는데 도움이 될까요? 보통 자격증하면 ADP, ADsP, SQLD, SQLP, 사회조사 분석사, 정보처리기사 등이 있습니다. 공부의 목적으론 괜찮을 수 있지만 취업 목적으론 효용이 있는지 모르겠습닌다 저는 사회조사 분석사 2급, 정보처리기사가 있지만 취업할 때 도움을 받은 기억은 없습니다 이 업계는 실력이 중요하기 때문에 실제 데이터 분석을 하거나, 프로젝트 진행, 캐글 등을 참여해보는 것이 좋을 것 같습니다 위 질문에 대해 답변을 남긴 FAQ를 참고하셔도 좋을 것 같습니다 이력서 작성이 어려운 분들을 위해 제 이력서를 보여드립니다. 참고만 하시면 좋을 것 같습니다 저는 구인본님의 이직초보 어느 개발자의 이력서 만들기를 보고 이력서를 만들었습니다 제가 만든 이력서 템플릿도 공유드립니다(pages 파일) : Dropbox Link",
    "tags": "diary",
    "url": "/diary/2019/04/05/how-to-study-datascience/"
  },{
    "title": "Genetic Algorithm 유전 알고리즘과 Python 구현",
    "text": "Genetic Algorithm, 유전 알고리즘에 대한 설명과 파이썬 구현 코드를 작성한 글입니다 Genetic Algorithm John Holland Larger class of Eolutionary Algorithms에 속해 있음 Evolution, natural selection, reproduction에 영감을 받음 Genetic crossoer, mutation, selection 특징 Population-based search(Not neighborhood) Binary encoding Chromosomes(염색체), genotype vs phenotype 용어 Crossover 2-point crossover Random cutoff points Single-point crossover 2-point crossover Multipoint crossover Mutation Elitism Fitness value Selection Reference 위키피디아 유전 알고리즘 [최적화/전역 최적화] 유전 알고리즘 (Genetic Algorithm)",
    "tags": "optimization data",
    "url": "/data/2019/04/02/genetic-algorithm/"
  },{
    "title": "Uber Kepler.gl : 지리 데이터 시각화 도구",
    "text": "Uber의 데이터 시각화 도구인 Kepler.gl에 대한 글입니다 Kepler.gl 우버에서는 쏟아지는 GPS 데이터를 분석하기 위해 데이터 시각화팀을 만듬 이 시각화 팀은 vis.gl라는 홈페이지를 운영하고 있는데, 다양한 프레임워크를 제공하고 있음 Kepler.gl, Movement, AVS 등 csv, json, geojson 데이터 포맷을 사용할 수 있고, GPU 지원 덕분에 대용량 데이터도 쉽게 렌더링됨 단, 대용량(250MB 이상) 데이터를 업로드하려면 사파리를 사용하는 것을 권장하고 있음 기능 지원하는 기능은 Arc, Line, Hexagon, Point, Heatmap, GeoJSON, Buildings 등이 있음 Arc Line Hexagon Point Heatmap GeoJSON Buildings 사용 방법 홈페이지 메인에서 GET STARTED 클릭 혹은 URL : https://kepler.gl/demo로 접속 데이터 추가 데이터를 추가하는 부분 만약 자신의 데이터가 있다면, (적절히 가공한 후) 업로드하면 됨 데이터가 없다면 우선 Sample Data를 클릭 택시 데이터가 마음에 들기 때문에, NYC taxi trips 클릭 기본 UI 총 6가지 Component로 나눠봤는데(=제 마음대로), 하나씩 설명하면 1) Layers, Filters, Interactions, Base map 선택 창 Layers는 일단 Pass Filters 여기서 Filter를 설정할 수 있음 시간 관련 Filter를 걸면 4)가 생김 int나 float 컬럼을 선택하면 값을 조절할 수 있는 bar가 생김 string 컬럼은 Search box가 생김 Interactions Tooltip은 마우스온할 때 어떤 값들이 추가적으로 나오는지 설정하는 부분으로 쉽게 추가하거나 뺄 수 있음 Brush는 커서로 영역을 강조 표시 할 수 있음. 브러쉬가 켜지면 모든 레이어가 어두워짐 마우스로 커서를 올려 놓은 부분만 밝아지고 특히 아크 레이어와 잘 작동 Tooltip or Brush 중 하나만 선택 가능 Base map Map Style을 선택할 수 있고, May Layers도 설정 가능 Label, Road, Building, Water 등을 ON/OFF 할 수 있고 레이어 순서를 결정할 수 있음 Map Style을 커스텀하고 싶으면 mapbox에서 자신만의 map style을 publish한 후, access token을 입력하면 사용 가능 2) Layers 이 부분엔 데이터의 타입(Point, Arc, Line, Grid, Cluster, Icon, Heatmap, H3, Polygon 등등)을 설정 가능 설정값에 따라 옵션이 세부적으로 조절 가능 레이어를 ON/OFF 가능하고 Label이 없는 경우엔 Point로 표현 가능하고, Label도 붙일 수 있음 이 부분은 백번 보는 것보다 직접 실행하는 것이 좋음 3) Layer Blending 레이어를 어떻게 섞을지, additive, normal, subtractive가 존재 4) Time Playback 시간 Filter가 걸릴 경우 활성화 Bound를 조절한 후, 재생 버튼을 누르면 시간순으로 데이터를 볼 수 있음 또한 재생 속도도 조절 가능 5) Map View mode dual map view, 3D Map, show legend 가능 Dual Map view를 한 후, visible layers를 다르게 설정할 수 있음 6) Share Share 옵션으로 Export Image, Export Data, Export Config, Export Map, Share Public URL(Dropbox)를 사용할 수 있음 config를 export해서 재사용 가능 2019 로드맵 기능에 대한 로드맵은 아래 Github 참고 Kepler.gl 2019 Roadmap FAQ Video File로 export할 수 있는지? kepler에선 불가능, Quicktime Player나 Giffy 등을 사용하면 됨 참고 링크 : Giphy Capture 지도에 추가할 수 있는 데이터 세트의 수에 제한이 있는지? 제한은 없지만 많을수록 성능이 저하될 가능성이 큼 레이어도 동일 최대 파일 업로드 크기는 얼마인지? 250mb 이하 파일을 허용함. 사파리에선 더 큰 파일을 로드할 수는 있지만 성능은 제한됨 Tutorial Vis.gl 블로그에 글이 많음 Animating 40 years of California Earthquakes Mapping the Parisian trees Visualizing Unemployment for U.S. Counties with kepler.gl Using Kepler.gl and Movement to Visualize Traffic Effects of a Rainstorm 자체 서버에 빌드하기 별도로 빌드해서 사용해야 하는 분들을 위해 작성한 부분 Git, Node.js, Yarn이 설치되어 있어야 함 MapboxAccessToken 있어야 함 홈페이지 가입 후 발급 역시 이런 것은 유료.. 월 499달러라.. git clone https://github.com/uber/kepler.gl.git cd kepler.gl yarn --ignore-engines export MapboxAccessToken=&lt;insert_your_token&gt; npm start 완료된 후, localhost:8080에 가면 아래 같은 화면이 뿅! Reference kepler.gl Github 공식 홈페이지 Kepler User Guide Developing Kepler.gl Vis.gl Medium",
    "tags": "mobility data",
    "url": "/data/2019/04/01/uber-keplergl/"
  },{
    "title": "Simulated Annealing 개념과 Python 구현",
    "text": "Simulated Annealing에 대한 설명과 파이썬 구현 코드를 작성한 글입니다 Simulated Annealing Kirkpatrick이 1983년에 만듬 뜨거운 욕조에서 재료의 냉각을 시뮬레이션하는 알고리즘에 기반 더 전통적인 방법의 변형 Local(neighborhood) search 확률론적 메타휴리스틱 방법 Annealing 내부 강도를 제거하기 위해 금속이나 유리를 가열하고 천천히 냉각시키는 방법 금속재료를 가열한 다음 조금씩 냉각해 결정을 성장시켜 그 결함을 줄이는 작업 열에 의해서 원자는 초기의 위치(내부 에너지가 극소점에 머무르는 상태)로부터 멀어져 에너지가 더욱 높은 상태로 추이됨 천천히 냉각함으로써 원자는 초기 상태보다 내부 에너지가 한층 더 극소인 상태를 얻을 가능성이 많아짐 Cooling하는 방법 Temperature를 낮추는 방법 Cooling rate Local minima/optima를 벗어나는 컨트롤된 방식으로 오르막 이동(더 나쁜 솔루션)을 허용함 확률로 더 나쁜 움직임을 받아들임 random number를 체크한 후, random이 적으면 나쁘게 움직이고 높으면 움직이지 않음 해를 반복해 개선하며 현재의 해 근방에 있는 해를 임의로 찾는데, 그 때 주어진 함수의 값과 전역 인자 T가 영향을 줌 위에서 말한 물리 과정과 비슷한 원리로 T(온도)의 값은 서서히 작아짐 처음엔 T가 크기 때문에 해가 크게 변화하지만, T가 0에 가까워짐에 따라 변화가 줄어듬 처음은 간단하게 비탈을 올라갈 수 있으므로, 지역 최적점에 빠졌을 때 대책을 생각할 필요가 없음 아이디어 자체는 모든 분야에 대하여 적용이 가능 임의의 경우의 수가 많은 경우 정해진 조건에서 대용량의 최적화를 찾을 때 유용하게 사용 Notations 빨간색이 Final Solution 방법 T_{0}, M, N, alpha, move operator의 종류를 설정 m = 1 Search space x_{i}에서 random point로 시작 move operator x_{t}를 사용해 다른 장소로 이동 근처를 둘러보고 그 중 하나로 이동 더 나아졌는지 확인 후, 나아졌으면 끝. 아니면 계속 나아지지 않았다면 random number를 취함 1/(e^{(f(x_{tmp})-f(x_{i})/T_{t})}랑 비교 작다면 취하고, 작지 않다면 다른 곳을 찾고 n = n + 1 N번 진행 m = m + 1 T_{t+1} = \\alpha * T_{t} 1time에 5~9 스텝을 반복 x_{Final}을 찾고 좋은 Solution으로 기록 Flowchart Himmelblau 구현 Himmelblau function z = (x^{2} + y - 11)^{2} + (x+y^{2}-7)^{2} x와 y는 -6 ~ 6 minimized z = 0.0000 1개의 극대값과 4개의 극소값을 가짐 Himmelblau 코드 import numpy as np import matplotlib.pyplot as plt x0 = 1 # Initial solution y0 = -1 k = 0.1 T0 = 1000 M = 300 N = 15 alpha = 0.85 z_int = ((x0**2)+y0-11)**2+(x0+(y0**2)-7)**2 print(f\"Initial X is {x0:.3f}\") print(f\"Initial Y is {y0:.3f}\") print(f\"Initial Z is {z_int:.3f}\") 처음엔 Z가 146으로 Optimal과 거리가 멈 import numpy as np import matplotlib.pyplot as plt x0 = 1 # Initial solution y0 = -1 k = 0.1 T0 = 1000 M = 300 N = 15 alpha = 0.85 z_int = ((x0**2)+y0-11)**2+(x0+(y0**2)-7)**2 print(f\"Initial X is {x0:.3f}\") print(f\"Initial Y is {y0:.3f}\") print(f\"Initial Z is {z_int:.3f}\") temp = [] min_z = [] # neighborhood search for i in range(M): for j in range(N): xt = 0 yt = 0 # move operator ran_x_1 = np.random.rand() ran_x_2 = np.random.rand() ran_y_1 = np.random.rand() ran_y_2 = np.random.rand() if ran_x_1 &gt;= 0.5: x1 = k*ran_x_2 else: x1 = -k*ran_x_2 if ran_y_1 &gt;= 0.5: y1 = k*ran_y_2 else: y1 = -k*ran_y_2 xt = x0 + x1 yt = y0 + y1 of_new = ((xt**2)+yt-11)**2+(xt+(yt**2)-7)**2 of_final = ((x0**2)+y0-11)**2+(x0+(y0**2)-7)**2 ran_1 = np.random.rand() form = 1/(np.exp((of_new-of_final)/T0)) if of_new &lt;= of_final: x0 = xt y0 = yt elif ran_1 &lt;= form: x0 = xt y0 = yt else: x0 = x0 y0 = y0 temp = np.append(temp, T0) min_z = np.append(min_z, of_final) T0 = alpha*T0 print(f\"X is {x0:.3f}\") print(f\"Y is {y0:.3f}\") print(f\"Final OF is {of_final:.3f}\") plt.plot(temp, min_z) plt.title(\"Z vs Temp\", fontsize=20, fornweight='bold') plt.xlabel(\"Temp\", fontsize=18, fornweight='bold') plt.ylabel(\"Z\", fontsize=18, fornweight='bold') plt.xlim(1000, 0) plt.xticks(np.arrange(min(temp), max(temp), 100), fornweight='bold') plt.yticks(fontweight='bold') plt.show() &gt;&gt;&gt; X is 3.584 &gt;&gt;&gt; Y is -1.848 &gt;&gt;&gt; Final OF is 0.000 x0를 2로 바꾸고, y0을 1로 바꾸고 실행해보기 Quadratic Assignment Problem QAP Wikipedia 8개의 department와 8개의 location이 있음 objective = minimize flow costs between the placed departments flow cost is flow * distance optimal flow costs is 107(or 214) 코드 import numpy as np from matplotlib import pyplot as plt import pandas as pd Dist = pd.DataFrame( [[0, 1, 2, 3, 1, 2, 3, 4], [1, 0, 1, 2, 2, 1, 2, 3], [2, 1, 0, 1, 3, 2, 1, 2], [3, 2, 1, 0, 4, 3, 2, 1], [1, 2, 3, 4, 0, 1, 2, 3], [2, 1, 2, 3, 1, 0, 1, 2], [3, 2, 1, 2, 2, 1, 0, 1], [4, 3, 2, 1, 4, 2, 1, 0]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], index=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) Flow = pd.DataFrame( [[0, 5, 2, 4, 1, 0, 0, 6], [5, 0, 3, 0, 2, 2, 2, 0], [2, 3, 0, 0, 0, 0, 0, 5], [4, 0, 0, 0, 5, 2, 2, 10], [1, 2, 0, 5, 0, 10, 0, 0], [0, 2, 0, 2, 10, 0, 5, 1], [0, 2, 0, 2, 0, 5, 0, 10], [6, 0, 5, 10, 0, 1, 10, 0]], columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"], index=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]) T0 = 1500 M = 250 N = 20 alpha = 0.9 X0 = [\"B\", \"D\", \"A\", \"E\", \"C\", \"F\", \"G\", \"H\"] New_Dist_DF = Dist.reindex(columns=X0, index=X0) New_Dist_Arr = np.array(New_Dist_DF) # Make a dataframe of the cost of the initial solution objfun1_start = pd.DataFrame(New_Dist_Arr * Flow) objfun1_start_Arr = np.array(objfun1_start) sum_start = sum(sum(objfun1_start_Arr)) print(sum_start) Temp = [] Min_Cost = [] for i in range(M): for j in range(N): ran_1 = np.random.randint(0, len(X0)) ran_2 = np.random.randint(0, len(X0)) while ran_1 == ran_2: ran_2 = np.random.randint(0, len(X0)) xt = [] xf = [] A1 = X0[ran_1] A2 = X0[ran_2] # Make a new list of the new set of departments w = 0 for i in X0: if X0[w] == A1: xt = np.append(xt, A2) elif X0[w] == A2: xt = np.append(xt, A1) else: xt = np.append(xt, X0[w]) w = w + 1 # print(X0) # print(A1, A2) # print(xt) new_dis_df_init = Dist.reindex(columns=X0, index=X0) new_dis_init_arr = np.array(new_dis_df_init) new_dis_df_new = Dist.reindex(columns=xt, index=xt) new_dis_new_arr = np.array(new_dis_df_new) # Make a dataframe of the current solution objfun_init = pd.DataFrame(new_dis_init_arr * Flow) objfun_init_arr = np.array(objfun_init) # Make a dataframe of the new solution objfun_new = pd.DataFrame(new_dis_new_arr * Flow) objfun_new_arr = np.array(objfun_new) sum_init = sum(sum(objfun_init_arr)) sum_new = sum(sum(objfun_new_arr)) rand1 = np.random.rand() form = 1 / (np.exp(sum_new - sum_init) / T0) if sum_new &lt;= sum_init: X0 = xt elif rand1 &lt;= form: X0 = xt else: X0 = X0 Temp.append(T0) Min_Cost.append(sum_init) T0 = alpha * T0 print() print(\"Final Solution\", X0) print(\"Minimized Cost:\", sum_init) plt.plot(Temp, Min_Cost) plt.title(\"Cost vs Temp\", fontsize=20, fontweight='bold') plt.xlabel(\"Temp\", fontsize=18, fontweight='bold') plt.ylabel(\"Cost\", fontsize=18, fontweight='bold') plt.xlim(1500, 0) plt.xticks(np.arange(min(Temp), max(Temp), 100), fontweight='bold') plt.yticks(fontweight='bold') plt.show() Reference Simulated Annealing, SA, 담금질 기법 - 위키피디아 Simulated Annealing 동전 뒤집기와 Simulated Annealing",
    "tags": "optimization data",
    "url": "/data/2019/04/01/simulated-annealing/"
  },{
    "title": "Uber H3 : 육각형 계층의 인덱스",
    "text": "Uber의 그리드 시스템인 H3에 대한 글입니다 그리드 시스템 Grid 시스템은 대용량 데이터를 분석하고, 지구의 영역을 구분 가능한 그리드셀로 분할할 때 중요 한국 같은 경우 행정 구역 단위가 있지만(시군구동…) 이 단위는 행정을 위한 단위기 때문에 분석시 유용하지 않음 강남구는 생각보다 길고, 큼. 강남역 왼쪽은 서초구 우버에선 ride price과 dispatch을 효율적으로 최적화하기 위해 그리드 시스템인 H3을 개발하고 오픈소스로 공개함 유튜브 영상 H3 Youtube Surge Pricing 하며 겪은 이슈 우버에서 처음엔 도시 단위로 오퍼레이션 했는데, Boundary Effect가 생김 Surge Cliffs에서 취소가 생김 Pantom Demand: 유령 수요(너무 넓은 지역) 프랑스는 도시가 매우 복잡하고 잘게 쪼개져 있음 도시보다 작은 단위가 필요함을 깨달음 왜 육각형(헥사곤)을 사용했는가? Smooth gradients of demand를 구현할 수 있음 Clear center of demand Dynamic neighborhoods 각 육각형끼리 거리가 동일함(반면, geohash 같은 사각형은 좌우상하와 대각선의 거리가 다름) 아래 관점으로 여러 실험 Neighbor Traversal(이웃 순회) Subdivision(재조합) 사각형이 완벽하게 재조합이 되나, 헥사곤은 그렇진 않음. 우버는 완전 똑같을 필요는 없다고 함(약간의 에러를 감수) Distortion(왜곡) 헥사곤을 선택한 중요한 이유 중 하나 지구는 sphere(구체)고, 평평하지 않음. 그러나 grid는 평평함. Map Projection을 통해 평평해짐 H3를 사용하면 지리 데이터를 분석해 여러 결정을 내릴 수 있음 그리드를 사용한 분석 사례 매 순간 라이더가 라이딩을 요청하고, 운전자는 여행을 시작, 배고픈 사용자는 음식을 요청 각 이벤트는 특정 위치에서 발생 이런 이벤트를 분석해 시장에 대해 더 잘 이해하고 최적화할 수 있음 도시의 특정 지역에서 공급보다 수요가 많아 가격을 조정하거나 특정 드라이버에게 가까운 거리에 승차 요청이 있다고 알릴 수 있음 도시 전체 데이터를 분석해야 하고, 미세한 단위로 수행되어야 함 헥사곤은 quantization error를 최소화함 H3 Hexagonal global grid system의 장점 + hierarchical indexing 시스템을 결합하기 위해 H3을 만듬 지구상 3차원 위치에서 2차원 점으로 이동하려면 투영(Projection)이 필요 Mercator Projection이 유명한데, 크기 왜곡이 발생해 셀의 영역이 달라짐 정사각형 그리드는 여러 계수가 필요로 함 결국 지도 투영은 이십면체(Icosahedron)를 중심으로 하는 gnomonic projection을 사용함 20면체는 다양한 방법으로 펼쳐져 2차원 지도를 생성할 수 있으나, H3는 전개하지 않고 20 면에 그리드를 배치해 geodesic discrete global grid 시스템을 만듬 육각형 육각형은 중심점과 이웃 점 사이에 단 하나의 거리를 가지고 있음 그라디언트에 대한 분석/스무딩을 단순화함 H3 그리드 122개의 기본 셀을 지구상에 배치하고 한 면에 10개의 셀을 배치해 구성함 16개의 해상도(resolution)을 지원 계층적 특성으로 인덱스의 해상도를 효율적으로 자르고 복구할 수 있음 육각형 색인은 64 비트 정수로 표현됨 육각형의 중심에서 주위 육각형 어디로 움직여도 동일한 거리를 이동함(지오해시는 방향에 따라 다름) H3 사용하기 Python h3-py cc, make, cmake가 깔려있는지 확인 which cc /usr/bin/cc which make /usr/bin/make which cmake /usr/bin/cmake 설치 pip3 install h3 geo_to_h3 함수 위도, 경도, 해상도를 통해 h3 인덱스를 반환하는 함수 arg : lat 위도, lng 경도, hex resolution from h3 import h3 h3_address = h3.geo_to_h3(37.3615593, -122.0553238, 5) h3_to_geo 함수 h3 인덱스를 통해 헥사곤의 중심점(lat, lng)을 반환하는 함수 hex_center_coordinates = h3.h3_to_geo(h3_address) h3_to_geo_boundary 함수 h3 인덱스를 통해 헥사곤의 boundary를 반환하는 함수 hex_boundary = h3.h3_to_geo_boundary(h3_address) k_ring_distance 함수 h3 인덱스를 통해 거리가 k 안에 있는 h3 인덱스를 반환하는 함수 h3.k_ring_distances(h3_address, 4) Folium과 결합해 Jupyter Notebook에서 사용하는 예시 SQL에서 사용하고 싶으면, UDF를 만들어서 사용할 수 있음 H3를 토대로 나온 데이터를 kepler.gl에서 시각화할 수 있음 관련 블로그 글 : Uber Kepler.gl : 지리 데이터 시각화 도구 Reference H3: Uber’s Hexagonal Hierarchical Spatial Index H3 Github h3-py Github H3 Youtube",
    "tags": "mobility data",
    "url": "/data/2019/03/31/uber-h3/"
  },{
    "title": "메타휴리스틱 기법과 탐색 방법, Metaheuristics and Search Technique",
    "text": "메타휴리스틱 이론, 메타휴리스틱 기법에 대해 작성한 글입니다 Overview Operations Research OR 수학적 모델링이나 통계 분석, 최적화 기법 등을 이용해 복잡한 의사결정 문제에서 최적해 혹은 근사최적해를 찾아내 이익, 성능, 수익 등을 최대화하거나 손실, 위험, 비용 등을 최소화하는 현실적인 문제를 해결할 때 사용 2차 세계 대전에서 기원 Scheduling Transportation Inventory management Warehousing Facility allocation Energy distribution 예제 의자는 15BF, 6시간 노동력이 필요하고 식탁은 24BF, 5시간의 노동력이 필요 Maximize profit =&gt; z = 12X + 10Y Chair : 12달러, Table : 10달러 Subjected to 15X + 24Y &lt;= 300 6X + 5Y &lt;= 120 OR 구성 요소 목적 함수(Objective function) 결정 변수(Decision variables) 목적 함수식이나 제약조건에서 미지수로 나타나는 변수 제약 조건(Constraints) Types of Solutions and Constraints Solutions Infeasible : 모든 제약조건에 만족하는 solution이 없는 경우 Feasible Optimal Near-Optimal Constraints Hard constraints : 필수 제약조건 Soft constraints Continuous vs Combinatorial Continuous 연속적 문제 결정 변수가 연속적인 문제 Combinatorial 결정 변수가 이산적인 문제 이산적 문제, 조합 최적화 문제 정수 계획법(Integer Programming : 결정 변수가 정수인 최적화 문제)이 대표적 예시 Traveling Salesman Problem(TSP) Vehicle Routing Problem(VRP) Knapsack PRoblem(KP) Quadratic Assignment Problem(QAP) P vs NP Problems P problem : 짧은 다항식 문제 NP problem : 짧은 non-deterministic 문제 메타휴리스틱 휴리스틱 합리적인 계산 비용으로 최적 또는 거의 최적의 솔루션을 찾는 기술 메타휴리스틱 특정 문제에 특화되지 않고 자연에서 영감을 얻은 경험적 방법 NP Problem은 문제가 커지면서 점점 더 어려워짐 자주 사용되는 메타휴리스틱 방법 Genetic Algorithm(GA) Tabu Search(TS) Ant Colony Optimization(ACO) Partical Swarm Optimization(PSO) Simulated Annealing(SA) Search Techniques Local search vs global search local : 이웃에 기반함, 그리드 서치 global : search space Deterministic vs stochastic deterministic : non-random 참고 링크 Continuous Problem 특정 인풋으로부터 어떤 output이 나오는 함수 Minimization or maximization 문제 Himmelblau’s function Combination Problem discrete elements의 조합 문제 Minimization or maximization 문제 TSP 문제 5개의 도시 : A, B, C, D, E minimum route : D-A-E-C-B (최소 거리) 다른 것도 가능 : A-D-C-E-B Neighborhood vs Population Search Neighborhood search local search 계산적으로 어려운 최적화 문제를 해결하는 휴리스틱 방법론 잠재적 솔루션을 취하고 주변 환경(일종의 이웃)을 체크 Simulated Annealing and Tabu Search Population Based Search 각 iteration에서 솔루션의 population를 사용하는 방법 잠재성이 있는 솔루션을 Evaluate 랜덤하게 다른 솔루션을 생산하기 위한 솔루션을 선택 Genetic Aglorithm, Evolutionary Strategies, Particle Swarm Optimization 메타휴리스틱은 어떻게 작동하는가? Random initial solution Neighborhood search 초기 솔루션부터 근처를 돌아다니며 실행 모든 스텝을 evaluate 각 스텝의 목적 값(objective value)를 추적 Population-Based search 랜덤 초기 솔루션을 여러개 생성 더 “나은” 솔루션을 생성 Diverse Reference 휴리스틱 이론 - 위키피디아",
    "tags": "optimization data",
    "url": "/data/2019/03/26/metaheuristics/"
  },{
    "title": "KafkaKRU(Kafka 한국사용자 모임) 2회 미니밋업 후기",
    "text": "카프카 한국자 사용자모임에서 진행한 2회 미니밋업 후기입니다 행사 정보 Facebook Event URL 1. 제목: KafkaKRU(Kafka 한국사용자 모임) 제2회 미니밋업 2. 신청: https://www.onoffmix.com/event/171028 3. 날짜: 3월 26일 화요일 오후 6시 50분 4. 장소: 서울특별시 송파구 올림픽로35길 125 삼성SDS 잠실 Campus 동관 지하2층 다목적홀 5. 인원: 35명 6. 후원: (1) 삼성 SDS: 장소 및 다과 (2) 카카오: 카카오 스티커, 카카오 프렌즈 상품 (3) 책만 출판사: &lt;대시보드 설계와 데이터 시각화&gt; 3권, &lt;블록체인 기업으로 가는 길&gt; 3권 (4) 젯브레인 7. 일정 18:50 ~ 19:00 인사 + 네트워킹 19:00 ~ 19:20 Kafka를 활용한 캐시 로그 처리 - 김현준(카카오) 19:20 ~ 19:40 Kafka를 활용한 Elasticsearch 실무프로젝트 소개 - 이은학(메가존) 19:40 ~ 19:50 브레이크 타임 19:50 ~ 20:10 Kafka를 활용한 rabbitmq 로그 처리 - 정원빈(카카오) 20:10 ~ 20:30 Kafka를 Microservice Architecture에 활용하기 - 이동진(Apache Software Foundation) 20:30 ~ 20:50 Kafka 프로듀서 &amp; 컨슈머 - 강한구(카카오 모빌리티) 21:00 ~ 뒷풀이 인사 발표 자료 : https://github.com/kafkakru/meetup 나중에 스티커를 만든다고 하심! Kafka를 활용한 캐시 로그 처리 사례 카카오 김현준님 1도 모르는 사람이 Kafka &amp; ELK로 캐시로그 처리해보기 캐시 시스템과 실시간 로그분석의 필요성 캐시 시스템 오픈소스를 활용한 구성(nginx + ATS) 자체 개발 캐시서버 img/static image(css/js), 동영상 등을 처리하기 위한 캐시 시스템 1차 Cache ⇒ 2차 Cache ⇒ 원본(Origin) 실시간 로그 분석 필요성 Image size, 404, 응답 지연 등 ⇒ 하루 전 데이터를 가지고 모니터링 캐시 로그 분석 시스템 구축 운영의 니즈를 반영하고자 시스템 구축 시작 ELK로 시작, 1차 Cache에서 access_log를 그림 요청이 많고, 트래픽이 많음 ⇒ 로그도 많음 로컬 캐시 서버에 로그를 쌓기 싫어서, nginx에서 바로 쏨 문제 인지와 Kafka 도입 그리고 Lag 키바나 그래프를 보다보니 로그가 너무 조금만 들어오고 있음!? 분석 시작 Syslog 전송 과정에서 누락? Logstash 성능? 주변에 물어보니 대용량이나 많은 트래픽은 logstash 앞단에 큐 하나! redis나 rabbitmq를 찾아보다가 승범님이 카프카를 쓰길 권함 lag이 계속 쌓임…! lag은 처리하지 못한 수치 파티션을 쪼개던지, 컨슈머를 늘리거나 등등 로그 분석이 돈 버는 것은 아니라 컨슈머를 늘리기엔 눈치가 보이고.. 파티션 조정해도 효과가 적음 logstash 설정이 잘못된 것 같으니 다시! ⇒ 모두 동일.. auto.commit.interval.ms=5000과 auto.commit= true interval을 줄이니 lag이 사라짐 현재는 수백대의 cache 서버 로그를 초당 15만건 이상 처리 대략 30테라? Kafka를 활용한 Elasticsearch 실무프로젝트 소개 메가존 이은학님 빅데이터 플랫폼 구축 및 활용솔루션 개발 중 프로젝트 카드사 비정형 / 정형 / 외부 데이터 ⇒ 분산되어 있는데 통합! 빅데이터 허브 : 엘라스틱 서치 + 하둡 사이언스 플랫폼 프로젝트에서 Elastic Stack 활용 영역 비정형 데이터 분석 Google analytics 고객 상담데이터(STT) ⇒ RESTFul 한글 형태소 분석기(Nori) 웹/앱 로그 아키텍쳐 스트리밍 데이터(24시간 발생하는 웹, 고객 상담) ⇒ 카프카 GA 데이터 ⇒ Batch로 logstash 카프카 사용 이유 데이터가 많고, 다른 곳에서 사용 카드사라 고객 상담 데이터 중 탈퇴라는 단어나 카드론은 민감 ⇒ 이 단어가 나오면 해당 부서에 Push ⇒ 대응(카드론은 수익에 연결된 단어) 프로젝트 이슈 사용자 쿼리 기록해야 함 ⇒ 금융감독원 감사 6.5.1부터 UserID를 남길 수 있음 xpack.security.audit.enabled; true xpack.security.audit.outputs: logfile 통신 노드도 남음 ⇒ 제외할 ID를 설정해야함 kibana, 다른 id Audit 활용 사례 건수 확인 (curl) 조회(페이징) 주의 사항 : 엘라스틱은 기본적으로 10000건까지만 반환함. 설정 필요 max_result_window를 설정하고 조회한 후, 해제(null) ⇒ 해제 안하면 부담이 될 수 있음 개인정보 비식별화 비정형 데이터 특정상 개인정보의 식별범위 모호 처음엔 할 수 없다고 했는데, 생각해보니 몇 패턴이 있음 고객 상담을 전문으로 하는 분들은 단어 선택을 또박 또박하게 함 ⇒ 음성 분석 파일을 분석하니 ~~ 회원님 ~~님이라고 정확하게 발음하는 것을 확인함 ⇒ 로직 처리 카드 번호, 주민번호 : 연속되는 숫자 체크 치환 : 주소 Ruby Filter(str.rb) 작성 후, str.conf(logstash 적용) 빅데이터 에코시스템 연계 Elasticsearch → Hadoop ES Hadoop 사용 일별을 index로 지정 N번 실행시 중복 방지하도록 overwriting table Hadoop → Elasticsearch 명사 별도 적재 사례 적재된 한글 문장 데이터 중 명사만 별도 field로 적재 필요 이유 : Nori 형태소 분석 결과를 타 빅데이터 시스템에서 활용 REST Call이 힘드니 하둡에 적재 아이디어1 : 인덱스 데이터는 날짜별로 쌓임 아이디어2 : logstash를 통해 reindex가 가능 아이디어3 : Cron 형태의 배치가 가능 INDEX 조회시 source와 토큰을 동시 조회한 후, 동일 INDEX에 source와 토큰 업데이트! 소스도 올라와있음! Kafka를 활용한 RabbitMQ 로그 처리 카카오, 정원빈님 RabbitMQ erlang으로 구현된 AMQP 메시지 브로커 Advanced Message Queue Protocol AMQP 퍼블리셔가 Exchange → 큐에 전달(메세지를 저장하는 저장소) → Consumer에서 push/pull 방식으로 가져감 Exchange, queue가 VHOST 논리 단위로 생성 퍼블리셔가 제대로 보냈는지, 컨슈머가 제대로 받았는지 확인할 수도 있음 TCP 연결 체크하는 로직 RabbitMQ 모니터링 Server resource 메모리를 모니터링 RabbitMQ LOG AMQP OPERATION 밑에 2개를 카프카를 사용함 Kafka + Elastic Search로 로그 처리 시스템을 개발 Rabbit MQ에서 Filebeat/Worker에서 → APP Topic, AMQP Topic을 카프카로 전송 → Elastic Search로 저장 → Kibana 토픽 중 특정 단어가 포함되었거나 AMQP 중 확인하고 싶은 오퍼레이션을 파싱, 구별해서 관리자에게 알람이 가도록 구성 RabbitMQ? Kafka? 둘다 메세지 브로커 RabbitMQ도 멀티컨슈머가 가능하긴 한데, 추가될 때마다 큐를 추가적으로 생성해야 함 Exchange는 큐의 메세지만큼 복사해서 전달 Kafka는 Consumer를 추가하면 그냥 추가하면 끝 하나의 동일한 메세지를 여러개의 컨슈머가 동시에 할 수 있어서 기능 확장이 용이함 메시지 관점에서 상세 비교 RabbitMQ는 큐 기반이고 메모리에 저장됨 Consumer가 메세지 수신시 즉시 삭제 Kafka는 로그 기반이고 디스크에 저장 리텐션 설정하면 그 기간동안 상시 보관 메세지 처리 방식도 다름 RabbitMQ는 발송 확인 / 수신 확인 가능 Prefetch Count : Consumer에 이슈가 있을 때(제대로 수신하지 못할 떄) 다른 consumer나 다시 전송하도록 하는 설정 메모리 사용률이 높을 때 차단 Exchange를 통해 특정 큐에 라우팅 가능 Kafka는 발송 확인 가능 / 수신 확인 불가능 Prefetch나 메모리 사용률 높을때 차단하는 설정 없음 RabbitMQ는 똑똑하지만 조금 느린 브로커, Kafka는 게으르지만 매우 효율적인 브로커 서비스에 따라 적당한 것이 따로 있다고 생각함 동시에 모두 사용도 가능 RabbitMQ는 제이피모건에서 많이 사용함 Kafka Streams : Interactive Queries Apache Software Foundation, 이동진님 Kafka Topic에 저장된 내용을 Microservice에서 참조하고 싶은데, 어떻게 해야 하나요? Redis 같은데 저장해서 써야 하나요? Producer → Kafka → Consumer → Redis → Microservice? 이런 방법이 아니라! Interactive Query 기능 또는 Queryable Store 기능! Kafka Streams Kafka 0.10에 도입된 Stream 처리 library Kafka Topic을 실시간으로 받아서 처리하는 루틴을 간편하게 정의 가능 High Level DSL : KStream, KTable Low Level API도 지원 장점 Task 관리가 필요 없음 필요한만큼(=파티션 수) 알아서 작업(task)을 생성하고 thread pool에 분배 하나의 작업이 하나의 (topic, partition)에 대한 처리 전담 consumer group 기능을 사용해서 구현됨 → coordination이 필요 없음 프레임워크가 아닌 라이브러리 ⇒ 갖다 쓰면 됨 wordcount 예제 props와 topology 객체를 넣어주면 카프카 스트림즈 객체가 됨 스트림즈에 start method를 실행하면 지정한 로직대로 진행 Key-value storage Kafka Streams가 처리의 효율성을 위해 내부적으로 만든 저장소 RocksDB로 구현됨(Facebook에서 만든 inmemory db) 직접 생성할 수도 있고, 사용자가 임의로 생성할 수도 있음 KTable 객체를 생성하면 반드시 하나가 함께 생김 Interactive Query Kafka Streams 내부에 생성된 key value storage의 내용과 위치를 조회할 수 있도록 해주는 기능 Queryable Store : interactive query 기능이설정된 key value storage 수정은 안됨 Interactive Query 예제 결론 KTable을 사용하면 Kafka Topic의 내용을 표 형태로 읽어올 수 있다 그 내용을 Interactive Query를 사용해서 열어볼 수 있음 kafka Streams가 해주는 것 현재 프로세스에서 잡고 있는 partition에 포함된 key에 대한 value 값 현재 프로세스에서 잡고 있는 partition에 포함된 포트 값 Spark Streaming은 언제 쓸까? 여러 데이터 소스에서 읽고 복잡한 쿼리를 사용한다면 Spark Streaming Kafka 프로듀서 &amp; 컨슈머 카카오 모빌리티, 강한구님 AB Test 플랫폼을 만들 때 Flink, Vertical Service에서 ETL을 담당 예전엔 카프카 쓰세요! Default만 해도 된다고 했는데 요새는 더 깊게 알아야 할 것 같아 자료를 만듬 Producer 메세지를 전송 producer객체를 생성할 때 Accumulator와 Network Thread가 생성됨 Accumulator : 사용자가 send한 reocrd를 메모리(recordbatch)에 차곡차곡 쌓아줌 Network Thread : RecordBatch를 브로커로 전송 For문을 돌 때 buffer.size, batch.size로 지정 브로커로 전송하는 속도보다 쌓이는 속도가 많으면 내부적으로 어큐물레이터에 쌓이게 될테고, 버퍼 메모리만큼 차게 되면 어플리케이션에서 블락 됨 Network Thread가 별도의 쓰레드가 떠서 계속 쉴 틈 없이 작업을 수행 해당 레코드 배치가 어떤 브로커로 갈지 브로커를 상태 확인 브로커별 레코드 배치를 재배열 데이터 전송 linger.ms → send 속도가 너무 느려서 레코드 배치에 쌓이는 것이 별로 없을 때 이 시간만큼 기다렸다가 가져감 max.request.size → 바이크에 탈 사람의 수로 비유 max.in.flights.requests.per.connection (발표 자료가 아래쪽으로 되어있어서 정확히 못봄..! 자료 나중에 공개되면 확인) Broker 메세지를 저장 [Topic name] - [partition] 폴더 구조 confluent 블로그에서 본 내용인데, 하드 4테라 1개보다 1테라 하드 4개가 더 좋다! Segment 단위로 파일 저장 *.index, *.log, *.timeindex 첫 offset이 파일명이 됨 Consumer 메세지를 가지고 옴 fetcher와 coordinator Fetcher poll 함수가 실행되면 적절한 크기의 record 리턴하고, 내부에 record가 없다면 브로커에게 record드를 요청하고 저장. 그리고 적절한 크기의 record 리턴하는 역할 Coordinator fetcher가 열심히 일할 수 있도록 정보 제공 어떤 토픽, 파티션을 consume할지 Broker의 group coordinator와 통신 Heartbeat, offset, commit, consumer group join도 함 코드 예제를 통해 설명해주셨는데, 천천히 보면 좋을듯 Fetcher에 레코드가 있는 경우 바로 리턴 max.poll.records : 한번에 가져올 레코드 수 max.poll.interval.ms : poll이란 함수를 호출하는 최대 시간 Fetcher에 레코드가 없는 경우 max.partition.fetch.bytes fetch.min.bytes fetch.max.wait.ms 부록 Consumer Rebalance Consumer Group offset 0.9 미만에선 zookeeper에 consumer offset을 저장 ⇒ 껏다가 켰다가 ⇒ 무리가 감 0.9 이상에선 __consumer_offset 토픽을 사용 후기 카프카를 많이 사용하진 않고, GCP에서 Pub/Sub 정도만 사용했지만 꾸준히 관심을 가져온 카프카! 새로운 시각을 얻을 수 있었음 잠실의 삼성 SDS에서 행사가 진행되었는데, SDS 건물 처음 들어가서 신기했음..! 행사장에 가니 서브웨이 샌드위치도 준비해주시고 + 중간에 피자도 주셔서 배부르게 먹었음..! 감사합니다 Kafka는 ElasticSearch와 쓰는 조합이 정말 많은 것 같은데, 진짜일까? 확인해보기 Kafka Streams와 Machine Learning을 합쳐서 사용하는 사례도 발견 : kafka-streams-machine-learning-examples 오랜만에 만난 분들과 인사해서 좋았음! Kafka 한국 사용자 모임 가입 안하셨다면 가입 필수 :) 마지막 Google 설문지 링크는 bit.ly 같은 것으로 줄여서 bit.ly/kafkakru-미니밋업-2회-설문 이렇게 하셨으면 더 좋았을 것 같음..! (사실 설문하려고 했는데 수많은 텍스트 문자 있고, 뒤에서 안보여서 시도했다가 포기..OTL)",
    "tags": "lecture etc",
    "url": "/etc/2019/03/26/kafkakru-2nd-review/"
  },{
    "title": "pandas-gbq에서 인증(Authentication) 설정하기",
    "text": "pandas-gbq에서 인증(authentication) 하는 방법에 대해 정리한 글입니다 pandas-gbq는 pandas에서 Google BigQuery의 데이터를 쉽게 조회할 수 있는 파이썬 라이브러리입니다 Authentication 1) Service Account Key 사용한 인증 2) User Account를 통한 인증 Service Account Key Service Account Key는 Creation Page에서 쉽게 만들 수 있음 공식 문서 JSON key type을 선택해 다운로드 장점 간편하게 사용 가능 Remote 서버에서 사용할 경우 매우 유용 Remote 서버를 위한 Key를 발급한 후, IAM을 특정 권한만 설정 단점 처음엔 사용하기 쉬우나 회사에서 이 key를 통해 많은 사람들이 쿼리를 날린다면, 보안의 문제가 있을 수 있음 JSON key를 잘 관리할 수 있는지?가 핵심 이슈 이 이슈를 위해 2) User Account를 사용 사용 예시 private key 파라미터를 사용할 경우 deprecated 예정 import pandas as pd query = \"SELECT * FROM `dataset.table`\" pd.read_gbq(query=query, project_id=\"project_id\", private_key=\"path/to/key.json\", dialect='standard') credentials 파라미터를 사용할 경우 pandas 0.24.0, pandas-gbq 0.8.0 버전부터 사용 가능 from google.oauth2 import service_account import pandas as pd credentials = service_account.Credentials.from_service_account_file('path/to/key.json') query = \"SELECT * FROM `dataset.table`\" df = pd.read_gbq(query=query, project_id=\"project_id\", credentials=credentials, dialect='standard') User Account 이 방식은 Colab에서 Google drive 권한 획득하는 것처럼 웹을 띄운 후, G Suite 또는 Gmail 로그인으로 인증하는 방식 pydata_google_auth 라이브러리 사용 장점 Key가 아닌 User Account로 인증하기 때문에 GCP의 IAM으로 관리할 수 있음 단점 캐시된 인증이 끝난 후, 다시 작업해야 하는 점 사용 예시 import pandas as pd import pydata_google_auth SCOPES = [ 'https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/bigquery' ] credentials = pydata_google_auth.get_user_credentials( SCOPES, auth_local_webserver=True) query = \"SELECT * FROM `dataset.table`\" df = pd.read_gbq(query=query, project_id=\"project_id\", credentials=credentials, dialect='standard') 혹은 pd.read_gbq() 인자로 reauth=True, auth_local_webserver=True 를 넣어도 가능함 결론 어떤 방식이 좋은지는 “상황”에 따라 다름 각 방식의 차이점이 무엇인지 인지하고, 적절하게 사용하는 것이 핵심! 공식 google-cloud-bigquery에서 인증하는 방법이 궁금하면 공식 문서 참고 Reference pandas_gbq",
    "tags": "basic gcp",
    "url": "/gcp/2019/03/17/pandas-gbq-auth/"
  },{
    "title": "Terraform 소개 및 정리",
    "text": "코드로 인프라를 관리할 수 있는 테라폼에 대해 정리한 글입니다 코드로서의 인프라스트럭처 “Write, Plan, and Create Infrastructure as Code” 인프라스트럭처를 코드로서 작성, 계획, 생성하는 도구 하나의 패러다임 Infrastructure as Code 인프라스트럭처의 정의 리소스들의 집합 물리적 환경 : 네트워크 장비, 서버 컴퓨터 등 클라우드 : 가상 컴퓨팅 자원, 매니지드 서비스 등 코드로서 인프라스트럭처 IaC 도구 리소스를 코드로 관리하는 도구 관리하는 도구들 서버 스크립팅(Server Scripting) 서버에서 스크립트 실행 설정 관리(Configurations Management) 대상 : 서버의 상태 리소스 선언 도구(Resource Management) 대상 : 리소스를 정의 설정 관리 셰프, 퍼핏, 앤서블 등 DSL 코드로 서버의 상태를 정의하고 관리 서버 명령어보다 추상회되어 있음 다수의 서버와 다양한 환경을 지원 완벽하지는 않지만 서버가 우리가 정의한대로 되도록 멱등성을 지원 Chef Solo 입문 : 인프라스트럭처 자동화 프레임워크라고 되어있음 일반적인 소프트웨어 개발 코드 작성 Git 저장소로 변경 관리 Github로 협업 이슈 관리 풀 리퀘스트와 코드 리뷰 지속적 통합 인프라스트럭처가 코드가 되면 소프트웨어처럼 개발할 수 있음! 리소스 선언 도구 테라폼 Terraform, 클라우드 포메이션 등 DSL 코드로 리소스를 선언하고 관리 주로 클라우드의 가상화된 리소스를 관리 물리적 장비는 대상이 아님 코드가 곧 아키텍처 테라폼 HashiCrop에서 만든 오픈소스 인프라 관리 도구 HCL를 사용해 인프라스트럭처를 코드로 정의 다양한 프로바이더를 지원 AWS, GCP, Azure 커맨드라인 인터페이스 의존성 표현도 가능 어떤 것이 먼저 필요한지 테라폼이 정의해서 생성 엔터프라이즈 서비스 제공 HCL과 테라폼 기초 HashiCorp Configurations Language 하시코프에서 만든 설정 전용 DSL 컨설, 테라폼에서 지원 JSON과 호환 변수, 조건문, 인터폴레이션 등 지원 HCL: 리소스 resource \"aws_instance\" \"bastion\" { ami = \"${ var.ami_id }\" instance_type = \"${ var.type }\" } aws_instance : 리소스 타입 bastion : 리소스 이름(테라폼 내부에서 참조하기 위해 사용) ami, instance_type : 속성 이름 ${ var. ~} : 속성 값 테라폼은 리소스를 계속 정의 테라폼 Workflow Write -&gt; Plan -&gt; Create(Apply) Write resource \"aws_instance\" \"bastion\" { ami = \"${ var.ami_id }\" instance_type = \"${ var.type }\" } 로컬에 aws_instance.bastion이 있고 AWS 계정엔 아무것도 없음 Plan aws_instance.bastion이 생성 계획 정의 terraform plan Create apply를 통해 AWS에 리소스를 생성 상태를 terraform.tfstate에 저장 용어 계획=plan 적용=apply 상태=tfstate 테라폼 삽질기 테라포밍 테라폼을 적용하는 두 가지 방법 새로 만드는 리소스를 테라폼으로 작성 운영 중인 클라우드 자원을 테라폼 코드로 작성 기존의 리소스 임포트 테라폼 import 명령어 현재 상태만을 tfstate에 임포트 tf 파일은 생성되지 않음 tfstate에만 자원이 있다면 삭제하려고 함(위험!!!) 로컬에 없으면(instance.tf) 없는걸 aws상에 동기화하려하니.. 삭제가 되는꼴 기존에 사용중인 인프라 가져오기 참고 루비의 terraforming AWS 리소스를 tf / tfstate로 임포트 지원 리소스 타입별로 통째로 임포트 모든 리소스 타입을 지원하진 않음 임포트 결과가 완벽하지 않음 테스트 및 수작업으로 보정 의존 관계를 표현하지 못함 구조 서비스 단위로 분리하고, 모듈로 서비스 리소스 분리 프로젝트 별 리소스 공유 거대한 프로젝트를 만들면 서로 리소스를 공유할 수 있음 프로젝트간에는 리소스를 직접 공유할 수는 없고, 간접적으론 가능 더 궁금하면 테라폼 기초 튜토리얼 참고 Reference 테라폼 도입기 클라우드 인프라스트럭처 코드로 재정의하기 - 김대권 테라폼 기초 튜토리얼 Google Cloud Platform에 Terraform 설정하기",
    "tags": "devops development",
    "url": "/development/2019/03/15/terraform/"
  },{
    "title": "크롬 개발자 도구 Console에서 Google Play Review 크롤링하기",
    "text": "Crawling Google Play App Review with Developer Tools 개발자 도구를 사용해 Google Play App 리뷰 크롤링하는 과정에 대해 작성한 글입니다(정말 빠르고 쉽게 크롤링하기!) 부록으로 Word Cloud 그리는 Colab도 첨부했습니다 Crawling Google Play Review 구글플레이 Review 데이터를 크롤링하는 방법은 많음 Requests와 BeautifulSoup을 사용하는 방법 Selenium을 사용하는 방법 이런 방법들은 모두 “파이썬 스크립트”를 작성해, 파이썬을 실행해야 함 스크립트 언어라 간단한 편이지만 그래도 더 간단히 크롤링하는 방법이 있음 크롬 개발자 도구를 사용한 구글 플레이 리뷰 데이터 크롤링! 이 방법이 적절한 경우 (주관적 생각) 시간이 없을 경우 리뷰 데이터가 생각보다 적은 경우 URL 확인 Google Play에 접속한 후, 리뷰를 보고 싶은 앱을 검색 요새 미세먼지가 너무 심해서, 많이 다운로드될 것 같은 미세미세를 검색 리뷰 확인 리뷰 모두 보기 클릭 URL은 https://play.google.com/store/apps/details?id=cheehoon.ha.particulateforecaster&amp;showAllReviews=true 스크롤 내리기 개발자 도구 열고 Console로 이동 Mac OS에서 단축키는 command + option + i 처음엔 리뷰가 모두 보이진 않음 리뷰를 계속 내리거나 “더보기” 버튼을 클릭해야 함 스페이스를 계속 내리기 위해 아래 코드를 Console에서 실행 코드 실행하지 않고 스페이스 계속 누르고 더보기 클릭하는 수작업도 나름 빠름 window.scrollTo(0, document.body.scrollHeight || document.documentElement.scrollHeight); 4번 스크롤을 내리면 “더보기” 버튼이 생기는데, 클릭하는 코드 document.getElementsByClassName(\"RveJvd snByac\")[0].click() 만약 리뷰가 너무 길 경우 “전체 리뷰” 버튼이 생기는데, 클릭하는 코드 ([0]에서 숫자값을 조절해야 할 수 있음) document.getElementsByClassName(\"LkLjZd ScJHi OzU4dc \")[0].click() 크롤링 어느 정도 리뷰를 끝까지 내렸다고 가정하고 Console에서 아래 코드 실행 참고로 별점이랑 이름, 리뷰 등을 따로 가져올 순 있지만 한방에 HTML로 가져옴 var reviews = document.querySelectorAll('div[class=\"d15Mdf bAhLNe\"]') var data = [] reviews.forEach(v =&gt; data.push({body: v.outerHTML})) 이제 콘솔에서 data하고 엔터를 치면 아래와 같이 나옴 (840) [{…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, {…}, …] 현재 리뷰 840개가 저장되었다는 뜻 copy(data)를 통해 클립보드에 데이터를 저장 이제 일단 데이터 크롤링 끝! 파일 생성 이제 터미널에서 아래 코드 실행 vi review.json 수정 모드를 위해 i를 한번 입력한 후, command + v 리뷰가 많다면 시간이 꽤 소요될 수 있음. 커피 마시고 오는거 추천 다 되었으면 ESC + :wq 로 저장하고 나오기 파이썬으로 데이터 전처리 이미 크롤링은 끝났지만, 전처리를 쉽게 하고싶은 분들을 위한 부분 아래 코드로 데이터 전처리 import pandas as pd df = pd.read_json(\"review.json\") p = re.compile(r'\\d+') def parser(body): bs = BeautifulSoup(body, 'html.parser') user_name = bs.find('span', class_='X43Kjb').text date = bs.find('span', class_='p2TkOb').text rating = bs.find('div', {'role':'img'})['aria-label'] rating = p.findall(rating)[-1] review_text = bs.find('span', {'jsname':'bN97Pc'}).text return user_name, date, rating, review_text df['user_name'], df['date'], df['rating'], df['review_text'] = zip(*df['body'].map(parser)) del df[\"body\"] 이제 user_name, date, rating, review_text를 가지는 데이터프레임이 생성됨 아래 코드로 date 처리 df['date'] = pd.to_datetime(df['date'], format='%Y년 %m월 %d일') rating 개수 그래프 그리기 import seaborn as sns sns.factorplot('rating',kind='count',data=df) Word Cloud 그리기 부록으로 Word Cloud 그린 Colab 링크 공유 :) 전체 코드가 작성되어 있음 5점 평점의 Word Cloud 2점 이하 평점의 Word Cloud 마무리 약간 생소하지만 간단한 개발자 도구를 통해 데이터를 수집해봤습니다! 도구는 선택의 문제이니 모두 상황에 맞는 적절한 도구를 찾으면 좋겠습니다! 개발자 도구로 할 수 있는 일이 더 궁금하신 분들은 링크를 참고해주세요 :)",
    "tags": "web development",
    "url": "/development/2019/03/12/crawling-in-developer-tools-console/"
  },{
    "title": "Flask에서 Unit Test하기",
    "text": "Flask에서 Unit Test 하는 방법에 대해 작성한 글입니다 Test 테스트 자동화의 중요성 시스템 테스트에서 가장 중요한 것은 테스트의 자동화 사람이 직접 실행하는 매뉴얼 테스트만 거칠 경우, 사이드 이펙트가 생길 수 있음 테스트를 최대한 자동화해서 테스트가 반복적으로, 자주 실행될 수 있도록 해야하며 항상 정확하게, 빠지는 부분이 없도록 테스트가 실행되도록 하는 것이 굉장히 중요함 테스트 방법 1) UI test / End-to-End test 2) integration test 3) unit test UI test / End-To-End test UI Test는 시스템의 UI(User Interface)를 통해서 테스트하는 것 웹이라면 웹 브라우저를 통해 웹사이트에 접속하고, UI에 직접 입력하고 클릭하는 등을 통해 기능이 정상적으로 작동하는지 테스트 장점 사용자가 실제로 시스템을 사용하는 방시과 가장 동일하게 테스트 단점 시간이 가장 많이 소요되는 테스트 프론트엔드 ~ 백엔드까지 모든 시스템을 실행시키고 연결해야 함 자동화하기 가장 까다로움 Selenium 같은 UI Test 프레임워크를 사용해 어느정도 자동화가 가능하지만 100% 자동화는 어려움 특히 화면 렌더링에서 문제가 발생 단점 때문에 전체 테스트 중 대략 10% 정도만 UI test 방식을 통해 실행하는 것을 추천 ( 주로 마지막에 테스트 ) Integration test 여태까지 미니터 API를 개발하며 해왔던 방식 API 서버를 실행시키고 HTTP Request를 실행해 Response가 올바른지 파악 테스트하고자 하는 서버를 실제로 실행시키고 테스트 HTTP 요청을 실행해 테스트해보는 방식 하나의 시스템만 실행해서 UI test에 비해 테스트 설정이나 실행 시간이 더 짧고 간단 하지만 unit test에 비해 자동화에 걸리는 공수가 더 크고 실행 속도도 더 느릴 수 밖에 없음 전체 테스트 중 대략 20% 정도만 할당하는 것을 추천 Unit test 시스템을 테스트한다는 개념보다는 코드를 직접 테스트하는 개념 즉, 코드로 코드를 테스트함 실행하기 쉬우며 실행 속도도 빠름 디버깅도 비교적 쉬움 함수 단위로 테스트해서 파악이 쉬울 수밖에 없음 단점은 함수 단위로 테스트하다보니 전체적인 부분을 테스트하기엔 제한적일 수 밖에 없음 이런 단점을 integration test와 UI test를 통해 보완 전체 테스트의 70%를 unit test pytest 파이썬 내장 라이브러리인 uniitest보다 사용하기 직관적인 pytest를 사용할 예정 설치 pip3 install pytest pytest에선 test_라고 되어있는 파일들만 테스트 파일로 인식하고, 함수도 test_라고 prefix가 있어야만 test할 함수로 인식함 예시 (test_multiply_by_two.py) def multiply_by_two(x): return x * 2 def test_multiply_by_two(): assert multiply_by_two(4) == 7 터미널에서 pytest를 입력해 실행하면 오류가 발생 미니터 API unit test 순서 test_endpoints.py에 unit test 코드 구현 테스트용 데이터베이스 생성 config.py test_db = { 'user': 'test', 'password': '1234', 'host': 'localhost', 'port': 3306, 'database': 'test_db' } test_config = { \"DB_URL\": f\"mysql+mysqlconnector://{test_db['user']}:{test_db['password']}@{test_db['host']}:{test_db['port']}/{test_db['database']}?charset=utf8\" } Flask는 unit test에서 엔드포인트들을 테스트할 수 있는 기능을 제공함(test_lcient) pytest.fixture 데코레이터를 사용하면 같은 이름의 함수의 리턴값을 해당 인자에 넣어줌 @pytest.fixture def api(): app = create_app(config.test_config) app.config['TESTING'] = True api = app.test_client() return api test ping def test_ping(api): resp = api.get(\"/ping\") assert b'pong' in resp.data test tweet tweet을 생성하기 위해선 사용자가 있어야 함 해당 사용자로 인증 절차를 거친 후, access token으로 tweet 엔드포인트를 호출해야 함 def test_tweet(api): new_user = { \"email\": \"zzsza@naver.com\", \"passowrd\": \"1234\", \"name\": \"변성윤\", \"profile\": \"test profile\" } resp = api.post( \"/sign-up\", data=json.dumps(new_user), content_type=\"application/json\" ) assert resp.status_code == 200 # get id fo the new uesr resp_json = json.loads(resp.data.decode(\"utf-8\")) new_user_id = resp_json[\"id\"] # login resp = api.post( \"/login\", data=json.dumps({\"email\": \"zzsza@naver.com\", \"password\": \"1234\"}), content_type=\"application/json\" ) resp_json = json.loads(resp.data.decode(\"utf-8\")) access_token = resp_json[\"access_token\"] # tweet resp = api.post( \"/tweet\", data=json.dumps({\"tweet\": \"Hello World\"}), content_type=\"application/json\", headers={\"Authorization\": access_token} ) assert resp.status_code == 200 # tweet check resp = api.get(f\"/timeline/{new_user_id}\") tweets = json.loads(resp.data.decode(\"utf-8\")) assert resp.status_code == 200 assert tweets == { \"user_id\": 1, \"timeline\" : [ { \"user_id\": 1, \"tweet\": \"Hello World\" } ] } test에서 사용된 데이터를 테스트 종료 후 삭제해줘야 다른 테스트에 영향을 끼치지 않음 pytest에선 setup_function과 teardown_function을 사용하면 됨 def setup_function(): ## Create a test user hashed_password = bcrypt.hashpw( b\"test password\", bcrypt.gensalt() ) new_users = [ { 'id' : 1, 'name' : '변성윤', 'email' : 'zzsza@naver.com', 'profile' : 'test profile', 'hashed_password' : hashed_password } ] database.execute(text(\"\"\" INSERT INTO users ( id, name, email, profile, hashed_password ) VALUES ( :id, :name, :email, :profile, :hashed_password ) \"\"\"), new_users) def teardown_function(): database.execute(text(\"SET FOREIGN_KEY_CHECKS=0\")) database.execute(text(\"TRUNCATE users\")) database.execute(text(\"TRUNCATE tweets\")) database.execute(text(\"TRUNCATE users_follow_list\")) database.execute(text(\"SET FOREIGN_KEY_CHECKS=1\")) Reference 깔끔한 파이썬 탄탄한 백엔드",
    "tags": "python development",
    "url": "/development/2019/03/05/unit-test-with-flask/"
  },{
    "title": "Flask에 인증(Auth) 붙이는 방법",
    "text": "Flask에 인증을 붙이는 방법(Flask with auth)를 작성한 글입니다 특히 사용자의 비밀번호 암호화에 대해 자세히 알아볼 예정 인증 많은 API에서 인증(Authentication)은 공통적으로 구현되는 엔드포인트들 중 하나 Private한 API나 Public한 API도 기본적 인증을 요구함 Private API 사용할 수 있는 사용자나 클라이언트를 제한 Public API 사용자나 클라이언트를 제한하진 않지만 사용 횟수 제한, 남용 방지, 사용자 통계 등의 이유로 인증을 대부분 필요로 함 인증은 사용자의 신원을 확인하는 절차로 일반적으로 웹사이트에서 사용자가 로그인해 아이디와 비번을 확인하는 절차를 이야기함 즉, 로그인 기능을 구현하는 것이 인증 엔드포인트임 로그인 기능 구현 절차 사용자 가입 절차를 진행해 사용자의 ID, 비밀번호를 생성해야 함 가입한 사용자의 ID, 비밀번호를 데이터베이스에 저장하고 사용자의 비밀번호는 암호화해서 저장 사용자가 로그인할 때 본인의 ID, 비밀번호를 입력 사용자가 입력한 비밀번호를 암화한 후, 암호화되어서 DB에 저장된 비밀번호와 비교 비밀번호가 일치하면 로그인 성공 로그인에 성공하면 API 서버는 access token을 프론트엔드 혹은 클라이언트에 전송 프론트엔드 서버는 로그인 성공 후 해당 사용자의 access token을 첨부해 request를 서버에 전송해 매번 로그인하지 않아도 되도록 함 사용자 비밀번호 암호화 사용자의 비밀번호를 암호화해서 저장해야 하는 이유 1) 외부 해킹 공격에 의해 데이터베이스가 노출되었을 경우를 대비 2) 내부 인력에 의해 데이터베이스가 노출되었을 경우에 대비 사용자의 비밀번호를 암화할 때는 단방향 해시 함수(one way hash function)가 일반적으로 쓰임 단방향 해시 함수는 복호화를 할 수 없는 알고리즘 단방향 해시 함수 구현 import hashlib m = hashlib.sha256() m.update(b\"test password\") m.hexdigest() bcrypt 암호 알고리즘 단방향 해시 암호 알고리즘도 충분히 해킹할 수 있음 대표적으로 rainbow attack 미리 해시 값들을 계산해놓은 테이블을 생성하고 해시 함수값을 역추적해서 본래 값을 찾음 이런 해시 함수의 취약점을 보완하기 위해 2가지 방법을 사용 1) salting 음식에 간을 맞추기 위해 소금을 더하듯 비밀번호에 추가적으로 랜덤 데이터를 더해 해시 값을 계산 기존 비밀번호 + 특정 스트링을 붙이고 해싱 진행 2) key stretching 단방향 해시 값을 계산하고 또 해시하고, 여러번 반복하는 방법 해시 알고리즘의 실행 속도가 너무 빠르기 때문에 이런 방식을 사용 2가지 방법을 구현한 해시 함수 중 널리 사용되는 것이 bcrypt 설치 pip3 install bcrypt 암호화 import bcrypt bcrypt.hashpw(b\"password\", bcrypt.gensalt()) bcrypt.hashpw(b\"password\", bcrypt.gensalt()).hex() Access Token HTTP는 stateless라 이전에 어떤 HTTP 통신들이 실행됬는지 알지 못함 이전에 인증되었는지 알지 못하게 됨 로그인 정보를 HTTP 요청에 첨부해서 보내야 API 서버에서 로그인된 상태를 처리 이런 로그인 정보를 담고있는 것이 access token access token을 생성하는 방법 대표적으로 쓰이는 기술이 JWT(JSON Web Token) JSON 데이터를 token으로 변환하는 방식 유저가 로그인 요청하면 API 서버가 인증 확인 후, access token을 떨굼 그 토큰을 쿠키 등에 저장했다가 요청할 때 사용 가짜 JWT를 전송할 경우 백엔드 API에서 자신이 생성한 JWT인지 아닌지 확인함 JWT 구조 xxxxx.yyyyy.zzzzz header : x 토큰 타입과 사용되는 해시 알고리즘을 지정 { \"alg\": \"HS256\", \"typ\": \"JWT\" } payload : y JWT를 통해 실제로 서버 간에 전송하고자 하는 데이터 HTTP 메세제의 body와 비슷 signature : z JWT가 원본 그대로라는 것을 확인할 떄 사용되는 부분 Base64URL 코드화된 header, payload, JWT Secret을 헤더에 지정된 암호 알고리즘으로 암호화해 전송 프론트엔드가 JWT를 백엔드 API로 전송하면 서버에서 JWT signature 부분을 복호화해 서버에서 생성했는지 확인 누구나 원본 데이터를 볼 수 있는 부분(Base64URL 코드화)이 라 민감한 데이터는 저장하지 않도록 해야 함 PyJWT 파이썬에서 JWT를 구현할 떄 사용할 수 있는 라이브러리 설치 pip3 install PyJWT 사용법 import jwt data_to_encode = {\"some\": \"payload\"} encryption_secret = \"secrete\" algorithm = \"HS256\" encoded = jwt.encode(data_to_encode, encryption_secre, algorithm=algorithm) jwt.decode(encoded, encryption_secret, algorithms=[algorithm]) 엔드포인트 구현 /sign-up 수정 : 암호화해서 비밀번호 저장 @app.route(\"/sign-up\", methods=[\"POST\"]) def sign_up(): new_user = request.json new_user['password'] = bcrypt.hashpw(new_user['password'].encode('UTF-8'), bcrypt.gensalt()) new_user_id = app.database.execute(text(\"\"\" INSERT INTO users ( name, email, profile, hashed_password ) VALUES ( :name, :email, :profile, :password ) \"\"\"), new_user).lastrowid row = app.database.execute(text(\"\"\" SELECT id, name, email, profile FROM users WHERE id = :user_id \"\"\"), { \"user_id\": new_user_id }).fetchone() create_user = { \"id\": row[\"id\"], \"name\": row[\"name\"], \"email\": row[\"email\"], \"profile\": row[\"profile\"] } if row else None return jsonify(create_user) 인증 엔드포인트 수정 HTTP Post request에 JSON 데이터로 사용자의 아이디와 비밀번호를 전송받아 데이터베이스에 저장된 사용자의 비밀번호와 동일한지 확인 @app.route(\"/login\", methods=[\"POST\"]) def login(): credential = request.json email = credential[\"email\"] password = credential[\"password\"] row = app.database.execute(text(\"\"\" SELECT id, hashed_password FROM users WHERE email = :email \"\"\"), {'email': email}).fetchone() if row and bcrypt.checkpw(password.encode(\"UTF-8\"), row[\"hashed_password\"].encode(\"UTF-8\")): user_id = row[\"id\"] payload = { \"user_id\": user_id, \"exp\": datetime.utcnow() + timedelta(seconds=60 * 60 * 24) } token = jwt.encode(payload, app.config[\"JWT_SECRET_KEY\"], \"HS256\") return jsonify({ \"access_token\" : token.decode(\"UTF-8\") }) else: return '', 401 인증 절차를 다른 엔드포인트에 적용하기 tweet, follow, unfollow 엔드포인트에 인증을 적용 공통적인 기능을 필요로 하는 경우 파이썬의 데코레이터를 사용 functools의 wraps를 사용해 데코레이터 생성 인증 decorator 함수는 전송된 HTTP 요청에서 Authorization 헤더 값을 읽어 JWT access token을 읽고 복호화해서 사용자 아이디를 읽음 =&gt; 로그인 여부 결정 인증 Decorator 함수 def login_required(f): @wraps(f) def decorated_function(*args, **kwargs): access_token = request.headers.get(\"Authorization\") if access_token is not None: try: payload = jwt.decode(access_token, current_app.config[\"JWT_SECRET_KEY\"], \"HS256\") except jwt.InvalidTokenError: payload = None if payload is None: return Response(status=401) user_id = payload[\"user_id\"] g.user_id = user_id g.user = get_user_info(user_id) if user_id else None else: return Response(status=401) return f(*args, **kwargs) return decorated_function 인증 Decorator 적용하기 def tweet, def follow, def unfollow 위에 아래처럼 명시하면 됨 @app.route(\"/tweet\", methods=[\"POST\"]) @login_required def tweet(): ~~~~ 확인 이제 먼저 로그인을 해야 함, 로그인하지 않고 아래 request를 날리면 오류 발생 http -v POST localhost:5000/tweet tweet=\"Hi!\" 액세스 토큰 생성 http -v POST localhost:5000/login email=zzsza@naver.com password=1234 access_token을 복사한 후 아래와 같이 요청 http -v POST localhost:5000/tweet tweet=\"Hi!\" \"Authorization:eyJ0e~~~~~~~~~\" 기존엔 tweet할 때 id도 붙여야 했는데, 이젠 인증으로 처리해서 id도 필요 없음. 코드 살짝 수정 @app.route(\"/tweet\", methods=[\"POST\"]) @login_required def tweet(): user_tweet = request.json user_tweet[\"id\"] = g.user_id # 이 부분 tweet = user_tweet[\"tweet\"] if len(tweet) &gt; 300: return \"300자를 초과했습니다\", 400 app.database.execute(text(\"\"\" INSERT INTO tweets ( user_id, tweet ) VALUES ( :id, :tweet ) \"\"\"), user_tweet) return \"\", 200 Reference 깔끔한 파이썬 탄탄한 백엔드",
    "tags": "web python development",
    "url": "/development/2019/03/04/auth-with-flask/"
  },{
    "title": "Hash 함수를 사용한 AB Test Sampling",
    "text": "Hash 함수를 사용한 AB Test Sampling에 대한 글입니다 AB Test AB Test는 통계적 가설 검정의 한 형태로, 단일 변수에 대한 두 가지 버전을 비교하는 방법 A와 B는 사용자의 행동에 영향을 미칠 수 있는 하나의 변형을 제외하면 동일 Push 메세지를 보낼 때 메세지의 워딩을 다르게 하는 것도 AB Test의 일종으로 볼 수 있고, 웹페이지에서 배너의 위치에 따른 클릭률을 비교하는 것도 AB Test로 볼 수 있음 참고 자료 강규영님의 A/B 테스팅이란 : AB 테스팅의 단점 언급 1) 테스트를 많이 할 경우 단기적으로 손해 발생 가능 2) AB Test 결과는 시간의 흐름에 따라 바뀔 수 있음 3) AB Test만 해선 Local minima에 머물게 될 위험이 있음 박장시님의 A/B 테스트에서 p-value에 휘둘리지 않기 위키피디아 AB 테스트 AB Test Sampling 보통 실무에서 AB Test를 할 경우 user_id를 가지고 하곤 함 예 : 1의 자리가 홀수인 그룹 A, 1의 자리가 짝수인 그룹 B 단, 그룹을 나누기 전에 사용자들의 분포를 확인해야 함 이런 방법은 실험이 많지 않을 경우엔 유용할 수 있지만, 다른 실험을 또 설계할 경우에 이슈가 발생 예 : 그럼 이번엔 10의 자리가 홀수인 그룹을 A, 10의 자리가 짝수인 그룹을 B! 이렇게 계속 만들다 보면 어느덧 user_id의 자리수를 모두 채우고, 점점 조건이 애매할 수 있음 그래서 생각한 방법이 Hash 함수를 사용해 user_id를 숫자로 만들고, 해당 숫자를 어떤 기준점 기준으로 A와 B로 나누는 방법 Hash 함수 해시 함수는 특정 길이의 데이터를 고정된 길이의 데이터로 매핑하는 함수 암호학적 해시 함수와 비암호학적 해시함수로 구분 암호학적 해시 함수 : MD5, SHA계열 비암호학적 해시 함수 : CRC32 Salt 솔트는 단방향 해시 함수에서 추가하는 임의의 문자열로, 원본 메세지(여기선 user_id) 뒤에 문자 열을 추가함 실험의 관리성을 위해 실험의 이름을 솔트로 설정 이런 함수를 사용할 때, 백엔드쪽과 분석쪽에서 같은 결과가 나는지 꼭 확인해야 함 백엔드에서 user_id -&gt; hash -&gt; A/B에 따라 다른 API 데이터가 쌓인 곳에서 user_id -&gt; hash -&gt; A/B로 파악 Hash 함수를 사용하면 균일하게 나눠질 것이라고 생각 코드 파이썬에서 user_id를 Hash하는 코드 sha1을 사용 Python def hash_user_id(user_id, salt, ratio_threshold): \"\"\" user_id(int) : user_id salt(str) : salt 문자열 ratio_threshold(float) : 기준 ratio. 예 : 0.5 \"\"\" id_and_salt = str(user_id) + salt h = hashlib.sha1() h.update(id_and_salt.encode('utf-8')) res_hash = h.hexdigest() total = pow(16, len(res_hash)) hash_to_int = int(res_hash, 16) ratio = hash_to_int / total if ratio &gt;= ratio_threshold: return 'A' else: return 'B' SQL에서 필요할 경우, 대부분 Hash 함수를 지원하기 때문에 쉽게 user_id를 Hash할 수 있음(없다면 UDF 만들어서 사용) Salt 설정 AB Test마다 일정한 규칙 안에서 salt를 설정 예: 날짜-이름의 규칙을 가진다고 하면 20190223-test 이런 형식 그리고 특정 문서에 꾸준히 기록! Reference 강규영님의 해시 기반 샘플링 Deterministic A/B tests via the hashing trick 김재석님의 A/B Testing에 대한 기초적인 정보들 버즈빌의 Sampling부터 Interpretation까지",
    "tags": "analytics data",
    "url": "/data/2019/02/23/ab-test-sampling-with-hash-function/"
  },{
    "title": "시계열 예측을 위한 Facebook Prophet 사용하기",
    "text": "페이스북이 만든 시계열 예측 라이브러리 Prophet 사용법에 대해 작성한 글입니다 Prophet은 Python, R로 사용할 수 있는데, 본 글에선 Python로 활용하는 방법에 대해서만 다룹니다 Prophet 페이스북이 만든 시계열 예측 라이브러리 통계적 지식이 없어도 직관적 파라미터를 통해 모형을 조정할 수 있음 일반적인 경우 기본값만 사용해도 높은 성능을 보여줌 내부가 어떻게 동작하는지 고민할 필요가 없음 (개인 의견) 사실 이게 개인으로선 아쉬운 점.. 내부 알고리즘을 공개하진 않고 Linear Model이다! 정도만 알려줌 Python, R로 사용 가능 현재 Version 0.4(2018.12.18)로 아직 1점대 미만이라 언제든 API가 변경되도 이상하지 않음 관련 자료 공식 홈페이지 Prophet 논문 영어라 읽기 힘들다면 모든이들을 위한 FACEBOOK PROPHET PAPER 쉬운 요약정리 참고하면 좋을 것 같음! 설치 pip3 install fbprophet Prophet 구성 요소 Growth, Seasonality, Holidays y(t)=g(t)+s(t)+h(t)+error ARIMA 같은 시계열 모델은 시간에 종속적인 구조를 가지는 반면 Prophet은 종속적이지 않고 Curve Fitting으로 문제를 해결 학습 속도가 빠르고, 빈 구간을 interpolate하지 않아도 됨 직관적으로 이해할 수 있는 파라미터를 통해 모형을 쉽게 조정 가능 Growth Linear Growth(+Change Point) Change Point는 자동으로 탐지 예측할 때는 특정 지점이 change point인지 여부를 확률적으로 결정 Non-Linear Growth(Logistic Growth) 자연적 상한성이 존재하는 경우, Capacity가 있음 Capacity는 시간에 따라 변할 수 있음 Seasonality 사용자들의 행동 양식으로 주기적으로 나타나는 패턴 방학, 휴가, 온도, 주말 등등 푸리에 급수(Fourier Series)를 이용해 패턴의 근사치를 찾음 공돌이의 수학정리노트, CT Fourier Series 나무위키 푸리에 해석 Holidays 주기성을 가지진 않지만 전체 추이에 큰 영향을 주는 이벤트가 존재 이벤트의 효과는 독립적이라 가정 이벤트 앞뒤로 window 범위를 지정해 해당 이벤트가 미치는 영향의 범위를 설정할 수 있음 Model Fitting Stan을 통해 모델을 학습 probabilistic programming language for statistical inference 2가지 방식 MAP (Maximuam A Posteriori) : Default, 속도가 빠름 MCMC (Markov Chain Monte Carlo) : 모형의 변동성을 더 자세히 살펴볼 수 있음 Analyst in the loop Modeling 통계적 지식이 없어도 직관적 파라미터를 통해 모형을 조정할 수 있음 일반적인 경우 기본값만 사용해도 높은 성능을 가능 내부가 어떻게 동작하는지 고민할 필요가 없음 요소 Capacities : 시계열 데이터 전체의 최대값 Change Points : 추세가 변화하는 시점 Holidays &amp; Seasonality : 추세에 영향을 미치는 시기적 요인 Smoothing : 각각의 요소들이 전체 추이에 미치는 영향의 정도 Prophet 사용법 1) 데이터를 Prophet에 맞도록 가공 필요한 컬럼은 ds, y 2개!(컬럼 이름을 맞춰야함) from fbprophet import Prophet import pandas as pd df = pd.read_csv(\"./data/example_wp_log_peyton_manning.csv\") 예측값의 상한과 하한을 제어해야 하면 cap, floor 컬럼에 값 지정 df['cap'] = 6 df['floor'] = 1.5 2) Prophet 객체 생성하고 Fit Prophet 객체는 1번만 Fit 할 수 있음. 만약 여러번 Fit 하고 싶으면 새로운 객체를 생성해야 함 m = Prophet() # Default growth='linear' m.fit(df) 상한과 하한을 설정할 경우엔 Prophet 객체를 생성할 때 growth=’logistic’ 추가 m = Prophet(growth='logistic') m.fit(df) 3) 미래 Dataframe 생성 future = m.make_future_dataframe(periods=365) future.tail() 1)에서 예측값의 상한이나 하한을 설정했으면 동일하게 지정 future['cap'] = 6 future['floor'] = 1.5 4) 예측 forecast = m.predict(future) forecast.tail() 5) forecast 결과 확인 yhat_lower, yhat_upper 같이 범위로 제공 forecast.tail() forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(60) 6) 시각화 forecast 시각화 fig1 = m.plot(forecast) 점선들이 하한선과 상한선 forecast component 시각화(Trend, Weakly, Yearly) fig2 = m.plot_components(forecast) Trend Change Points Prophet에선 기본적으로 트렌드가 변경되는 지점을 자동으로 감지해 트렌드를 예측함 감지하는 것을 사용자가 조절할 수 있음 Prophet 객체를 생성할 때 changepoint_range, changepoint_prior_scale, changepoints을 조절 1) changepoint_range 기본적으로 Prophet은 시계열 데이터의 80% 크기에서 잠재적으로 ChangePoint를 지정 90%만큼 ChangePoint로 지정하고 싶다면 아래와 같이 설정 m = Prophet(changepoint_range=0.9) 2) changepoint_prior_scale Change Point의 유연성을 조정하는 방법 오버피팅이 심하면 너무 유연한 그래프가 나와서 모든 값에 근접하고, 언더피팅일 경우 유연성이 부족 기본 값은 0.05 이 값을 늘리면 그래프가 유연해지고(=언더피팅 해결), 이 값을 줄이면 유연성이 감소(=오버피팅 해결) m = Prophet(changepoint_prior_scale=0.05) 3) changepoints(list) 잠재적으로 change point일 수 있는 날짜들 명시하지 않으면 잠재적인 changepoint가 자동으로 설정됨 m = Prophet(changepoints=['2019-02-04'. '2019-02-05']) 시각화 from fbprophet.plot import add_changepoints_to_plot fig = m.plot(forecast) a = add.changepoints_to_plot(fig.gca(), m, forecast) 빨간 점선 : ChangePoint 빨간 실선 : Trend Seasonality, Holiday Effects, And Regressors Modeling Holidays and Special Events 휴일이나 모델에 반영하고 싶은 이벤트가 있으면 Dataframe을 생성해 반영할 수 있음 이벤트는 과거 데이터와 미래 데이터가 모두 포함되어 있어야 함 주변 날짜를 포함시키기 위해 lower_window, upper_window를 사용해 업데이트의 영향을 조절 가능 예제는 Play Off 경기일과 SUperbowl 경기날을 Holiday로 설정 playoffs = pd.DataFrame({ 'holiday': 'playoff', 'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16', '2010-01-24', '2010-02-07', '2011-01-08', '2013-01-12', '2014-01-12', '2014-01-19', '2014-02-02', '2015-01-11', '2016-01-17', '2016-01-24', '2016-02-07']), 'lower_window': 0, 'upper_window': 1, }) superbowls = pd.DataFrame({ 'holiday': 'superbowl', 'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']), 'lower_window': 0, 'upper_window': 1, }) holidays = pd.concat((playoffs, superbowls)) 사용하는 방법은 간단, Prophet 객체를 생성할 때 holidays 인자로 넘기면 됨 m = Prophet(holidays=holidays) forecast = m.fit(df).predict(future) holiday effect를 아래 코드로 확인할 수 있음 forecast[(forecast['playoff'] + forecast['superbowl']).abs() &gt; 0][ ['ds', 'playoff', 'superbowl']][-10:] plot_component로 시각화할 경우 holidays의 영향도 볼 수 있음 fig = m.plot_components(forecast) 만약 holidays에 오버피팅된 것 같으면 holidays_prior_scale을 조정해 smooth하게 변경 가능(기본값은 10) m = Prophet(holidays=holidays, holidays_prior_scale=0.05).fit(df) forecast = m.predict(future) forecast[(forecast['playoff'] + forecast['superbowl']).abs() &gt; 0][ ['ds', 'playoff', 'superbowl']][-10:] Built-in Country Holidays Prophet 객체를 생성한 후, m.add_country_holidays(country_name=’US’) 이렇게 작성하면 국가의 휴일을 사용할 수 있음 그러나 한국은 없음 Github 참고해서 만들어도 될 듯, 혹은 커스텀 이벤트를 생성 Fourier Order for Seasonalities Seasonalities를 부분 푸리에의 합을 사용해 추정 이 부분은 논문에 자세히 나오고, 위키피디아도 참고하면 좋음 푸리에 급수는 주기함수를 삼각함수의 급수로 나타낸 것 yearly_seasonality 파라미터의 default는 10 만약 시즈널리티가 자주 발생한다고 생각하면 이 값을 20으로 수정하면 됨. 단, 오버피팅 조심! from fbprophet.plot import plot_yearly m = Prophet(yearly_seasonality=10).fit(df) a = plot_yearly(m) Specifying Custom Seasonalities 커스텀 시즈널리티를 생성할 수 있음 기본적으로 weekly, yearly 특성 제공 m.add_seasonality로 추가하며 인자는 name, period, fourier_order가 있음 m = Prophet(weekly_seasonality=False) m.add_seasonality(name='monthly', period=30.5, fourier_order=5) forecast = m.fit(df).predict(future) fig = m.plot_components(forecast) prior_scale을 조절해 강도를 조절할 수 있음 Additional regressors add_regressor 메소드를 사용해 모델의 linear 부분에 추가할 수 있음 예제에선 NFL 시즌의 일요일에 추가 효과를 더함 def nfl_sunday(ds): date = pd.to_datetime(ds) if date.weekday() == 6 and (date.month &gt; 8 or date.month &lt; 2): return 1 else: return 0 df['nfl_sunday'] = df['ds'].apply(nfl_sunday) m = Prophet() m.add_regressor('nfl_sunday') m.fit(df) future['nfl_sunday'] = future['ds'].apply(nfl_sunday) forecast = m.predict(future) fig = m.plot_components(forecast) Multiplicative Seasonality 단순한 seasonality가 아닌 점점 증가하는 seasonlity를 다룰 때 사용하면 좋은 기능 데이터가 엄청 많을 경우 유용할 듯 사용하는 방법은 매우 단순. seasonality_mode의 인자로 multiplicative 지정 m = Prophet(seasonality_mode='multiplicative') Uncertainty Intervals 불확실성의 범위가 나타나는 원인 1) Trend의 불확실성 2) Seasonality 추정의 불확실설 3) 추가 관찰되는 잡음 Uncertainty in the trend 예측을 하면 yhat_lower, yhat_upper가 나타나는데 이 범위도 사용자가 조절할 수 있음 interval_width의 기본 값은 80% changepoint_prior_scale을 조절하면 예측 불확실성이 증가함 forecast = Prophet(interval_width=0.95).fit(df).predict(future) Uncertainty in seasonality 시즈널리티의 불확실성을 알기 위해 베이지안 샘플링을 사용해야 함 mcmc.samples 파라미터를 사용. 이 값을 사용하면 최초 n일에 대해 적용한다는 뜻 m = Prophet(mcmc_samples=300) forecast = m.fit(df).predict(future) fig = m.plot_components(forecast) 이제 불확실성의 범위가 보임 Outliers 위와 같은 예측 그래프를 보면, 2016년부터 Uncertainty Intervals이 너무 큼 너무 튀는 값이 존재해서 예측할 때 영향이 미침 이런 값들은 제외하고 예측해야 함 NA, None로 설정 또는 상한선, 하한선 설정 df.loc[(df['ds'] &gt; '2010-01-01') &amp; (df['ds'] &lt; '2011-01-01'), 'y'] = None model = Prophet().fit(df) fig = model.plot(model.predict(future)) Sub-daily Data 여태까지 사례는 모두 Daily 데이터였는데, 더 짧은 단위도 예측할 수 있음 Timestamp로 YYYY-MM-DD HH:MM:SS 형태로 저장 사실 데이터 형태만 다른거고 여태와 동일함 Data with regular gaps 정기적인 gap이 있는 데이터도 예측할 수 있음 매달 1일의 데이터만 있어도 월별로 예측 가능(부정확성이 더 늘겠지만!) 기타 공식 문서는 Tutorial정도로만 충실하고, 추가되는 API에 대한 설명이 없음(=소스코드 까서 직접..) fbprophet의 diagnostics.py엔 아래 기능이 구현되어 있음 cross_validation performance_metrics mse, rmse, mae, mape, coverage 깔끔하고 간단하게 짜여있으니 참고해도 좋을듯 plot.py엔 아래 기능이 구현되어 있음 plot_yearly, plot_weekly, plot_seasonality, plot_cross_validation_metric 등 마무리 시계열 분석에 대해 잘 알지는 못하지만, 예측을 해야될 경우 사용하면 좋은 라이브러리입니다 다른 방식의 접근(예를 들면 카테고리컬로 regression)과 동시에 진행하고 Stacking해도 좋을 것 같아요! 구현된 코드는 시계열 예측할 때 자주 사용될 코드들이 있어서 구현 코드를 파악해도 좋을 것 같아요! 참고 코드 fengyin123의 Github AidanCooper의 Github harshitssj4의 Github 2017년 은(Silver) 가격 Backtesting한 Github Reference 공식 홈페이지 이민호님의 Slideshare",
    "tags": "time-series data",
    "url": "/data/2019/02/06/prophet/"
  },{
    "title": "Kubernetes StatefulSet를 사용해 MongoDB 실행하기",
    "text": "2019 클라우드 스터디 잼 입문반에서 진행하는 Kubernetes in the Google Cloud 퀵랩을 듣고 정리한 내용입니다 Basic Stateless Application의 특징 디스크에 중요한 데이터가 없음 필요한 만큼 여러 똑같은 컨테이너를 시작, 종료할 수 있음 컨테이너가 죽으면 보관중 데이터는 사라짐 Stateful Application의 특징 Container-specific 암호, 인증키, 설정값을 Instance에 개별 할당 Stateful 정의 Stateful means the computer or program keeps track of the state of interaction, usually by setting values in a storage field designated for that purpose 컴퓨터, 프로그램이 스토리지 필드에 값을 설정하고 상호 작용에 대한 상태를 지속적으로 기록 StatefulSet은 stateful한 애플리케이션을 관리하기 위해 사용하는 workload Pod 각각에 대해 sticky identity를 유지 StatefulSet의 복제본은 배포, 크기 조정, 업그레이드 및 종료에 대한 정상적이고 순차적인 접근 방식을 따름 StatefulSet을 사용하면 복제본이 다시 예약될 때 명명 규칙, 네트워크 이름 및 저장소가 그대로 유지 기본 설정 컴퓨터 영역 설정 gcloud config set compute/zone us-central1-f 새 클러스터 생성 3개의 노드(가상 머신)가 있는 새 클러스터 생성 gcloud container clusters create hello-world 저장소 등급 저장소 등급 설정 MongoDB 복제본 세트를 설정하기 위해 StorageClass, Headless Service, StatefulSet 설정 예제 구성 파일 복제 git clone https://github.com/thesandlord/mongo-k8s-sidecar.git cd ./mongo-k8s-sidecar/example/StatefulSet/ 저장소 등급 생성 저장소 등급 구성 확인(SSD와 하드디스크) cat googlecloud_ssd.yaml 저장소 등급 배포(kubectl apply 명령어 사용) kubectl apply -f googlecloud_ssd.yaml Headless Service 및 StatefulSet 배포 Headless Serice 구성 확인 clusterIP가 ‘None’으로 설정되어 있어서 Headless Service를 확인할 수 있음 StatefulSet과 함께 사용시 포등 ㅔ직접 액세스할 수 있는 DNS 주소를 제공받을 수 있음 cat mongo-statefulset.yaml StatefulSet 확인 cat mongo-statefulset.yaml apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: mongo spec: serviceName: \"mongo\" replicas: 3 template: metadata: labels: role: mongo environment: test spec: terminationGracePeriodSeconds: 10 containers: - name: mongo image: mongo command: - mongod - \"--replSet\" - rs0 - \"--smallfiles\" - \"--noprealloc\" ports: - containerPort: 27017 volumeMounts: - name: mongo-persistent-storage mountPath: /data/db - name: mongo-sidecar image: cvallance/mongo-k8s-sidecar env: - name: MONGO_SIDECAR_POD_LABELS value: \"role=mongo,environment=test\" volumeClaimTemplates: - metadata: name: mongo-persistent-storage annotations: volume.beta.kubernetes.io/storage-class: \"fast\" spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 100Gi StatefulSet 객체에 대한 설명, 메타데이터 부분은 레이블과 복제본의 수를 지정 spec 부분엔 포드 사양을 작성. name이 mongo와 mongo-sidecar가 존재하는데, 사이드카 컨테이너가 복제본 세트를 자동으로 구성 volumnClaimTemplates는 전에 생성한 StorageClass를 호출해 볼륨을 프로비저닝하고 MongoDB 복제본당 100GB 디스크를 프로비저닝함 배포 kubectl apply -f mongo-statefulset.yaml MongoDB 복제본 세트에 연결 아래 명령어를 출력해 DESIRED와 CURRENT 수가 동일한지 확인 kubectl get statefulset 클러스터 3개 포드 확인 kubectl get pods 첫 복제본 세트 멤버에 연결 kubectl exec -ti mongo-0 mongo MongoDB REPL에 연결됨. 기본 구성으로 복제본 세트 시작 rs.initiate() 복제본 세트 구성 인쇄 rs.conf() 종료 exit MongoDB 복제본 세트 확장 복제본을 3개에서 5개로 늘리기 kubectl scale --replicas=5 statefulset mongo 아래 명령어로 확인 kubectl get pods 5개에서 3개로 줄이기 kubectl scale --replicas=3 statefulset mongo 아래 명령어로 확인 kubectl get pods MongoDB 복제본 세트 활용 Headless Serivce가 지원하는 StatefulSet의 각 포드엔 안정된 DNS 이름이 존재. .의 형식 3개의 복제본 세트의 DNS 이름은 아래와 같음 mongo-0.mongo mongo-1.mongo mongo-2.mongo 연결 문자열 URI는 아래와 같음 \"mongodb://mongo-0.mongo,mongo-1.mongo,mongo-2.mongo:27017/dbname_?\" 삭제하기 StatefulSet 삭제 kubectl delete statefulset mongo Headless Service 삭제 kubectl delete svc mongo 볼륨 삭제 kubectl delete pvc -l role=mongo 클러스터 삭제 gcloud container clusters delete \"hello-world\" Reference Kubernetes in the Google Cloud Qwiklabs Kubernetes Tutorial Kubernetes StatefulSet 개요 &amp; Nginx Web Cluster(1/5) Azure cluster workloads",
    "tags": "kubernetes development",
    "url": "/development/2019/01/27/kubernetes-statefulset/"
  },{
    "title": "Kubeflow 소개 및 Intro",
    "text": "Kubernetes를 사용한 ML Workflow Reference Kubeflow Document",
    "tags": "kubeflow mlops",
    "url": "/mlops/2019/01/27/kubeflow-intro/"
  },{
    "title": "Kubernetes Engine을 사용한 배포 관리(with Jenkins)",
    "text": "2019 클라우드 스터디 잼 입문반에서 진행하는 Kubernetes in the Google Cloud 퀵랩을 듣고 정리한 내용입니다 Intro 지속적 배포, Blue Green 배포, 카나리 배포 같은 배포 시나리오에 대해 배울 예정 kubectl 도구 연습 배포 yaml 파일 생성 배포 실행, 업데이트, 확장 배포 및 배포 스타일 업데이트 연습 배포 서로 다른 2개 이상의 인프라 환경, 지역을 연결해 기술, 운영상의 요구를 해결 배포 세부 사항에 따라 하이브리드, 멀티 클라우드, 공개-비공개 배포라고도 부름 배포가 단일 환경, 지역으로 제한될 경우 발생하는 이슈 리소스 한도 도달 제한된 지리적 범위 단일 환경이면 멀리 떨어진 사용자들이 모두 하나의 배포에 액세스해야됨 제한된 가용성 제공업체 종속 경직된 리소스 일회성 배포가 아닌 절차를 가진 배포를 설계해야 함 반복 가능하고 프로비저닝, 구성, 유지보수에 있어 검증된 방식을 사용해야 함 시나리오는 대표적으로 멀티 클라우드 배포, 프론트엔드 내부 데이터, 지속적 통합/지속적 배포 프로세스 실습 샘플 코드 clone git clone https://github.com/googlecodelabs/orchestrate-with-kubernetes.git cd orchestrate-with-kubernetes/kubernetes 5개의 n1-standard-1 노드를 갖는 클러스터 생성 gcloud config set compute/zone us-central1-a gcloud container clusters create bootcamp --num-nodes 5 --scopes \"https://www.googleapis.com/auth/projecthosting,storage-rw\" 배포 객체 알아보기 배포 객체 살펴보기 kubectl explain deployment 모든 필드를 보려면 --recursive 옵션 추가 kubectl explain deployment --recursive 배포 객체의 구조와 개별 필드 이해 kubectl explain deployment.metadata.name 배포 만들기 image 변경 vi deployments/auth.yaml i containers: - name: auth image: kelseyhightower/auth:1.0.0 배포 객체 생성 kubectl create -f deployments/auth.yaml 배포 확인 kubectl get deployments 배포 복제본 생성 kubectl get replicasets 복제본 세트가 생성되면 단일 포드 생성 kubectl get pods 인증 배포 서비스 생성 kubectl create -f services/auth.yaml hello 배포 만들고 노출 kubectl create -f deployments/hello.yaml kubectl create -f services/hello.yaml fronted 배포 생성하고 노출 kubectl create secret generic tls-certs --from-file tls/ kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf kubectl create -f deployments/frontend.yaml kubectl create -f services/frontend.yaml 외부 IP를 선택한 후 curl로 상호 작용 kubectl get services frontend curl -ks https://&lt;EXTERNAL-IP&gt; kubectl의 출력 템플릿을 사용해 curl을 한 줄로 사용할 수 있음 curl -ks https://`kubectl get svc frontend -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"` 배포 확장 spec.replicas 필드를 업데이트해 확장 필드 설명 확인 kubectl explain deployment.spec.replicas kubectl scale 명령을 사용해 replicas 필드 업데이트 가능 kubectl scale deployment hello --replicas=5 실행 중인 인증 포드 5개인지 확인 kubectl get pods | grep hello- | wc -l 애플리케이션 축소 후 확인 kubectl scale deployment hello --replicas=3 kubectl get pods | grep hello- | wc -l 지속적 업데이트 이미지가 새 버전으로 업데이트 되도록 지원 배포가 새 버전으로 업데이트되면 새로운 복제본 세트가 생성되고 기존 복제본 세트의 복제본이 줄어들며 새 복제본 세트의 복제본 수가 서서히 늘어남 배포 업데이트 kubectl edit deployment hello image를 kelseyhightower/hello:2.0.0로 변경 편집기에서 저장하면 업데이트된 배포가 클러스터에 저장되고 쿠버네티스에서 지속적 업데이트 시작 새 복제본 세트 조회 kubectl get replicaset 롤아웃 내역에서 확인 kubectl rollout history deployment/hello 지속적 업데이트 일시중지 kubectl rollout pause deployment/hello 롤아웃 현재 상태 확인 kubectl rollout status deployment/hello 포드에서 확인 kubectl get pods -o jsonpath --template='{range .items[*]}{.metadata.name}{\"\\t\"}{\"\\t\"}{.spec.containers[0].image}{\"\\n\"}{end}' 지속적 업데이트 재개 롤아웃이 일시중지되면 일부만 새 버전이고 나머지는 기존 버전으로 유지(resume 명령 사용) kubectl rollout resume deployment/hello 상태 확인 kubectl rollout status deployment/hello 업데이트 롤백 새 버전에서 버그가 발견된 경우 새로운 포드에 연결된 사용자들이 문제를 겪음 이전 버전으로 롤백해 조사하고 다시 수정된 버전을 릴리즈 rollout 명령을 사용해 롤백 kubectl rollout undo deployment/hello 롤백 확인 kubectl rollout history deployment/hello 모든 포드가 이전 버전으로 롤백되었는지 확인 kubectl get pods -o jsonpath --template='{range .items[*]}{.metadata.name}{\"\\t\"}{\"\\t\"}{.spec.containers[0].image}{\"\\n\"}{end}' 카나리 배포(Canary Releases) 설명 글 사용자 하위 집합을 사용해 프러덕션의 새 배포를 테스트를 사용하고싶은 경우 선택하는 방법 소수의 사용자에 변경 사항을 릴리즈 카나리 배포 만들기 새 버전의 별도 배포와 안정적인 일반 배포 및 카나리 배포로 이루어짐 새 버전에 해당하는 카나리 배포 생성 cat deployments/hello-canary.yaml kubectl create -f deployments/hello-canary.yaml 확인 kubectl get deployments curl curl -ks https://`kubectl get svc frontend -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`/version Blue-Green 배포 지속적 업데이트는 최소한 오버헤드, 성능 영향, 다운타임으로 천천히 배포할 수 있어서 이상적 완전히 배포된 후 새 버전을 가리키도록 부하 분산기를 수정하는 편이 유용할 경우가 존재하는데, 이 경우 Blue-Green 배포 사용 기존 Blue 버전과 새로운 Green 버전의 배포를 각각 하나씩 생성해 구현 새로운 Green 버전이 실행되면 서비스를 업데이트해 이 버전을 사용하도록 전환 단점 클러스터의 리소스가 2배 이상 필요 서비스 업데이트 kubectl apply -f services/hello-blue.yaml Green 배포 버전 라벨 및 이미지 경로 업데이트 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hello-green spec: replicas: 3 template: metadata: labels: app: hello track: stable version: 2.0.0 spec: containers: - name: hello image: kelseyhightower/hello:2.0.0 ports: - name: http containerPort: 80 - name: health containerPort: 81 resources: limits: cpu: 0.2 memory: 10Mi livenessProbe: httpGet: path: /healthz port: 81 scheme: HTTP initialDelaySeconds: 5 periodSeconds: 15 timeoutSeconds: 5 readinessProbe: httpGet: path: /readiness port: 81 scheme: HTTP initialDelaySeconds: 5 timeoutSeconds: 1 Green 배포 생성 kubectl create -f deployments/hello-green.yaml 현재 버전 사용하는지 확인 curl -ks https://`kubectl get svc frontend -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`/version 서비스 업데이트 kubectl apply -f services/hello-green.yaml 서비스가 업데이트되는 즉시 Green 배포가 사용됨 curl -ks https://`kubectl get svc frontend -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`/version Blue-Green 롤백 기존 버전으로 업데이트 kubectl apply -f services/hello-blue.yaml 서비스를 업데이트하면 롤백 성공 curl -ks https://`kubectl get svc frontend -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\"`/version Kubernetes Engine에서 Jenkins를 사용한 지속적 통합(Conitnuous Delivery) 젠킨스를 사용해 지속적 통합을 진행 빌드, 테스트, 배포의 파이프라인을 통합 미리 선행해야 하는 일 쿠버네티스 엔진 클러스터에 젠킨스 Provision 젠킨스에 Helm Package Manager 설정 Feature들 탐험 젠킨스 파이프라인 생성하고 테스트 Kubernetes Engine GCP의 Managed Kbernetes Jenkins 빌드, 테스트, 배포 파이프라인을 유연하게 조절할 수 있는 도구 프로젝트를 신속하게 반복할 수 있도록 지원 Repo 준비 gcloud config set compute/zone us-central1-f git clone https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.git cd continuous-deployment-on-kubernetes Provisioning Jenkins Kubernetes 클러스터 생성 gcloud container clusters create jenkins-cd \\ --num-nodes 2 \\ --machine-type n1-standard-2 \\ --scopes \"https://www.googleapis.com/auth/projecthosting,cloud-platform\" 완료된 Task 테스트 gcloud container clusters list gcloud container clusters get-credentials jenkins-cd kubectl cluster-info &gt;&gt;&gt; is running at ~ Helm 설치 wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz tar zxfv helm-v2.9.1-linux-amd64.tar.gz cp linux-amd64/helm . # permission kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account) # cluster-admin role kubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-admin-binding --clusterrole=cluster-admin --serviceaccount=kube-system:tiller ./helm init --service-account=tiller ./helm update # version 확인 ./helm version 젠킨스 설정 및 설치 설정 Deploy ./helm install -n cd stable/jenkins -f jenkins/values.yaml --version 0.16.6 --wait 젠킨스 pod 확인 kubectl get pods 젠킨스 UI에서 클라우드 쉘로 포워딩 설정 export POD_NAME=$(kubectl get pods -l \"component=cd-jenkins-master\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl port-forward $POD_NAME 8080:8080 &gt;&gt; /dev/null &amp; 젠킨스 서비스 확인 kubectl get svc &gt;&gt;&gt; cd-jenkins ClusterIP 10.11.255.161 &lt;none&gt; 8080/TCP 4m cd-jenkins-agent ClusterIP 10.11.251.179 &lt;none&gt; 50000/TCP 4m kubernetes ClusterIP 10.11.240.1 &lt;none&gt; 443/TCP 27m 젠킨스 마스터가 요청할 때 빌더 노드가 필요할 때 자동으로 실행되도록 쿠버네티스 플러그인을 사용 작업이 완료되면 자동으로 해제되고 리소스가 클러스터 리소스 풀에 다시 추가됨 젠킨스 연결 실행 printf $(kubectl get secret cd-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo 포트 8080 생성 ID : admin, PW : 위에서 나온 것을 복붙 배포 Production : Live Canary : samller-capacity site cd sample-app kubectl create ns production kubectl apply -f k8s/production -n production kubectl apply -f k8s/canary -n production kubectl apply -f k8s/services -n production Production 환경 scale up kubectl scale deployment gceme-frontend-production -n production --replicas 4 fronted 설정 kubectl get pods -n production -l app=gceme -l role=frontend backend 설정 kubectl get pods -n production -l app=gceme -l role=backend 외부 IP 노출 kubectl get service gceme-frontend -n production Export export FRONTEND_SERVICE_IP=$(kubectl get -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" --namespace=production services gceme-frontend) Curl curl http://$FRONTEND_SERVICE_IP/version 젠킨스 파이프라인 생성 gceme 복사하고 Cloud Source Repository에 Push gcloud alpha source repos create default Init git init git config credential.helper gcloud.sh git remote add origin https://source.developers.google.com/p/$DEVSHELL_PROJECT_ID/r/default config git config --global user.email \"[EMAIL_ADDRESS]\" git config --global user.name \"[USERNAME]\" add, commit, push git add . git commit -m \"Initial commit\" git push origin master Reference Kubernetes in the Google Cloud Qwiklabs",
    "tags": "kubernetes development",
    "url": "/development/2019/01/20/kubernetes-engine-deployment/"
  },{
    "title": "MLflow 소개 및 Tutorial",
    "text": "머신러닝 라이프 사이클을 관리할 수 있는 오픈소스인 MLflow에 대한 소개 및 간단한 Tutorial에 대한 글입니다 MLflow MLflow는 End to End로 머신러닝 라이프 사이클을 관리할 수 있는 오픈소스 데이터브릭스에서 만듬 데이터브릭스 hosted version 주요 기능 1) MLflow Tracking 파라미터와 결과를 비교하기 위해 실험 결과를 저장 2) MLflow Projects 머신러닝 코드를 재사용 가능하고 재현 가능한 형태로 포장 포장된 형태를 다른 데이터 사이언티스트가 사용하거나 프러덕션에 반영 3) MLflow Models 다양한 ML 라이브러리에서 모델을 관리하고 배포, Serving, 추론 REST API, CLI를 통해 모든 기능에 액세스 할 수 있기 때문에 모든 라이브러리, 프로그래밍 언어에서 사용 가능 API는 Python, R, Java 존재 설치 및 예제 코드 준비 virtualenv 설정 virtualenv env source env/bin/activate 설치 pip3 install mlflow # virtualenv 없이 pip3 install mlflow하니 [Errno 13] Permission denied: '/usr/local/man' 발생 예제 코드 clone git clone https://github.com/mlflow/mlflow cd mlflow/examples Tracking Tracking Document Code Version Git commit hash used to execute the run, if it was executed from an MLflow Project. Start &amp; End Time Start and end time of the run Source Name of the file executed to launch the run, or the project name and entry point for the run if the run was executed from an MLflow Project. Parameters Key-value input parameters of your choice. Both keys and values are strings. Metrics Key-value metrics where the value is numeric. Each metric can be updated throughout the course of the run (for example, to track how your model’s loss function is converging), and MLflow will record and let you visualize the metric’s full history. Artifacts Output files in any format. For example, you can record images (for example, PNGs), models (for example, a pickled scikit-learn model), or even data files (for example, a Parquet file) as artifacts. 예제 코드 # mlflow_tracking.py import os from random import random, randint from mlflow import log_metric, log_param, log_artifacts if __name__ == \"__main__\": print(\"Running mlflow_tracking.py\") log_param(\"param1\", randint(0, 100)) log_metric(\"foo\", random()) log_metric(\"foo\", random() + 1) log_metric(\"foo\", random() + 2) if not os.path.exists(\"outputs\"): os.makedirs(\"outputs\") with open(\"outputs/test.txt\", \"w\") as f: f.write(\"hello world!\") log_artifacts(\"outputs\") 실행 python3 mlflow_tracking.py &gt;&gt;&gt; Running mlflow_tracking.py 웹 UI 실행 기본적으로 프로그램을 실행할 때마다 tracing API가 mlruns 디렉토리에 파일을 작성. 그 후 웹 UI 실행 가능 mlflow ui MLflow Project 실행 MLflow Projects Document default 설정은 디펜던시가 conda에 있음 conda를 사용하고 싶지 않을 경우 --no-conda 추가 mlflow run --no-conda git@github.com:mlflow/mlflow-example.git -P alpha=5 &gt;&gt;&gt; Elasticnet model (alpha=5.000000, l1_ratio=0.100000): RMSE: 0.8594260117338262 MAE: 0.6480675144220314 R2: 0.046025292604596424 mlflow run tutorial -P alpha=0.5 Experiments list 확인 mlflow experiments list Saving and Serving Model MLflow Models Document 학습 python3 sklearn_logistic_regression/train.py # train.py import numpy as np from sklearn.linear_model import LogisticRegression import mlflow import mlflow.sklearn if __name__ == \"__main__\": X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1) y = np.array([0, 0, 1, 1, 1, 0]) lr = LogisticRegression() lr.fit(X, y) score = lr.score(X, y) print(\"Score: %s\" % score) mlflow.log_metric(\"score\", score) mlflow.sklearn.log_model(lr, \"model\") print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid) Serving train.py를 실행하면 RUN ID가 나오는데, 그걸 토대로 아래처럼 serving mlflow pyfunc serve -r &lt;RUN_ID&gt; -m model --no-conda --port 1234 curl curl -d '{\"columns\":[\"x\"], \"data\":[[1], [-1]]}' -H 'Content-Type: application/json; format=pandas-split' -X POST localhost:1234/invocations Tutorial linear regression 모델 학습 재사용, 재생산 가능하도록 패키징 간단한 HTTP 서버에 배포 모델 학습 examples/sklearn_elasticnet_wine/train.py &lt;alpha&gt; &lt;l1_ratio&gt; 여러번 테스트 import os import sys import pandas as pd import numpy as np from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import ElasticNet import mlflow import mlflow.sklearn # Run from the root of MLflow # Read the wine-quality csv file wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"wine-quality.csv\") data = pd.read_csv(wine_path) # Split the data into training and test sets. (0.75, 0.25) split. train, test = train_test_split(data) # The predicted column is \"quality\" which is a scalar from [3, 9] train_x = train.drop([\"quality\"], axis=1) test_x = test.drop([\"quality\"], axis=1) train_y = train[[\"quality\"]] test_y = test[[\"quality\"]] alpha = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 0.5 l1_ratio = float(sys.argv[2]) if len(sys.argv) &gt; 2 else 0.5 with mlflow.start_run(): lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42) lr.fit(train_x, train_y) predicted_qualities = lr.predict(test_x) (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities) print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio)) print(\" RMSE: %s\" % rmse) print(\" MAE: %s\" % mae) print(\" R2: %s\" % r2) mlflow.log_param(\"alpha\", alpha) mlflow.log_param(\"l1_ratio\", l1_ratio) mlflow.log_metric(\"rmse\", rmse) mlflow.log_metric(\"r2\", r2) mlflow.log_metric(\"mae\", mae) mlflow.sklearn.log_model(lr, \"model\") 웹 서버 접속 mlflow ui 웹 서버 탐색 학습 코드 패키징 MLproject란 파일 생성해서 아래와 같이 설정 name: tutorial conda_env: conda.yaml entry_points: main: parameters: alpha: float l1_ratio: {type: float, default: 0.1} command: \"python train.py {alpha} {l1_ratio}\" conda.yaml 작성 (pip로 할 경우 사용하지 못하는가?) name: tutorial channels: - defaults dependencies: - numpy=1.14.3 - pandas=0.22.0 - scikit-learn=0.19.1 - pip: - mlflow mlflow run tutorial -P alpha=0.42으로 실행 만약 Github에서 올라간 소스를 바로 실행하고 싶으면 아래와 같이 사용 가능 mlflow run git@github.com:mlflow/mlflow-example.git -P alpha=0.42 Serving the Model REST API를 통한 실시간 서비스 또는 Apache Spark의 배치 inference 같은 다양한 도구에서 사용할 수 있는 형식이 존재 예제 코드에선 선형회귀 모델을 학습한 후 MLflow 함수가 모델을 아티팩트로 저장함 mlflow.sklearn.log_model(lr, \"model\") 웹 UI에서 확인하면 설정 확인 가능 아래 명령어를 통해 serving mlflow pyfunc serve /Users/mlflow/mlflow-prototype/mlruns/0/7c1a0d5c42844dcdb8f5191146925174/artifacts/model -p 1234 API Test curl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data '{\"columns\":[\"alcohol\", \"chlorides\", \"citric acid\", \"density\", \"fixed acidity\", \"free sulfur dioxide\", \"pH\", \"residual sugar\", \"sulphates\", \"total sulfur dioxide\", \"volatile acidity\"],\"data\":[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]}' http://127.0.0.1:1234/invocations &gt;&gt;&gt; {\"predictions\": [6.379428821398614]} 클라우드에서 배포 AzureML mlflow azureml export -m &lt;model path&gt; -o test-output SageMaker mlflow sagemaker build-and-push-container mlflow sagemaker deploy &lt;parameters&gt; Reference MLflow Document 이동진님의 mlflow",
    "tags": "mlflow mlops",
    "url": "/mlops/2019/01/16/mlflow-basic/"
  },{
    "title": "Kubernetes 개요 및 애플리케이션 배포",
    "text": "2019 클라우드 스터디 잼 입문반에서 진행하는 Kubernetes in the Google Cloud 퀵랩을 듣고 정리한 내용입니다 Introduction to docker Dokcer를 사용하면 인프라에서 어플리케이션을 분리하고 인프라를 managed 어플리케이션처럼 사용할 수 있음 Docker는 코드를 신속하게 제공하고 테스트, 배포 속도를 높이고 코드 작성과 실행 사이의 주기를 단축해줌 Dokcer 컨테이너는 쿠버네티스에서 쉽게 사용할 수 있음 Docker의 핵심을 배우며 쿠버네티스 및 컨테이너 응용 프로그램을 개발할 수 있음 Hello World hello-world docker run! docker run hello-world docker 이미지 출력 docker images docker running 컨테이너 보기 docker ps &gt;&gt;&gt; 처음엔 아무것도 뜨지 않음 모든 docker 이미지를 보려면 -a 추가 docker ps -a docker run –name [container-name] hello-world container 이름을 앞의 글자 3개만 사용해도 됨! Build test 폴더 생성 mkdir test &amp;&amp; cd test Dockerfile 생성 더 궁금할 경우 Docker command reference 참고 cat &gt; Dockerfile &lt;&lt;EOF # Use an official Node runtime as the parent image FROM node:6 # 이미지 지정 # Set the working directory in the container to /app WORKDIR /app # 컨테이너 작업 디렉토리 설정 # Copy the current directory contents into the container at /app ADD . /app # 현재 디렉토리 내용을 컨테이너에 추가 # Make the container's port 80 available to the outside world EXPOSE 80 # 80 포트를 연결을 허용 # Run app.js using node when the container launches CMD [\"node\", \"app.js\"] # node 명령을 실행해 프로그램 시작 EOF app.js 생성 cat &gt; app.js &lt;&lt;EOF const http = require('http'); const hostname = '0.0.0.0'; const port = 80; const server = http.createServer((req, res) =&gt; { res.statusCode = 200; res.setHeader('Content-Type', 'text/plain'); res.end('Hello World\\n'); }); server.listen(port, hostname, () =&gt; { console.log('Server running at http://%s:%s/', hostname, port); }); process.on('SIGINT', function() { console.log('Caught interrupt signal and will exit'); process.exit(); }); EOF build docker build -t node-app:0.1 . # Dockerfile이 있는 디렉토리에서 명령어 실행 # -t는 이미지에 태그, name:tag docker images 확인 Run Run docker run -p 4000:80 --name my-app node-app:0.1 다른 터미널에서 아래 명령어 입력 curl http://localhost:4000 &gt;&gt;&gt; Hello World docker stop and remove docker stop my-app &amp;&amp; docker rm my-app 다시 run docker run -p 4000:80 --name my-app -d node-app:0.1 docker ps docker logs docker logs [container_id] 호스트의 8080 포트를 컨테이너의 80포트로 연결 docker run -p 8080:80 --name my-app-2 -d node-app:0.2 docker ps Debug 작동중 container 로그 출력 docker logs -f [container_id] Interactive Bash session을 시작해 container로 진입 docker exec -it [container_id] bash inspect metadata 검사 docker inspect [container_id] --format을 사용하면 특정 필드를 json타입으로 리턴 docker inspect --format='' [container_id] Publish 자신의 이미지를 Google Container Registry에 push! gcr이 호스팅하는 비공개 레지스트리에 이미지를 푸시 형식 : [hostname]/[project-id]/[image]:[tag] Project ID 확인 gcloud config list project docker tag docker tag node-app:0.2 gcr.io/[project-id]/node-app:0.2 push gcloud docker -- push gcr.io/[project-id]/node-app:0.2 모든 컨테이너 stop and remove docker stop $(docker ps -q) docker rm $(docker ps -aq) 컨테이너 이미지 삭제 docker rmi node-app:0.2 gcr.io/[project-id]/node-app node-app:0.1 docker rmi node:6 docker rmi $(docker images -aq) # remove remaining images docker images gcloud docker pull gcloud docker -- pull gcr.io/[project-id]/node-app:0.2 Hello Node Kubernetes 쿠버네티스 Notebook, 멀티노드 클러스터, 클라우드에서 내부 배포할 수 있고 다양한 환경에서 실행할 수 있는 오픈소스 Kubernetes Engine(Google의 Managed Service)를 사용해 인프라 설정보다 쿠버네티스를 경험하는 것에 집중 kubectl document Node.js 애플리케이션 만들기 쿠버네티스 엔진에 배포할 애플리케이션 작성 server.js var http = require('http'); var handleRequest = function(request, response) { response.writeHead(200); response.end(\"Hello World!\"); } var www = http.createServer(handleRequest); www.listen(8080); node server.js 이걸 docker 컨테이너로 패키징 vi Dockerfile FROM node:6.9.2 # docker 허브의 node 이미지로 시작 EXPOSE 8080 # 포트 8080 노출 COPY server.js . # 사용자가 만든 server.js를 이미지로 복사 CMD node server.js # node 서버 시작 이미지 빌드 docker build -t gcr.io/PROJECT_ID/hello-node:v1 . Docker 컨테이너 이미지에서 포트 8080로 데몬 실행 Docker run -d : Detached mode docker run -d -p 8080:8080 gcr.io/PROJECT_ID/hello-node:v1 Docker 컨테이너 중지 docker ps에 나오는 컨테이너 중지 Google Container Registry로 이미지 Push Container Registry는 Google Cloud 프로젝트에서 액세스 가능 gcloud docker -- push gcr.io/PROJECT_ID/hello-node:v1 이제 쿠버네티스가 접근할 수 있는 도커 이미지 만들어짐 클러스터 만들기 Container Engine 클러스터 생성 하나의 클러스터는 쿠버네티스 마스터 API 서버와 노드로 구성, 노드는 Computer Engine 가상 머신 Kubernetes Engine에 접속 gcloud container clusters create hello-world \\ --num-nodes 2 \\ --machine-type n1-standard-1 \\ --zone us-central1-f 포드 만들기 포드는 관리 및 네트워킹 용도로 서로 연결된 컨테이너 그룹 포드는 하나 또는 여러 개의 컨테이너를 포함할 수 있음 kubectl run hello-node \\ --image=gcr.io/PROJECT_ID/hello-node:v1 \\ --port=8080 deployment “hello-node” created란 문구가 나오면 배포 개체를 만든 것! 포드를 만들고 확장할 땐 배포를 사용하는게 좋음 배포 확인(마치 docker ps 같음) kubectl get deployments 배포 만들어진 포드 확인 kubectl get pods 클러스터 정보 확인 kubectl cluster-info 클러스터 config 확인 kubectl config view pod 로그 확인 kubectl get events kubectl logs &lt;pod name&gt; 외부 트래픽 허용 Pod는 클러스터에 포함된 내부 IP로만 액세스 가능 가상 네트워크 외부에서 컨테이너에 액세스 하려면 포드를 쿠버네티스 서비스로 노출시켜야 함 kubectl expose 명령에 –type=”LoadBalancer” 플래그를 사용! kubectl expose deployment hello-node --type=\"LoadBalancer\" 직접 노출시키는 것이 아닌 배포를 노출시킴!!!! 서비스가 이 배포에서 관리하는 모든 포드에 걸쳐 트래픽의 부하를 분산 IP 주소 확인 외부 부하 분산 IP 1개 보임 kubectl get services 서비스 규모 확장하기 쿠버네티스의 강력한 기능은 애플리케이션을 간편하게 확장할 수 있는 것 갑자기 용량이 필요해졌을 경우 복제 컨트롤러에 새로운 복제를 관리하라고 명령할 수 있음 kubectl scale deployment hello-node --replicas=4 배포에 관한 설명 확인 kubectl get deployment kubectl get pods 서비스의 업그레이드 롤아웃 배포한 애플리케이션에 버그 수정이나 추가 기능을 적용해야 할 경우 server.js 하단에 response.end(“Hello Kubernetes World!”); 추가 빌드 및 push docker build -t gcr.io/PROJECT_ID/hello-node:v2 . gcloud docker -- push gcr.io/PROJECT_ID/hello-node:v2 실행 중인 컨테이너의 이미지 라벨을 기존 hello-node-deployment를 수정해야 함 gcr.io/PROJECT_ID/hello-node:v1 =&gt; gcr.io/PROJECT_ID/hello-node:v2 이를 위해 kubectl edit 명령 사용 kubectl edit deployment hello-node containers의 image를 수정 새로운 이미지 배포 kubectl get deployments 그래픽 대시보드 kubernetes 클러스터 대시보드에 액세스 하려면 아래 명령어 실행 gcloud container clusters get-credentials hello-world \\ --zone us-central1-f --project &lt;PROJECT_ID&gt; kubectl proxy --port 8081 URL에서 ?authuser=0을 삭제하고 끝이 /ui를 붙임 근데 대시보드 안나오는데요..? Kubernetes를 통한 클라우드 조정 kubectl을 사용해 Docker 컨테이너를 배포하고 관리 하나의 애플리케이션을 마이크로 서비스로 나눔 영역 설정 gcloud config set compute/zone us-central1-b 클러스터 설정(시간이 좀 소요됨) gcloud container clusters create io 코드 clone git clone https://github.com/googlecodelabs/orchestrate-with-kubernetes.git cd orchestrate-with-kubernetes/kubernetes 쿠버네티스 데모 Kubernetes 시작하는 가장 쉬운 방법 : kubectl run ngninx 컨테이너 인스턴스 실행 kubectl run nginx --image=nginx:1.10.0 실행 중인 nginx 컨테이너 보기 kubectl get pods Kubernetes 외부로 노출 공개 IP 주소로 도달한 클라이언트는 서비스 뒤에 있는 포드로 라우팅 kubectl expose deployment nginx --port 80 --type LoadBalancer 서비스 나열 kubectl get services curl http://:80 로 확인 Pod 쿠버네티스의 핵심 요소 하나 이상의 컨테이너가 포함된 집합 종속도가 높은 여러 컨테이너가 있을 경우 컨테이너들은 단일 포드로 패키징 볼륨 포드가 활성화되어 있어야 사용 가능한 데이터 디스크 포드는 컨텐츠에 대한 공유 네임스페이스를 제공 예제 포드에 속한 두 컨테이너가 서로 통신할 수 있으며 연결된 볼륨도 공유함 포드는 네트워크 네임스페이스도 공유 포드마다 IP 주소 하나씩 존재 포드 만들기 pods/monolith.yaml 구성 파일 출력 apiVersion: v1 kind: Pod metadata: name: monolith labels: app: monolith spec: containers: - name: monolith image: kelseyhightower/monolith:1.0.0 args: - \"-http=0.0.0.0:80\" - \"-health=0.0.0.0:81\" - \"-secret=secret\" ports: - name: http containerPort: 80 - name: health containerPort: 81 resources: limits: cpu: 0.2 memory: \"10Mi\" 포드는 하나의 컨테이너로 구성 시작할 때 몇 개의 인수를 컨테이너에 전달 http 트래픽을 위해 포트 80을 개방 kubectl을 사용해 모놀리식 포드 생성 kubectl create -f pods/monolith.yaml kubectl get pods를 사용해 포드 나열 모놀리식 포드 정보 요약 kubectl describe pods monolith 포드 상호 작용 포드는 기본적으로 비공개 IP 주소가 할당되고 클러스터 외부에 도달할 수 없음 kubectl port-forward 명령을 사용해 로컬 포트를 모놀리식 포드의 포트로 매핑 (새 터미널에서) 포트 전달 kubectl port-forward monolith 10080:80 통신 확인 curl http://127.0.0.1:10080 로그인 curl -u user http://127.0.0.1:10080/login 비밀번호 : password를 입력하면 토큰이 생성됨! 복사해서 사용 TOKEN=$(curl http://127.0.0.1:10080/login -u user|jq -r '.token') 안전한 엔드포인트 도달 curl -H \"Authorization: Bearer $TOKEN\" http://127.0.0.1:10080/secure 모놀리식 포드 로그 조회 kubectl logs monolith 새 터미널을 열고 실시간 로그 스트림 받기 kubectl logs -f monolith 쿠버네티스 포드의 대화형 셸 실행하기 kubectl exec monolith --stdin --tty -c monolith /bin/sh 서비스 서비스 문서 포드는 영구적인 것이 아님 활성 여부나 준비 확인의 실패 등 여러 이유로 중단되거나 시작될 수 있음 포드 집합과 통신하려고 하면? 다시 시작될 때는 다른 IP 주소가 할당될 수도 있는데, 이 떄 서비스를 사용함 서비스는 포드를 위한 안정적인 엔드포인트를 제공함 서비스는 라벨을 사용해 서비스가 수행될 포드를 판단 포드에 올바른 라벨이 있으면 서비스에서 자동으로 포드를 선택해 노출 서비스가 포드 집합에 제공하는 액세스 수준은 서비스 유형에 따라 다름 ClusterIP(내부) : 클러스터 내부에서만 볼 수 있음 NodePort : 클러스터의 각 노드에 외부 액세스가 가능한 IP 제공 LoadBalancer 서비스 만들기 서비스를 만들기 전에 https 트래픽을 처리할 수 있는 안전한 포드 생성 cd ~/orchestrate-with-kubernetes/kubernetes cat pods/secure-monolith.yaml kubectl create secret generic tls-certs --from-file tls/ kubectl create configmap nginx-proxy-conf --from-file nginx/proxy.conf kubectl create -f pods/secure-monolith.yaml 서비스 구성 파일 탐색 cat services/monolith.yaml kind: Service apiVersion: v1 metadata: name: \"monolith\" spec: selector: # 포드를 자동으로 찾아 노출 app: \"monolith\" secure: \"enabled\" ports: - protocol: \"TCP\" port: 443 targetPort: 443 nodePort: 31000 # 31000에서 443로 외부 트래픽 전송 type: NodePort 모놀리식 서비스 생성 kubectl create -f services/monolith.yaml service “monolith” created라는 출력은 포트를 사용해 서비스를 노출하고 있다는 의미 노출된 포트에서 트래픽 허용 gcloud compute firewall-rules create allow-monolith-nodeport \\ --allow=tcp:31000 curl curl -k https://&lt;EXTERNAL_IP&gt;:31000 시간이 초과됨 포드에 라벨 추가 현재 위 모놀리식 서비스에는 엔드포인트가 없음 kubectl get pods 명령과 라벨 쿼리를 사용해 문제를 해결할 수 있음 kubectl get pods -l \"app=monolith\" secure=enabled 추가 kubectl get pods -l \"app=monolith,secure=enabled\" 누락된 secure=enabled 라벨을 포드에 추가 kubectl label pods secure-monolith 'secure=enabled' kubectl get pods secure-monolith --show-labels 모놀리식 서비스의 엔드포인트 조회 kubectl describe services monolith | grep Endpoints 이제 접근 가능 curl -k https://&lt;EXTERNAL_IP&gt;:31000 Kubernetes를 사용한 애플리케이션 배포 프러덕션 단계에서 컨테이너를 확장하고 관리하는 방법에 대해 배움 실행 중인 포드의 수가 사용자가 지정한 포드 개수와 일치하도록 보장 앱의 세 부분 1) 인증 : 인증된 사용자의 JWT 토큰 생성 2) 환영 : 인증된 사용자에게 인사 3) 프론트엔드 : 인증 및 환영 서비스로 트래픽 라우팅 서비스마다 배포를 하나씩 생성 내부 서비스와 프론트엔드 배포의 외부 서비스를 정의할 예정 완료되면 모놀리식 앱에서처럼 마이크로 서비스와 상호 작용할 수 있음 각 부분의 독립적 확장 및 배포가 가능해짐 배포 객체 생성 kubectl create -f deployments/auth.yaml 인증 배포 서비스 생성 kubectl create -f services/auth.yaml 환영 배포를 만들고 노출 kubectl create -f deployments/hello.yaml kubectl create -f services/hello.yaml 프론트엔드 배포 및 노출 kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf kubectl create -f deployments/frontend.yaml kubectl create -f services/frontend.yaml 외부 IP 확인 후 curl kubectl get services frontend curl -k https://&lt;EXTERNAL-IP&gt; 정리 리소스 정리 cat cleanup.sh chmod +x cleanup.sh ./cleanup.sh gcloud container clusters delete io --zone Reference Kubernetes in the Google Cloud Qwiklabs",
    "tags": "kubernetes development",
    "url": "/development/2019/01/11/kubernetes-and-deployment/"
  },{
    "title": "디스크 캐싱과 레디스를 활용한 크롤링",
    "text": "캐싱을 사용해 크롤링하는 방법에 대해 작성한 글입니다 디스크 캐싱부터 레디스까지 다룹니다 관련 코드는 Github에 있습니다 캐싱을 사용해야 하는 경우 캐싱 오프라인 상태(데이터 분석 또는 개발 목적)에서 웹 페이지에 접근할 수 있도록 하기 때문에 사용자에게 도움이 됨 가장 최신 정보나 현재 정보를 얻는 것이 최우선이면 캐싱이 적합하지 않을 수 있음 대규모 크롤링이나 반복적인 크롤링을 계획하지 않으면 매번 페이지를 스크래핑하고 싶음 캐싱을 구현하기 전에 얼마나 자주 페이지를 스크래핑할지, 얼마나 자주 새로운 페이지를 스크래핑할지, 얼마나 자주 캐싱을 삭제해야 할지 등에 대해 생각해야 함 링크 크롤러에 캐싱 기능 추가하기 URL을 다운로드하기 전에 캐싱을 확인 download 함수 안, 다운로드할 경우 쓰로틀링(Throtting)을 조절 매번 다운로드할 때 매개 변수를 전달할 필요가 없도록 클래스로 구현 설명 __call__ 메소드 이 클래스의 객체가 함수처럼 호출되면 실행되는 함수 다운로드 전에 캐싱을 확인(캐싱은 딕셔너리) # throttle.py from urllib.parse import urlparse import time class Throttle: \"\"\" Add a delay between downloads to the same domain \"\"\" def __init__(self, delay): # amount of delay between downloads for each domain self.delay = delay # timestamp of when a domain was last accessed self.domains = {} def wait(self, url): domain = urlparse(url).netloc last_accessed = self.domains.get(domain) if self.delay &gt; 0 and last_accessed is not None: sleep_secs = self.delay - (time.time() - last_accessed) if sleep_secs &gt; 0: # domain has been accessed recently # so need to sleep time.sleep(sleep_secs) # update the last accessed time self.domains[domain] = time.time() from random import choice import requests from throttle import Throttle class Downloader: def __init__(self, delay=5, user_agent='wswp', proxies=None, cache={}, timeout=60): self.throttle = Throttle(delay) self.user_agent = user_agent self.proxies = proxies self.cache = cache self.num_retries = None self.timeout = timeout def __call__(self, url, num_retries=2): self.num_retries = num_retries try: result = self.cache[url] print('Loaded from cache:', url) except KeyError: result = None if result and self.num_retries and 500 &lt;= result['code'] &lt; 600: # 캐싱 결과가 서버 에러면 무시하고 다운로드를 재시도 result = None if result is None: # 캐싱에서 결과를 로드하지 않아 다운로드 self.throttle.wait(url) proxies = choice(self.proxies) if self.proxies else None headers = {'User-Agent': self.user_agent} result = self.download(url, headers, proxies) self.cache[url] = result return result['html'] def download(self, url, headers, proxies): print('Downloading:', url) try: resp = requests.get(url, headers=headers, proxies=proxies, timeout=self.timeout) html = resp.text if resp.status_code &gt;= 400: print('Download error:', resp.text) html = None if self.num_retries and 500 &lt;= resp.status_code &lt; 600: # recursively retry 5xx HTTP errors self.num_retries -= 1 return self.download(url, headers, proxies) except requests.exceptions.RequestException as e: print('Download error:', e) return {'html': None, 'code': 500} return {'html': html, 'code': resp.status_code} 디스크 캐싱 urlsplit 함수를 사용해 URL 파싱 from urllib.parse import urlsplit component = urlsplit(\"https://www.google.com/webhp\") print(component) &gt;&gt;&gt; SplitResult(scheme='https', netloc='www.google.com', path='/webhp', query='', fragment='') print(component.path) &gt;&gt;&gt; /webhp filename 지정 if not path.enswith('/'): path += '/index.html' elif path.enswith('/'): path += 'index.html' filename = component.netloc + path + components.query __getitem__ 메소드 item을 get __setitem__ 메소드 item을 set 캐싱 테스트하기 diskcache.py, advanced_linke_cralwer from diskcache import DiskCache from advanced_link_crawler import link_crawler link_crawler('http://example.webscraping.com/places/default', '.*/(index|view)/.*', cache=DiskCache()) 첫 실행시 2분정도 소요되지만 다시 실행하면 1초만에 종료! (캐싱!) 디스크 공간 절약하기 위해 압축(zlib 사용) 오래된 캐싱 데이터 만료하기 디스크 캐시의 현재 버전은 키 값을 디스크에 저장한 후 나중에 이 키를 요청할 때마다 다 값을 리턴 웹 페이지 컨텐츠가 변경되면 캐싱 데이터가 만료돼 적합하지 않을 수 있음 __set__ 메소드는 만료 타임 스탬프를 키로 result 딕셔너리에 저장하고 __get__ 메소드는 현재 UTC 시간을 만료 시간과 비교 DiskCache의 단점 URL에서 지원되지 않는 문자를 사용할 경우 다른 URL이 동일한 파일 이름으로 매핑됨 서로 다른 255자가 넘는 긴 URL의 단축 버전은 동일한 파일 이름으로 매핑 URL 길이 제한을 피하기 위해 URL의 해시를 얻은 후 해당 해시를 파일 이름으로 사용 파일시스템이 가지는 문제, FAT32 파일시스템이면 디렉터리당 허용되는 최대 파일 수는 65,535개, 이럴 경우 디렉터리를 분할 레디스를 사용한 캐싱 key-value 저장소 캐싱 크롤링할 때 대량의 데이터를 캐싱해야 할 경우 key-value store를 사용 파이썬 딕셔너리와 매우 흡사 저장소의 각 엘리먼트에는 key, value가 있음 레디스 : REmote DIctionary Server의 약자 트위터에서 레디스를 대용량 스토리지로 사용 크롤링하고 추가 정보나 검색이 필요하면 ElasticSearch 또는 MongoDB 사용하는 것을 추천 레디스 설치 # In mac brew install redis # python library pip3 install redis 레디스 서버 실행 redis-server 레디스 사용하기 레디스 3.0부터 dict 타입을 바로 넣을 수 없고, byte나 string으로 변경해야 들어감 import redis import json r = redis.StrictRedis(host='localhost', port=6379, db=0) r.set('test', 'answer') &gt;&gt;&gt; b'True r.get('test') &gt;&gt;&gt; b'answer' url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239' html = '...' results = {'html': html, 'code': 200} results = json.dumps(results).encode('utf-8') r.set(url, results) r.get(url) 컨텐츠 업데이트하기 r.set(url, b'{\"html\": \"new html\", \"code\": 200}') r.get(url) 컨텐츠 살펴보고 삭제 r.keys() r.delete('test') r.keys() 레디스 캐싱 구현 __getitem__ def __getitem__(self, url): record = self.client.get(url) if record: if self.compress: record = zlib.decompress(record) return json.loads(record.decode(self.encoding)) else: raise KeyError(url + ' does not exist') __setitem__ def __setitem__(self, url, result): data = bytes(json.dumps(result), self.encoding) if self.compress: data = zlib.compress(data) self.client.setex(url, self.expires, data) setex 메소드 datetime.timedelta 또는 초 값을 받음 지정된 시간 내 레코드를 자동으로 삭제할 수 있는 메소드 캐싱 테스트 from rediscache import RedisCache from advanced_link_crawler import link_crawler link_crawler('http://example.webscraping.com/places/default', '.*/(index|view)/.*', cache= RedisCache()) requests-cache 탐색 경우에 따라 내부적으로 requests를 사용하는 라이브러리를 캐싱하거나 캐싱 클래스를 직접 처리하고 싶지 않을 경우 사용 requests를 통해 URL에 접근하는 모든 get 요청은 먼저 캐싱 확인하고 캐싱이 없는 경우에만 페이지 요청 설치 pip3 install requests-cache 캐싱 여부 확인 및 만료 설정 import requests_cache import requests from datetime import timedelta requests_cache.install_cache(backend='redis') requests_cache.clear() url = 'http://example.webscraping.com/places/default/view/239' resp = requests.get(url) resp.from_cache &gt;&gt;&gt; False resp = requests.get(url) resp.from_cache &gt;&gt;&gt; True # 만료 설정 requests_cache.install_cache(backend='redis', expire_after=timedelta(days=30)) 레디스는 메세지를 주고받는 PubSub도 지원하니 참고하면 좋을듯 :)",
    "tags": "python development",
    "url": "/development/2019/01/08/crawling-with-caching/"
  },{
    "title": "Reinforcement Learning 3강. Planning by Dynamic Programming",
    "text": "David Silver의 Reinforcement Learning 강의를 한국어로 해설해주는 팡요랩 영상을 보고 메모한 자료입니다 Planning MDP에 대한 모든 정보를 알 때(=Environment, State) 더 나은 policy를 찾아가는 과정 Policy Evaluation policy가 고정되었을 때 value function을 찾는 것 Dynamic Programming 복잡한 문제를 푸는 방법 큰 문제를 작은 문제로 나누고 작은 문제에 대해 솔루션을 찾고 다 모아서 큰 문제를 품 강화 학습 Model Free : environment가 어떤 것을 던져줄지 모를 경우(완전한 정보가 없을 경우) Model based : environment에 대한 모델이 있는 경우 이 문제를 해결할 때 Planning, dynamic programming이 쓰임 Dynamic Programming의 조건 1) Optimal substructure 작은 문제로 나뉠 수 있어야 함 2) Overlapping subproblems 한 서브 문제를 풀고 나온 솔루션을 저장해(cached) 다시 사용할 수 있음 MDP는 이 조건을 만족함 Bellman 방정식이 recursive value function이 작은 문제들의 해 Planning by Dynamic Programming DP는 MDP에 대해 모두 알고 있다고 가정함 State transaction, reward, … 2가지 문제가 있음 1) For prediction value function을 학습하는 것 MDP가 있고 policy가 있을 때 그 policy를 따를 경우의 value function poliy evaluation 2) For control optimal policy를 찾는 것 MDP만 있고 optimal policy를 찾음 policy iteration, value iteration Policy Evaluation 이 policy를 따라갔을 때, return을 얼마 받는가? policy를 평가함 즉, value function을 찾는 문제 Bellman expectation backup을 사용해서 계속 적용 backup 메모리에 저장 synchronous backup 예시 MDP가 있고 policy가 있을 떄 value를 찾는 prediction 문제 주어진 바보같은 policy를 평가만 했을 뿐인데 평가된 value에서 greedy하게 움직이면 optimal policy가 찾을 수 있음 모든 문제에서 이게 됨! 신기 무한하게 갈 필요가 없음! 평가하게 greedy하게 움직이는 것을 만들자 Policy Iteration Evaluate the policy(value function을 찾고) Improve the policy by acting reddily(value function에 대해 greedy하게 움직이는 새로운 policy를 만들면) 이 Evaluate, Improve를 반복하면 converge됨 예제 Jack’s Car Rental 좋은 문제인진 모름.. 최대 20곳의 차가 있을 수 있음 A는 포아송 분포로 차가 옴 A에서 B로 B에서 A로 자꾸 차를 옮겨야 함 하나 빌릴때 10달러 policy를 추측할 수 있음 b는 수요가 더 많음 a에서 차가 적어도 어쩔 경우엔 b로 옮기는 것이 나을 수 있음 x축 : b 지점에 있는 차의 수, y축 : a 지점에 있는 차의 수 iteration하면 수렴한다! 정도의 감만 잡으면 ok 증명 무조건 이전 policy보다 좋은가? 수렴 포인트는 optimal Modified Policy Iteration 꼭 수렴할 때까지 해야되는가? 일찍 끝내면 안되는가 k번만 하고 evaluation, improve해도 되지 않는가? 이렇게 해도 합리적임! Value Iteration Principle of Optimality Deterministic Value Iteration 서브 프러블럼의 솔루션을 알면 Bellman Optimality Equation을 이용해 s에서의 솔루션을 구할 수 있음 바로 이전 점에서 목적지까지 최단 거리를 구함. 그 전에서 구함.. 반복 value만 가지고 놈(value만 이터레이티브하게 update) policy 없음 예시 한 스텝들은 bellman optimality equation을 이용해 품 끝이란 것을 알 수 없어서 모두 다 돌아야 함 잠시 정리 Asynchronous Dynamic Programming 여태 나온 DP 방법들은 모든 stete들이 parallel되게 하는 synchronous backup을 사용했음 어떤 state만 하거나 순서를 다르게 하는 asynchronous한 방법을 이용하면 computation을 줄일 수 있음 모든 state가 골고루 뽑히면 수렴 방법론 In-place dynamic programming 코딩 테크닉 state가 n개 있으면 table이 있어야 함 이전 step의 테이블 정보와 새로운 table inplace는 하나만 가지고 있음! Proritised sweeping 순서를 중요한 친구 먼저! 중요한 정의 : bellman error가 큰 것 Real-time dynamic programming state space가 넓고 agent가 가는 곳은 한정적일 경우 agent를 움직이게 하고 정해진 곳을 방문하면 update Full-width Backups DP는 full-width backup을 사용 큰 문제에선 DP를 사용할 수 없음(차원의 저주..) Sample Backups state가 많아도 고정된 cost로 할 수 있음 model free에서도 할 수 있음 오늘은 model based를 생각했었음. model free는 어디로 갈 지 모르는 상황, 액션을 하며 샘플! Reference 팡요랩",
    "tags": "rl data",
    "url": "/data/2019/01/06/dynamic-programming/"
  },{
    "title": "머신러닝 오퍼레이션 자동화, MLOps",
    "text": "Euroscipy 2018에서 진행한 Scalable Data Science: The State of MLOps in 2018 발표 자료를 보며 정리한 내용입니다 awesome-machine-learning-operations Repo에 엄청 좋은 자료들이 가득합니다 :) ML + DevOps = MLOps 데이터 사이언스는 2개의 workflow로 일반화됨 Model Development Model Serving(Prediction) 만약 팀이 작을 경우 유지할 수 있는 소수의 모델 데이터 과학자들이 머리속에 모델에 대해 기억할 수 있음 각자 진행 상황을 추적할 수 있는 방법이 있음 그러나 팀이 점점 성장하면 생기는 이슈들 1) Data 흐름의 복잡도 증가 데이터 처리 workflow가 많음 표준화된 흔적 없이 데이터가 수정됨 flow와 스케쥴의 복잡성을 관리하는 것이 점점 힘듬 2) 각자 선호하는 도구가 다름 Tensorflow, R, Spark, Keras, PyTorch, MXNet 등 3) Serving model은 점점 어려워짐 다른 환경에서 실행되는 다른 모델 버전 모델 배포 및 복귀(롤백)가 더 복잡해짐 4) 무엇이 잘못되었는지 추적하기 힘듬 데이터 과학자들은 파이프라인 버그라고 하는데 데이터 엔지니어는 모델의 문제라고 함 직군별 Task Data Scientist 모델 개발 Data Engineer 데이터 파이프라인 개발 DEVOPS / DATAOPS / MLOPS Engineer 모델, 데이터 파이프라인 및 Production 부분 담당 컨셉 모델(이나 기술)이 성장하며 인프라를 사용해야 함 ML-Ops의 2가지 원칙 Reproducibility ( 재현성 ) Orchestration ( 배치/정렬 자동화 ) 원칙 1. MODEL &amp; DATA VERSIONING 각 단계별 특정한 data in/out이 있고, code나 config가 존재 추상화하면 Data cleaning, Feature Generator, Model 재현성이 가능하게 해주는 것 오류를 디버깅할 때 추적 가능 결과가 균일 재사용할 수 있도록 컴포넌트 모듈화 여러 라이브러리를 추상화 이전 릴리즈로 되돌아 가야 할 경우 재현 원칙 2. MODEL DEPLOYMENT ORCHESTRATION Production에서 모델 serving의 복잡도를 다룸 CI / CD / 모니터링과 유사하나 완벽히 다름 모니터링 모델의 Quality Control 과정은 반드시 필수!!!! Compliance 언제 무엇이 왜 어떻게 발생했는지? Guidelines for and properties of compliant ML 오류를 찾거나 디버깅, 중요한 이슈 report 등이 가능해짐 컴퓨터 자원 할당 다양한 계산 그래프가 있는 경우 적절한 리소스를 할당해야 함 아래와 같은 소프트웨어에서 가능 ETL framework HDFS-based service Kubernetes cluster Distributed framework 자원 할당이 만드는 차이 - 기존(전통) 서버 할당과 비교 가능 auto scale serverless Existing Tools Github 참고 PMML sklearn2pmml Github Storage 및 serialisation을 위한 기계학습 모델 파이프라인을 표현하는 방법 여러 시스템이 이 방법으로 플랫폼간 모델 전송 from sklearn import datasets, tree iris = datasets.load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) from sklearn_pandas import DataFrameMapper default_mapper = DataFrameMapper( [(i, None) for i in iris.feature_names + ['Species']]) from sklearn2pmml import sklearn2pmml sklearn2pmml(estimator=clf, mapper=default_mapper, pmml=\"D:/workspace/IrisClassificationTree.pmml\") DATA VERSION CONTROL(DVC) link 데이터 과학 프로젝트를 위한 버전 컨트롤 시스템 iterative.ai에 의해 빌드된 git fork는 데이터, config, 코드를 그룹화할 수 있음 # add your data dvc add images.zip # connect data input, model output and code dvc run -d images.zip -o modl.p ./cnn.py # add repository location (s3) dvc remote add myrepo s3://mybucket # push to the location specified dvc push ModelDB Github 함수 자체를 확장해 라이브러리 수준에서 input, output, config를 추적하는 암묵적인 방법 사용된 모든 단계와 모델 결과, prediction을 저장 저장된 모델을 사용 가능 def fit_and_predict(data): model.fit_sync(data) preprocessor.transform_sync(data) model.predict_sync(data) PACHYDERM Github 재현 가능한 파이프라인 정의를 허용하는 End to End 모델 버전 관리 프레임워크 # create a repo $ pachctl create-repo training $ pachctl list-repo NAME CREATED SIZE training 2 minutes ago 0 B # commit data (with -c flag) into the repo $ pachctl put-file training master -c -f data/iris.csv # Build # pytrain.py ...import dependencies cols = [ \"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\", \"Species\" ] features = [ \"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\" ] # Load training data irisDF = pd.read_csv(os.path.join(\"/pfs/training\", \"iris.csv\"), names=cols) svc = svm.SVC(kernel='linear', C=1.0).fit( irisDF[features], irisDF[\"Species\"]) # Save model to pachyderm /pfs/out joblib.dump(svc, os.path.join(\"/pfs/out\", 'model.pkl')) # dockerfile FROM ubuntu:14.04 ...install dependencies # Add our own code. WORKDIR /code ADD pytrain.py /code/pytrain.py # computational pipeline 정의 { \"pipeline\": { \"name\": \"model\" }, \"transform\": { \"image\": \"pachyderm/iris-train:python-svm\", \"cmd\": [\"python3\", \"/code/pytrain.py\",] }, \"input\": { \"atom\": { \"repo\": \"training\", \"glob\": \"/\" } } } ORCHESTRATION을 위한 도구 MLeap Github 모델 직렬화에 대한 깊은 다이빙 # run the server $ docker run \\ -p 65327:65327 \\ -v /tmp/models:/models \\ combustml/mleap-serving:0.9.0 # load a model curl -XPUT -H \"content-type: application/json\" \\ -d '{\"path\":\"/models/&lt;my model&gt;.zip\"}' \\ http://localhost:65327/model Seldon-core Github",
    "tags": "basic mlops",
    "url": "/mlops/2018/12/28/mlops/"
  },{
    "title": "Reinforcement Learning 2강. MDP",
    "text": "David Silver의 Reinforcement Learning 강의를 한국어로 해설해주는 팡요랩 영상을 보고 메모한 자료입니다 2강. Markov Decision Process MDP 강화학습이 적용되는 도메인이 다 MDP MDP에서 최적의 Sequential한 Decision Making을 어떻게 풀어나갈 것이냐? ← 강화학습이 풀고자 하는 문제 MDP가 무엇인지, 어떤 용어와 개념이 있는지 차근차근 배울 예정 이전 history를 버리고 state만 기억하면 될 경우 markov property Markov Process Markov Reward Processes Markov Decision Processes 순으로 진행하고 Extensions to mdps는 실버 강의에서 다루지 않아서 패스! Introduction to MDPs Markov decision process는 RL에서 환경을 표현 Environment는 모두 관측가능한 상황! 즉, 현재 state가 프로세스의 특징을 완전히 나타냄 대부분의 RL 문제는 MDP 형태로 만들 수 있음 Optimal control 문제를 continuous MDP로 다루고 Bandit 문제도 1 state를 가진 MDP Markov Property The future is independent of the past given the present P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1}, ..., S_{t}] State가 모든 관련된 정보를 가지고 있어서 state만 필요할 뿐 The state is a sufficient statistic of the future State Transition Matrix Environment를 설명할 때 필요하는 것들 시간 t일때 s에 있고 액션이 없음. 다음 스텝으로 옮길 때 (t+1) 여러 액션으로 전이할 확률 s에서 s 프라임으로 갈 확률 Markov Process(or Markov Chain) 상태들이 N개 있음 discrete하게 뚝뚝 끊어지며 state를 움직임 계속 옮겨다님 memoryless random process memoryless : 내가 어느 경로로 왔는지 상관 없이 이 곳에 도착하는 순간 미래가 정의 random process : 샘플링을 할 수 있음 S, P로 정의됨 S는 유한한 state들의 세트 P는 state transition probability matrix 어떤 조건을 만족하면 최종 분포가 stationary가 됨 1억명을 state에 두고 계속 전이하면.. staet마다 있는 수가 일정하게 됨 Student Markov Chain 에피소드를 샘플링한다는 표현을 쓸 예정 샘플링을 한 것은 확률 변수가 있고 이벤트가 발생(시뮬레이션) 여기서 Transition Matrix로 표현하면 Markov Reward Process Markov Process는 S, P만 있었는데 이젠 R, gamma가 추가됨 R은 reward funtion, 어떤 state에 도달하면 reward를 몇을 줘라! 이런 것을 정의 n개 있으면 됨 gamma : discount factor, 0~1 state에만 reward를 줌 예시 Return 강화학습은 Return을 maximize! Reward랑 다름 Return은 total discounted reward from time step t 감가상각 개념! 미래.. 감마가 0에 가까울수록 근시안(myopic), 1에 가까우면 큰 그림(far-sighted) Discount를 꼭 해야 해? 솔직한 이유는 수학적으로 편리해서 Discount때문에 수렴성이 증명! 수렴해야 크고 작음을 표시할 수 있음 동물과 사람의 행동이 즉각적 리워드를 선호함 만약 모든 시퀀스가 종료되는 것이 보장되면 감마를 1로 해도 될 수 있음 문제에 따라 discount가 필요한 문제, 안 필요한 문제가 있음 Value Function(MRP) Return의 기대값! MRP에서 Value Function은 state에 왔을 때, 그 state에서 계속 샘플링하며 에피소드를 만들어감 에피소드마다 Return이 생김 샘플링에 따라 return이 달라짐 return의 평균 State S에 왔을 때 G의 기대값(어느 정도의 return을 받을지 예측할 수 있음) return을 G_{t}로 표현, 확률 변수 Bellman Equation for MRPs 정말 중요! Value based method에선 엄청 나옴 value funtion이 학습되는 것은 모두 벨만 방정식에 근거해서 학습 벨만 방정식 감마로 묶어! G_{t+1}이 v 점화식처럼 표현! S에서 value는 한 스텝을 가고 + 다음 스테이트에서 value에 감마를 곱한 것 매트릭스 형태로 표현하면 식을 정리하면 v = (1 - \\gamma P)^{-1}R r, P, 감마가 주어지면 v를 구할 수 있음 계산 복잡도는 O(n^{3})라서 비싼 연산 Direct solution은 오직 작은 MRP에서만 가능 큰 MRP의 경우 다른 방법 Dynamic programming Monte-Carlo evaluation Temporal Difference learning Markov Decision Process MDP는 MRP에서 A(Action)이 추가됨 A : action들의 집합 action마다 reward가 주어짐 action을 하면 확률적으로 state로 가게 됨(그림에선 pub) state에 있으면 확률분포에 의해 넘겨줬는데, 이번엔 정책에 따라 액션이 달라짐 Policies Plicy is a distribution over actions given states \\pi (a|s) = P[A_{t}=a|S_{t}=s] S_{t}에 있을 때 a를 할 확률 Agent의 행동을 정의해줌 현재 state만 있으면 됨(history 필요 없음) Value Function 아깐 파이가 없었음 이젠 파이가 있어야 함 어떤 스테이트에서 폴리피 파이를 끝까지 했을 때, 에피소드가 1개 나옴 ⇒ 여러개를 계속 뽑아서 샘플링 ⇒ 평균이 Value Function State Value Function input이 state 하나만 들어갈 경우 Action Value Function input이 state, action이 들어갈 경우 q_{\\pi}(s,a) = E_{\\pi}[G_{t}|S_{t}=s, A_{t}=a] 큐함수 큐러닝, DQN 모두 이걸 학습! Bellman Expectation Equation s에서 a를 했을 떄, 파이를 따라서 게임을 끝까지 하면 얻을 return의 기대값 어떤 state에 떨어질지 모름. 모든 state에 갈 수 있음 각 state에 떨어질 확률가 그 state에서 value를 곱하고 더함 감마는 discount action을 어떻게 해야되는진 아직 배우지 않음 Optimal Value Function Optimal state-value function 파이가 아닌 star로 표현 어떤 policy를 따르든(세상에 다양한 policy.. 무한의 value..) 그 중 제일 나은 것 Optimal action-value function 할 수 있는 모든 policy를 따른 q 함수 중에 max optimal value function을 아는 순간 MDP는 풀렸다(Solved)라고 함 Optimal Policy 두 폴리시가 있을 때 하나가 나은지 비교 partial ordering 어떤 두개에 대해 이게 더 낫다고 존재한다고 할 수 있음 q star를 알고 그걸 따라가면 optimal policy다 MDP에선 deterministic optimal policy가 존재 deterministic! 가위바위보에 해당 ⇒ 무조건 가위만 Solving the Bellman Optimality Equation non-linear라서 closed form이 없음 많은 반복적 솔루션이 존재 Value Iteration Policy Iteration Q-learning Sarsa Reference 팡요랩",
    "tags": "rl data",
    "url": "/data/2018/12/27/mdp/"
  },{
    "title": "이준구 교수님의 인간의 경제학",
    "text": "총평 오랜만에 읽은 경제학 서적 (경영학 전공이라 예전에 이런 책을 많이 읽었는데..) 앞 부분은 매우 좋았으나, 뒷부분부터 흠.. 음… 뭔가 중의적인 느낌으로 책을 작성한 느낌? 행태경제학 자체가 약간 중의적으로 설명하는 것 같기도 하고.. 나중에 다시 곱씹어봐야 할 듯 혹은 다른 행동경제학 책도 읽어보고 비교할 것! 일반적 경제학을 뒤엎는 이야기를 하는 것은 매우 좋았음! (매번 이런 것에 의문을 가졌기 때문에..) 프롤로그 전통적 경제이론도 정책을 잘못된 방향으로 이끌 가능성이 있음 인간 본성에 대한 비현실적 가정에서 출발해 사람들의 행동을 제대로 예측하지 못함 어떻게 반응할지 모르는 상황에서 만든 정책은 실패할 가능성이 큼 행태경제학 (behavioral economics) 다른 사람에 비해 엄청나게 적은 이득밖에 얻지 못하는 상황에 불만을 갖는 것은 자연스러운 반응 남과의 상대적 맥락에서 나를 생가하는 것은 인간의 보편적 특성 인간 본연의 모습이 무엇인지를 알아내려고 노력 인간이 정말로 이기적이고 합리적인 존재인지를 검증해 보자고 제안 행동 경제학이 아닌 행태 경제학이라고 작성한 이유 행동 그 자체가 아니라 행동의 방식이 연구의 주요 대상 어떤 행동을 하느냐가 아닌 어떤 방식으로 행동하느냐가 주요 관심 1장. 합리적 인간의 실상과 허상 1) 경제학의 거울게 비친 인간의 모습 경제학은 차가운 느낌을 줌 경제학이 전형적 인간형으로 설정한 호모 이코노미쿠스 경제적 인간이 유일하게 관심을 갖고 있는 것은 물질적 측면이며, 오직 물질적 동기에 의해서만 움직임 한정된 자원으로 욕망을 최대한 충족시키기 위해선 자원을 효율적으로 활용해야 함 ⇒ 합리성이 핵심적 요건 모든 행동의 근저에 이기심이 깔려있음 자신의 이익을 우선적으로 고려 행테경제학은 호모 이코노미쿠스를 전형적 인간형으로 볼 수 있는지 의문을 제기해 전통적 경제 이론에 반기를 듬 기본 입장은 인간의 합리성과 이기심에 명백한 한계가 있음 2) 우리는 얼마나 합리적일까? 합리성 : 주변 여건을 정확히 파악하고 상황에 맞는 적절한 의사결정을 할 수 있음 그러나 현실은 상황 파악 능력에 한계가 있고, 적절한 의사결정에 필요한 복잡한 계산을 할 줄 모름 예시 큰 종이 한장을 50번 계속 접으면 두께가 얼마나 될까? 112,590,000km.. 3) 우리는 얼마나 이기적일까? 기본적으로 이기적이기는 해도, 언제나 이기적 행동만 하는 것은 아님 4) 경제학의 깜짝 스타, 행태경제학 제한된 합리성(bounded rationality)만을 갖는 현실의 인간이 추구하는 목표는 극단적인 합리적 선택이 아니라, 스스로 만족스럽다고 생각되는 수준에 도달하는 것 즉, 전통적 경제이론에서 상정하는 무제한적인 합리성과 대조되는 개념 2장. 휴리스틱의 세계 아무리 치밀한 사람이어도 매사에 백과사전을 찾고 계산기를 두드려 가며 살아가는 것은 아님 어림짐작으로 끝내는 경우가 많음 ⇒ 주먹구구식 판단을 휴리스틱이라 부름 1) 늘 계산기 두드려 가며 사는 것은 아니다 휴리스틱 현실의 상황을 판단하는 일이 무척 복잡하기 때문에 이를 단순화하기 위해 사용하는 주먹구구식 원칙. 지적 능력와 정보의 부족함을 메워 주는 긍정적 측면과 더불어 사물에 대한 객관적 인식을 방해하는 부정적 측면을 동시에 갖고 있음 2) 꼼꼼한 사람이면 은행원일 가능성이 크다? 대표성 휴리스틱 사람들은 특정한 직업을 가진 사람들의 전형적 특성에 대해 선입견을 가짐 대표성 휴리스틱(representative heuristics) 어떤 사람에 대한 묘사가 특정 직업의 전형적 특성을 얼마나 잘 대표햐느냐에 의해 그의 직업을 짐작하는 방법 이 대표성 휴리스틱에 의한 판단은 심각한 오판을 가져올 수 있음 확률의 법칙을 무시 3) 기억에만 의존하는 것은 위험하다 가용성 휴리스틱 가용성 휴리스틱 자신의 기억에 떠오르는 사건 또는 상황을 고려해 판단을 하는 방법 (마치 베이즈 정리같네요) 현실에 대한 오판으로 이어질 수 있음 언론보도가 사람들의 기업에 쉽게 떠오를 수 있게 만들어주는 역할 4) 휴리스틱을 만만하게 보지 마라 지명도가 높은 기업의 주식을 우선적으로 구입하는 투자방식 무지에 기초한 의사결정 방식 상당히 위력이 있음 3장. 행태경제학자의 눈에 비친 인간의 진솔한 모습 1) 의미 없는 숫자의 마력 닻내림효과 사람들에게 유엔 가입국들 중 아프리카 국가가 차지하는 비율이 몇%인지 짐작하라고 말함 짐작하기 전에 제비뽑기로 0~100 숫자 하나를 뽑음 두 수치는 관계가 없지만, 뽑힌 숫자를 짐작하는 과정의 출발점으로 삼음 닻내림효과 배가 어느 지점에 닻을 내리면 이리저리 움직여봤자 그 부근에서 맴돌게 되는 것처럼, 아무 의미 없는 수자가 제시된다 해도 어떤 것에 대한 최종적 판단이 그 숫자에서 크게 벗어나지 않는 현상 닻내림효과가 발생한 이유는 아주 어려운 질문에 직면해 당황스러워하는 사람은 아무리 사소한 힌트라 하더라도 놓치지 않으려는 태도를 보이기 때문 2) 겸손이 미덕이다? 처음에 가격을 높게 부르고 ⇒ 점점 내림 기준점이 높을수록 거래가격이 더 높음 3) 폭탄세일의 진실 미끼 상품을 폭탄세일 ⇒ 다른 상품도 쌀 것이라는 기대 4) 손해를 보는 것은 정말로 싫다. 손실기피성향 암묵적 가정 사람이 느끼는 만족감은 그의 소득 혹은 재산의 크기에 의해 결정 행태경제학자들은 큰 재산을 모으면 만족감이 크겠지만, 어느 정도 시간이 지나면 그 상태에 익숙해져 만족감이 별로 크지 않을 가능성이 높음 재산의 어떤 기준점을 설정하고 재산이 그것보다 커졌는지 작아졌는지에 관심을 가짐 이득보다 손해에 훨씬 민감하게 반응 5) 나쁜 돈이 좋은 돈을 몰아낸다 부정적인 존재가 긍정적 존재에 비해 큰 영향력을 가짐 6) 갖고 있는 것은 놓치기 싫다. 부존효과 손실기피적인 태도와 관련 부존효과 어떤 물건을 갖고 있는 사람은 그것을 갖고있지 않은 사람에 비해 그 가치를 더 높게 평가하는 경향이 있는데, 이 원인이 부존효과. 자신이 소유하고 있는 물건을 포기하기 싫어하는 태도에서 발생함 7) 단지 표현만 바꿨을 뿐인데. 틀짜기효과 논리적으로 똑같은 문장이 표현에 따라 다른 방식으로 인식 틀짜기효과 똑같은 상황이라도 여러 가지 인식의 틀이 있을 수 있는데, 이때 어떤 틀에 의해 상황을 인식하느냐에 따라 사람들의 행태가 달라지는 것 8) 돈마다 주소가 따로 있다? 심적회계 심적회계 사람들이 마음속에 일종의 장부를 갖고 있어 어떻게 생긴 돈이고 어디에 쓸 돈 인지에 따라 들어오고 나가는 것을 기록한다고 보는 개념 우연히 생긴 돈은 편하게 써도 된다고 생각하고 연봉이 늘어난 것은 편하게 써도 된다는 태도를 취하기 어려움 4장. 인간, 당신은 연약한 갈대 1) 혼자 힘으로 다이어트를 할 자신이 있습니까? 자신을 통제하는 능력에 대한 의심 ⇒ 돈을 사용해서 통제 (PT) 자제력에 대한 과도한 믿음으로 비합리적 선택을 하는 경우 헬스클럽 연간 정기권을 구입하고 거의 이용하지 않는 것 정기권 구입 순간엔 헬스클럽을 자주 찾을것이라 생각하고 자신함 ⇒ 허나… 2) 돌이킬 수 없는 것에 연연하는 우리 매몰비용 후회(regret) 사람들은 후회를 싫어함. 후회하지 않으려고 매몰비용을 견딤 3) 휴먼의 눈에 비친 기회비용과 매몰비용 기회비용 : 눈에 보이지 않더라고 비용의 성격을 가지면 모두 비용에 포함 예 : 미용실에 가서 10,000원을 주고 머리를 잘랐는데 왕복 소요시간은 1시간. 이럴 경우 1시간과 관련된 비용도 포함해야 함 기회비용과 매몰비용의 구분이 어려움 20달러 주고 산 와인을 가지고 있는 사람이 있음. 현재는 와인 가격이 75달러로 올랐음. 지금 이 와인을 먹으면 비용이 얼마인가? (1) 0 : 이미 가격을 지불 (2) 20 : 처음에 지불한 가격 (3) 20+이자 : 이자비용 포함 (4) 75 : 지금 와인을 팔면 받을 수 있는 금액 (5) -55달러 : 75달러의 가치를 가진 와인을 마셨지만 20달러만 지불 경제학적 관점은 (4)번이 답 ⇒ 20% (1)을 고른 사람은 와인을 사기 위해 지출된 비용이 매몰비용이라고 오해했지만, 다시 팔 수 있기에 20달러가 매몰비용은 아님 ⇒ 30% (5)번은 심적회계의 원칙을 적용한 결과 ⇒ 25% 4) 고민하는 것은 딱 질색이야 경제학의 기본 입장 선택가능성이 많으면 많을수록 좋음 마음에 안 드는 선택가능성은 그냥 버려도 됨 그러나 현실적으로 선택에 심리적 비용이 든다는 사실을 고려하면 선택가능성이 많을수록 더 좋다는 보장이 없음 선택에 심리적 비용이 크면 선택가능성이 적을수록 더 좋음 따라서 선택가능성을 일부러 없애기도 함 ⇒ 선택하지 않기를 선택하는 것 선택과 관련된 비용을 절약하기 위해 선택의 자유를 포기 부모님 세대가 주로 패키지 여행가는걸 생각하면..! 5) 귀차니즘은 우리 삶의 현실이다 현상유지 편향 (status quo bias) 현재 상황에서 좀처럼 벗어나려 하지 않는 습성 그 상황이 유지되기를 바라는 성향 기정편향 (default bias) 사람들이 귀찮음을 싫어하기 때문에 미리 정해진 것을 그대로 따르려는 경향이 있음 3개월 무료체험 후, 내가 말하지 않으면 계속 구독 의사가 있다고 간주하는 것 ⇒ 기정편향을 사용해 이윤 증대함 6) 조금씩 가격을 올려도 무방하다? 합리성의 원칙과 부합되는 행동 일반적으로 사람들은 다른 가게로 옮겨가는데 드는 수고의 가치가 옮기고 얻는 이득보다 더 크다 생각하면 다른 카게로 옮겨가 물건을 삼 그러나 현실에선 절대적 절약폭보다 절약되는 비율을 더 중시 이 논리를 거꾸로 적용하면 애당초 어떤 가격을 이런저런 이유를 붙여 적은 비율로 올려도 소비자들이 크게 반발하지 않음을 짐작할 수 있음 자동차에서 가격이 3,300만원이라 하고 추후 옵션에 따라 금액이 증가됨 이걸 4,100만원에서 옵션을 빼면 싸진다고 하면 큰 이득을 본다고 생각하지 않음 7) 백화점은 왜 끊임없이 세일을 하는가? 거래효용 ( transaction utility) 거래의 조건 그 자체에서 얻은 효용 전통적 의미의 효용(어떤 물건을 소비해 얻은 만족감)과 성격이 다름 8) 뉴욕 택시 운전사들의 독특한 행태 손님이 많은 날엔 일찍 퇴근하고 손님이 적으면 늦게까지 일함 독특한 심적회계 방식 하루 단위로 수입을 평가 하루 목표 수입을 설정하고 달성되지 못하면 손실이라 생각 만약 택시 운전사들이 한 달 단위로 수입을 평가하는 심적회계의 방식을 사용하면 손님이 없는 날엔 일찍 귀가할 것임 9) 생각이 다른 사람을 설득하는 것은 불가능한 일이다 현실의 인간은 자신의 신념과 어긋나는 정보가 아무리 많아도 좀체 신념을 바꾸지 않음 확증편향 어떤 증거가 갖는 의미를 자신의 신념과 일관되는 방향으로 비틀어 해석하는 경우(선택적으로 해석) 신념이 굳을수록 현저하게 나타남 사람들은 자기가 보고 싶은 것만 보려하고 듣고싶은 것만 들으려 함 10) 미운 사람은 뭘 해도 밉다 심리학자들은 사람들이 어떤 대상을 좋아하는지 여부에 따라 판단을 다르게 하는 경향을 보인다는 사실을 발견 감정휴리스틱 대상에 대해 좋은 감정을 가지면 평가가 좋아짐 정치 분야에서 많이 보임! 기술에서도 보임 ⇒ 특정 기술이 짱임! 11) 혹독한 얼차려가 좋은 효과를 발휘한다? 평균회귀 능력은 평균의 회귀하는 현상을 보임 경기를 못하고 ⇒ 얼차려 받고 좋은 효과를 냈다는 것은 신빙성이 없음 12) 그럴 줄 알았어. 사후확신편향 실현된 결과에 대해선 예전에 말한것보다 더 높은 확률을 말했다고 기억하고 실현되지 않은 결과에 대해선 그리 크지 않을것이라고 대답한 것으로 기억 어떤 일의 결과를 보고 자신의 예측을 사후에 정당화하는 경향 결과의 좋고 나쁨에 따라 평가를 내리게 만드는 요인 머신러닝에서 자주 보이는 듯? 사후확신편향은 결과편향이라는 또 다른 편향을 발생시킴 13) 입학사정관이 정말로 우수한 학생을 골라낼 수 있을까? 심리학자들 왈 : 전문가를 자처하는 사람들이 자신 있게 내리는 평가가 아주 단순한 공식을 적용해 얻은 예측보다 부정확한 경우가 많다고 함 전문가들이 여러 사항을 복잡하게 고려해 답을 얻으려 하는데 이와 같은 태도가 문제를 일으킴 대부분의 상황에서 단순한 방식이 더 정확한 예측에 이름 14) 우리는 왜 복권을 사는가? 실제로 일어날 확률이 작은데도 발생확률을 과장해 평가하는 경우가 많음 광우병, 벼락맞아 사망할 확률보다 당첨확률이 더 작다는 복권을 사는 해우이 등 15) 나는 다르다고 자신하는 우리 자기위주편향 자신이 남들보다 특별히 더 운이 좋다거나 더 능력이 있다고 생각하는 것 자신감 과잉 자신이 평균보다 더 능숙하다고 생각하는 것 자신을 냉철하게 돌아보지 못함 5장. 우리는 얼마나 이기적인가? 1) 남의 떡이 더 커 보인다. 몫 나누기 게임 케이크를 나눌 때 한 사람이 자르고, 다른 사람이 먼저 선택하도록 만들면 두 사람 모두 만족할 수 있음 대부분의 사람들은 이기적으로 행동해도 되는 상황에 남에게 양보하는 태도를 보임 자신에게 돌아오는 이익만 중요하다고 생각하는게 아닌 체면이나 공정성, 배려도 중요하다고 생각 2) 칼자루를 쥐었다고 마음대로 휘두르지는 않는다. 싫으면 말고 게임 이기심이 얼마나 중요한 역할을 하는지 살펴보는 것이 실험의 목적 많은 사람들이 이기적으로 행동해도 무방한 상황에서 이기심을 자제하는 의외의 행동을 보임 최후통첩게임 3) 좀더 생각해 봐야 할 것들 원시 부족은 이기적인 태도가 관찰됨 공동체적 삶이 강할거라 생각하지만 오히려 이기심이 더..? 그들이 가진 독특한 문화적 배경이 그런 행동을 하게 만들었을 것이라고 보아야 함 공정성의 개념이 다른 사회와 다를 수 있음 독재자게임 제안자는 상대방이 어떤 태도를 취할지 신경쓸 필요 없음 인색하게 굴지 않고 공정성을 중요하게 생각 4) 모두가 무임승차를 하려 들까? 생각보다 무임승차 경향이 약함 언제나 이기적으로 행동하지는 않음 6장. 돈이 전부는 아니다 1) 호의에는 호의로, 악의에는 악의로 휴먼은 결과만 중요한 것이 아니라 과정도 중요하고 과정에서 공정성이 중요하다고 생각 기업과 소비자들 사이의 관계에서도 발견됨 일시적 품귀상태를 빌미로 가격을 올리면 소비자들의 반발을 불러일으킬 수 있음 ⇒ 기업에서 두려워 함 2) 우리는 무엇을 공정하다고 느끼는가? 경제학 관점에서 공급량이 고정된 상황에서 수요가 늘면 가격은 당연히 올라간다 그러나 사람들은 이런 행태를 정당하지 못하다고 느낌 ⇒ 궁색한 처지를 이용해 자기 배를 불리는 부당한 행동으로 봄 자선단체에 기부한다는 단서가 달리면 ⇒ 여전히 부당 모든 사람이 동등한 권리를 가져야 마땅한 물건을 경매에 붙일 경우 부당하다는 인식 발생 3) 아 다르고 어 다르다 기준점을 설정해 공정성 여부를 판단하는 경향 새로 고용된 사람은 기준점이 다르다고 생각하는 경향 고용주의 직종이 바뀜에 따라 기준점이 다르다고 생각하는 경향 공정성에 대한 인식은 매우 미묘함 4) 중앙은행이 인플레이션을 원하고 있다? 5) 받은 만큼 일한다 6) 소비자의 분노를 사지 않고 가격을 올리는 방법 암묵적 비용보다 자신의 주머니에 나가는 경우 더 민감 차를 정가에서 200만원 깎고 팔다가 인기가 너무 많아 웨이팅이 길어져서 정가 그대로 받기 시작할 경우 공정하다는 대답을 58% 들음 기업에서 정가는 유지한 채 할인을 해주는 쪽을 선택하는 이유가 이 맥락 7장. 내일을 향해 쏴라 1) 로또 상금을 20년 후에 받는다면? 할인율의 개념 할인율 미래에 주고받은 금액을 현재 가치로 계산하는 것을 할인이라 부르는데, 이 할인의 과정에서 적용되는 비율 사람들이 기다리기 싫어하기 때문에 할인을 함 주식을 투자하고 존버하는게 참을성이 큰 사람 ⇒ 할인율이 낮음 2) 먼 미래의 일일수록 더 느긋해진다 사람들은 상황에 따라 들쑥날쑥학 할인율을 적용해 미래의 이득과 비용을 평가 3) 스타와의 데이트, 날짜를 언제로 잡으시겠습니까? 2주 후에 보겠다고 말하는 사람이 많음 천천히 음미하면서 소비하는 것과 같은 현상, 설레는 상태로 2주 반면 전기충격을 받아야 하면, 바로 받겠다고 하는 사람이 많음 두려움을 계속 안고 살기 싫다는 의미. 매를 빨리 맞자 4) 생활수준은 점차 높아지는 것이 좋다 평평한 모양의 소득 흐름보다 위로 올라가는 모양의 소득 흐름이 더 좋다고 대답 생산성이 높아지는 것처럼 임금도 높아져야 한다고 생각 8장. 금융시장에서 생긴 일 1) 대박 터트리는 방법은 없다. 효율시장이론 주식시장엔 극도의 합리성을 발휘하려고 함 효율시장이론 모든 투자자들이 합리적으로 행동한다고 할 떄 주식의 가격은 그것의 기초가치 수준에서 맴돌기 되기 때문에 어떤 투자자가 특정한 회사의 주식을 사들여 대박을 터트리는 것은 불가능함 2) 오마하의 마술사 워런 버핏을 보라 가치주전략 과거의 주식 가격, 수익, 배당금, 장부가치 등에 비추어 볼 때 그 가격이 낮은 수준에 형성되어 있다고 말할 수 있는 주식을 가치주라 부르는데, 이런 주식에 집중적으로 투자하는 전략 주가-수익비율이 낮은 주식이 바로 가치주라고 판단 주식 시장에서도 평균회위의 경향이 보일 수 있음 3) 쌍둥이 주식의 가격 차이를 이용하는 방법도 있다 4) 모멘텀전략이라는 것도 있다 최근 얼마동안 수익률이 좋았던 주식을 사고 수익률이 나빴던 주식을 파는 전략 수익률이 좋았던 주식들은 앞으로 당분간 수익률이 좋은 경향을 보이는 모멘텀이 존재한단 사실을 사용",
    "tags": "book etc",
    "url": "/etc/2018/12/25/behavioral-economics/"
  },{
    "title": "2018년 회고, 2019년 다짐",
    "text": "매년 어김없이 돌아오는 회고 글입니다 이 글을 읽기 전에 2017년 회고 글을 먼저 보고 오셔도 좋을 것 같습니다 :) 2017년 회고, 2018년 계획 월별 정리 월별 정리는 짧게 어떤 일을 했는지 위주로 정리! 1월 새해 첫 근무일에 작성한 메모 새해 첫 근무일! 야근하고 집에 12시에 갔지만, 자발적 야근! 내 실력을 더 키우고 싶어서 야근한다. 더 노력! 지금은 실력이 부족해도 계속 정진하다보면 좋은 결과를 낼 것이라고 믿음! 잘하고 있다 성윤아 BigQuery + Dataflow를 사용해 데이터 처리 파이프라인을 개선 컨텐츠 큐레이션 관련 (추천) 업무 진행 Airflow 사용 블로그 글 7개 작성 2월 패스트캠퍼스 파이토치 딥러닝 조교(With 성동) 리드미에서 데이터 사이언스 직무 특강 및 상담 블로그 글 4개 작성 3월 파이썬 깊게 이해 파이썬 코딩의 기술, 전문가를 위한 파이썬 광역버스를 타고 출퇴근하는데 교통사고를 겪음.. 병원을 꾸준히 다님 중앙대에서 데이터 사이언스 입문 특강, 학생분들 상담 퇴사를 결정하게 된 시기 블로그 글 5개 작성 4월 배운 것이 많은 회사를 퇴사하고, 폐관수련하듯 공부 시작! OKKY에서 데이터 사이언스 Intro 특강 Little Big Data에서 바닥부터 시작하는 데이터 인프라 발표 네이버 AI 해커톤 결선에 참여해 지식인 9등 패스트캠퍼스 데이터사이언스 스쿨 7기 특강 블로그 글 10개 작성 5월 데이터 사이언스 공부도 좋지만, 경영학 전공이었기 때문에 개발적 역량이 부족함을 깨닫고 개발적 역량을 꾸준히 공부한 기간 패스트캠퍼스 데이터엔지니어 Extension 스쿨 조교 활동 시작 모교인 인하대에서 데이터 사이언스 특강! 인하광장에 직접 글올리고 기안도 잡고 특강했는데, 학교에서 연결해주면 좋을텐데 직접 진행해서 번거로웠음 동네에 살고, 동갑이고, 퇴사 기간이 겹쳤던 웅원이랑 CS231n 스터디 시작 웅원이에게 많은 것을 배웠고, 나랑 정반대의 학습 스타일인 사람과 같이 스터디할 경우 시너지 효과를 느낌 운동을 해야겠다고 다짐해서 집 앞에서 PT 시작! 블로그 글 16개 작성 6월 이 기간엔(~9월) 정말 일상의 반복이었던 기간 운동하고 공부하고 다른 회사 방문하던 시기 Gap Year 이야기에 보면 더 자세한 내용을 볼 수 있습니다! 숙명여대 데이터 사이언스 취업 특강 블로그 글 10개 작성 7월 보아즈에서 데이터 사이언스 특강 블로그 글 13개 작성 8월 본격 취업을 고민하며 면접을 많이 본 기간 블로그 글 14개 작성 9월 다양한 회사에서 고민한 결과, 쏘카로 이직 회사가 뚝섬이라 출퇴근 5시간(…)이었지만 바로 추석이었고, 바로 자취 시작 블로그 글 4개 작성 10월 자취방에 침대, 책상 등 아무것도 없어서 방을 채우는데 집중했던 기간 블로그 글 2개 작성 MAB에 대해 심층적으로 학습 Uber, Lyft, Grab, Airbnb 기술 블로그 대부분을 읽었음 11월 글또 2기 시작 1기와 다르게 2기는 슬랙에서 진행 관심사별로 나눴더니 유대감이 조성되는 느낌 나한테 중앙화된 것을 분리하고자 퍼실분들을 두고 진행! 감사합니다 블로그 글 5개 작성 재미로 리액트 공부 시작 타다 관련 프로젝트 진행 시뮬레이터 구현 확률분포 기반으로 시뮬레이터를 구현하는 사람을 거의 못봤는데, 처음 하면서 재미를 느낌 어렵지만 재미있어서 즐기는 중 아마 이 내용으로 추후 블로그 글을 작성할 듯! 12월 정규직 전환 완료 입사 3개월간 벌써 사내 발표를 2번했는데, 앞으로도 사내에 무언가를 꾸준히 공유하면 좋을 듯 블로그 글 5개 작성 2018년 블로그 회고 2018년간 총 95개의 글 작성 사실 2017년 12월부터 블로그를 시작했으니 이제 1년된 블로그 주로 작성한 테마 데이터 엔지니어링 Apache Spark, Apache Kafka, BigQuery Airflow 개발 Python, Docker, Linux OS 머신러닝 및 딥러닝 CS231n 논문 리뷰 Kaggle Coursera 강의 책 후기 인스타그램으로 돌아보는 2018년 SELECT * FROM instragram_photos WHERE year=2018 ORDER BY date 확실히 책도 많이 읽고, 공부도 많이한 듯 작년에 작성한 2018년 목표 1. 블로그 글쓰기 모임 진행 : `글또`라는 이름으로 지었다. 아마 1월부터 진행할 예정인데, 강제로 돈을 걷고 포스팅하면 돌려드리는 형식으로 진행할 예정이다. 그렇기 때문에 많은 인원을 내가 커버할 수가 없다. 아마 지인이나 소수로 운영하다 잘되면 크게 넓힐듯..! 2. 건강 챙기기 : 체력이 하락하고 살은 찌고 있다..^^.. 이제 운동을 하며 건강을 챙겨야겠다 3. 현재 하고있는 일에 대한 전문성 + 딥러닝쪽 특화 4. 캐글은 꾸준히 참여 5. 멘토링도 간간히 진행 캐글 꾸준히 참여는 못했지만, 다시 하려고 다짐중! 의지의 문제니 조금만 더 의지를! 딥러닝쪽 특화라고 되어있는데, 이젠 딥러닝 특화라기보다 Solver의 입장에서 딥러닝은 그저 도구일 뿐.. 제한된 시간과 리소스를 통해 최대의 효과를 낼 수 있는 도구를 사용하자는 마인드로 바뀜(데이터 엔지니어링도 필요하다면 하고!) 2018년 짧은 리뷰 이직을 경험한 해 개발적 역량을 늘리고 싶어 선퇴사하고 3개월간 공부에 집중한 시기 (여행을 못간게 사실 아쉽지만!) 그래도 계속 쭉쭉 성장하고 있는 해였고, 데이터 분야에서 세부적인 방향을 잡고 더 추진력을 받은 해 Github 커밋은 이제 딱히 신경쓰지 않음! 대신 캘린더에 세부적인 일정을 매일 작성하는 중 Notion도 사용하고 있는데 정말 마음에 드는 도구! 2019년 다짐 2018년의 컨셉은 빡세게였다면, 2019년의 키워드는 부지런히, 다양하게 아래의 리스트는 하고 싶은 리스트지만 모든 것을 할 수는 없으니 시기를 적절히 나누고, 무리하지 않기! 과한 욕심으로 아무것도 못할 수 있음 독서 개발 서적은 많이 읽지만, 그 외에 인문교양 책들도 많이 읽기! 리디셀렉트를 통해 읽기 요리 회사 DBA분이 주말에 뭐해요?라고 물었을 때 “코딩, 공부”라고 답했음 생각해보니 이제 개발 이외에도 관심을 두면 좋지 않을까? 생각으로 요리를 선택! 마침 자취도 시작했꼬! 타코를 만드는게 목표입니다 (나름 쉽지 않을까요?) 책 집필 발표를 많이 했고, 상담도 누적 100회 이상 진행해서 나름 비전공자 혹은 이 분야 입문자에 대한 고민을 많이 들었음(나도 비전공자, 비석사기도 하고) 이런 내용들을 책으로 내면 어떨까- 고민! 그리고 빅쿼리 관련 책도 쓸 예정! 툴에 집중하기보다 데이터 엔지니어링의 한 부분으로 쓰지 않을까 생각중 회사에서 다짐 쏘카에 입사하고 느낀 것은 정말 다양하고 재미있는 데이터가 많고 해결해야 할 문제도 많음. 이런 문제들을 하나씩 처리할 예정 문제를 해결하는 Solver의 입장으로 다양한 지식을 흡수하고 적용하기 구성원들에게 도움이 되는 사람이 되고, 함께 성장하기 오픈소스에 기여하기 회사에 Apache 커미터 경험자분들이 계신데, 멋있어 보임 단순히 사용만 하는 입장이 아닌 이젠 오픈소스에 기여하고 싶음 글또 글쓰기 모임 글또를 2기째 운영하고 있는데, 1기에 비해 더욱 잘되고 있기에(25명의 글을 피드백해야되는 나로서는 약간 피곤하지만) 앞으로 어떻게 진행할지 고민중 글 첨삭 관련 딥러닝 모델을 몇개 만들어볼까.. 하는데 자연어처리에 사실 흥미가 적어서(..) 고민 중 강화학습 점점 강화학습을 해야될 것 같은 느낌-? 공부하고 시뮬레이터에 적용해보며 강화학습이랑 친해지기! Deep Racer도 관심이 많기 때문에 아마 구입할 듯..! GDE 예전부터 지켜보고 있던 Google Developers Experts에 도전해볼까 고민중! 클라우드 분야로 해볼까 고민- 그런데 진행 절차같은 것이 잘 안나와 있음 ;ㅁ; 어떻게 해야될까요? 어쨋든 버킷리스트!",
    "tags": "diary",
    "url": "/diary/2018/12/22/2018-retrospect/"
  },{
    "title": "Contextual Multi-Armed Bandits 정리",
    "text": "Multi Armed Bandits(멀티 암드 밴딧) 방법 중 하나인 Contextual Multi-Armed Bandits에 대해 정리한 글입니다 Multi Armed Bandit 멀티 암드 밴딧은 비즈니스에서 엄청 좋은 효과를 거두는 방법으로 강화학습 책에서도 등장하는 내용입니다 MAB 관련 내용은 아래 책과 두 글을 추천합니다 Bandit Algorithms for Website Optimization 원서 송호연님의 브런치 김우승님의 A/B테스트를 보완하는 MAB 알고리즘 구현 코드 bandits Bandit Algorithms for Webstie Optimization의 Github BanditLib slots Contextual Multi-Armed Bandits Contextual MAB 방법은 단어 그대로, 맥락을 고려하는 MAB라고 보면 됩니다 관련 논문 Abstract 순서가 있는 독립적인 시도에서 online 알고리즘은 주어진 상황(부가적인 정보)에 기반해 가능한 행동 중 하나의 action을 선택해 선택된 행동의 총 지불을 극대화 이전의 연구들(context-free MAB)은 부가적 정보 없이 선택된 행동에만 의존하는 모델 목적 광고의 클릭률(CTR)을 극대화 Introduction 검색 엔진들의 목표 클릭 가능성을 높이고 예상 수익을 높이기 위해 사용자에게 가장 관련있는 광고를 표시 이 목표를 달성하기 위해 검색 엔진은 시간의 흐름에 따라 다른 검색어에 대해 가장 관련성이 높은 광고에 대해 학습해야 함 현재 관련있는 광고를 expolit하고 잠재적으로 관련성이 높은 광고를 explore context : 사용자 쿼리 (검색어) \\mu(x, y) : 쿼리 x에서 광고 y가 클릭될 확률, CTR 가능한 알고리즘 : Bayes optimal 주어진 쿼리에 대해 가장 높은 CTR을 가진 광고를 표시 이를 위해 CTR을 알아야 하지만 CTR을 몰라도 베이즈 최적를 사용하면 점근적으로 동일 모든 알고리즘 A에 대해 베이즈 최적의 클릭수와 T 쿼리에 대해 A 알고리즘이 받는 클릭수간의 예상 차이를 고려 이 예상 차이를 regret of A라 부르고 R_A(T)로 표시 어떤 쿼리 시퀀스에 대해 쿼리당 regret이 0에 접근하면 베이즈 최적으로 점근적임 Asymptotically Bayes Optimal 알고리즘의 표준 지표는 라운드당 regret이 0에 가까워지는 수렴 속도 추가로 읽을 자료 Deep Contextual MAB MAB Survey 전상혁님 MAB 글 숨니의 무작정 따라하기 블로그 위키피디아",
    "tags": "mab data",
    "url": "/data/2018/12/16/contextual-mab/"
  },{
    "title": "함께 자라기(애자일로 가는 길) 후기 및 정리",
    "text": "김창준님의 함께 자라기(애자일로 가는 길)를 읽고 정리한 글입니다 김창준님 발표를 전혀 들어보진 않았지만, 재미있게 읽었습니다 나는 어떻게 하고 있는가? 우리 조직은? 등에 대해 고민할 수 있었습니다 올해를 회고하기 전에 읽어서 정말 다행이네요! 특히 1장의 내용이 정말 좋았습니다 :) 성장, 코칭, 채용, 팀, 달인의 비결에 대한 내용이 계속 기억에 남네요 김창준님 블로그에도 좋은 내용이 많이 있습니다 서론 내가 정말 자랄 수 있을까? 우리가 정말 함께 자랄 수 있을까? 우리가 정말 매일매일 함께 자랄 수 있을까? 협력(우리) + 학습(자라다) + 접근 방법(매일매일) 함께 자라기 : 애자일의 핵심 1장. 자라기 자라기 : 학습 상반된 의견이 넘치는 현실 스스로 생각하는 훈련이 반드시 필요 야생 학습 현실에선 야생 학습이 더 많이 필요 협력적 비순차적 자료에 한정 없음 명확한 평가가 없음 정답이 없음 목표가 불분명하고 바뀌기도 함 불확실성이 높을 경우일수록 학습이 중요 학습 방법을 학습 당신은 몇 년 차? 개발자 가치 : 그 사람이 업계에서 얼마나 오래 살아남았는지로 결정 직원을 뽑을 때 무엇이 그 사람의 실력을 잘 예측할까? 미국 데이터로 메타 분석 메타 분석 : 연구마다 편향과 오류가 있을 수 있기 때문에 여러 연구를 통합해 통계 분석하는 방법. 실험 연구보다 더 강한 증거력 고려 변수별 직무 성과의 상관성 작업 샘플 테스트(실제 해야되는 업무 테스트) : 0.54 아이큐 같은 지능 테스트 : 0.51 구조화된 인터뷰(모든 후보에게 같은 순서의 동일 질문) : 0.51 성실성 같은 성격 테스트 0.41 레퍼런스 체크 : 0.26 연차 : 0.18 경력이 얼마 되지않을 경우엔 연차의 상관성이 높음. 연차가 높아질수록 상관성이 낮아짐 대학 갓 졸업자 vs 2년차는 후자의 실력이 높을 확률이 크지만 5년차와 10년차는 연차 차이가 의미 없음 학력 : 0.10 필체, 나이는 0에 가까움 소프트웨어 개발에서 경력과 실력 실력이 뛰어난 사람은 문제를 이해하는 데 시간을 적게 쓰는 것 대부분의 영역에서 최소의 경험치만 넘어가면 경력 연수와 실제 직무 성과의 상관성이 낮음 개발자의 경험이 얼마나 폭넓고 다양했는지가 실제 직무성과와 관련 양적인 면이 아닌 질적인 면 잘 뽑는 것 이상으로 중요한 것 사람을 뽑고 어떻게 교육, 훈련시키고 성장시킬지 회사에선 고민하지 않는 경우가 많음 시스템 개발자들이 할 수 있는 것 (김창준님 예측) 소프트웨어 개발에서 점차 경력 연수를 중시하는 문화가 사라질 것이고 다른 것에도 신경을 써야 함 의도적 수련 자신의 기량을 향상시킬 목적으로 반복적으로 하는 수련 피드백을 짧은 주기로 얻는 것 10분 후, 1시간 후, 하루 후, 일주일 후 등 실수를 교정할 기회가 있는 것 뛰어난 전문의 예시 뛰어난 전문의는 진단시 자신이 무얼 생각하는지 많은 기록을 하고, 자신이 얼마나 정확한지 나중에 확인 ⇒ 동료들과 차별화하는 중요한 요소 자신이 언제 어떻게 나아지고 있는지 잘 알 수 있음 자기 계발은 복리로 돌아온다 회고 되짚어 보는 것 : 나 자신에게 얼마나 투자를 했나 습득한 지식이나 능력은 복리로 이자가 붙음 김창준님의 힌트 자신이 이미 갖고 있는 것들을 잘 활용하기 새로운 것만 유입시키지 말고, 그 지식을 얼마나 어떻게 활용하는지 반성 이미 갖고 있는 것들을 하이퍼링크로 서로 촘촘히 연결. 노드 간 이동 속도가 빨라질 수 있도록 고속 도로를 놔라. 다양한 분야를 넘나들며 시너지 효과! 새로운 것이 들어오면 이미 갖고 있는 것들과 충돌을 시도 현재 내가 하는 일이 차후에 밑거름이 될 수 있도록 하라 외부 물질을 체화하라 주기적인 외부 자극을 받기 외부 자극을 받고 재빨리 자기화 자신을 개선하는 프로세스에 대해 생각해보기 A 작업을 되돌아보는 회고/반성 활동을 주기적으로 하기 나를 개선하는 과정을 어떻게 개선할 수 있을지 고민 피드백을 자주 받아라 사이클 타임을 줄여라, 순환율을 높여라 일찍, 그리고 자주 실패하기. 실패에서 학습하기 자신의 능력을 높여주는 도구와 환경을 점진적으로 만들기 학습 프레임과 실행 프레임 실행 프레임 : 잘하기 주어진 과업이 좋은 성과를 내는 것으로 인식 ex) 인정받아 다음 단계로 올라가냐에 관심 목표가 학습을 통한 성장이면 이런 프레임은 좋지 않음 학습 프레임 : 자라기 주어진 과업을 내가 얼마나 배우느냐로 인식 코딩 구루 이야기 : 입사 1년이 안된 두 사람의 선택, 행동, 반응이 다름 가장 학습하기 힘든 직업이 살아남는다 학습하기 힘든 환경과 주제 목표가 모호하고 주관적, 동적 매 순간 선택할 수 있는 행동/선택의 종류가 불확실 매 순간 목표에 얼마나 근접했는지 알기 어려움(빠른 피드백이 어려움) 열린 시스템에서 일함 과거의 선택과 결과에 대해 구조화된 기록이 없음 컴퓨터가 병목을 가지는 부분 지각과 조작, 창의적 지능, 사회적 지능 독창성 : 문제를 해결하는 창의적 방법 사회적 민감성 : 타인의 반응을 알아차리고 왜 그렇게 반응하는지 이해 협상 : 사람들을 화해시키고 서로 간 차이를 조정 설득 : 다른 사람들의 마음이나 행동을 바꾸게 설득 타인을 돕고 돌보기 : 타인에 대한 개인적 도움 제공 무엇에 집중할 것인가 딱 정해진 일만 할 수 있는 환경이 축복이 아닌 저주일 수 있음 암묵지와 직관을 배우고 수련! 달인이 되는 비결 달인이 되려면 1) 실력을 개선하려는 동기가 있어야 하고 2) 구체적인 피드백을 적절한 시기에 받아야 함 의도적 수련의 필수 조건, 적절한 난이도 나의 실력와 난이도가 비슷해야 함 비슷한 부분에서 몰입! 업무 시간에서 불안함이나 지루함을 느끼는 때가 대부분이면 실력이 늘지 않는 환경에 있는 것 팀장이 실력에 맞는 일을 주기보다 자신이 몰입할 수 있도록 전략을 세워야 함 지루함을 느끼는 경우 : a1 실력 낮추기 평상시 즐겨 쓰던 보조 도구를 사용하지 않기 지루함을 느끼는 경우 : a2 난이도 높이기 100rps면 되는 시스템을 1,000rps 기준으로 만들기 익숙한 작업을 새로운 언어로 진행 리팩토링, 자동화 남들보다 일을 더 효율적/효과적으로 하기 위해 나만의 도구 방법! 불안함을 느끼는 경우 : b2 실력 높이기 사회적 접근 나보다 뛰어난 전문가의 도움을 얻기 짝 프로그래밍, 스택 오버플로우, 튜토리얼 문서 도구적 접근 내 능력을 확장시켜 줄 도구 Auto ML 내관적 접근 비슷한 일을 했던 경험을 머릿속에서 되살려보기 불안함을 느끼는 경우 : b1 난이도 낮추기 가장 간단하고 핵심적인 결과물, 프로토타입을 첫 목표로 삼기 쉬운 작업을 먼저 하고 어려운 작업으로! 쉬운 작업을 먼저 할 경우 정확도가 높아진 비교 연구가 있음 동적인 균형 지속적으로 자신의 감정 상태를 살펴야 함 메타인지 전략 : 알아차림(mindfulness) 팀장이 할 수 있는 일 팀장들에겐 팀원의 상태를 파악하고 그들이 몰입으로 가도록 도와주는 것이 의도적 수련일 수 있음 프로그래밍 언어 배우기의 달인 인지적 작업 분석(Cognitive Task Analysis), 역 엔지니어링(Reverse engineering) 잘하는 사람이 어떻게 잘하는지 분석 새로운 언어를 빠르게 배우던 S님의 비결 1) 튜토리얼을 읽을 때 뭘 만들지 생각하고 읽음 읽을 때 다음 작성할 프로그램을 염두 작성할 수 있겠단 생각을 하면 코딩 시작 프로그램 완성하면 다시 문서 읽음 이런 방법을 적극적 읽기! 첫 프로그램은 단어 개수 세기 루프는 어떻게? 글자 하나를 읽으려면 어떻게? 출력은 어떻게? 라는 질문을 가지고 읽음 2) 공부를 할 때 표준 라이브러리 소스코드를 읽음 언어의 스타일! 언어의 결을 배우고 따르는 것이 중요 3) 공부 중 다른 사람의 코드에 내가 필요한 기능을 추가 자신이 만들 수 있는 작고 간단한 추가 기능을 생각 바로 적용 전문가에게 답을 얻을 수 있는 방법 : 구체적 사건에 대해 말하도록 유도 실수는 예방하는 것이 아니라 관리하는 것이다 실수 예방 문화 : 실수를 하지 마라! 실수한 사람 비난 실수 관리 문화 실수가 나쁜 결과를 내기 전에 빨리 회복하도록 돕고, 실수를 공유하고 실수에 대해 서로 이야기하며 배움 실수 관리 문화에 가까울수록 그 기업의 혁신 정도가 더 높음 실수가 없으면 학습하지 못함 뛰어난 선생에 대한 미신 대부분의 훈련은 6개월 정도만 지나도 효과가 거의 사라짐 복합적인 이유 : 학습자, 교사, 교육 방식과 내용, 조직 환경 등의 이유 교육의 목적 : 학생이 ‘더 잘하게’ 도와주는 것 인지적 작업 분석을 사용해보기 선생 입장에서 “내가 이 문제를 해결할 때 어떤 과정을 거치는가”를 생각하며 질문, 분석 그리고 학생들이 배우며 어떤 생각을 하는가 직접 관찰하고 질문을 던지고 분석 이런 부분을 생각하며 다른 사람을 가르치거나 배우는 자리를 마련 나홀로 전문가에 대한 미신 고독한 천재? x 전문가는 사회적 자본과 사회적 기술 또한 뛰어남 뛰어난 개발자일수록 타인과 인터랙션에 더 많은 시간을 씀 2장. 함께 소프트웨어 개발을 잘 관리하기 위한 3가지 근본적 능력 1) 복잡한 상황을 이해하는 능력 프로젝트를 계획한 다음 관찰하고 행동해 프로젝트를 진행되게 하거나 계획을 바꿀 수 있어야 함 2) 관찰하는 능력 무엇이 벌어지고 있는지를 관찰하고, 효과적인 적응 행동을 하기 위해 관찰한 것이 어떤 의미인지 이해할 수 있어야 함 3) 행동하는 능력 어려운 대인 상황에서 화나거나 숨고 싶을 때도 적절하게 행동해야 함 짝 프로그래밍 두 사람이 대화를 통해 추상화를 높임 다른 사람과 협동하고 대화, 같이 그림도 그리고 소스 코드를 편집! 신뢰를 깎는 공유인가 신뢰를 쌓는 공유인가 신뢰 자산이 높은 조직은 커뮤니케이션 효율이나 생산성이 높음 신뢰를 쌓을 때 사용되는 방법 투명성과 공유, 인터랙션 복수 공유 하나 공유나 최고 공유는 “이 작업에 대해 흉을 보면 어쩌지?”라는 방어적 대응 전략을 생각해둠 또한 상대의 시안을 솔직하게 말하면 “내가 한 말로 나를 싫어하면 어쩌지?”라고 생각할 수 있음 복수 공유는 위에 나온 불안감이 상대적으로 적음 여러 개를 준비했으니 하나를 까도 나에 대한 공격은 아님 신뢰도 높아지고 성과도 좋음 팀장 자리에 있으면 새로운 아이디어 전파가 쉬울 것이라고 생각하는 것은 환상 상대방에 대해 이해하고, 많은 대화를 해야 함 이것도 모르세요? 공감하고 이해하려는 대화 (혹시 이 부분이 문제있으면 연락주세요!) 이 부분은 링크 참고 훌륭한 팀장이라면 그 사람의 사고 과정을 전략적으로 이해하려고 함 행동을 유도하는 대화 하향식 접근의 함정 바텀업 : 탐색적인 성격 대부분 탐다운 식으로 일함 잘 정의된 문제(well-defined)와 잘 정의되지 않은(ill-defined) 문제에 따라 접근 방법이 다름 어려운 문제는 탑다운과 버텀업을 섞어 씀 전문가팀이 실패하는 이유 전문가들 모아서 팀 만든다고 반드시 잘하는 것은 아님 오히려 성과가 떨어질 수 있음 정보 공유하고 협력을 잘하기 위한 명시적인 도움이 필요하며 소셜 스킬 등이 뛰어난 제너럴리스트가 있으면 도움이 됨 구글이 밝힌 탁월한 팀의 비밀 팀에 누가 있는지보다 팀원들이 서로 어떻게 상호작용하고 자신의 일을 바라보는지가 중요 심리적 안전감이 높은 예측력 팀 토론 등 특별히 생긴 활동을 통해 심리적 안전감을 개선 심리적 안전감 : 내 생각이나 의견, 질문, 걱정, 실수가 드러났을 때 처벌받거나 놀림받지 않을 거라는 믿음 쾌속 학습팀 리더가 팀 학습 속도에 미치는 영향 단순히 기술적 탁월함만을 갖춘 사람보단 학습 환경을 만들 수 있는 리더가 필요 학습 환경의 차이 학습이 빠른 팀은 팀원을 뽑을 때 협동적으로 이루어짐(디자이너를 뽑을 때 개발자가 관여) 단순 업무 수행 능력뿐만 아니라 다른 사람과 협력을 얼마나 잘하는지, 애매한 상황을 즐길 수 있는지, 자기보다 지위가 높은 사람에게도 자신있게 의견을 제안할 수 있는지 등을 보고 뽑음 개개인이 새로운 기술을 획득해야 한다고 보지 않고, 함께 일하는 새로운 방법을 만들어야 한다고 생각 심리적으로 보호가 되어야 함! 3장. 애자일 협의의 애자일 불확실성이 클 때 우리가 어떻게 해야 하는지를 고민한 결과물 더 짧은 주기로 피드백을 받고, 다양한 사람에게 피드백 받음 광의의 애자일 삶을 사는 방식까지 확장 우리의 삶도 불확실하기 때문에 학습과 협력",
    "tags": "book etc",
    "url": "/etc/2018/12/16/agile-together/"
  },{
    "title": "데이터 분석의 힘 후기 및 정리",
    "text": "회사 그룹장님이 추천해주셔서 읽은 책! 사실 제목이 진부한 느낌이었는데, 의외로 2018년에 8월에 나온 신간! 수식을 사용하지 않는 데이터 분석 입문서 인과 관계 분석에 대해 다룸 후기 가볍게 읽기 좋은 내용, 생각보다 알찬 내용! 단 일본어 표현이 조금 생소할 수도 있음 우버 사례는 흥미진진! Yes24 링크 Chapter 1. 데이터의 상관관계는 인과관계가 아니다 인과 관계를 입증하기 어려운 이유 1) 다른 요인이 영향을 미쳤을 가능성이 있음 2) 인과 관계가 반대일 가능성이 있음 상관 관계가 밝혀져도 그것만으론 인과 관계가 있다고 말할 수 없음 잠복 변수 V를 최대한 모은 다음 통계 분석에 의해 V의 영향을 배제해야 함 문제는 데이터 수가 아닌 편향(bias) 잘못된 데이터 분석에서 도출된 오차 Chapter 2. 최선의 데이터 분석법, RCT 잠재적 결과를 이용한 인과관계 분석(potential outcome approach) 가격 인상 후의 전력 소비량을 Y_1, 인상 전의 소비량은 Y_0 개입 효과 트리트먼트 효과 Y_1 - Y_0 가격 인상(X)이 A의 소비량(Y)에 미친 영향 한 사람에 대해 Y1, Y0 2개가 나타날 수 없음 개입 집단(treatment group)과 비교 집단(control group)로 나눔 가정 : 가격 인상이라는 개입(X)이 없을 경우 비교 집단의 평균 소비량과 개입 집단의 평균 소비량은 같다 무작위 비교 시행 (RCT) AB Test 핵심 : 소비자를 집단으로 나눌 때는 무작위로(random) 실험 전에 두 집단의 평균값의 차는 0이어야 함(정말 무작위라면!) 원칙 1) 적절하게 집단을 나눈다 2) 집단은 반드시 무작위로 나눈다 3) 집단별로 충분한 표본의 수를 채운다 단점 비용, 시간, 노력이 많이 듬 파급 효과가 없어야 함 개입집단에 대한 개입이 비교집단에는 영향을 주지 않는다 대책 1) 개입집단을 어느 수준으로 설정할지 생각 2) 개입효과 외에 개입의 파급효과도 분석할 수 있도록 실험 설계 Chapter 3. 급격한 변화의 경계선을 찾는 RD 디자인 회귀 불연속 설계법(RD 디자인) 키워드 : 불연속, 경계선 의료비 본인부담금은 X, 결꽈인 의료 서비스 이용 수는 Y로 표시할 경우 다음과 같은 가정이 필요 만약 경계선에서 본인부담금(X)이 변화하지 않는다면 의료 서비스 이용자 수(Y)도 점프하지 않는다 원칙 1) 경계선을 기준으로 한 가지 요인(X)만 비연속적으로 변화하는 상황을 찾아냄 2) 경계선 부근에서 X 이외의 요인이 비연속적으로 변화하지 않는지 검증 단점 가상의 데이터에 기초하므로 데이터로 입증하는 것은 불가능 경계선 부근에 있는 사람에 대한 인과관계만 측정할 수 있는 단점 지리적 경계선을 이용한 RD 디자인 사례 Chapter 4. 계단식 변화가 있는 곳엔 집군분석 집합군 사이의 관계를 통해 인과관계를 밝혀내는 것 원칙 1) 계단식 인센티브를 분석에 이용할 수 있는지 확인 2) 분석하고 싶은 변수(X)만 계단식으로 변함. 다른 변수는 계단의 경계점 부근에서 비연속적으로 변화하지 않음 3) 인센티브가 크게 바뀌는 경계선에서의 데이터 집적을 분석해 개인/기업이 인센티브의 변화에 어떻게 반응했는지 검증 강점 가정이 성립하면 경선 부근에서 마치 RCT가 실시된 듯한 상황 결과를 그래프로 보여줌 RCT가 불가능할 경우 유용 약점 분석에 필요한 가정이 성립할 것이라는 근거를 제시할 수는 있지만 입증할 수는 없음 계단식만 가능해서 유용성이 떨어짐 Chapter 5. 시간의 흐름에 따른 패널 데이터 분석 패널 데이터 : 복수의 집단에 대해 복수의 기간에 걸쳐 수집한 데이터 RCT가 불가능할 경우 사용할 수 있는 자연 실험 기법 중 하나 이중 차분법(difference in differences methods) 원칙 1) 개입을 전후해서 개입집단과 비교집단 양쪽의 데이터를 입수할 수 있는지 확인 2) 평행 트렌드 가정이 성립하는지 검증 3) 평행 트렌드 가정이 성립할 가능성이 높다면 두 집단의 평균값 추이를 그래프로 그려 개입 효과의 평균값을 측정 가정 개입이 일어나지 않았다면 개입집단의 평균값과 비교집단의 평균값은 평행한 추이를 보임 가정을 위해 조사해야 할 것 개입 이전의 데이터를 모아 개입집단과 비교집단 사이에 평행 트렌드 가정이 성립하는지 조사 개입집단에만 영향을 미친 다른 사건이나 변수가 없었는지 확인 Chapter 6. 데이터는 어떻게 전략이 되는가 우버 사례 수요 곡선을 추정하는 프로젝트 목적지를 입력하고 운전자를 부르면 요금이 표시 이 순간 소비자는 표시된 요금으로 운전자를 부를지 말지 선택 ← 수요 곡선을 추정하기 위한 단서 그 가격을 보고 소비자가 서비스를 이용했는지 기록 RCT 사용 운전자 수와 이용자 수를 실시간으로 확인해 수급 핍박 지수(surge generator)를 계산 지수가 클수록 거리에 나와있는 운전자 수에 비해 이용자 수가 많음 가로 : 수요핍박지수 세로 : 우버 택시를 부르려고 했던 소비자 가운데 스마트폰에 가격이 표시된 이후 실제로 운전자를 부른 소비자의 비율 가격이 오르면 얼마나 줄어드는지 수치로 확인 Chapter 7. 불량 분석을 피하기 위한 방법 분석 결과의 외적 타당성 X가 Y에 미친 영향을 과학적으로 분석할 수 있음 여기서 얻은 결과는 분석에 사용된 표본에 적용되는 인과관계 분석 결과 인과관계가 도출되면 내적 타당성이 확보되었다고 함 실험으로 얻은 분석 결과를 다른 대상에게도 적용할 수 있느냐를 외적 타당성의 문제라고 부름",
    "tags": "book etc",
    "url": "/etc/2018/12/10/power-of-data-analysis/"
  },{
    "title": "Uber Experimentation Platform(XP) 이해",
    "text": "우버 테크 블로그에 있는 Under the Hood of Uber’s Experimentation Platform 을 읽고 정리한 글입니다 제가 관심있는 부분 위주로 정리했기 때문에 자세한 이해를 하고 싶은 분들은 원문을 읽어보길 추천합니다! Uber’s Experimentation Platform 우버에선 실험을 통해 고객 경험(UX)를 향상 새로운 기능 테스트 및 앱 디자인 향상시 사용 새로운 아이디어, 제품 feature, 마케팅 캠페인, promotion 및 기계학습 모델의 효과를 측정, 디버깅, 측정 및 모니터링 가능 Uber Rider, Ubet Eats, uber Freight 앱 전반에서 지원 A/B/N Test, 인과 분석, Multi-armed Bandit(MAB)에 기반한 지속적 실험 통계 방법을 크게 4가지로 사용 1) 고정된 horizon A/B/N Test(t-test, 카이제곱, rank-sum test) 2) Sequential probability ratio tests(SPRT) 3) causal inference test (synthetic control and diff-in-diff tests) 4) continuous A/B/N test using bandit algorithms(톰슨 샘플링, upper confidence bounds, contextual multi armed bandit test를 통한 베이지안 최적화) 유형 1, 유형 2 오류의 확률을 계산시 편차 보정을 측정하는 회귀 방법을 사용하고, standard error를 측정하기 위해 block bootstrap와 delta methods를 사용 Classic AB Test 실험군과 대조군을 설정한 후, 그룹간 metric을 측정해 실험 효과를 측정 Feature release 실험시 주로 사용 측정값의 평균, lift(treatment effect)가 중요한지, 샘플 사이즈는 충분히 큰지 등을 나타냄 (역자) 이런 기능은 통계적 모델링에서 항상 필요하기 때문에, 한번 만들면 계속 재사용 가능! 이 부분 추가 학습하기 통계 엔진 Data validation -&gt; check metric type -&gt; Data preprocessing -&gt; 각 방법론 적용 -&gt; P value calculation -&gt; A/B/N test -&gt; Lift 계산 -&gt; 검정력, sample 사이즈 계산 무작위 실험할 때 첫 단계 : 의사 결정(decision) metric 결정 decision metric 예 : 라이더 총 예약 수 측정 항목에 따라 Google 통계 엔진은 다양한 통계 가설 테스트 절차를 적용해 보고서 생성 (역자) 아키텍쳐 참고할 수 있을듯 데이터가 들어와서 각 과정을 check 이렇게 거대하지 않아도 되고, simple하게 먼저 구축 시작해도 좋을듯 통계적 방법의 핵심 구성요소 데이터를 수집하고 2가지 주요 문제를 감지 표본 크기의 불균형 실험군과 대조군의 표본 크기 비율이 예상한 것과 크게 다를 경우 이런 시나리오는 무작위 추출 메커니즘을 2번 확인해야 함 Flickers(깜빡임?) 실험군에서 대조군으로 전환된 경우 예 : 안드로이드에서 아이폰으로 바꾼 사람 이럴 경우 우버는 분석에서 제외한다고 함 (역자) 아예 실험군/대조군을 전환될 수 없도록 설정하면 될 듯 User level의 metrics 1) Continuous metrics 유저당 예약 비율같은 1가지 숫자로 나타낼 수 있는 지표 2) Proportion metrics 회원가입을 하고 어떤 여행을 완료한 비율을 Test, binary 3) Ratio metrics 2개의 숫자 값을 사용한 metric 예를 들면 (총 트립 요구수/완료된 트립의 수) AB Test의 퍼포먼스를 증대시키기 위한 전처리 1) Outlier detection clustering-based 알고리즘을 사용해 아웃라이어를 탐지하고 제거 2) Variance reduction 분산 감소는 가설 테스트의 통계 능력을 향상시킬 때 도움이 됨 사용자가 적거나 정확성을 희생하지 않고 조기에 실험 끝낼 경우 유용 CUPED 방법 사용 3) Pre-experiment bias 다양한 유저가 존재하기 때문에 그룹간 실험 편향을 교정할 필요가 있음 diff-in-diff를 사용 어떤 내용인지는 설명 안나옴 P value 계산이 핵심 p value는 결과가 중요한지 여부를 판별 - p value와 오 탐지율(Type 1 오류, 0.05)을 비교 상황마다 다른 방법을 활용 1) Welch’s t test 연속 측정과 같은 기본 Test 예 : 완료된 트립 2) Mann-Whitney U test 데이터의 극심한 skewness를 탐지할 때 사용되는 rank sum에서 효과적(?) 3) Chi-squared test 라이더 리텐션, propertion metric을 사용할 경우 4) Delta method and bootstrap methods 샘플 크기가 작거나 ratio metrics 실험일 경우 standard error estimation 좀 더 검색 필요 검정력(power)를 위해 2종 오류도 check Metrics management 사용 가능한 메트릭 추천 1000개 이상의 측정 항목이 있음. 사용자가 측정 항목 결정하는게 점점 어려워짐 Item-item collabortive filtering 방법 사용 2가지 score를 측정해서 제일 높은 것을 추천 Popularity score 함께 선택될수록 해당 측정 항목에 대한 점수가 높아짐 Jaccard_index 사용 Absolute score 피어슨 상관 점수 Sequential testing 전통적인 AB Test는 샘플을 반복적으로 가져와 1종 오류를 부추김 순차적 테스트는 핵심 비즈니스 메트릭을 지속적으로 모니터링 하는 방법 제공 1종 오류를 부풀리지 않으며 confidence를 조절 진행중인 실험에 대한 모니터링 Methodologies Metrics 모니터링 목적으로 mSPRT(mixed Sequential Probability Ratio Test)와 FDR을 사용한 분산 추정을 위한 테스팅을 활용 Mixture Sequential Probability Ratio Test 모니터링할 때 가장 보편적으로 사용하는 방법 (역자) 추가 공부 필요 Variance estimation with FDR control 순차 테스트를 올바르게 적용하기 위해 가능한 정확한 분산을 추정해야 함 비교군 대조군 그룹의 누적 차이를 모니터링 delete-a-group jackknife variance estimation과 block bootstrap 방법을 사용해 연관된 데이터에서 mSPRT 테스트를 일반화 Continuous experiments driver, rider, eater, restaurant, delivery-partner 경험을 지속적 실험을 통해 최적화하려고 함 bandit과 최적화에 초점을 둔 강화학습 방법을 구현해 실험 최근 컨텐츠 최적화를 위한 기술을 사용해 고객 engagement(참여)를 향상 Case Study Moving forward 점점 더 지능적인 실험 플랫폼을 구축하려고 함 (역자) 아마 지금은 더 좋아졌을 것 같음 머신러닝 관련은 글에 안나와서 아쉬움 다른 블로그 글 도 읽어볼만함! 단, 2017년 글 Reference 우버 테크 블로그 글",
    "tags": "engineering data",
    "url": "/data/2018/12/09/uber-experimentation-platform/"
  },{
    "title": "딥모닝 5주차. PR12-022~025",
    "text": "PR12 동영상을 하루에 1개씩 보는 “딥모닝” 스터디에서 본 영상을 정리하는 글입니다 PR-022: InfoGAN PR-023: YOLO9000: Better, Faster, Stronger PR-024: Pixel Recurrent Neural Network PR-025: Learning with side information through modality hallucination PR-026는 생략 PR-022: InfoGAN 선정 이유 Cool idea Cool 수학적 백그라운드 결과도 좋음 Cool 구현체 다른 paper들과 연결 GAN Generator : noise Discriminator : fake 이미지 구분 Information theory Entropy 확률분포 p를 최적의 coding scheme으로 coding했을 때 필요한 bit의 수 p가 얼마나 무질서하게 퍼져있는지 Cross Entropy true distribution p에 대한 coding scheme으로 unnatural distribution q를 coding했을 때 필요한 bit의 수 Mutual information X를 알 때 Y의 entropy가 어떻게 변할까? 두 확률분포가 서로 얼마나 의존하고 있을까? 서로 독립이면 I(X; Y) = 0 InfoGAN Motivation GAN을 학습하면 representation이 어떻게 구성되어있는지 알 수 없음(disentangled) =&gt; 활용하기 힘듬 유의미한 확률 분포를 나타내도록 뽑아낼 수 있지 않을까? Main idea GAN처럼 G, D를 학습하되 latent code c~P(c)에 대해서 c와 G(z,c)가 의존 관계에 놓이도록 학습하자! Variational Mutual Information Maximization Q(c x)를 가져와 근사 Expectation을 처리하기 위해 Lemma 5.1 사용 Lemma 5.1 Random variables X, Y and function f(x,y) under suitable regularity conditions Reconstruction loss! Disco GAN 자료 reconstructure loss가 발생해서 oscillation PR-023: YOLO9000: Better, Faster, Stronger Better (YOLOv2) Batch normalization High resolution classifier Convolution with anchor boxes Dimension clusters Direct location prediction Fine-grained features Multi-scale training Faster (YOLOv2) Darknet-19 Training for classification Training for detection Stronger (YOLO9000) Hierarchical classification Dataset combination with Wort-tree Joint classification and detection Introdcution &amp; Motivation Detection은 class 개수가 너무 작음 Classification만큼 올리고 싶은데 힘든 이유가 labeling이 어렵기 때문 따라서 Detection과 Classification 데이터를 잘 짬뽕해서 만들려고 함 Joint training algorithm YOLO 성능이 떨어지는 편 Recall이 낮음 Better Batch Normalization 사용 High Resolution Classifier 448x448 fine tuning Convolution with Anchor Boxes 앵커 박스 도입 416x416 : 홀수, 13x13이면 이미지의 가운데! 모든 앵커박스마다 class, objectness를 예측 Dimension Clusters 어떤 anchor 박스가 좋을까? IoU가 제일 큰 곳으로 k means Direction Location Prediction 이미지 전체로 이동할 수 있음 Fine-Grained Features 13x13 feature maps Multi-Scale Training Faster Darknet-19 Mostly 3x3 filter Global average pooling 사용 파라미터 획기적으로 줄일 수 있음 Training for classification ImageNet 100 class 448x448 fine-tuning Training for detection Adding 3x3 conv layer 5 boxese with 5 coordinates each and 20 classes per box Stronger YOLO 9000 Hierarchical classification ImageNEt labels are pulled from WordNet structured as a directed graph, not a tree conditional probabilities를 예측 Join Classification 9000 classes from the full ImageNet release 앵커박스 3개 classification loss만 backpropagation Dataset Combination with WordTree COCO : general concepts ImageNet : specific concepts 결론 YOLOv2는 빠르고 정확함 YOLO9000은 데이터셋을 합치는 것이 의의 있음 PR-024: Pixel Recurrent Neural Network Generative Model의 한 종류 이미지 픽셀을 Recurrent Neural network로! Generative Model Generative Model도 다양한 계보가 있음! Data distribution을 어떻게 모사할까?가 Pixel RNN쪽 Intuition 픽셀이 많아지면 이들의 관계를 어떻게 모델링할 것인가? Sequential Model로 Generate! 픽셀 하나가 주어졌을 때, 다음 픽셀을 예측 i에서 i-1 … 1까지 픽셀을 사용해 원하는 p(x)를 구함 Autoregressive Model 자기 자신을 두고 자기 자신을 나타내는 모델 단순하고 직관적, 학습이 잘됨 latent value가 없음 tractable likelihood Mask 미래의 픽셀은 정보를 받지 않기 위해, 계산을 할 때 마스크를 씀 미래 픽셀은 0 채널이 여러개일 경우 R부터 시작 Mask A, B Receptive Fields Architecture Residual 붙여서 더 깊게! Row LSTM Diagonal LSTM PixelCNN LSTM이 너무 느리니.. (성능은 좋지만) 개선! CNN으로 레이어를 여러번 쌓음 256 classes KL Divergence 데이터를 제대로 이해하고 있으면 compress할 때 완벽하게 알고 있기 때문에 추가적인 정보가 필요가 없음 compress할 때 얼마나 데이터가 필요한가로 설명할 수 있음 Negative Likelihood PR-025: Learning with side information through modality hallucination hallucination : 환각, 복사 Learning with side information multi modal RGB, Depth 등등 다양한 것을 input Test를 위해서도 똑같이 필요한데 이게 문제.. Missing Input during test Zero padding 잘 안될 수 있음 Generate해서 RGB를 동시에 넣음 Generate가 어려움 Feature Space를 hallucination Hallucination 빨간색을 카피할 수 있는 파란색 Loss function Several issues First train the RGB &amp; D-Net, copy the D Nte to H-Net Pool5 layer가 hallucinate",
    "tags": "paper data",
    "url": "/data/2018/12/08/deepmorning-week5/"
  },{
    "title": "딥모닝 4주차. PR12-017~021",
    "text": "PR12 동영상을 하루에 1개씩 보는 “딥모닝” 스터디에서 본 영상을 정리하는 글입니다 PR-017: Neural Architecture Search with Reinforcement Learning PR-018: A Simple Neural Network Module for Relational Reasoning (DeepMind) PR-019: Continuous Control with Deep Reinforcement Learning PR-020: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification PR-021: Batch Normalization PR-017: Neural Architecture Search with Reinforcement Learning 2016년 논문 Motivation for Architecture Search 뉴럴 네트워크를 디자인하는 것은 힘들고 튜닝할 때 노력과 시간이 많이 걸림 Feature Engineering =&gt; Neural Network Engineering 패러다임으로 변화 딥러닝 구조를 만드는 딥러닝 구조 Neural Architecture Search 핵심은 Configuration string 한 layer의 configuration : Filter Width: 5, Filter Height : 3, Num Filters : 24 RNN(controller)를 사용해 아키텍쳐를 명시하는 string 생성 RNN을 수렴하게 학습해서, string의 정확도를 알 수 있음 RNN sequence to sequence Training with REINFORCE Distributed Training 사용 아키텍쳐 세부 Generated Convolutional Network from Neural Architecture Search Skip Connection을 너무 좋아함 직사각형 필터를 좋아함(7x4) 첫 convolution 이후 모든 layer와 연결하는 것을 좋아함 Recurrent Cell Prediction Method LSTM, GRU와 비슷한 RNN cell을 찾기 위해 search space 생성 PR-018: A Simple Neural Network Module for Relational Reasoning (DeepMind) RN이라고도 불림 키워드 Plug and Play Module : 그냥 꽂으면 된다 Achieved SOTA, super-human performance : 사람보다 더 잘한다 Reasoning the Relations 관계를 reasoning하는 것이 중요 키 차이가 가장 큰 나무는? 나무간의 관계를 잘 알아야 함 The difficulty of reasoning relations Symbolic approaches 관계를 로직 language로 정의해서 수식적으로 풀이 정리하기 쉽지 않고 많은 문제에선 힘듬. 확장성 부족 Statistical learning 어텐션같은 방법 data-poor problem이 존재 Relation Networks Object들이 있고, g라는 relation 함수가 존재 모든 조합을 함수에 통과시키고, Sum 그 후 f라는 함수에 통과 RN의 장점 관계를 잘 추론(Infer) 실제 관계를 몰라도 됨 smantic 몰라도 됨 의미 몰라도 됨 All-to-all이 아닌 일부만 해도 됨 RN은 g 함수 하나로 통과시켜서 compute도 좋음 Order invariant함 Model - for VisualQA Model - for Natural Language Thoughts Simple components to apply Try with other existing models DeepCoding with RN? 가능할까? PR-019: Continuous Control with Deep Reinforcement Learning Deep Q러닝의 뒷 이야기 Introduction Deep Q Network High-dimensional observation spaces discrete and low-dimensional action spaces High-dimensional continuous action space에 Deep RL을 어떻게 구현할 수 있을까? Q-Learning discrete한 것에 맞는 방법 Background DQN Algorithm DQN을 continuous action space에 바로 적용이 가능할까? No! argmax로 주어지는데, action space가 high dimensional에서 optimal을 찾느 느린 문제가 됨 Actor-Critic Approach based on Deterministic Poligy Gradient를 사용! Critic Network (Value) : DQN처럼 구성 Actor Network (Policy) : loss function을 2번으로 나눔. update를 2개의 곱으로 표현(chain rule) 전체 네트워크 구성 PR-020: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification Parameter ReLU Weight 초기화를 다른 방법으로! Definition a값을 학습! no weight decay decay를 주면 일반 ReLU처럼 나온다 초기 a값을 0.25로 시작 Weight Initialization Zero는 매우 나쁨 Constant도 매우 나쁨 Use small random values! varinace를 어떻게 둘 것인가? 이 논문이 나오기 전엔 Xavier Initialization을 많이 씀 Variance를 2/(n_in+n_out) Initialization in PReLU Case Summary PReLU 제안 Initialization method 제안 PR-021: Batch Normalization DNN : 학습이 어려움 많은 파라미터 때문 weights의 작은 변화가 upper layer에 다른 값을 전달하게 됨 이런 현상을 Interval Covariate Shift라고 부름 Training, Testing 분포가 다르면 학습이 잘 안되는것처럼 input이 달라지면 학습이 잘 안됨 Batch Normalization 분포의 변화가 적으면 학습이 잘 될것이다 라는 가정 Scale을 조절 =&gt; 기존 분포와 유사하게 됨 Dropout을 쓸 필요가 없음 결론 Internal covariate shift를 해결하기 위해 제안 장점 빠른 속도 Less careful initialization Regularization effect Other methods ELU는 batch norm이 없어도 됨 SELU(Self-Normalizing Neural network)",
    "tags": "paper data",
    "url": "/data/2018/12/02/deepmorning-week4/"
  },{
    "title": "딥모닝 3주차. PR12-012~016",
    "text": "PR12 동영상을 하루에 1개씩 보는 “딥모닝” 스터디에서 본 영상을 정리하는 글입니다 PR-012 : Faster R-CNN PR-013 : Domain Adversarial Training of Neural Network PR-014 : On Human Motion Prediction using RNNs PR-016 : Yon Only Look Once(YOLO): Unified, Real-Time Object Detection PR-012 : Faster R-CNN 블로그 포스팅 참고 PR-013 : Domain Adversarial Training of Neural Network Source 데이터를 받고 실제 Test 데이터가 들어왔을 때 잘 classification했으면 좋겠음 문제를 단순히 봐서 라벨이 2개밖에 없는 binary classification 문제로 설명 empirical error를 정의 모든 데이터의 true error가 우리가 가진 sample 데이터의 empirical error + 모델의 복잡도로 upper bound된다고 증명 empirical 에러를 줄일 때, 모든 unseen 데이터에 대해 bound가 성립 Occam razor principle 일반적인 supervised learning의 setting : Training과 test의 domain이 같다고 가정 예측할 때 새로운 domain에 적용하는 경우가 많음 high quality - low quality posed - in the wild Domain Adaptation 소스 도메인에만 label 새로 올 target 도메인은 다를 수 있음 3 main class 샘플 데이터와 타겟 데이터들의 분포가 bias가 있으면 bias를 조절해서 서로 맞춤 feature 도메인을 학습 잘 해서, new representation space를 찾음 여기를 주로 파악할 예정 Ajustement/Iterative 방법 pseudo-labeld 정보로 통합 예시 도메인이 다른 곳에서 사용 공통적으로 잘 작동할 feature를 찾기 1 step Error 기존 전략 : 최대한 적은 파라미터로 training error가 최소인 모델을 찾자 DANN에선 불가능 PR-014: On Human Motion Prediction using RNNs 선정 이유 RNN을 사용한 Human Motion을 공부하기 위해 Motion forecasting 쉽게 말하면 Generation Given을 주고 예측 풀 스윙이 있으면 그 다음 모델이 어떤 모습일지? Senetence completion과 유사한 듯 a high-dimensional and nonlinear version of sentence completion Background RNN 뉴럴넷에서 히든 뉴런에 recurrent한 뉴런이 존재 Gradient vanishing / exploding 문제 존재 LSTM/GRU 게이트 유닛 그 전의 값을 pass할지 넘길지 등을 담당 4배의 weight와 파라미터가 들어감 GRU : LSTM보다 계산이 간단한데 비슷한 성능 다양하게 사용되는 RNN one to one one to many many to one many to many many to many 간단한 접근 joint angle 데이터를 모두 LSTM에 적용 Recurrent Network Models for Human Dynamics 논문 ERD : Encoder-Recurrent-Decoder gradually add noise 발이 삐끗하면 새로운 지역이라 이상한 행동을 할텐데, 애초에 학습시 noise를 추가해 보정 단, noise를 잘 넣어주는게 어려움 Structural-rnn : Deep learning on spatio-temporal graphs 머리, 다리, 팔 패턴을 다르게 학습하자! (위 방법은 모든 것을 한번에 넣었음) Hierarchical RNN Motion Forecasting using RNN (short term) given ground truth랑 예측된 것의 앵글 포인트를 에러 측정 (long term) 사람이 눈으로 봐서 그럴듯한지? 기존 RNN은 long term에서 이상하게 되는 문제, short term은 50까지 학습했으면 51을 할 때 jump가 생김 Noise 스케쥴링 테스트할 때 노출될 수 있는 에러를 미리 예측, 에러도 누적 점점 노이즈도 누적 일종의 curriculum learning 쉬운 예제부터 어려운 예제까지 이 논문에선 노이즈 스케쥴링 하지 않음(Residual에서 튀는 값들이 없어짐) 더 심플한 모델 제안하고 모든 데이터를 넣었음 Proposed solution Sequence to sequence 아키텍쳐 Residual 아키텍쳐 마지막 프레임에 안 움직이는거랑 비교하면 어떨까? (신선한 아이디어) 결론 PR-015 PR-016 : Yon Only Look Once(YOLO): Unified, Real-Time Object Detection 1번만 봄, 여태까지 나온 것을 다 합쳤다, 실시간이다 Faster RCNN에서 성능을 줄이더라도 속도를 늘리겠다! Main Concept Detection 문제를 regression 문제로 바꿈 1번의 feed forward(이미지 전체를 한번 본다) 다른 도메인에서도 잘 된다(General representation) Object Detection as regression problem 과거 window moving으로 점점 찾고, 합치는 방식 기존 한번에 찾고 끝내보자 Unified Detection 이미지를 S x S 그리드로 나눔 grid cell, bounding box를 x,y,w,h로 표현 물체가 많이 포함할 것 같은 곳이 더 찐함 Network Design Inception v1 Modified GoogLeNet 1x1 reduction layer Training Feature Extractor, Object Classifier로 구성 1) ImageNet 1000개 데이터로 pretrain 2) Network on Convolutional Feature Maps 3) 7x7x30 텐서가 나옴 그리드 셀 Loss Function SSE(sum-squared-error)를 사용해 단순화 Limitation of YOLO",
    "tags": "paper data",
    "url": "/data/2018/11/25/deepmorning-week3/"
  },{
    "title": "딥모닝 2주차. PR12-007~011",
    "text": "PR12 동영상을 하루에 1개씩 보는 “딥모닝” 스터디에서 본 영상을 정리하는 글입니다 PR-007 : Deep Photo Style Transfer PR-008 : Reverse Classification Accuracy(역분류 정확도) PR-009 : Distilling the Knowledge in a Neural Network PR-010 : Auto-Encoding Variational Bayes PR-011 : Spatial Transformer Networks PR-007 : Deep Photo Style Transfer A Neural Algorithm for Artistic Style 논문을 먼저 살펴봄 어떤 사진이라도 명화처럼 만들자! 사진 : Content 명화처럼 : Style 이 논문 이전엔 feature를 뽑는데만 encoding만 관심 가짐 Understanding Deep Image Representations by Inverting Them Preliminary 1 conv layer로 feature를 뽑음 ⇒ content generation 레이어가 깊어질수록 왜곡이 존재, 정보를 많이 날림 Loss function Image x를 iterative하게 만듬, weight 학습이 아님 여기선 y와 w페어가 주어짐 ⇒ x 업데이트 Preliminary 2 Texture synthesis 오리지날 텍스쳐를 가지는 것을 feature map으로 만들자 ⇒ style generation texture와 periodicity가 유사하다고 봄, correlation에 texture(style) 정보가 숨어있다 Neural Algorithm of Artistic Style Style generation from feature map Content generation from feature map 두 마리 토끼를 한번에! content를 적당히 뭉개며 style을 적당히 가미 Loss는 2개를 더해줌 Deep Photo Style Transfer Photorealism Regularization ⇒ 사진처럼 만들기 Augmented Style loss with semantic segmentation Image Matting : foreground object를 뽑음, foreground object를 잘 뽑아내지 못하면 패널티 affine transform Sky + building으로 구성된 이미지의 gram matrix를 생각해보자 sky^2+buidling^2+2*sky*building (cross term) cross term을 없애버리기 feature map에 semantic segmentation mask를 통과시킴 sky, building에 대한 feature가 나옴 람다가 커질수록 원래 하늘과 유사해짐 PR-008: Reverse Classification Accuracy(역분류 정확도) 세분화의 성능을 테스트 데이터 라벨 없이 예측하는 방법 문제 1 : 의료 이미지 세분화 문제 2 : 데이터가 모자르다 Proposed method 모든 데이터를 학습 Pseudo Ground truth : 가짜 정답 reverse classifier : 기존 모델과 비슷한 것을 학습 테스트 데이터와 가짜 정답을 모델에 학습 역분류 정확도 학습용 데이터에 대한 역분류기의 성능을 다양한 척도로 확인 논문은 RCA가 실제 classifier의 결과와 선형적 관계를 지닐 것이라고 추정 Calibration Classifier와 Reverse classifier에 calibration 정확도 보정 실습 old_data.shape -&gt; (2500, 100) new_data.shape -&gt; (2500, 100) Y.shape -&gt; (100,) from sklearn.linear_models import LogisticRegression Classifier = LogisticRegression() Classifier.it(old_data, Y) Y_pred = Classifier.predict(new_data) Classifier.fit(new_data, Y_pred) Classifier.score(old_data, Y) 요약 데이터에 대한 정확도 평가하는 방법 필요 의료 데이터는 검증 데이터 구하기가 어렵기 때문에 평가가 어려움. 평가가 제대로 안되면 오버피팅이 될 가능성이 있음 RCA를 통해 세분화의 정확도 판단 PR-009 : Distilling the Knowledge in a Neural Network 힌튼이 발표한 논문 앙상블의 계산 시간이 느리기 때문에 이런 것을 해결하기 위해 앙상블의 정보를 싱글 모델로 이전하는 것 Distilling 앙상블의 정보를 싱글 모델로 이동시키는 것 불순물이 섞였을 때 순수 물질만 남기는 것 앙상블을 빠르게 학습하는 내용도 있는데 이건 다루지 않을게요 뉴럴넷 파라미터가 많아서 오버피팅이 생길 수 있음 오버피팅을 피하기 위해 앙상블을 사용 이니셜 파라미터를 다르게 하는 것이 효과가 있음 단점 저장 공간이 많이 들고, 계산 시간이 오래 걸림 병렬 처리를 해도 모델의 개수가 코어 개수보다 많으면 더 오래 걸림 모바일 폰엔 저장 공간 이슈로 사용 불가능 딥러닝도 용량이 큼(VGG는 500메가쯤) Distilling Ensemble : Single Model 싱글, shallow한 모데을 만들고 싶음 좋은 성능 적은 계산 시간, 저장 공간이 조금만 앙상블을 만들어 싱글 shallow에 전이 1) 관측치가 많다면 앙상블을 쓰지 않아도 성능이 일반화될 것이다 over sampling ⇒ 레이블이 붙어있지 않은 상태로 진행 기존에 학습된 앙상블 모델으로 예측 ⇒ 오버 샘플링된 데이터의 클래스에 레이블을 붙임 RMSE로 평가했는데, training size가 커질수록 성능이 좋아짐 2) 클래스의 확률 분포를 알면 잘 학습이 되지 않을까 ba라는 사람이 제안 레이블 대신 logit값을 줘서 분포를 알아보자 logit : 클래스의 점수, 확률분포 로짓에 noise를 줘서 학습 ⇒ regularizer 역할 다시 single shallow에서 학습 힌튼은 soft max를 사용해 확률값 구함 확률을 계산해서 학습하는 것이 regularizer라고 힌튼이 말했음 activation 함수를 sigmoid에서 softmax로 변환 모든 확률값을 더했을 때 1이 되도록 temperature 도입 이게 낮으면 logit이 큰 값에 대해선 확률값이 1이고 나머지는 0이 됨 확률이 클래스가 1이 되면 확률 분포를 알 수 없음 (모두 1을 가짐) 2~5 정도 사용 너무 높으면 모두 같은 확률값을 가짐 (0.33) 모든 경우에 대해 실험을 해봐야 함 cross entropy의 변형을 사용 실험 결과 결론 앙상블만큼 좋은 성능을 가지고 적은 계산 시간을 가짐 softmax보다 logit이 더 좋았다고 말하는 중 PR-010 : Auto-Encoding Variational Bayes Generative가 핫해지는 중 레퍼런스가 다양하고 각자 설명하는 방식이 다 다름 추천 글 Motivation(논문을 선택한 이유) 제대로 이해해 보자! 2014년 논문이라 Classic 다양한 관점으로 해석이 됨 수식이 어려워 보이지만 그렇게 어렵진 않음(학부 졸업생 수준으로 커버 가능) Manifold hypothesis 딥러닝 자체가 manifold에 기반 우리가 다루는 데이터들이 사실은 저차원의 벡터 공간이 살고 있다 mnist가 784차원인데, 실제로 데이터는 작은 차원에 augmentation을 한 것 그 공간을 찾아내자!가 generative 모델의 목표 Autoencoder Input이 들어오면 hidden layer를 통해 다시 output이 생김 latent presentation을 학습 Linear Regression 점들의 경향성을 찾는 것 여기에 확률적 관점을 가미하면 점들의 확률 분포를 찾는 문제로 변형 그래피컬로 표현 가능 Autoencoder가 선을 찾는 것이라면 variational autoencoder는 점들의 확률분포까지 찾아내는 것 각 축(Z, X)을 고차원으로 높이면 latent representation 데이터를 찾아내는 것이 목표 이론 Maximum likelihood를 사용 그러나 p_{x}를 모름, 체인룰을 사용해 전개해도 여전히 모름 Variational inference를 사용 복잡한 p_{x}를 간단한 q_{x}로 근사하겠다 어떤 확률 분포가 있고, 그걸 직접 계산하기 어려울 때 계산 어려운 것을 인정하고 단순한 확률 분포로 근사 Math KL-divergence 기본적으로 distance 개념인데 distance는 아니다..라고 말함 P,Q를 바꾸면 값이 달라짐 Distance 비슷한 것이다 라고 생각하면 될 듯 확률 분포 P와 Q 차이를 구하는 것 식 전개 천천히 전개하고, 모르겠으면 영상 13분 참고 L을 최대화 몬테카를로 q(zㅣx)로 샘플링한 값으로 L을 계산 몇개를 뽑는거니 variance가 존재 전개를 조금 다르게 하면, 애널리틱하게(수학적으로) 표현 가능 variance를 적게 가능 전개할 때 2번째 식을 주로 사용 논문의 어펜딕스에 있음 Reparametrization Tricks 중간에 샘플링 과정이 있는데, 샘플링은 미분이 안됨 그래서 샘플링을 밖으로 빼내고 표현력은 유지함 네트워크 구조 평균과 시그마를 inference하고 x를 계산 stochastic autoencoder + regularization using prior prior는 노말 정규 분포(0,1)로 가정 결과 Visualization 결과 다 잘 찾는중 벡터 스페이스 커지면 좋음 왜 오토 인코더는 안될까? prior를 걸어주지 않음 레퍼런스 수학을 사용하지 않고 전개한 쉬운 글 PR-011: Spatial Transformer Networks 2015년 닙스, 구글 딥마인드 Introduction 동적으로 변하는 네트워크를 설계하겠다 CNN의 한계 spatial invariant하지 못하다 Scale, Rotation : No Translation : 부분적으로 max pooling 이게 spatial invariant하게 만들긴 하나 유동적이진 않음 Architecture Localisation net : transformer 할 좌표를 찾음 마지막단에 regression이 있어야 함 Grid generator : 좌표를 변환(Affine transform) Sampler : 하나씩 읽어옴, interpolation integer sampling bilinear sampling Differentiable image sampling : 미분 가능하도록! multiple spatial transformer가 가능 앞에, 뒤에, 병렬 등 다양하게 사용 가능 Experiment MNIST 번지 표지판 앞단에 1개만 중간에 여러개 넣음 (4개) 새 200종 Conclusion deformable을 offset을 계산을 따로 spatial은 트랜스폼의 파라미터만 찾아줌 샘플링하며 intepolation할 때 계산량이 적음 그러나 간단한 연산만 가능",
    "tags": "paper data",
    "url": "/data/2018/11/17/deepmorning-week2/"
  },{
    "title": "글또 2기 다짐글",
    "text": "글또 2기를 시작하는 다짐 글입니다 :) 시작 글 작년부터 꾸준히 진행하고 있던 개발자 글쓰기 모임 “글또”의 2기를 시작했습니다! 1기 다짐글 1기 분들이 모두 만족하셔서 운영자로 뿌듯하네요 :) 처음 시작할 땐 지인 위주로 사람을 선정했고, 이번엔 그런 제한조건 없이 모두 받았습니다 그 결과 총 27명과 함께 하기로 결정했습니다. 사실 이 모든 분들을 제가 케어할 수 있을지 고민이 되긴 하네요 변경된 점 팀 제도 운영 주요 관심사별로 팀을 묶었습니다. 관심사가 같다면 서로 피드백을 많이 줄 것이라고 예상합니다 슬랙 사용 페이스북 그룹은 뭔가 숙제 검사하는 느낌이 들어 슬랙으로 결정했습니다 글또를 외부에 알리기엔 좋지 않을 수 있지만, 외부보다 내실이 더 중요하기 때문에 이렇게 결정했습니다 퍼실 임명 팀별로 퍼실을 임명해 조금 더 원활한 운영이 가능하도록 변경했습니다 글 쓰기 제한 변경 1기땐 400자 이상, 800자 이상 이런 조건을 걸었는데, 이번엔 그런 조건 없이 글을 작성하면 됩니다 너무하다 싶으면(=날림) 개인적으로 연락드리기로.. 그리고 첫 글은 다짐 글, 마지막 글은 회고 글입니다 어떤 일을 하면 회고는 반드시 필요하다고 생각합니다. 1기땐 회고 글 작성을 안한 것이 아쉽네요 ;ㅁ; Pass권 추가 제공 사람들은 각자의 사정이 있기에 글을 작성하기 어려울 때가 있습니다. 1기땐 Pass권이 총 2회였는데 이번엔 총 3회입니다 마감 시간 변경 마감 시간도 일요일에서 월요일 넘어가는 자정이었는데, 이제 월요일 새벽 2시까지 작성하면 됩니다 생각보다 야행성분들이 많아서.. 그렇게 배려했습니다 다음 기수에는 또 어떤 점이 바뀔지 기대됩니다 다짐 모임 관리자 관점의 목표는 글또분들이 꾸준히 글을 쓰는 습관이 생기도록 잘 서포트하는 것입니다 첨삭, 의견 등 다양하게 드릴 예정입니다 개인적인 목표는 회사의 업무와 연관된 내용을 습득한 후, 블로그에 글을 차곡차곡 작성하는 것입니다! 현재 쏘카에서 Dynamic Pricing 문제를 풀고 있는데, 경제학적 관점에서 생각하고, 이것 저것 많은 고민을 담아 풀고 있습니다 이런 문제들을 잘 정리해서 공유할 예정입니다! 고민 매번 고민은 이 모임을 언제까지 진행할 수 있을까? 입니다 2 기수를 진행하면 나이 1살이 증가됩니다! 계속 진행하기 위해서 플랫폼을 구축해야 할까..ㅋㅋㅋㅋ 라는 생각이 있네요 그래도 일단 현 기수에 최선을 다 하고 나중에 고민하려고 합니다! 아이디어(및 실행 여력) 있으시면 제게 연락을…",
    "tags": "diary",
    "url": "/diary/2018/11/11/geultto2/"
  },{
    "title": "딥모닝 1주차. PR12-001~006",
    "text": "PR12 동영상을 하루에 1개씩 보는 “딥모닝” 스터디에서 본 영상을 정리하는 글입니다 PR-001 : Generative adversarial nets PR-002 : Deformable Convolutional Networks PR-003 : RNN encoder-decoder PR-004 : Image Super-Resolution Using Deep Convolutional Networks PR-005 : Playing Atari with Deep Reinforcement Learning PR-006 : Neural Turing Machines PR-001 : GAN(Generative adversarial nets) Generative 생성! 파인만 “내가 창조할 수 없는 것은 이해할 수 없다” 뉴럴넷이 그림을 그릴 수 있다는 것은 라벨의 Feature를 품고있는 것이고 classification은 당연히 따라옴 Unsupervised Learning에 속함 No label or curriculum =&gt; self learning 1) 볼츠만 머신 2) Auto-encoder or Variational Inference 3) Generative Adversarial Network Stacker autoencodder - SAE 자기 자신을 답으로 사용하고 L2 loss 인코딩으로 latent space에 넣고 다시 디코딩 Variational autoencoder Variational inference를 사용 우리가 알 수 있는 모델로 하한을 계산하고, 하한을 높이는 방식을 취함(Variational approximations) Likelihood는 logistic regression과 유사 =&gt; min 값을 학습 =&gt; blur Adversarial Generator, Discriminator 모델 2개 존재 모르는 Z space에 넣은 후, 우리가 가진 x와 g로 나온 아웃풋과 비교 경찰과 지폐위조범 사례 z값을 줬을 때 x 이미지를 내보내는 모델(Q)를 정의하고 실제 데이터 모델 P에 가깝도록! 왼쪽에서 점점 우측으로 이동하며 원본과 유사해짐 수식이 min max가 정말 어려워 보이지만 극단적 예시를 넣으면 쉽게 이해할 수 있음 Discriminator가 잘한다고 했을 때, D(x)=1이라 로그 부분은 0이 됨 Discriminator 입장에선 0인게 max Generator 입장에선 속이는 것이 좋기 때문에 min Two Step Approach 아래와 같은 내용을 증명해야 함 Proposition 1 a \\log(y)+b \\log(1-y)를 미분하면 a(1-y)-by=0 따라서 y = \\frac{a}{a+b} G가 fixed 되었을 때 D의 optimal 포인트는 다음과 같이 나타낼 수 있음 D_{G}^{*}(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} D에 관한 것을 껴넣고 표현! Main Theorem D가 optimal에 도달했을 때 G의 입장에서 어쩔 때 Global optima를 가지는지 확인 p_{g}=p_{data}일 경우 D_{G}^{*}(x)=\\frac{1}{2}, 대입해서 풀면 C(G)=-log(4) KL 다이버전스..! 검색해보기 C(G) : discriminator를 fix한 상태에서 g로만 dependent한 loss function Convergence of the proposed algorithm minmax problem이 global optimal을 가짐 모델 distribution이 실제 데이터 distribution과 정확히 일치할 때 뿐 D에 대해 supream(max)에 대한 loss 함수가 있고, 모든 p_{g}에 컨벡스하면 d_optimal 서드 드리프트(미분들의 set)에 포함됨 convex 문제는 미분해서 gradient descent하면 global optimal에 도달하는 것이 보장되어 있음 Gan do 이미지 생성 vector arithmetic(벡터 산수) 이미지에서도 사용 가능!(DCGAN) Rotation 되는 것처럼 이미지를 생성할 수 있음 Difficuties 생성된 모델이 잘된건지 파악하기 힘듬 Inception Score Mode Collapse min max problem때문에 생기는 문제 뉴럴넷 입장에선 max min로 볼수도 있음 Related works Super-resolution(SRGAN) Img2Img Translation(CycleGAN) Find a code(infoGan) PR-002 : Deformable Convolutional Networks Input 데이터에 대해 Transform(rotation, add noise 등)을 적용하는데 이걸 사람이 정해주는게 아닌, 자동으로 할 수 있을까?에 대한 논문 Abstract 우리의 한계는 늘 고정된 convolution filter를 사용(3by3, 5by5) 이런 것을 알고리즘이 배우게 하자! Convolution, RoI pooling을 자동으로 배워보자 Introduction data augmentation한 것도 모두 같은 label crop, rotate, flip, jitter, occlude 레이블을 바꾸지 않는 data augmentation을 알고 있어야 함 그러나 특정한 도메인(의료, 암)은 size 스케일링을 하면 안됨 Transformation-invariant feature를 찾는 것도 좋지만, 쉽지 않음 drawback : assumed fixed and known 배경, 큰 물체, 작은 물체를 학습해야 할 때 같은 필터를 사용하는 것은 비효율..! filter size를 flexible하게! 핵심 1) Convolution -&gt; deformable convolution 점을 움직일 수 있도록 하면 되지 않을까 2) RoI pooling -&gt; deformable RoI pooling RoI pooling 다양한 사이즈에서 fixed size를 얻음 Spatial Transform Network(STN) 딥마인드 인풋이 틀어진 경우 정방향으로 갖다두는 네트워크 분류하기 좋도록 classification Deformable convnet Deformable convolution p_{n}은 offset \\Delta p_{n}는 분수로 사용 interpolation하면 점을 얻을 수 있음 학습할 때 convolution layer 사용 Deformable ROI Pooling 중심 값에 대해서 진행하는데, 역시 오프셋을 도입하자 학습할 때 fully connected layer 사용 Deformable ConvNets 기존 conv layer를 살짝만 바꿔도 작동 1) conv feature를 학습 2) classification, task specific network Result 효과적으로 늘어나는 것을 볼 수 있음 구현물 Scaled MNIST에선 deformable이 더 잘됨 다른 것과 비교 STN와 비교 Linear Transform을 학습 =&gt; 한계 정면으로 바꾸는 것(interpolation), 각 픽셀에 대해 진행하니 expensive deformable convolution은 filter weight에 집중하지 않고 filter의 어떤 샘플(x)을 쓰느냐에 신경 Effective Receptive Field receptive fielld size는 루트에 비례해서 커짐 deformable은 확확 커짐 Atrous convolution 팽창하는 conv, 정방 deformable은 정방이 아닌 틀어져도 가능 Dynamic Filter 상황에 맞게 dynamic filter를 다양하게 생성 deformable은 filter가 아닌 x를 중요시 질문 연산량 STN보단 조금임 서브 네트워크 하나를 더 배우는 것 PR-003 : RNN encoder-decoder 기계번역을 위한 자연어 표현 학습 RNN Encoder-Decoder RNN을 활용해 문장 구절 학습 (다양한 길이에 적용) Encoder-Decoder라는 새로운 구조 제안 새로운 Hidden Unit 제안 Stastical Machine Translation의 성능을 개선 제안하는 RNN 모델로 기존의 SMT의 성능 개선 SMT 이외의 가능성도 보여줌 학습된 Vector가 의미와 문법을 포착 RNN sequence data를 사용 김성훈 교수님 강의 참고 기계 번역(Machine Translation) Rule-based MT : dictionary, 문법 기반의 번역(Parser, Analyzer, Transfer Lexicon) Statistical MT : 이미 번역된 문서들을 바탕으로한 통계 기반의 번역, 확률로 나타냄 Hybrid MT : Rule-based + Statistical Neural MT : Depp Learning을 활용한 번역 SMT RNN Encoder-Decoder 구조 소개 decoder : 제네레이터같은 역할, c를 포함 Hidden Unit 게이트가 2개 존재 r이 완전히 열리면 이전 state를 버림 update gate는 새로운 것을 얼마나 반영할 것인가 RNN Encoder RNN Decoder 학습 활용 SMT에 적용 RNN Encoder-Decoder에서 구한 확률 값이 feature 함수로 들어감 왜 성능이 잘 나올까? CSLM의 contribution과 많이 겹치지 않음 통계적으로 도출된 Score가 아님 출현 빈도가 많지 않은 표현들에 대한 보완이 가능 RNN Encoder-Decoder가 짧은 표현을 선호 일반적으로 BLEU 값은 짧은 표현에서 높게 나옴 이외의 활용 방안 SMT를 완전히 대체 범용적인 Phrase Representation 의미와 문법 모두 담아냄 word2vec처럼 표현, 의미를 담아냄 결론 SMT의 성능을 높일 수 있었다 기존 번역 시스템에 쉽게 적용 완전히 SMT를 대체할 수 있음 범용적인 Phrase Representation PR-004 : Image Super-Resolution Using Deep Convolutional Networks ILL-posed problem 다양한 솔루션이 존재하는 문제 저해상도 이미지로부터 고해상도 이미지를 만드는 문제 위성 영상, 의료 영상, 천체 영상 등에서 많이 사용되고 있음 적은 수의 데이터를 사용하고 있음 배경지식 평가 방식 영상이 얼마나 개선되었는지, 우리가 만드려고 하는 ground truth랑 우리가 만든 것이 얼마나 유사한지 평가하는 방식들 PSNR (Peak Signal-to-Noise Ratio) 최대 신호 대 잡음비(영상 화질 손싱정보에 대한 평가) SSIM (structural similarity index) 밝기, 명암, 구조를 조합해 두 영상의 유사도 평가 MS-SSIM (Multi-scal SSIM) 다양한 배율에 대한 SSIM 평가 IFC(Information fidelity criterion) 서로 다른 두 확률분포에 대한 의존도 평가 SISR 관련 선행 연구 Example-Based Super-Resolution 데이터셋이나 학습 모델을 구축하기 어려움 Image Super-resolution sparse를 사용해 어떻게 고해상도를 얻을까 방법론 핵심 아이디어 Convolutional Neural Networks with 3-layers super-resolution 문제에서 최초로 딥러닝을 적용 index를 바꿔서 쓰는 중, image가 Y고 ground truth가 X resolution image에서 하나의 픽셀(값)을 regression하는 문제 수식화 6분쯤 이미지 추가 conv-relu-conv-relu-conv 차이가 작으면 작을수록 PSNR이 올라감 실험 결과 나비 이미지 사례 필터 사이즈를 늘렸을 때, PSNR이 늘어남(동시에 트레인 시간도) RGB 채널을 그대로 사용하지 않고, YCbCR에 적용 리뷰 의견 Sparse-coding과 같은 맥락의 표현을 CNN으로 구현 End to End라 효율적 Single Image Super Resolution 문제에서 최초로 딥러닝 적용 모델 구조가 독창적이진 않았음 2014년 이후 모두 SRCNN을 기반으로 함 질문 patch based로 하면 바운더리 펙트로 문제가 생기는데, 학습을 한 후 파인튜닝을 하는가? 그러진 않고 inference시 전체 이미지 사용 GAN에 적용해서 사용할 수 있을까? 합성된 이미지에서 적용해 성능을 늘릴 수 있을까? Generative는 structure 정보가 없기 때문에 해상도는 늘릴 수 있지만 원점(앵커 포인트)는 틀어지는 문제가 생길 수 있음 =&gt; 진짜 고해상도 이미지인지? data augmentation 언급은 없음 2배로 해상도 늘리고 나온 아웃풋을 다시 인풋으로 넣으면 계속 해상도가 늘어날 수 있을까? 8배까지가 눈으로 봤을 때 정상적이고 16배부턴 잘 안되는듯 Accurate Image Super Resolution using Very Deep Convolutional Networks 이번엔 논문 2개 모델을 복잡하게 할 수 있을까? 방법론 Receptive field를 키우고, scale factor를 2배 3배 4배 고려 이미지 learning decay 레이어 자체를 vgg처럼 비슷하게 3x3을 20개 쌓고, 마지막에 residual 부분을 넣음 원 이미지를 학습하는 것이 아닌 경계 값만 학습 두 이미지의 차는 달라지는 부분이 없음 너무 high하지 않도록 gradient를 조절 실험 결과 경계 부분이 확실히 더 살아남 scale factor가 깊어질수록 좋아짐 리뷰 의견 Edge 부분만 학습해서 뛰어남 SRCNN을 20개의 레이어로 확장해서 표현력 증가됨 Residual learning과 adhustable gradient clipping을 통해 빠른 학습 가능 Image restoration 문제에서 input-output의 유사성을 residual learning 적용 학습 속도 대폭 개선하고, SRCNN과 비교해 확연히 개선됨 PR-005 : Playing Atari with Deep Reinforcement Learning 논문의 스토리 텔링이 엄청 좋음 모범적인 논문 Abstract First deep learning model Successfully learn control policies directly High-dimensional sensory input CNN model trained on variant of Q-learning 아타리 2600 게임 3은 사람보다 잘함 Introduction 문제 제기 High dimensional sensory input으로부터 직접 학습하고 싶음 대부분 RL은 hand-crafted feature를 가지고 했음 인사이트 컴퓨터 비전과 음성 인식에서 잘되고 있음 그냥 딥러닝을 Reinforcement에 적용하면 안됨, 그 이유는 DL은 레이블된 데이터를 사용 이걸 해결하기 위해 CNN with a variant Q-learning을 사용 대부분 딥러닝 논문은 데이터 샘플이 independent라는 것을 가정, 지금은 시퀀스 데이터를 받음 Experience replay Background 수학은 딱 1페이지 Agent, Environment State Policy : 특정 상황에서 내가 뭘 해야되는지 알려줌 Deterministic하면 바로 나오고, Stochastic하면 확률로 나옴 Value Fuction : 어떤 상태에 내가 있다면 가치가 얼마인지? Q를 학습하는 과정 Value-Based learning Q-Networks w라는 weight를 하나 더 갖는 함수로 정의, Q(s, a, w) Deep Q-Networks(DQN) 딥러닝할 때 (셔플하고) 배치로 읽는 것과 비슷한 아이디어 state를 다 저장했다가, 셔플해서 사용 이 작업으로 correlation을 제거 DQN in Atari 화면 하나만 보여주면 어디로 움직이는지 모름 화면 4개 이미지를 동시에 사용하고, 액션을 예측 Training and Stability 그 당시엔 Q가 converge될 것인가를 네트워크에서 설명하지 못했는데, 이걸 설명함 에피소드마다 리워드가 어떻게 변화하는지 보여줌, 마치 학습이 잘 안되는 것처럼 왔다갔다 함 그러나 우리는 리워드가 아니라 Q를 approximate하고 싶은 것, Q를 보니 점수가 계속 상승하고 있음 Visualizing the value functions 적과 같은 선상에 있으면 Q의 기대치가 상승 맞기 직전에서 상승 적이 없어지면 Q의 기대치가 하락 Experience Replay 네트워크를 target과 학습하려는 weight으로 분리 이 부분은 여기서 자세히 다루진 않음 이걸 진행하면 3배 높은 스코어를 얻을 수 있음 PR-006 : Neural Turing Machines 2014년에 처음 나온 논문 Tuning Machine 컴퓨터 모델 읽기 쓰기 장치 존재 프로그램이 칸을 이동하며 Read/write Discrete해서 backpropagation 불가 Neural Turing Machine 알고리즘은 못하지만 일반화를 잘 함 프로그램을 배울 수 있음 Differentiable(미분 가능한) Turing machine Sharp functions made smooth and can train with backpropagation 계산과 메모리가 분리된 뉴럴넷 뉴럴넷 : CPU, 메모리 : RAM 알고리즘의 입력과 출력을 보고 알고리즘을 배울 수 있음 복사, 반복, 정렬 알고리즘에 대한 실험 결과를 나타냄 구조 그냥 Turing 머신과 다른 점은 read/write 헤드가 여러개 있을 수 있는 점 여러 헤드가 있어서 더 효율적 RNN or LSTM RNN이나 LSTM은 메모리 수를 증가시키면 네트워크가 커지고, 계산량이 많아짐 RNN은 계속 내용이 업데이트 되서 전 내용을 쓸 수 없음 LSTM이 이걸 해결해주긴 하는데, 이 논문에선 이 아이디어를 더 앞서서 NTM을 만듬 Hidden state의 size를 증가시키기 위해 사용 Innovations 메모리와 뉴럴넷이 나뉘어짐 2014년에 Attention 메커니즘이 거의 없었음 Memory Networks에서 발전된 End-to-End Memory는 Read만 있는데 NTM은 Read, Write 둘 다 있음 Detail Addresing 주소 찾기 Content Only 위치를 대략적으로 찾음 Location Only Shift iterates from the last focus Content -&gt; interploation -&gt; Conv Shift -&gt; Sharpening Content Addressing 근사값을 찾는 과정 softmax Interpolation Location addressing contetn와 locate 방식을 얼마나 할지 정함 Convolutional Shift 얼마나 많은 지점에 영향을 받는지 계산 Sharpening 최종 주소 weight를 구함 Writing Erase 후 Add Erase w는 어디를 지울지 e는 얼마나 지울지 w가 커질수록 더 많이 지워짐 Add w는 어디를 추가할지 a는 뉴럴넷에서 나온 것 Erase후 Add하면 업데이트됨! Read w는 어디를 읽을지 Flow Input을 정렬되지 않은 리스트와 정렬된 리스트(정답값)을 받음 뉴럴넷을 통해 Addressing, Writing, Memory 연산, Reading, r_{t}를 구함 What to improve Memory management probelm : Dynamic Allocation Time Retrieval Memory in Order : Temporal Matrix Graph Algorithm for wider range of tasks Reinforcement Learning",
    "tags": "paper data",
    "url": "/data/2018/11/11/deepmorning-week1/"
  },{
    "title": "David Silver-Reinforcement Learning 1강",
    "text": "David Silver의 Reinforcement Learning 강의를 한국어로 해설해주는 팡요랩 영상을 보고 메모한 자료입니다 Introduction 강화 학습의 특징 Supervisor가 없고 오직 reward(보상) signal만 존재 Supervisor가 없다는 것은 답이 없다는 의미 목적은 사람이 정해주는데 그것을 달성하려면 어떻게 해야할까? 알아서 Reward를 maximise해라! 피드백이 늦어질 수 있음(즉각적이지 않음) 내가 지금 한 행동이 바로 보상으로 연결되지 않을 수 있음 시간이 중요 sequential한 순서가 있는 데이터 에이전트의 액션이 그 이후의 데이터에 영향을 줌 어떻게 Fitting하느냐에 따라 데이터가 달라짐 강화 학습의 예시 헬리콥터에서 스턴트 가동 backgammon에서 월드 챔피언 물리치기 투자 포트폴리오 관리 휴머노이드 로봇이 걷게 하기 아타리 게임 용어 설명 Rewards R_{t}은 scalar feedback signal 벡터가 아닌 방향성이 없고 더하고 빼는 것만 있는 값 scalar reward가 아닐 경우 rl로 풀 수 없을수도 있음 t번째 타임에 R_{t}만큼의 스칼라(숫자) 하나가 주어짐 모든 목적(Goal)은 Cumulative reward를 maximise하는 것! Reward를 잘 설계하는 것이 중요. 예를 들면 로봇이 걷는 문제에서 1초간 안넘어지면 +1, 넘어지면 -100 등을 줌! Sequential Decision Making Sequential하게 결정을 잘 해야 함! Action이 long term consequences Reward가 딜레이 될 수 있음 Agent and Environment Agent : 우리가 학습시키는 것, 뇌로 표현 Agent의 바깥에 있는 것이 모두 Environment Agent가 action을 하면 environment가 2가지 신호를 줌(reward, observation) History and State History ( H_{t} ) Agent가 observations, actions, rewards를 시간순으로 저장 사실 history는 많이 안쓰임 State 다음에 뭘 할지 결정하기 위해 쓰이는 정보들 State is the information used to determine what happens next State는 history의 함수로 표현 S_{t}=f(H_{t}) Environment State S_{t}^{e} : environment의 다음 값을 계산하기 위해 내적을 가진 표현형. 보통 Agent에게 보이지 않음 Atri Example observation reward action 1024개를(게임기 안에 있는 정보) 참고해 다음 화면을 계산 =&gt; Environment state Agent State 내가 history를 보고 다음 action을 해야하는데 필요한 정보들 다음 행동을 할 때 쓰이는 정보들 Markov State(=Information State) 어떤 State가 Markov한지 안한지? 결정할 때 바로 이전 state만 보고 결정을 할 경우 The future is independent of the past given the present S_{t}가 미래를 결정한다, State는 미래에 대한 통계치다 예를 들어 헬리 콥터를 조종하는데 현재 위치 기울어진 각도, 가속도, 바람의 세기가 주어진 경우 이 헬리콥터가 어떻게 움직여야 중심을 잡을 수 있을까? 10분 전의 상태를 기억하고 할 필요가 없음. 직전 상황이 중요 Environment state는 Markov History H_{t}는 Markov 강화학습은 거의 Markov 상태에서만 문제를 품 예시 Rat Example 전구를 키는 것, 레버를 당기는 것, 종을 울리는 것 3가지의 신호 3번째에 감전을 당할까? 치즈를 받을까? State 정의에 따라 다름 맨 앞을 무시하고 맨 뒤 3번을 보면, 3번째도 감전이겠구나! 생각할 수 있음 전구의 개수와 레버 개수 종의 개수를 State로 정의하면 치즈를 얻겠구나! 생각할 수 있음 state를 4개 모두로 하면 알 수 없음 Fully Observable Environments environment state를 볼 수 있는 환경을 fully observable하다고 함 이럴 경우 O_{t}=S_{t}^{a}=S_{t}^{e} Formally, Markov decision process(MDP)라고 부름 Partial Observability Agent state와 environment state가 같지 않음 RL Agent의 구성 요소 3가지를 모두 가지고 있을 수 있고, 하나만 가지고 있을 수 있음 1) Policy Agent의 행동을 나타냄 State를 넣으면 Action을 뱉어냄 Deterministic policy : s를 넣어주면 action가 나옴 - Stochastic policy : 각 액션별 확률을 뱉음 2) Value function 상황이 얼마나 좋은지 게임이 끝날 때까지 받을 수 있는 총 미래의 reward의 합산을 예측 현재 state가 좋은지 안좋은지를 판단 수식에 익숙해져야 함!! v_{\\pi}(s)=E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+ \\gamma^{2}R_{t+3}+... | S_{t}=s] 이 state로부터 특정 policy 파이를 따라갔을 때 받을 수 있는 총 리워드의 기대값 3) Model 환경(environment)이 어떻게 될지 예측 Enviornment의 역할 2개를 모델링 Reward 예측 State transition 예측 이 모델을 RL에서 사용할 수도 있고 아닐수도 있고 RL Agent의 분류 (1) 1) Value Based Value Function만 존재 이 친구만 있어도 Agent 역할 가능 어느 위치가 좋은지만 알면 그곳을 Go 2) Policy Based Policy만 존재 3) Actor Critic Policy, Value Function 2개 존재 RL Agent의 분류 (2) 1) Model Free 내부적으로 모델을 만들지 않고 Policy 또는 Valu Function만 가지고 행동 2) Model Based 내부적으로 모델(환경의)을 추측해서 만들어서 움직임 Learning and Planning 강화 학습이 크게 두개의 문제에 직면 Reinforcement Learning Environment를 모름. 그냥 던져짐 환경과 인터랙션을 하며 계속 개선 Planning Environment를 알고 있음 Reward가 어떻게 되는지, Transition을 알고 있음 Agent가 내부적으로 연산을 통해 다른 곳을 갈 수 있음(알파고의 몬테카를로) Exploration and Exploitation Exploration Environment에 관한 더 많은 정보를 찾음 Exploitation Reward를 maximise! 2개는 Trade-off 관계 예시 Prediction and Control Prediction 문제 Value function을 잘 학습시키는 것 Policy가 주어졌을 때 미래를 평가하는 것 State에 따른 value를 찾는 것 Control 문제 Best policy를 찾음 미래를 최적화하는 것 State에서 어떻게 움직여야 하는지를 찾는 문제 Example Maze Example Reference 팡요랩",
    "tags": "rl data",
    "url": "/data/2018/11/01/reinforcement-learning-intro/"
  },{
    "title": "Gap Year 및 쏘카 이직 이야기",
    "text": "이 글은 Gap Year 동안 있었던 일에 대해 작성한 글입니다. 더불어 이직과 관련된 내용도 포함되어 있습니다 데이터야놀자 2018 이그나이트에서 발표한 내용 기반으로 작성되었습니다(발표보다 이 글이 더 자세해요!) 작성하고 보니 글이 너무 많지만 어려운 수식은 없으니 천천히 읽어보셔도 좋을 것 같아요 :) 이 글은 웹 환경에서 최적화되어 있습니다! 목차 01 : 나는 누구인가? 퇴사 자아 성찰 02 : Gap Year 이야기 일기 공부 상담 회사 탐방 중간 점검 이직 시점 03 : 쏘카 어때요? 정리 및 맺음말 01. 나는 누구인가 퇴사 잘 다니던 회사를 2018년 4월에 퇴사했습니다 보통 이직할 회사에 합격한 후, 퇴사를 진행하는 분들이 많지만 저는 다음 회사에 대한 고민 없이 과감히 퇴사했습니다 이런 생각을 한 이유는 1) 자신에 대해 생각할 시간이 필요해서 2) 개인 공부할 시간을 확보하기 위해서 3) 취업은 언젠가 다시 할 수 있단 긍정적 마인드 자신이 나아갈 방향을 설정하는 시간인 Gap Year를 계획적으로 보내기로 다짐했습니다 특히 앞으로 어떻게 커리어를 쌓을 것인가?에 대해 계속 고민했습니다 자아 성찰 위 이미지는 퇴사하고 처음 자아 성찰을 하며 작성한 글입니다 저에 대해 간단히 말씀드리면 레트리카라는 스타트업에서 데이터 분석가로 1년 2개월간 근무했습니다 데이터 분석가였지만 데이터 엔지니어의 부재로 데이터 엔지니어링 업무까지 같이 진행했습니다 사수가 없었지만 부족한 부분은 페이스북에서 경력이 있는 분들을 직접 찾아뵙고 의견을 들어가며 채웠습니다 진행한 일들은 파이프라인 생성 및 관리, 대시보드 구축, 데이터 이벤트 설계, 데이터 QA, 데이터 분석, 머신러닝 등 폭 넓고 다양하게 일할 수 있었습니다 더 궁금하시면 [바닥부터 시작하는 데이터 인프라] 발표 자료 참고 데이터와 관련된 다양한 직군을 경험한 결과, 제 성향이 어디에 근접한지 알 수 있었습니다 저는 나와있는 이론을 구현해 실용적으로 제품에 녹이는 일을 좋아했습니다. 그 과정에서 데이터 엔지니어링, 머신러닝을 모두 즐겼습니다. 중요한 것은 해결해야 하는 문제라고 생각합니다 :) 따라서 앞으론 머신러닝 엔지니어로 커리어를 계속 나아가려고 합니다 머신러닝 엔지니어를 하기 위해 제가 부족한 부분을 생각했습니다 머신러닝 “엔지니어”이기 때문에 컴퓨터 공학적인 부분은 당연히 알면 좋다고 생각했습니다 컴퓨터 사이언스 전반(OS, 데이터 구조, 알고리즘, 네트워크 등)을 공부하기로 다짐했습니다 또한 데이터 엔지니어링(카프카, 스파크) 및 딥러닝(Tensorflow, Computer Vision) 공부도 꾸준히 하기로 다짐했습니다 (위 내용은 제 개인적인 생각이라 사람마다 다르게 생각할 수 있습니다) 어떤 회사를 갈 것인가도 계속 생각했습니다 과연 어떤 회사가 나와 맞는 회사일까? 스타트업과 대기업 B2B 기업과 B2C 기업 등등에 대해 고민했습니다 나는 왜 일을 하는가? 일을 하는 이유에 대해 구체적으로 생각해보지 않았지만, 조금 생각해봤습니다 경영학을 전공하고 광고(미디어 플래닝), 디자인, 공기업, 창업 등 다양한 업무를 진행하며 진짜 하고 싶은 일을 찾으려 했습니다 하고 싶은 일을 찾고 고민하니 해당 업계에서 정말 열심히 성장하고 싶었습니다!!!!!! 제가 일하는 이유는 성장입니다!! 02. Gap Year 이야기 일기 자아 성찰을 하고, 거의 매일 짧은 일기를 작성했습니다 같은 시기에 Gap Year를 보내는 딥러닝을 공부하는 청년백수 모임에 활동한 것이 짧은 일기를 작성할 때 도움이 되었습니다 일기의 형태보다 Daily To Do List 형태로 작성했습니다 물론 가끔은 길게 일기를 작성한 날도 있습니다! 저는 첫 회사부터 꾸준히 이렇게 일기를 작성하고 있습니다! 초반엔 업무 도메인에 대해 작성하고, 그 이후엔 오늘은 무엇을 공부했고 이런걸 찾았다 등등을 작성합니다 오랜 시간이 지난 후, 다시 보면 생각보다 내가 많은 것들을 공부했구나! 느낄 수 있습니다 :) 회사 적응하는 과정이나 고민들도 작성해서 재밌습니다(일기는 항상 다시 보면 재밌..) 일기 형태로 작성하지 않아도, 메모장에 꾸준히 작성하길 추천드리고 싶습니다. 저는 Jupyter Notebook에서 마크다운으로 슝슝 작성합니다 공부 퇴사하고 공부한 것들 중 기억나는 것만 작성하면 네이버 AI 해커톤 결선 최종 9등 : 블로그 후기 CS231n : 블로그 Python 코딩 : Github Scala : Github 데이터 엔지니어링(주로 스파크) : 블로그 OS : 블로그 네트워크 : 하루 3분 네트워크 교실 알고리즘 : 헬로 코딩 알고리즘, Github 코딩인터뷰 완전 분석 Linux 쉘 스크립트 다시 공부 : 블로그 Coursera Kaggle 강의 : 블로그 마이크로 소프트웨어 잡지 구독 전반적인 개발 지식이 부족한 제게 이 잡지는 정말 유익했어요!(다양한 주제를 다룸) 좋은 블로그 글 구독 개인 용도의 슬랙을 생성해, 좋은 블로그 글을 RSS 구독하고 있습니다 또한 인터넷에서 접할 수 있는 좋은 글들은 별도의 채널(Ex:딥러닝-자료)에 보관하고 있습니다 여기서 핵심은 제가 무엇이 공부했다가 아니라 블로그 또는 Github에 메모를 한 것이에요! 사람은 언제나 망각의 동물이기 때문에 1년 후의 저를 위해 글을 작성합니다 개발자 글쓰기 모임인 “글또”를 진행했기 때문에 더 꾸준히 글을 작성할 수 있었습니다 공부도 했지만 운동을 너무 안하는 것 같아서 PT 30회도 했습니다!!! 또한 가족과의 시간을 많이 보냈어요 상담 경영학 전공, 학부 졸업, 패스트캠퍼스 스쿨 수료 제가 가진 이력이 특이해서 많은 분들이 상담 요청을 하는 것 같습니다(인스타그램, 이메일, 페이스북 등을 통해) 그리고 대학교 취업특강, OKKY 발표 등에서 연락처를 알려드렸는데, 그 경로로도 종종 들어오고 있습니다 과거에 저도 방황했던 기억이 있기 때문에 최대한 답변을 드리고 있습니다(팩트 폭력을 많이 하기도 하지만;;;) 대부분 문의하시는 내용이 큰 맥락에서 겹쳐서 자주 묻는 질문에 대해 글을 작성했습니다 I-want-to-study-Data-Science 최신 정보도 계속 업데이트하고, 부족한 부분도 보완할 예정이에요! 여전히 도움이 필요하신 분들에게 도움을 드리고 싶습니다. 물리적 제약으로 오프라인 모임은 피하고 있지만 궁금하신 내용이 있으면 메신져로 언제든 연락주세요-! 단, 질문할 때 추상적인 질문(ex : 데이터 분야 어떻게 공부해요?)은 피해주세요. 위에 제가 작성한 문서를 먼저 보신 후, 구체적인 이야기를 해주셔야 좋은 답변을 드릴 수 있습니다 그리고 질문하시는 분의 데이터를 최대한 많이 작성해주시면(=맥락을 이해할 수 있도록) 더욱 좋습니다 회사 탐방 퇴사하고 시간적 여유가 많아 다양한 회사에 방문했습니다 IT 대기업, 통신사 대기업, 게임 회사, 스타트업 등 15곳 이상에 다녀왔습니다 “어떤 회사가 나랑 잘 맞을까?”에 대해 알고 싶었기 때문에 회사 탐방을 했습니다 구직자 입장에서도 충분히 회사에 대해 인지하고 입사해야 입사 후 괴리감이 덜할 수 있다고 생각합니다 업계의 이슈도 알 수 있었고, 여전히 데이터 업계엔 (좋은) 인력이 부족하다고 느꼈습니다 페이스북 메신저로 대표님 또는 데이터 관련 담당자분에게 연락드려 회사에 방문했습니다 이 때 그냥 놀러가는 것이 아닌, 회사에 대한 고민을 꼭 하고 가면 좋겠어요 회사의 제품은 무엇인지, 최근엔 어떤 이슈가 있는지, 특정한 앱에 대한 생각 등 이런 의견을 나누는 것은 면접 질문에 대비할 수도 있고, 대화하신 분에게 좋은 인상을 남길 수 있습니다 주로 파악한 것 회사가 무엇을 하려고 하는가? 회사의 구성원은? 팀이 존재하는가? 회사의 비전은? 회사 구성원들의 표정은? 회사에 다녀온 후 바로 느낌에 대해 작성해주세요 저는 회사 선택할 때 이 내용을 많이 봤어요!(면접 날에도 작성했어요) 중간 점검 퇴사하고 6주가 지난 시점에 중간 점검을 진행했습니다 중요한 것은 솔직한 중간 점검입니다! 저는 솔직하게 제가 욕심쟁이인 것과 프로젝트보다 공부를 더 즐긴다는 것을 인정했습니다. 인정하고 그에 맞는 계획을 다시 세우면 됩니다! 마치 Global Optima를 찾아가는 과정처럼! 이직 시점 평소에 회사 탐방을 꾸준히 했기 때문에 많은 회사에 대해 인지하고 있었습니다 회사에 대해 많은 정보가 있는 것도 중요하지만 더 중요한 것은 회사 선정시 고려할 우선 순위입니다 신념이라고도 말할 수 있습니다 사람마다 가치관이 다르기 때문에 많은 고민을 해보시면 좋을 것 같습니다 저는 아직 한참 갈 길이 먼 주니어라고 생각해서, 이번엔 팀이 있는 환경에서 일하고 싶은 생각이 있었습니다 또한 데이터쟁이니까 데이터도 재미있는지(=그 도메인에 내가 관심이 있는지)도 꼭 체크했습니다 신념과 정보를 토대로 총 9개의 회사에 지원했고, 최종적으로 4곳의 합격에 오퍼 레터를 받았습니다 그리고 많은 고민 끝에 쏘카에 입사하기로 결정했습니다 03. 쏘카 선택한 이유 VCNC를 인수하고 팀 빌딩을 단단하게 하고 있는 중 많은 사람들이 데이터의 중요성을 강조하고 이해하고 있음 저는 운전을 싫어해서 자율주행이 빨리 대중화되길 바라고 있는데, 그걸 국내에서 실현할 회사 중 하나가 쏘카라고 생각합니다 어때요? 입사하고 1달이 지났습니다. 요즘 만나는 분들에게 많이 받는 질문은 “쏘카 어때요? 성윤님은 행복하세요?”입니다 제 답변은 “엄청 재미있고, 좋아요. 행복해요!” 팀원들의 실력이 모두 좋고, 구성원 전체가 빠른 성장을 지향하고 있습니다! (팀원은 총 14명) 풀어야 하는 문제들이 대부분 엄청 재미있고 도전적입니다! 데이터 문화가 좋습니다. 쿼리를 작성할 수 있는 분들이 저희 그룹 말고도 많습니다 젊고 빠르게 일하고 있습니다! 새로운 서비스(타다)가 호평을 받고 있어 사내 분위기도 상승-! 아래부턴 1달간 찍은 소소한 사진입니다 뚝섬역에 위치한 쏘카 사무실 문 앞! 파랑파랑해서 좋아요 입사하고 받은 웰컴 키트! 간단한 다과와 캡슐 커피, 필기구 등이 있습니다! 저희 그룹장이신 김상우님의 데이터야놀자 발표 자료입니다 더 궁금하시면 링크 참고 저희가 요즘 풀고있는 문제 Zone 운영 Dynamic Pricing 타다 다양한 Task들을 진행하고 있습니다! 그 이외에도 다양한 문제가 많습니다 :) 아파치 커미터이신 이동진님 발표 Kafka Streams vs Spark Structured Streaming 이런 고퀄의 세미나를 회사에서 들을 수 있어서 정말 영광이었어요! 회사의 복지! 쏘카 이용할 때 할인을 받을 수 있습니다! 사내 모임 저는 다음 주에 처음 참여하는데, 맛있는 것을 먹거나 특별한 활동을 다른 팀분들과 함께하는 모임! 활동비가 월마다 지원되서 꾸준히 참여하면 재미있을 것 같아요! 휴가 3년 리프레시 휴가 제도를 보면 실용성이 있는가? 싶었는데 제 맞은편 자리에 계신 분이 떠나셨습니다!_! 그 외에도 건강, 교육, 전월세자금 대출 지원 등 있습니다 정리 및 맺음말 자신에 대해 생각해보고 부족한 점을 보완해보세요 회사에 대한 고민도 함께 하고, 가능하다면 회사 방문도 추천드려요 솔직한 중간 점검을 통해 궁극적인 방향으로 나아가기! 여러분은 충분히 멋진 사람이니 조급해하지 말고 여유를 가지며 하시면 좋을 것 같습니다 :) 아직도 부족한 저는 더 더 더! 노력하러 가보겠습니다! ★ 인재 모십니다 ★ 현재 쏘카에서 풀고 있는 문제 Zone Management 현재 : 데이터 기반(차량당 매출, 수요 데이터 등)을 기반으로 차량을 재분배 미래 : 머신러닝을 통해 존 관리 어시스트! Pricing 미래 : 수익성을 유지하면서 체감 가격을 떨어트리는 Dynamic Pricing! 타다 미래 : 차량 수요를 예측 궁금하시면… 회사로..! 쏘카에서 데이터 분석가, 데이터 사이언티스트, 데이터 엔지니어, 머신러닝 전문가 상시 모집하고 있습니다 데이터 분석가 Wanted 공고 : 링크 지원할 생각이 있으신 분은 제게 연락을…! kyle@socar.kr",
    "tags": "diary",
    "url": "/diary/2018/10/26/gap-year-and-socar/"
  },{
    "title": "DEVIEW 2018 2일차 후기",
    "text": "네이버 DEVIEW 2018 2일차 후기입니다 :) 입장 입장해서 받은 물건들! 물과 간단한 간식, 커피 쿠폰 쿠팡에서 준 컵케잌 이미지를 이해하는 이미지검색: 텍스트기반 이미지검색에 CNN 이용하기 조근희님 발표 자료 산티아고 순례길 검색 결과 이상적인 검색 결과가 나왔음 순례길 경로, 풍경 사진 이미지 검색 웹검색, 뉴스, 도영상과 더불어 가장 기본이 되는 검색 서비스 가장 많이 쓰는 검색 grep CTRL + F SELECT 쿼리문 위 검색은 Boolean Retrieval Model 이진 값에 근거해 순위 구분없이 연관/비연관으로 결정 높은 Recall, 색인이 필요 없음 but 소셜 데이터엔 엉망 Term Weighting Models TF-IDF : 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치 BM25 : TF-IDF와 문서의 길이까지 고려 이 모델도 여전히 제대로된 결과를 보이진 않음 Relevance Feedback 사용자 클릭 등 사용자의 피드백을 고려해 검색품질을 향상 잘못된 데이터가 나와서 클릭 로그가 오염될 수 있음 전통적인 정보검색 기법 Similarity + 문서의 Quality(조회수, 이웃수, 좋아요, 중복수, 해상도 등) / 사용자 피드백 / 문서 확장 전통적인 방법은 여전히 한계 존재 이미지 검색을 하는 이유 Why People Search for Images using Web Search Engines Explore/Learn 또는 Locate/Acquire 또는 Entertain를 위해 검색 인물 질의가 대부분이고 사진, 배경, 캐릭터 등을 찾음 연령대/성별 이미지 검색어 분석 이미지 이해를 위한 내용분석이 필요 CNN 사용해서 내용 분석! CNN 사용 질의를 class로 labeling 고양이 사진에 고양이라고 태깅 학습된 건 잘 찾지만, 텍스트 match는 NLP 영역(더 어려운 문제) CNN Feature를 사용해 이미지 유사도를 구함 n개의 대표 이미지로 m개의 후보 이미지를 Re-ranking 발표 자료 공개되면 아키텍쳐 파악하기 검색이 잘 안될 경우, 정답을 사람이 알려주는 시스템 CNN을 사용해 사용자 요구 분류 검색 필터로 보는 사용자 요구 구글, 바이두에서 사용. 아바타 사진 / 일러스트레이션 / 얼굴 / 만화 그림 / 정지 사진 등등 8개의 Label을 가진 CNN 모델 사용 실사, 색칠공부, 컬러만화, 스케치, 클립아트, 상품, 사람, 텍스트 아키텍쳐는 정말 간단하게 사용 질의, 이미지 클릭 정보를 기반으로 공통적으로 클릭한 이미지의 실제 질의를 확인 클릭 로그, 검색 컬렉션을 조인 예를 들어 색칠공부 label은 공주색칠공부, 포켓몬색칠공부, 햄스터그리기 등의 질의로 들어온 것을 확인 가능 37가지 사용자 의도 분류 모델 생성 사용자 의도를 8가지에서 37가지 Label로 확장 검색어를 주제 분류하고, 자주 나오는 쿼리로 label을 추가 악보, 도면, 테이블, 그래프, bj방송, 지도 등 클러스터링 기법 응용 1만개로 확장 High Level(100개), Low Level(10000개) 검색 피처로 활용하기 Query Time에 구현 1) Rule-based : 특정 질의의 패턴 파악 2) Topic Modeling : 주제 분류를 이용한 동적 weight 부여 3) Re-ranking : Initial search의 속성 분포 벡터를 이용해 동적 weight 부여 이미지 검색에서 중복 이미지가 웹문서보다 큼 Gray Mean-Block을 사용해 중복 제거 그러나 여전히 품질적 문제가 있어서 cnn feature를 포함해 PCA해서 추가 보완 작업 추가적으로 관심있는 Task 텍스트를 이미지처럼 검색하기 이미지를 설명하는 텍스트 찾기 TensorRT를 활용한 딥러닝 Inference 최적화 한재근님 Inference 다양한 환경에서 사용 모바일 / 데이터 센터 입력된 데이터에 대해 학습된 딥러닝 모델로 예측하는 행위 대용량 Inference를 위해 성능과 최적화에 대한 이해 필요 대용량 Inference를 위한 이해 처리 속도 외에 고려할 것들 처리량(Throughput) 응답 속도(Latency) 고효율(Efficiency) : 메모리 사용 서로 맞물려 있음 고성능과 응답 속도(GPU vs CPU) 응답시간 내에 얼마나 많은 요청을 처리할 수 있는가를 고민 GPU의 경우 16 배치까진 리니어하게 처리량이 증가 처리 속도와 정확도의 Tradeoff 연산량과 속도 작은 모델이 속도 빠름 연산량과 정확도 큰 모델이 정확도가 좋음 Inference 최적화 방향 가벼운 모델 사용 연산량이 적을수록 속도가 빠름 MobileNet, SqeezeNet, SEP-Nets 모델 압축 큰 모델의 성능 획득. TensorRT의 적용 범위 Quantization and Binarization, Network Weight PRunning and Sharing, Knowledge Distillation, Low-Rank Factorization and Sparsity TensorRT를 이용한 Inference 최적화 TensorRT 기본 디자인 Optimize, Runtime으로 나뉨 TensorRT의 일반적인 절차 Tensorflow 모델 적용 절차 TensorRT에서 제공하는 최적화 기능 성능 향상 TensorRT를 사용하면 처리량이 증대 OpenPose TensorRT Plugin Layer 적용사례 Realtime pose detection 실시간 Inference에 대한 수요 존재 TensorRT만으로 얼마나 성능 향상이 가능할까? 위 궁금증으로 OpenPose 선택 모델 성능 분석 Iteration당 실행 시간 확인 지원을 하는 레이어, 못하는 레이어가 있음 PReLU 지원 안함 TensorRT에 Layer 정보 추가하기 파싱한 후, 네트워크 정보 포함하기 Plugin Layer 개발 Optimization과 Runtime 2가지 과정에서 Plugin 사용 Layer 초기화 Enqueue Serialization Deserialization Plugin Factory 구성 Parser 연동 적용 모델 Benchmark 메모리 사용량 1/3로 줄음 Latency 1.75배 빨라짐 Throughput 초당 처리할 수 있는 수 증가 기계독해 QA: 검색인가, NLP인가? 서민준님 Question Answering 발표 자료 검색으로 찾는 QA 종합적으로 구성 내용 및 제목의 관련성 : 오늘은 이것만 다룰 예정 비슷한 검색을 한 유저가 읽은 문서 웹사이트의 신뢰도 문서의 인기도 검색자의 정보 Word Matching 검색한 단어가 존재하는 문서를 가져옴 제목에만 적용할 경우 꽤 효과적 TF-IDF 흔하지 않은 단어에 가중치 BM25 TF-IDF의 업그레이드 LSA Latent Semantic ANalysis Dense vector via SVD 단어에 추상적 태그를 달아줌 =&gt; 단어끼리 비교 가능 검색의 한계 단어 수준의 정보 습득은 가능하나 문법적 또는 의미적(semantic) 맥락은 파악 못함 문서나 문단 수준 이상으로 답 가져오기 힘듬 NLP로 읽는 QA 기계학습을 적용하기 위해 인풋, 아웃풋 정의 인풋 : 질문 아웃풋 : 답변 생성 or 추출 모델 Generative Model은 서비스 퀄리티가 안나오고 평가도 어려움(BLEU가 있긴 하지만..) 데이터 퀄리티 컨트롤이 어려움 결국 Extractive Neural Extractive QA Trend 자세한 내용은 데뷰 자료에서 참고하면 될 듯 검색과 NLP의 접점 검색 엔진이 잘못된 답을 내리면? 전단계의 에러 때문에 다음 단계에도 문제 위키피디아 560만개의 글 탐색할 때 시간이 많이 소요 Locality-Sensitive Hasing(LSH)을 통해 충돌 피함 Symmetric과 Asymmetric 문서 탐색은 이제 빨라짐 우리는 문서를 원한 것이 아님. 구문으로 변환하면? Phrase를 벡터로 모델링 비슷한 벡터 스페이스에 맵핑 수식으로 볼 경우, Decomposition이 어려움 PIQA를 1년간 삽질하셨음 자세한건 발표 자료 참고 Dense + Sparse vocab 사이즈를 크게 해서 concat하니 잘 됨 AI Serving Platform하루 수 억건의 인퍼런스를 처리하기 위한 고군분투기 현동석님 양은숙님 발표 자료 AiSP 커스텀 서버 만들기 학습하던 코드에 서버 코드를 추가해 서빙하는 방식 장점 있던 코드에 서버 코드를 추가 데이터 전후처리도 기존 코드 사용 인퍼런스 병목일 경우 서버 성능이 덜 중요 단점 비즈니스 로직과 인퍼런스가 커플링 물리 자원이 인퍼런스 성능에 최적화되지 않음 분업이 어려움 서버 자원과 인퍼런스 개별 스케일링 불가 웹서버의 기능을 직접 만들어야 함(톰캣, 아파치) 서빙용 서버 쓰기 Tensorflow Serving ZenDesk AnswerBot 데이터 처리나 비즈니스 로직도 분리해 개발, 배포 가능 DL Researcher, DL engineer로 분업 가능 인퍼런스만 떼어서 CPU, GPU 옵션 제공 단, 오직 TF만 사용해야 함 서빙 시스템 설계하기 분산 저장용에 개발용 배포용으로 나눠서 저장 LOGISS로 A/B Test AiSP 만들기 Docker 환경 지원 프론트엔드 만들기 클래스화 시켜서 저장 삽질1 : httpd + wsgi + django 리서쳐가 파이썬으로 파일 제공 그러나 이 방법은 요청 받을 때마다 죽음 gRPC로 넘길 때 stub가 리소스 해제 시점 문제로 프로세스 멈춤 해결했다고 나오지만 사실상 안됨 c++ 아파치 모듈로 구현 r.18만 적용 httpd module로 구현 삽질2 : 응답 데이터 접근하기 inception data 접근하는 방법이 나오지 않아서 아래로 해결 auth lenth = tensor.dim_size(1); auto data = reinterpret_cat&lt;cost float*&gt;(tensor.tesnor_data().data()); // 이제 data[i]로 접근 삽질3 : ArgMax 프론트엔드 서버에서 ArgMAx를 사용 불가(computation에서 진행) 그냥 만듬 딥러닝 모델 배포 플로우 tf.saved_model.builder.SavedModelBuilder(dir) 사용 SignatureDef 딥러닝 모델의 입출력을 정의하는 부분 DL 리서쳐가 정의해야 함(어디가 인풋이고 아웃풋인지 서빙에게 알려줘야 함) 하이퍼 파라미터 튜닝시 인풋으로 고려해야 함 컨테이너로 스케일 아웃 모델 관리 및 데이터 관리 사용된 데이터가 오래되면 다시 학습해야 함 주기적 업데이트 필수 추천 문서 검증하는 코드를 만들고 배포 전에 비교용으로 돌려보는 것 추천 인퍼런스 성능 측정하기 응답 시간이 균일하고, 배치 옵션으로 성능 향상 모니터링도 쉽게 가능 CPU는 임계치를 정하고 모니터링 GPU는 벤더의 진단도구(nvidia-smi) 활용 Papago Internals: 모델분석과 응용기술 개발 발표 자료 모델 연구/개발 응용 기술 연구/개발 앱/웹 서비스 개발 서버 개발 집중하는 것 품질 모델링, 전처리, 데이터 분석, 사용성 분석, CS 처리 편의성 웹 문서 번역, 오프라인 번역, 분석 도구 개발, 언어 감지기, 문장 분리기 최적화 품질 안정성 최적화, 속도 최적화, 서비스 최적화 학습과 수렴 권우님 발표 파파고 기계번역 모델 Transformer : Encoder, Decoder 학습이 매우 어렵고 불안정 따라서 학습과 수렴 개선을 위해 파파고에서 연구 연구 목표 품질 안정성 : 신뢰할 수 있는 학습 사이클 구축 연구 일반화된 모델 수렴 지점 선택 방법이 없음 다양한 학습 방법론을 실험 매 iteration마다 BLEU 점수의 추이 확인 각 언어별로도 BLEU 점수 추이 확인 그러나 BLEU 개선이 품질 개선을 보장하지 않음 전문가를 따로 섭외해 평가 추이를 함께 봄(그러나 상관관계가 낮음) 주요 연구 주제 서비스 품질 개선 서비스 속도 개선 인공신경망을 활용한 웹사이트 번역 목표 신경망 기반 번역을 통해 번역 품질 확보 후, 번역문의 적당한 위치에 원문의 태그를 삽입! Attention 적당한 위치를 찾기 위해 사용 기계 번역기가 번역 잘못하면 태그 복원 성능도 하락 실질적인 어려움 지저분한 웹 상의 문서 공백 종류가 20가지 같은 모양인데 표현 방식이 다른 경우 웹 표준을 지키지 않는 경우 번역할 부분만 뽑아내기 번역할 태그와 하지 않을 태그(스크립트) 구분 inline 태그만 고려하기 적절한 위치에 삽입 어텐션 매트릭스를 사용해 입력 토큰에 대한 출력 토큰 위치 찾음 한국어는 POS tagger로 조사 처리 빠르게 처리하기 Fronted : 번역할 사이트 작게 쪼갬 Website Translator : 번역할 텍스트와 태그 추출 / 문장 분리 및 태그 위치 기록 GPU Cluster : 분산 요청을 받아서 response Website Translator : 번역문 태그 복원 및 트리 재구성 Fronted : 렌더링 앞으로 연구 방향 기계 번역기 자체 성능 높이기 원문의 coverage를 높여 생략 줄이기 외부 지식을 활용해 고유 명사 번역 이전 문장을 고려해 다중 문장 번역 Alignment와 태그 복원 품질 높이기 phrase table을 활용한 supervised attention 입력 토큰 태그와 관련된 출력 토큰을 선택하는 휴리스틱의 다양화 캐릭터 레벨 기계 번역 모델의 어텐션 활용? NAVER 광고 deep click prediction: 모델링부터 서빙까지 발표 자료 온라인 광고의 Key Mission User의 어텐션을 얻고 싶음 유저에게 매력적인 광고를 전달 얼마나 광고를 많이 클릭할까? CTR 광고를 봤을 땐 impressions 클릭한다 안한다의 binary classification 문제로 변환 Project Mission 뿜, 네이버 뉴스 하단에 광고가 있음 아래 문제에 대한 고민 Scalability 광고는 계속 늘어나는데? Cold start problem 새로운 지면이 나올 경우엔? Case Study Facebook Ad Feature transform한 후, classification Serving 이야기 없음 Taboola Target repository에서 천개 정도의 아이템을 따로 뽑은 후(Candidate) 추천 YouTube candidate generation Google Play 얘네도 embedding 후 classification Modeling Key Idea Candidate model Embedding Embedding 그럴법한 광고 덩어리를 뽑아오면 됨 user, context 임베딩 벡터와 ad 임베팅 벡터를 비교 Feature Preprocessing Serving Key Idea Online serving 시점엔 분류 예측만 진행 Embedding은 제외 Candidate selection을 위해 최근접 이웃을 찾음 Prediction input size를 감소 Architecture Clipper Prediction Server CPU, Memory 이슈 TensorFlow Serving 사용 NN Search Spotify에서 작성한 ANNOY 구현 multiple distance metrics 지원 작은 memory 사용 다양한 프로세스간 memory 공유 전체 후기 전반적으로 재미있고 즐거웠던 행사! 이런 행사를 오면 꼭 공부해야겠단 생각을 엄청 가지고 집으로!!!!!",
    "tags": "lecture etc",
    "url": "/etc/2018/10/12/deview2018-review/"
  },{
    "title": "인프런 - 객체 지향 프로그래밍 입문",
    "text": "인프런 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다 들어가며 비용 코드 1줄 만드는데 비용이 최초 릴리즈엔 조금 들지만, 그 이후엔 비용이 많이 듬 첫 출시 이후엔, 거의 증가하지 않는 경우는 1줄 만드는 비용이 증가되는 것 예시 : 위 코드에서 아래 코드로 변경될 경우, 코드를 찾아 헤매는 시간이 점점 늘어남(비슷한 코드가 여러 곳에 있음) long start = System.currentTimeMillis(); ... long end = System.currentTimeMillis(); long elapsed = end - start; long start = System.nanoTime(); ... long end = System.nanoTime(); long elapsednano = end - start; 주요 원인 코드 분석 시간 증가 코드 변경 시간 증가 낮은 비용으로 변화할 수 있는 방법 패러다임 객체 지향, 함수형, 리액티브 코드, 설계, 아키텍처 TDD, DDD, 클린 아키텍처 업무 프로세스/문화 애자일, DevOps 객체 지향과 비용 캡슐화 + 다형성(추상화) 객체 절차 지향 데이터를 여러 프로시저가 공유하는 방식 처음엔 쉬움. 변수 선언하고, 필드 조작하고 등등 시간이 흘러갈수록 데이터를 공유하는 방식은 구조를 복잡하게 만들고, 수정을 어렵게 만드는 요인이 됨 객체 지향 데이터와 프로시저를 객체라는 단위로 묶음 특정 객체가 가지고 있는 데이터는 그 객체의 프로시저만 접근할 수 있음 다른 객체는 (다른 객체의) 데이터에 바로 접근할 수 없고, (다른 객체의) 프로시저를 호출하는 방식으로 연결 시간이 흐를수록 코드를 수정하기 수월해짐 (캡슐화에 대한 이야기) 객체 객체의 핵심 객체는 기능을 제공 내부적으로 가진 필드(데이터)로 정의하지 않음 예시) 회원 객체. 암호 변경하기 기능, 차단 여부 확인하기 기능 기능 명세 메서드(오퍼레이션)를 이용해 기능 명세 이름, 파라미터, 결과로 구성 객체와 객체 기능을 사용해 연결 기능 사용 = 메서드 호출 VolumnController volCont = new VolumnController(); volCont.increase(4); volCont.decrease(3); int currentVol = volCont.volume(); 메세지 객체와 객체 상호 작용을 메세지를 주고 받는다고 표현 메서드를 호출하는 메세지, 리턴하는 메세지, 익셉션 메세지 등 캡슐화(Encapsulation) 데이터 + 관련 기능 묶기 객체가 기능을 어떻게 구현했는지는 외부에 감춤 구현에 사용된 데이터의 상세 내용을 외부에 감춤 정보 은닉(Information Hiding) 의미 포함 외부에 영향 없이 객체 내부 구현 변경 가능 캡슐화하지 않으면 요구사항의 변화가 데이터 구조 및 사용에 변화를 발생 데이터를 사용하는 코드 A, B, C 수정 필요 요구사항 예시 자익 사용자에게 특정 기능 실행 권장 연장 계정 차단하면 모든 실행 권한 없음 Data를 LocalDateTime으로 변경 예시 왼쪽 코드는 변경되지 않음. Account 클래스 내부만 변경됨 캡슐화는 연쇄적인 변경 전파를 최소화 요구 사항의 변화가 내부 구현을 변경 캡슐화된 기능을 사용하는 코드 영향 최소화 캡슐화를 잘하면 기능에 대한 이해를 잘할 수 있음! 캡슐화를 위한 규칙 Tell, Don’t Ask 데이터를 달라하지 말고 해달라고 하기 데이터를 가지고 와서 확인하지 말고 데이터를 확인을 해주세요! Demeter’s Law 메서드에서 생성한 객체의 메서드만 호출 파라미터로 받은 객체의 메서드만 호출 필드로 참조하는 개체의 메서드만 호출 정리 캡슐화 : 기능의 구현을 외부에 감춤 캡슐화를 통해 기능을 사용하는 코드에 영향을 주지 않고(또는 최소화) 내부 구현을 변경할 수 있는 유연함을 가지게 됨 캡슐화 연습 1번째 예시 public AuthResult authenticate(String id, String pw) { Member mem = findOne(id); if (mem == null) return AuthResult.NO_MATCH; if (mem.getVerificationEmailStatus() != 2) { return AuthResult.NO_EMAIL_VERIFIED; } // 판단을 바꿔보기! // if(!mem.isEmailVerified()) { // return AuthResult.NO_EMAIL_VERIFIED; //} // public class Member { // private int verificationEmail Status; // public boolean isEmailVerified() { // return verificationEmailStatus == 2; // } if (passwordEncoder.isPasswordValid(mem.getPassword(), pw, mem.getId())) { return AuthResult.SUCCESS; } return AuthResult.NO_MATCH; } 2번째 예시 데이터를 들고 있는 쪽에 기능을 추가하며, 기능에 필요한 다른 값을 파라미터로 받는 예시! 3번째 예시 4번째 예시 데이터를 가지고 와서 판단한 후, 판단의 결과로 데이터를 다시 바꾸는 코드 추상화 다형성 여러(poly) 모습(morph)을 갖는 것 객체 지향에서는 한 객체가 여러 타입을 갖는 것 즉 한 객체가 여러 타입의 기능을 제공 타입 상속으로 다형성 구현 하위 타입은 상위 타입도 됨 예시 추상화 데이터나 프로세스 등을 의미가 비슷한 개념이나 의미있는 표현으로 정의하는 과정 두 가지 방식의 추상화 특정한 성질, 공통 성질(일반화) 간단한 예 DB의 USER 테이블 : 아이디, 이름, 이메일 Mony 클래스 : 통화, 금액 프린터 : HP, 삼성 GPU : 지포스, 라데온 서로 다른 구현 추상화 SCP 파일 업로드, HTTP 데이터 전송, DB 테이블에 삽입 이것들을 추상화하면 푸시 발송 요청으로 표현 가능 타입 추상화 여러 구현 클래스를 대표하는 상위 타입 도출 흔히 인터페이스 타입으로 추상화 기능에 대한 의미를 제공하고, 구현은 제공하지 않음 추상화 타입과 구현은 타입 상속으로 연결 EmailNotifier, SMSNotifier, KakaoNotifier 등이 콘크리트 클래스 추상 타입 사용 추상 타입을 이용한 프로그래밍 추상 타입은 구현을 감춤 기능의 구현이 아닌 의도를 더 잘 드러냄 추상 타입의 이점 : 유연함 콘크리트 클래스를 직접 사용하면 통제 방식이 바뀌면서 코드가 점점 변경됨 공통점을 도출해 추상 타입 사용 추상화는 의존 대상이 변경하는 시점에 진행 추상화 -&gt; 추상 타입 증가 -&gt; 복잡도 증가 아직 존재하지 않는 기능에 대한 추상화는 주의! 잘못된 추상화 가능성이 있고, 복잡도만 증가 실제 변경 및 확장이 발생할 때 추상화 시도 추상화를 잘 하려면 구현을 한 이유가 무엇 때문인지 생각하기 추상화 예제는 강의 참고! 상속보단 조립 상속 상위 클래스의 기능을 재사용, 확장하는 방법으로 활용 그러나 상속을 통해 기능 재사용의 단점 상위 클래스 변경 어려움 상위 클래스를 조금만 변경해도 하위 클래스가 모두 영향을 받음 클래스 증가 새로운 조합이 생기면 하위 클래스가 증가 어떤 것을 상속 받아야 하는지 애매해짐 상속 오용 불필요한 기능까지 모두 상속되서 꼬일 수 있음 조립 상속의 단점 해결 방법 여러 객체를 묶어서 더 복잡한 기능을 제공 보통 필드로 다른 객체를 참조하는 방식으로 조립 또는 객체를 필요 시점에 생성/구함 상속보다는 조립(Composition over inferitance) 상속하기에 앞서 조립으로 풀 수 없는지 검토 진짜 하위 타입인 경우에만 상속 사용 기능과 책임 분리 기능 분해 기능은 하위 기능으로 분해 기능은 곧 책임 분리한 각 기능을 알맞게 분배 하위 기능 사용 큰 클래스, 큰 메서드 클래스나 메서드가 커지면 절차 지향의 문제 발생 큰 클래스 : 많은 필드를 많은 메서드가 공유 큰 메서드 : 많은 변수를 많은 코드가 공유 여러 기능이 한 클래스/메서드에 섞여 있을 가능성 책임에 따라 알맞게 코드 분리 필요 책임 분배/분리 방법 패턴 적용 계산 기능 분리 외부 연동 분리 조건별 분기는 추상화 패턴 적용 전형적인 역할 분리 간단한 웹 컨트롤러, 서비스, DAO 복잡한 도메인 엔티티, 밸류, 리포지토리, 도메인 서비스 AOP Aspect(공통 기능) GoF 팩토리, 빌더, 전략, 템플릿 메서드, 프록시/데코레이터 등 계산 분리 포인트를 계산하는 부분을 별도의 클래스로 사용 연동 분리 네트워크, 메시징, 파일 등 연동 처리 코드 분리 조건 분기는 추상화 연속적인 if-else는 추상화 고민 역할 분리시 주의할 점 의도가 잘 드러나는 이름을 사용해야 함 HTTP로 추천 데이터 읽어오는 기능 분리시 RecommendService &gt; HttpDataService 역할 분리와 테스트 역할 분리가 잘 되면 테스트도 용이 의존과 DI 의존 기능 구현을 위해 다른 구성 요소를 사용하는 것 의존의 예 : 객체 생성, 메서드 호출, 데이터 사용 의존은 변경이 전파될 가능성을 의미 의존하는 대상이 바뀌면 바뀔 가능성이 높아짐 예 : 호출하는 메서드의 파라미터가 변경 예 : 호출하는 메서드가 발생할 수 있는 익셉션 타입이 추가 순환 의존 변경 연쇄 전파 가능성 존재 클래스, 패키지, 모듈 등 모든 수준에서 순환 의존이 없도록 의존하는 대상이 많다면? 의존하는 대상이 많으면 변경될 확률이 높음 의존하는 대상은 적을수록 좋음 의존 대상이 많을 경우 1) 기능이 많은 경우 한 클래스에서 많은 기능을 제공하는 경우 각 기능마다 의존하는 대상이 다를 수 있고 한 기능 변경이 다른 기능에 영향을 줄 수 있음 기능 별로 분리 고려!! 클래스 개수는 증가하나 클래스마다 필요로 하는 의존이 적어짐 2) 묶어보기 몇 가지 의존 대상을 단일 기능으로 묶어서 생각하면 의존 대상을 줄일 수 있음 기능 구현을 추상화! 의존 대상 객체를 직접 생성하면? 생성 클래스가 바뀌면 의존하는 코드도 바뀜(추상화 때 언급) 의존 대상 객체를 직접 생성하지 않는 방법 팩토리, 빌더 의존 주입(Depedency injection) 서비스 로케이터(Service Locator) 의존 주입(Depedency Injection) 외부에서 의존 객체를 주입 생성자나 메서드를 이용해 주입 조립기 조립기가 객체 생성, 의존 주입을 처리 예 : 스프링 프레임워크 DI의 장점 1) 상위 타입을 사용할 경우 의존 대상이 바뀌면 조립기(설정)만 변경하면 됨 2) 의존하는 객체 없이 대역 객체를 사용해서 테스트 가능 DI를 습관처럼 사용하기 의존 객체는 주입받도록 코드 작성하는 습관! 지금은 아니여도 점점 사용해보기 다음 학습 추천 강의 복습 TDD (개발 속도, 좋은 설계 가능성 높여줌) 함수형 프로그래밍 기초 (비용을 낮춰주는 다른 방법) 각 패러다임의 설계 패턴(지식/지혜 재사용) UML(도식화) 부록(DIP) 고수준 모듈, 저수준 모듈 고수준 모듈 의미있는 단일 기능을 제공 상위 수준의 정책 구현 저수준 모듈 고수준 모듈의 기능을 구현하기 위해 필요한 하위 기능의 실제 구현 그러나 고수준이 저수준에 직접 의존하면 저수준 모듈 변경이 고수준 모듈에 영향 고수준 정책이 바뀌지 않았으나 저수준 구현 변경으로 코드 변경 발생 Dependency Inversion Principle 의존 역전 원칙 고수준 모듈은 저수준 모듈의 구현에 의존하면 안됨 저수준 모듈이 고수준 모듈에서 정의한 추상타입에 의존해야 함 고수준 관점에서 추상화 고수준 입장에서 저수준 모듈을 추상화 구현 입장에서 추상화하지 말 것 DIP는 유연함을 높임 고수준 모듈의 변경을 최소화하면서 저수준 모듈의 변경 유연함을 높임 부단한 추상화 노력 필요 처음부터 바로 좋은 설계가 나오진 않음 요구사항/업무 이해가 높아지며 저수준 모듈을 인지하고 상위 수준 관점에서 저수준 모듈에 대한 추상화 시도 Reference 인프런, 객체 지향 프로그래밍 입문",
    "tags": "python development",
    "url": "/development/2018/09/09/oop/"
  },{
    "title": "Advanced Feature Engineering with Kaggle",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler, Feature engineering part1, 2를 듣고 정리한 내용입니다 추천 글 : Feature Engineering 기본 정리 변수 타입별 처리, missing value 처리 관련 내용은 Coursera Kaggle 강의(How to win a data science competition) 1주차 참고 Feautre Engineering Mean encodings Statistics and distance based features Matrix factorizations Feature Interactions t-SNE Mean encodings Powerful technique! 동의어 : likelihood encoding, target encoding Concept of mean encoding Mean Encoding Categorical Feature를 기준으로 하여 Target값과 통계적인 연산을 하는 것 Median, Mean, Std, Min, Max 등의 Feature 추가 Using target to generate features Moscow는 총 5개가 있고, target에 1인 값이 2개라서 0.4 Tver는 총 5개 있고, target에 1인 값이 4개라서 0.8 왜 이게 작동하는가? 1) Label encoding은 순서없이 random. target에 대한 correlation이 없음 2) mean encoding은 0부터 1까지로 나뉘어 Target값과 직접적 연관성이 생김 짧은 tree여도 더 나은 loss를 얻음 사용 예시 Tree depth를 증가시킬 때, 계속해서 점수가 증가하고 validation score도 같이 증가하면(오버피팅이 아니라는 뜻) Tree model이 분할에 대한 정보가 더 필요한 상황! 이럴 때 Mean Encoding이 사용 오버피팅! 우선 오버피팅을 잘 커버해야 함(정규화 필요) 아래와 같은 데이터여서 오버피팅 Regularization Mean encoding 자체는 유용하지 않을 수 있음. 데이터 정규화 필요 Regularization Training data에서 CV loop 돌리기 Smoothing(카테고리 수를) Random noise 추가 Sorting and calculating expanding mean CV loop Robust하고 직관적 보통 4-5fold를 진행하면 괜찮은 결과 LOO같은 극단적 상황을 조심할 필요가 있음 카테고리 수가 너무 작은 경우엔 오버피팅 Smoothing 하이퍼 파라미터 : 알파 보통 알파는 카테고리 사이즈와 동일할 때, 신뢰할 수 있음 Noise 노이즈가 encoidng의 quality를 저하시킴 얼마나 noise를 추가해야 할까? 고민해야 해서 불안정하고 잘 사용하기 어려움 보통 LOO와 함께 사용됨 Expanding mean 보통의 Mean ecnoding은 각 category 변수에 하나의 값으로 나오지만, expanding mean은 uniform하지 않음 leak이 적고 catboost에서 많은 성능 향상을 가져옴 Extensions and generalizations Regression and multiclass Regession엔 median, percentiles, std, distribution bins 등을 추가 (통계적 정보) multiclass엔 클래스만큼 다른 encoding. 각 클래스 encoding마다 다른 정보를 제공해 좋은 결과를 예상 Many-tomany relations cross product 아래의 예는 APP_id로 나눔 Time series Limitation Complicated features Rolling Interactions and numerical features Tree의 상호 작용을 분석 Tree Model이 Split할 때 feature1과 feature2 처럼되면 서로 상호작용하! 이런 분할이 많을수록 Mean Encoding시 좋음 Feature끼리 합하여 Target Mean Encoding 수행 Statistics and distance based features 1개의 feature를 group by 해서 다양한 통계값을 계산 More features How many pages user visited Standard deviation of prices Most visited page Many more.. Neighbors Explicit group is not needed More flexible 구현하기 힘듬 집의 개수, 평당 평균 가격, 학교/슈퍼/주차장의 개수, 가까운 지하철 역과 거리 KNN features in Springleaf 모든 변수에 대해 mean encoding 모든 포인트에 2000개의 이웃을 찾음(Bray-Curtis metric 사용) 그 2000개 이웃으로부터 다양한 feature 계산 5, 10, 15, 500, 2000의 Mean target 10개의 가까운 이웃과의 Mean distance Target 1에 대한 10개의 가까운 이웃과 mean distance Target 0에 대한 10개의 가까운 이웃과 mean distance Matrix factorizations for Feature Extraction 추천 시스템에서 자주 사용하는 matrix factorization으로 feature 생성 가능 위 방법처럼 feature를 혼합해 사용하기도 함 PCA 할 때, 전체 데이터를 concat한 후 진행하기! 차원 축소와 feature extraction할 때 유용한 방법 현실의 카테고리컬 feature들을 변경하는 방법 Feature Interactions One Hot 인코딩 후, 각 컬럼별로 곱함 곱, 합, 차, 나누기 등을 사용해 상호작용! t-SNE Manifold learning methods 데이터가 갖고 있는 변하지 않는 고유 특성(invariant properties)을 찾는데 사용 데이터 본연의 geometric 특성을 유지하며 고차원의 데이터를 저차원의 공간으로 projection 관련 내용은 ratsgo님의 t-SNE 참고 KNN features implementation 코드 참고 Reference Coursera How to win a data science competition Competitive-data-science Github Feature Engineering 기본 정리",
    "tags": "kaggle data",
    "url": "/data/2018/09/08/feature-engineering/"
  },{
    "title": "Kaggle Tip 및 대회 후기",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler Week4~5 내용을 정리한 내용입니다 Tips and tricks 5명의 강사분들이 경험한 내용을 전달! 누군가에겐 필요없는 내용일 수 있지만 도움이 되는 내용이 더 많을 듯 섹션별로 다른 강사가 이야기한 내용 대회에 참여하기 전 목표 설정하기 참여해서 얻고싶은 것은 무엇인가? 1) 흥미로운 문제에 대해 배우고 싶다 포럼에 토론이 많은 대회 추천! 주 관심사에 맞는 대회가 나오면 추천! 2) 새로운 software 툴을 사용하고 싶다 Tutorial이 있는 대회 추천! 3) 메달을 취득하고 싶다 참가자들의 제출 횟수를 파악 100회가 넘으면 아마 inconsistency of validation 횟수가 적으면 시도해볼 가치가 있음! 이 목표에 따라 참여할 대회를 결정할 수 있음 아이디어를 토대로 작업 1) 아이디어를 구조화 포럼을 읽고 흥미로운 것을 모아두기 2) 가장 중요하고 유망한 아이디어 선택 3) 왜 그것이 되거나(되지 않거나)를 이해해보기 하이퍼 파라미터 모든 것이 하이퍼 파라미터 Feature의 개수, 그라디언트 부스팅의 depth, cnn의 layer 수 등… 모든 파라미터를 아래 항목으로 정렬 1) 중요도 2) 실행 가능도 3) 이해도 Data loading csv/txt 파일을 hdf5/npy로 변경(빠르게 로딩하기 위해) default data는 64-bit array로 저장되어있음! 32-bits로 downcast해서 메모리를 2배 절약! 거대한 데이터셋은 chunks로 작업을 할 수 있음 chunks 사용시 메모리를 많이 사용하지 않음 Performacne 평가 추가적인 validation이 항상 필요한 것은 아님 처음엔 full cross validation loop 대신 simple split Full cv는 정말 필요할 때 진행 가장 빠른 모델로 시작(LightGBM) 모델 튜닝, 샘플링, 스태킹 등은 FEature engineering이 만족한 후에 진행 빠르고 더럽지만 항상 좋은 방법 코드 퀄리티에 신경쓰지 말기 Simple을 유지하기 : 중요한 것을 아끼기 컴퓨팅 리소스가 불편하면 큰 서버를 빌리기 초기 파이프라인 간단한(심지어 원시적인) solution부터 시작 전체 파이프라인을 디버깅 read data ~ writing submission 커널 또는 대회 주최자가 제공한 baseline을 읽고 완벽하게 숙지 From simple to complex 처음 시작할 땐 GBDT보다 파라미터를 튜닝할 필요가 없는 랜덤 포레스트를 선호 Best Practices from Software Development 좋은 변수 이름을 사용 코드가 읽기 어려우면 조만간 문제가 생길 수 있음 연구를 재생산 가능하도록 유지 랜덤 Seed 고정 Feature를 어떻게 생성했는지 작성하기 버전관리하기 코드 재사용 Train과 Test에서 feature 생성시 같은 코드를 사용하기 Read papers ML과 관련된 아이디어 ex) How to optimize AUC 도메인과 친해질 방법 특히 feature 생성시 유용함 My pipeline 포럼을 읽고 커널을 검사 항상 토론이 발생! 데이터의 버그가 있어 데이터가 모두 바뀔 수 있음. 그래서 대회 초창기에 참여하지 않는다고 함 EDA과 베이스라인 시작 데이터를 정확하게 load validation score가 안정한지 check Feature 추가 처음엔 만든 모든 변수를 추가 모든 변수를 한번에 평가함 하이퍼파라미터 최적화 처음엔 트레인 데이터를 오버피팅할 수 있는 파라미터를 찾음 그리고 모델을 다듬음 코드 관리 가장 중요한 것은 재생산! 코드를 깔끔하게 유지 긴 실행 history 피하기 global variables가 있을 수 있음 주기적으로 노트북 재부팅 변수 이름을 잘 정하기 One notebook per submission 그리고 git 사용하기 submission을 만들기 전에 커널을 restart test/val 처음에 train, test를 read한 후, train, val로 나누고 그 파일을 로컬에 저장해두기 매크로 사용 import numpy 같은 것을 귀찮으니 매크로로 편하게 사용 참고 노트북 Custom library 직접 구현해야 하는 경우 Out-of-fold predictions Averaging The Pipeline Understand the problem (1 day) Exploratory analysis (1-2 days) Define cs strategy Feature engineering (unitl last 3-4 days) Modeling (until last 3-4 days) Ensembling(lasy 3-4 days) 처음엔 커널이나 외부 정보를 전혀 받지 않고 스스로 문제해결 시도 그 후 다른 사람들의 커널을 보면 더 새로운 결과를 얻을 수 있음 문제를 넓게 이해하기 문제의 유형 이미지 인식, 음성 인식, 최적화 문제, tabular, 타임시리즈 등 데이터가 얼마나 큰가? 데이터가 얼마나 필요한가? 필요한 하드웨어 CPU, GPU, RAM, Disk space 딥러닝의 경우 GPU 필요 필요한 소프트웨어 TF, sklearn, Lightgbm, xgboost 아나콘다, virtual 환경 생성 테스트할 metric이 무엇인가? regression, classification, rmse, mae, … 유사한 대회를 찾아 정보를 찾아봄 EDA 변수들을 plot Train과 test에서 유사한 feature를 check 시간대별 feature, target별 feature plot 시간이 지나며 target이 변하는지 check Feature와 Target의 연관성 탐색 Feature간의 상호 관계 파악 Cross validation 전략 매우 중요 Time 관련 데이터가 중요하면 시간으로 데이터 나누기! Time-based validation 항상 과거 데이터로 미래를 예측해야 함 train보다 다른 entity가 있는 경우 Stratified validation 완벽하게 데이터가 random일 경우 Random validation(random K-fold) 위 사례의 Combination Feature engineering Data cleaning and preparation Handle missing value Generate new feature 다른 문제들은 다른 feature engineering이 필요! 계속 하다보면 반복되는 패턴도 있어, 자동화를 어느정도 할 수 있음 Modeling reference로 참고할 뿐 이게 진리는 아님!! Ensembling 단순 평균 ~ 멀티레이어 스태킹 등 다양한 방법으로 결합 협업 팁 대회 후기 Crowdflower Competition 최종 등수 : 2위 대회 목표 : 검색 결과의 적절성을 측정 Metric Quadratic weighted kappa Typical valu range 0 ~ 1 3 important points about data query들은 매우 짧음 unique query는 261개 쿼리는 train과 test에 동일 Solution Text features Similarities(유사도) query, title, description (query, title), (query, description) pair로 계산 matching 단어의 개수 TF-IDF의 코사인 거리 평균 word2vc의 거리 Levenshtein 거리 Symbolic n-grams Extending of queries Per-query models Sample weighting Bumper features Ensemble Kappa optimization Conclusion Key points Symbolic n-grams Expansion of queries Optimization of thresholds for Kappa Springleaf Marketing Response 최종 등수 : 3등 Feature packs Processed dataset data cleaning과 feature engineering Basic dataset Basic data cleaning and feature engineering Mean-encoded dataset Projecting features into homogeneous space KNN dataset distance features on mean-encoded Out-of-fold predictions xgboost oof predictions(meatafeatures) should be diverse each meatafeature should bring ‘new’ information about Y neural net StandardScaler, Ranks, Power Reference Coursera How to win a data science competition Competitive-data-science Github",
    "tags": "kaggle data",
    "url": "/data/2018/09/04/kaggle-tips-and-review/"
  },{
    "title": "Kaggle 동영상 강의 Week4 - Hyperparameter Optimization",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler Week4 : Hyperparameter Optimization 부분을 듣고 정리한 내용입니다 Hyperparameter Optimization 이번에 배울 내용 Hyperparameter tuning in general General pipeline Manual and automatic tuning What should we understand about hypterparameters? Model, libraries and hyperparameter optimization Tree-based models GBDT : XGBoost, LightGBM, CatBoost RandomeForest/ExtraTrees Neural networks Pytorch, Tensorflow, Keras Linear models SVM, logistic regression Vowpal Wabbit, FTRL Factorization Machines libFM, libFFM 그러나 이 부분은 수업에서 다루진 않지만 자료 찾아보기 Hyperparameter 튜닝하는 방법 1) 가장 큰 영향력을 가진 파라미터를 선택 모든 파라미터를 튜닝할 순 없음 2) 파라미터가 정확히 training에 어떤 영향을 미치는지 이해하기 이 파라미터가 변하면 어떤 일이 일어나는가? 3) Tune them! a. Manually (change and examine) b. Automatically (hyperopt, etc..) Hyperopt Scikit-optimize Spearmint GPyOpt RoBO SMAC3 def xgb_score(param): # run XGBoost with parameters 'param' def xgb_hyperopt(): space = { 'eta' : 0.01, 'max_depth' : hp.quniform('max_depth', 10, 30, 1), 'min_child_weight' : hp.quniform('min_child_weight', 0, 100, 1), 'subsample' : hp.quniform('subsample', 0.1, 1.0, 0.1), 'gamma' : hp.quniform('gamma', 0.0, 30, 0.5), 'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.1), 'objective':'reg:linear', 'nthread' : 28, 'silent' : 1, 'num_round' : 2500, 'seed' : 2441, 'early_stopping_rounds' : 100 } best = fmin(xgb_score, space, algo=tpe.suggest, max_evals=1000) Color-coding legend Underfitting (bad) Good fit and generalization (good) Overfitting (bad) 파라미터의 2 종류 red 파라미터 증가시 fitting을 방해 파라미터 증가시 오버피팅을 감소 파라미터 증가시 모델의 자유를 감소 제약 조건이 많아지며 오버피팅에서 언더피팅으로 모델을 바꿈 green 파라미터 증가시 train set에 더 나은 fit 모델이 underfit이면 파라미터를 증가 모델이 overfit이면 파라미터를 감소 언더피팅에서 오버피팅으로 모델을 바꿀 수 있음 green 파라미터를 더 사용 Tree-based models GBDT XGBoost LightGBM CatBoost : 이건 따로 공부해보기 RandomForest, ExtraTrees scikit-learn Others RGF(baidu) : Regulized Greedy Forest 그러나 아직 사용하기 어렵고 느림 작은 데이터에 시도해보기 GBDT 두 모델 모두 Tree를 생성한 후, given objective를 최적화 max_depth : Tree의 최대 depth, 증가할수록 train set에 더 빠르게 fit, 처음엔 7로 두고 해보기! (lightGBM) num_leaves : Tree가 매우 깊을 경우 이걸 조정하면 좋을 수 있음 subsample : 일종의 정규화를 도와줌 colsample_bytree, colsample_bylevel : 만약 모델이 오버피팅같으면 이 값을 줄이면 됨 min_child_weight : 이걸 증가하면 모델이 보수적이 됨, 가장 중요한 파라미터 중 하나. 데이터에 따라 다르지만, 넓게 범위를 잡는 것을 두려워하지 말기! eta : gradient descent 같은 필수적인 learning weight num_rounds : 얼마나 learning step을 수행할 것인가(=얼마나 트리를 만들 것인가) sklearn.RandomForest/ExtraTrees ExtraTrees는 Randomfrest의 더 랜덤한 버전. 파라미터는 동일 N_estimators : 트리 개수. 처음엔 이 값을 작은 값부터 큰 값까지 설정한 후, accuracy를 측정! 그래프를 통해 추론. 보통 50? max_depth : xgboost와 다르게 none 설정 가능(unlimited depth) 보통 7부터 시작 추천 min_samples_leaf : 정규화, min_child_weight와 유사 criterion : 지니 또는 엔트로피 Neural net models Framework Keras, Tensorflow, MxNet, PyTorch Keras, PyTorch 추천 여기선 Dense layer 이야기만 함(fully connected layer로 연결된) simple 레이어로 시작하기! Optimizers : SGD + momentum이 빠르게 수렴하긴 하지만 오버피팅일 수 있음 Batch size : 32 또는 64로 시작 Regularization : Dropout을 각 레이어의 마지막에 추가하거나 네트워크의 끝쪽에 추가 Static dropconnect 첫 hidden layer를 굉장히 큰 units으로 구성 정규화하기 위해 랜덤하게 99%를 drop Linear modesl Scikit-learn SVC/SVR Sklearn이 libLinear와 libSVM을 랩핑 멀티코어를 사용하려면 직접 컴파일 LogisticRegression/LinearRegression + regularizers SGDClassifier/SGDRegressor Vowpal Wabbit FTRL SVC에서 C를 천천히 상승 L1은 feature selection할 때 사용 L1/L2/L1+L2는 각각 모두 시도 Tips 하이퍼파라미터 튜닝에 너무 많은 시간을 쏟지 말기 더 이상 아이디어가 없거나 여분의 계산 리소스가 있는 경우에만 시도하기 참고 견디자 GBDT 또는 신경망을 수천번 돌려야 할 수도 있음 모든 것을 평균 파라미터도 평균! 참고 자료 Tuning the hyper-parameters of an estimator (sklearn) Optimizing hyperparams with hyperopt Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python Reference Coursera How to win a data science competition Competitive-data-science Github",
    "tags": "kaggle data",
    "url": "/data/2018/09/03/how-to-win-a-data-science-competition-week4/"
  },{
    "title": "Google Colab 사용하기",
    "text": "Google의 Colab 사용법에 대해 정리한 글입니다 이 글은 계속 업데이트 될 예정입니다! 목차 UI 상단 설정 구글 드라이브와 Colab 연동 구글 드라이브와 로컬 연동 Tensorflow 2.0 설치하기 PyTorch 사용하기 KoNLPy 설치 Github 코드를 Colab에서 사용하기 BigQuery 사용하기 Matplotlib에서 한글 사용하기 TensorBoard 사용하기 JDK 설치하기 Google Storage에서 파일 읽기 MNIST on TPU 소스 Kaggle 연동하기 코기 모드 런타임 연결 Google Colab 풀 네임은 Google Colaboratory Google Drive + Jupyter Notebook Google Drive처럼 협업 가능(동시에 수정 가능) https://colab.research.google.com/로 접속시 사용 가능 컴퓨터 사양(19년 12월 기준) Ubuntu 17.10 CPU 제논 2.3GHz 메모리 13G GPU : K80 또는 T4 : TPU도 사용 가능 GPU 사용시 최대 12시간 Github의 소스 코드를 Colab에서 사용 가능 UI 붉은 부분 해당 노트북의 목차 코드 스니펫 재사용 가능한 소스 코드로 다양한 예제 코드가 있음 Uploading files from your local file system, Using BigQuery, Listing files in Google Drive, Install library, etc 연결된 파일 노란 부분 헤더 보이기 유무 파란 부분 코드 부분 상단 설정 새 Python2, 3 노트 : Notebook 파일 생성(GPU 없는 설정) 노트 업로드 : 로컬에 있는 노트북 업로드 Github Gist에 사본 저장 : Gist에 Secret으로 저장(처음에 연동 설정 필요) Github에 사본 저장 : Github Repo의 특정 브랜치에 저장 런타임 유형 변경 클릭시 GPU 사용 가능 Colab 사용하기 OS 확인 !cat /etc/issue.net 하드웨어 사양 CPU !cat /proc/cpuinfo Memory !cat /proc/meminfo Disk !df -h GPU !nvidia-smi 구글 드라이브와 Colab 연동 from google.colab import auth auth.authenticate_user() from google.colab import drive drive.mount('/content/gdrive') colab에서 구글 드라이브 권한 획득 위 명령어 복사 붙여넣기 그 후 나오는 URL로 접속한 후, verification code 입력 단, 매번 이 작업을 해줘야 함….(일정 시간 이후엔 끊김) colab에서 drive란 폴더를 만든 후, 우리 구글 드라이브의 root와 drive 폴더를 연결(mount) !cd gdrive/data; ls-al; 구글드라이브의 root에 data란 폴더가 있었음 train_activitiy.csv 데이터를 읽어오기 import pandas as pd df = pd.read_csv(\"./gdrive/data/train_activity.csv\") 만약 apt-key output should not be parsed (stdout is not a terminal)란 Warning이 나오면 이미 인증이 완료되었다는 뜻이므로 바로 mount하면 됨 구글 드라이브와 로컬 연동 파일을 하나씩 업로드하지 말고 대량의 파일을 한꺼번에 업로드하고 싶은 경우 BackupAndSync를 사용해 로컬과 구글 드라이브를 연동 Dropbox처럼 내 로컬의 특정 폴더를 연동 맥북 환경에서 진행 위 링크를 클릭해 백업 및 동기화 다운로드 InstallBackupAndSync.dmg라는 파일을 클릭한 후, (열리지 않으면 우클릭 후 열기) 프로그램 설치 맥북 환경이 한글이신 분은 Google에서 백업 및 동기화라는 응용 프로그램이 추가됨(이것도 실행이 안되면 우클릭 후 실행) 환경 설정에서 동기화할 폴더 선택 “폴더 위치”라고 써있는 곳이 이제 Google Drive와 동일하게 연동 이제 “폴더 위치”에 원하는 데이터를 저장해두면 Colab에서 사용 가능 단, 크기가 큰 파일은 동기화 시간이 오래 걸릴 수 있음 Tensorflow 설치하기 2019년 12월 기준 Colab에 설치된 Tensorflow는 1.15.0입니다 Tensorflow 2.0을 설치하고 싶으면 아래처럼 입력하시면 됩니다 !pip install tensorflow==2.0.0-rc1 그 후, 런타임 - 런타임 다시 시작을 누르셔서 런타임을 다시 실행시키면 Tensorflow 2.0.0-beta1 설치된 것을 알 수 있습니다 import tensorflow as tf print(tf.__version__) PyTorch 사용하기 2019년 1월 26일부터 Colab에 기본적으로 PyTorch, torchvision, torchtext가 내장되었습니다 출처 : PyTorch 트위터 import torch import torchvision import torchtext print(torch.__version__) KoNLPy 설치 공식 문서엔 openjdk-7-jdk로 작성되어 있으나, 우분투 17.04에선 ppa를 추가해야 설치 가능 ppa를 추가하지 않고 8 버전을 설치 !apt-get update !apt-get install g++ openjdk-8-jdk !pip3 install konlpy 예제 코드 from konlpy.tag import Twitter twitter = Twitter() twitter.pos(\"질문이나 건의사항은 깃헙 이슈 트래커에 남겨주세요\") Github 코드를 Colab에서 사용하기 nbviewer나 htmlpreview처럼 사용 가능 https://github.com/~~~ 부분을 https://colab.research.google.com/github/~~~로 교체하면 됨 예시 https://github.com/zzsza/TIL/blob/master/python/tensorflow-1.ipynb 를 https://colab.research.google.com/github/zzsza/TIL/blob/master/python/tensorflow-1.ipynb로 변경 BigQuery 사용하기 google.colab의 auth를 통해 클라우드 권한을 얻은 후, 다양한 라이브러리 사용 google.cloud.bigquery 사용시 from google.cloud import bigquery from google.colab import auth project_id = '[your project ID]' auth.authenticate_user() client = bigquery.Client(project=project_id) for dataset in client.list_datasets(): print(dataset.dataset_id) pandas_gbq 사용시 (개인적으로 이 방법 추천) import pandas as pd from google.colab import auth auth.authenticate_user() query = \"SELECT * FROM &lt;YOUR TABLE&gt; LIMIT 10\" df = pd.read_gbq(query=query, project_id=&lt;your project id&gt;, dialect=‘standard’)) Matplotlib에서 한글 사용하기 폰트 설치 !apt-get install fonts-nanum* !apt-get install fontconfig !fc-cache -fv !cp /usr/share/fonts/truetype/nanum/Nanum* /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/ !rm -rf /content/.cache/matplotlib/* 그래프 그리기 import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib.font_manager as fm import numpy as np %matplotlib inline %config InlineBackend.figure_format = 'retina' mpl.rcParams['axes.unicode_minus'] = False # 그래프에서 마이너스 폰트 깨질 경우 대비 path = '/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf' font_name = fm.FontProperties(fname=path, size=18).get_name() plt.rc('font', family=font_name) fm._rebuild() # 이걸 해줘야 plt.rc가 작동 plt.plot(np.random.randn(4, 8), np.random.randn(4,8), 'bo--') plt.title('타이틀') plt.xlabel('X 라벨') plt.ylabel('Y 라벨') plt.show() Tensorboard 사용하기 Tensorboard 사용 준비 LOG_DIR를 ‘drive/tb_logs’로 설정하면 내 구글드라이브 root에 tb_logs 폴더가 생성 저는 ‘drive/data/tb_logs’로 지정 LOG_DIR = 'drive/data/tb_logs' !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip !unzip ngrok-stable-linux-amd64.zip import os if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR) get_ipython().system_raw( 'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &amp;' .format(LOG_DIR)) get_ipython().system_raw('./ngrok http 6006 &amp;') !curl -s http://localhost:4040/api/tunnels | python3 -c \\ \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\" 아래 예제 코드 실행 후, Epoch 지나고 위 코드에서 나온 URL 클릭하면 TensorBoard로 이동 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras import backend as K from keras.callbacks import TensorBoard batch_size = 128 num_classes = 10 epochs = 12 ### input image dimensions img_rows, img_cols = 28, 28 ### the data, shuffled and split between train and test sets (x_train, y_train), (x_test, y_test) = mnist.load_data() if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols) else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1) x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 x_test /= 255 print('x_train shape:', x_train.shape) print(x_train.shape[0], 'train samples') print(x_test.shape[0], 'test samples') ### convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation='softmax')) model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy']) tbCallBack = TensorBoard(log_dir=LOG_DIR, histogram_freq=1, write_graph=True, write_grads=True, batch_size=batch_size, write_images=True) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=[tbCallBack]) score = model.evaluate(x_test, y_test, verbose=0) print('Test loss:', score[0]) print('Test accuracy:', score[1]) JDK 설치하기 Python을 사용해 JDK 설치하기 아래 코드 입력 import os def install_java(): !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null #install openjdk os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" #set environment variable !java -version #check java version install_java() Google Storage에서 파일 읽기 gcsfs를 활용한 방법 storage에서 땡겨오기 때문에 colab에 파일을 옮기지 않아도 됨. 그러나 약간 시간 소요됨. 빠른 방법은 아래 gsutil 참고 !pip3 install gcsfs dask import gcsfs from google.colab import auth auth.authenticate_user() fs = gcsfs.GCSFileSystem(project='project_name') with fs.open('주소') as f: df = pd.read_csv(f) 위 방법보다 빠른 방법(gsutil 사용) 단, 세션이 초기화되면 매번 gsutil로 파일을 땡겨와야 합니다 from google.colab import auth auth.authenticate_user() import pandas as pd file_path = 'data.csv' !gsutil cp gs://&lt;your_bucket&gt;/{file_path} {file_path} df = pd.read_csv(query_result_path) Kaggle 연동하기 1) Kaggle beta API Json Key 다운 Kaggle - My Account - Dataset 옆에 있는 …을 클릭한 후, Account로 이동 하단에 API 부분에 Create New API Token을 클릭하면 Json Key가 다운로드 됨 이 Json 키를 매번 Colab에서 올려서 할 수도 있지만, 더 편하게 사용하고 싶어서 Google Storage에 Json 파일을 올리고, 거기서 키를 복사해오는 방법으로 진행합니다 2) Google Storage에 Json Key 저장 Google Storage로 이동한 후, Storage 버킷 선택 (버킷이 없다면 생성!) Colab에서 아래 명령어 입력 from google.colab import auth auth.authenticate_user() !mkdir -p ~/.kaggle !mv ./kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json 3) Kaggle 설치 및 데이터 다운로드 !pip install kaggle Competition 확인하기 !kaggle competitions list 데이터 다운로드는 각 대회 Data 부분에 나와있는 API 명령어를 입력하면 됨 !kaggle competitions download -c elo-merchant-category-recommendation 아래 명령어로 압축을 풀면 data 폴더에 파일들이 저장됨 !unzip '*.zip' -d ./data 복사하기 편하게 한번에 작성 from google.colab import auth auth.authenticate_user() !pip install kaggle !mkdir -p ~/.kaggle !mv ./kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json !kaggle competitions download -c elo-merchant-category-recommendation !unzip '*.zip' -d ./data 코기 모드 Colab을 조금 더 귀엽게 만드는 방법 도구 - 환경설정 - 기타에 코기 모드 / 아기고양이 모드 체크 조금 기다리면 귀여운 친구들이 뿅! 기타에 파워 레벨을 Many power로 설정하시면 코드를 칠 때마다 이팩트가 생깁니다 :) 런타임 연결 RAM, 디스크가 나오는 부분의 우측 삼각형을 클릭하면 아래와 같은 메뉴가 보입니다 호스트된 런타임에 연결(Connect to Hosted Runtime) Google Cloud의 새로운 머신 인스턴스에 연결 라이브러리를 다시 설치해야할 수도 있음 Colab의 연결이 끊길 경우 해당 버튼을 클릭해서 다시 연결할 수 있습니다(같은 인스턴스인지는 아직 확인해보지 못했습니다) 로컬 런타임에 연결(Connect to Local Runtime) 내 PC(로컬)을 사용 Reference Google Colab Free GPU Tutorial Google Colaboratory를 활용하여 Keras 개발환경 구축 유재흥님 블로그 정현석님 블로그",
    "tags": "dl data",
    "url": "/data/2018/08/30/google-colab/"
  },{
    "title": "Kaggle 강의 Week3 - Metrics optimization",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler Week3 Metrics optimization를 듣고 정리한 내용입니다 Metrics optimization 왜 Metrics은 많은가? Competition에서 왜 Metrics에 관심을 가져야 하는가? Loss vs metric 가장 중요한 metrics 분류와 회귀 Task Regression MSE, RMSE, R-squared MAE (R)MSPE, MAPE (R)MSLE Classification Accuracy, Logloss, AUC Cohoen’s (Quadratic weighted) Kappa metric의 최적화 베이스라인 metrics의 최적화 테크닉 Metrics 각각의 대회가 metric이 다른 이유는 대회를 주최한 회사가 그들의 특정 문제에 가장 적절한 것을 결정하기 때문 또한 문제마다 다른 metrics Online shop은 effectiveness를 최대화하는 것이 목표 Effectiveness를 정의 웹사이트에 방문한 횟수, 주문수로 나눌 수 있음 이런 것을 주최자들이 결정 더 나은 점수를 받기 위해 metric을 잘 이해하는 것은 필수 Regression metrics review 1 MSE Mean Sqaured Error \\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}})^{2} MSE는 우리의 예측값의 평균 squared error를 측정 Target value의 mean 참고 자료 RMSE Root mean square error \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}})^{2}} MAE Mean Absolute Error \\frac{1}{N}\\sum_{i=1}^{N}\\left|y_{i}-\\hat{y_{i}}\\right| MAE는 finance에서 널리 사용됨 Target value의 median Outliear의 영향을 받지 않음(outliear에게 robust) MAE vs MSE MAE 아웃라이어가 있는 경우 아웃라이어라고 확신하는 경우 MSE 우리가 신경써야 하는 관측하지 못한 값이라고 생각할 경우 MSE, RMSE, R-squared 최적화 관점에서 같음 MAE 아웃라이어에 robust Regression metrics review 2 Shop에서 매출을 예측하는 문제 Shop1 : predicted 9, sold 10, MSE = 1 Shop2 : predicted 999, sold 1000, MSE = 1 Shop1이 더 cirtical한데, MAE와 MSE는 동일함 그래프도 모든 Target value마다 동일(곡선이거나 V이거나) 위 문제점을 해결하기 위해 나온 것이 MSPE, MAPE relative error의 합에, 100%/N을 곱함 MSE와 MAE의 weight 버전이라고 생각할 수도 있음 MSPE MAPE 참고 자료 RMSLE Root Mean Squared Logarithmic Error MSPE와 MAPE와 동일한 상황에 사용 (relative error) prediction value와 target value가 큰 숫자일 때, 큰 차이를 벌점으로 내고싶지 않을 때 사용 과소평가된 항목에 패널티 Conclusion (R)MSPE Weighted version of MSE MAPE Weighted version of MAE (R)MSLE MSE in log space Classification metrics review Accuracy 분류기의 성능을 측정할 때 가장 간단히 사용할 수 있음 Best constant : predict the most frequent class 데이터셋의 라벨 A가 10%, B가 90%일 때 B로만 예측한 분류기는 90%의 accuracy를 가지는데, 이 분류기는 좋은 것일까? optimize하기 어려움 Logarithmic loss(logloss) logloss가 잘못된 답변에 대해 더 강하게 패널티 부여 Area Under Curve (AUC ROC) Binary task에서만 사용 특정 threshold를 설정 예측의 순서에 의존적이며 절대값엔 의존적이지 않음 설명 가능 AUC Pairs ordering AUC = # correctly ordered paris / total number of paris = 1 - # incorrectly ordered pairs / total number of pairs Cohen’s Kappa motivation Baseline accuracy는 라벨의 개수가 제일 많은 것으로 설정 Accuracy = 0.9(baseline) -&gt; my_score = 0 Accuracy = 1 -&gt; my_score = 1 Mse에서 r squared의 역할과 유사 일종의 normalization kappa는 baseline을 p_{e}로 지정(랜덤하게 계산) Weighted error General approaches for metrics optimization Loss and metric 동의어 : loss, cost, objective Target metrics : 우리가 최적화 하고 싶은 것 그러나 이걸 효율적으로 올리는 방법을 바로 알긴 어려움 Optimization loss : 모델을 최적화 하는 것 Target metrics을 직접 최적화하기 어려울 때 사용 Target metric 최적화 접근 방법 일단 올바른 모델을 실행 MSE, Logloss Train을 전처리하고 다른 metric을 최적화 MSPE, MAPE, RMSLE 후처리로 예측된 metric 최적화 Accuracy, Kappa Custom loss function 할 수 있으면 언제나! Early stopping을 사용 Regression metrics optimization RMSE, MSE, R-squared 대부분의 라이브러리에 loss function으로 구현되어 있음 L2 loss MAE L1 loss, Median regression MSPE and MAPE 둘 다 Weighted version이기 때문에 쉽게 구현 가능 RMSLE Classification metrics optimization 1 Logloss Probability calibration(눈금) Platt scaling 예측값에 Logistic Regression을 fit(마치 stacking처럼) Isotonic regression 예측값에 Isotonic Regression을 fit(마치 stacking처럼) Stacking 예측값에 XGBoost나 neuralnet을 fit Accuracy 이걸 직접적으로 optimize하는 쉬운 방법은 없음 다른 metric을 optimize Classification metrics optimization 2 AUC Pointwise loss single object에 대한 loss Pairwise loss pairwise loss Quadratic weighted Kappa Optimize MSE and find right thresholds Simple! Custom smooth loss for GBDT or neural nets Harder 구현 코드 Reference Coursera How to win a data science competition Competitive-data-science Github",
    "tags": "kaggle data",
    "url": "/data/2018/08/26/how-to-win-a-data-science-competition-week3/"
  },{
    "title": "Python 변수, 객체, 복사",
    "text": "Python 변수, 객체, 복사에 대해 작성한 글입니다 이야기할 내용들 파이썬 변수를 은유적 표현 변수는 이름표지 상자가 아님 객체의 정체성, 동질성, 별명 튜플은 불변형이지만 그 안에 있는 값은 바뀔 수 있음 얕은 복사, 깊은 복사 변수는 상자가 아니다 많은 책에서 변수를 상자로 표현하고 있는데, 객체지향 언어에서는 이 개념이 참조 변수를 이해하는데 방해가 됨 변수는 객체에 붙은 레이블이라고 생각하는 것이 좋음 상자로서의 변수를 설명할 수 없는 코드 a = [1, 2, 3] b = a a.append(4) b &gt;&gt;&gt; [1,2,3,4 변수를 상자로 생각하지 말고 변수를 포스트잇으로 생각하자 할당문의 경우 오른쪽 부분이 먼저 실행된다 객체가 생성된 후, 변수가 객체에 할당됨 class Gizmo: def __init__(self): print(\"Gizmo id : \" f'{id(self}) x = Gizmo() &gt;&gt;&gt; Gizmo id : 431489152 y = Gizmo() * 10 &gt;&gt;&gt; Gizmo id : 431489321 TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int' 할당문의 오른쪽에서 객체를 생성한 후, 레이블을 붙이듯 할당문 왼쪽에 있는 변수가 객체에 바인딩 여러 레이블을 붙이는 것을 별명이라 생각하기 정체성, 동질성, 별명 동일한 객체를 참조하는 사례 byeon = {'name': 'Seongyun', 'born': 1990} kyle = byeon kyle is byeon &gt;&gt;&gt; True id(kyle), id(byeon) &gt;&gt;&gt; (430048989, 430048989) kyle['age'] = 29 byeon &gt;&gt;&gt; {'name': 'Seongyun', 'born': 1990, 'age': 29} 값은 같으나 정체성이 다른 사례 byeonzzi = {'name': 'Seongyun', 'born': 1990, 'age': 29} byeonzzi == byeon &gt;&gt;&gt; True byeonzzi is not byeon &gt;&gt;&gt; True byeonzzi와 byeon은 동일한 값을 가지고 있기 때문에 == 연산자에 의해 동일하다고 판단 하지만 정체성은 다름. a is not b는 두 객체의 정체성이 다르다고 표현하는 파이썬 방식 정체성은 메모리 내의 객체 주소라 생각할 수 있으며 is 연산자가 두 객에츼 정체성을 비교 id() 함수로 정체성의 id를 반환. 실제 프로그래밍에선 주로 is 연산자 사용 == 연산자와 is 연산자의 선택 값을 비교하는 경우엔 == 사용이 더 많이 보임 그러나 변수를 싱글턴과 비교할 땐 is 연산자 사용 is 연산자는 오버로딩할 수 없고, 연산이 간단해서 속도가 빠름 a == b는 a.__eq__(b)의 편리 구문 튜플의 상대적 불변성 참조된 항목이 가변형이면 튜플 자체는 불변형이지만 참조된 항목은 변할 수 있음 t1 = (1, 2, [30, 40]) t2 = (1, 2, [30, 40]) t1 == t2 &gt;&gt;&gt; True id(t1[-1]) &gt;&gt;&gt; 4302515784 t1[-1].append(99) t1 &gt;&gt;&gt; (1, 2, [30, 40, 99]) id(t1[-1]) &gt;&gt;&gt; 4302515784 t1 == t2 &gt;&gt;&gt; False 위 사례를 보면 정체성(id)은 동일하지만 값은 변경됨 기본 복사는 얕은 복사 복사하는 가장 쉬운 방법은 자료형 자체의 내장 생성자를 사용하는 것 list(), tuple(), dict() 등 리스트나 가변형 시퀀스의 경우는 l2 = l1[:] 같이 [:]로도 사본을 생성할 수 있음 이 두 방법은 얕은 사본(shallow copy) 얕은 사본시 모든 항목이 불변형이면 메모리를 절약해 문제를 일으키지 않지만, 가변 항목이 들어가면 문제가 생김 pythontutor에서 아래 코드를 실행해보면 시각적으로 볼 수 있음 l1 = [3, [66, 55, 44], (7, 8, 9)] l2 = list(l1) l1.append(100) l1[1].remove(55) print('l1:', l1) &gt;&gt;&gt; l1: [3, [66, 44], (7, 8, 9), 100] print('l2:', l2) &gt;&gt;&gt; l2: [3, [66, 44], (7, 8, 9)] l2[1] += [33, 22] l2[2] += (10, 11) print('l1:', l1) &gt;&gt;&gt; l1: [3, [66, 44, 33, 22], (7, 8, 9), 100] print('l2:', l2) &gt;&gt;&gt; l2: [3, [66, 44, 33, 22], (7, 8, 9, 10, 11)] 리스트에서 += 연산자는 리스트를 변경 튜플에서 += 연산자는 새로운 튜플을 만들어 l2[2]에 다시 바인딩 깊은 복사와 얕은 복사 deepcopy() : 깊은 복사 객체에 순환 참조가 있으면 무한루프에 빠질 수 있음 너무 깊이 복사하는 경우도 존재 : 복사하면 안되는 외부 리소스나 싱글턴을 객체가 참조하는 경우 copy() : 얕은 복사 Reference 전문가를 위한 파이썬",
    "tags": "python development",
    "url": "/development/2018/08/25/python-object/"
  },{
    "title": "Python에서 데이터 시각화하는 다양한 방법",
    "text": "Python에서 데이터 시각화할 때 사용하는 다양한 라이브러리를 정리한 글입니다 데이터 분석가들은 주로 Python(또는 R, SQL)을 가지고 데이터 분석을 합니다 R에는 ggplot이란 시각화에 좋은 라이브러리가 있는 반면 Python에는 어느 춘추전국시대처럼 다양한 라이브러리들이 있습니다 각 라이브러리들마다 특징이 있기 때문에, 자유롭게 사용하면 좋을 것 같습니다 Zeppelin도 시각화할 때 사용할 수 있지만, 라이브러리는 아니기 때문에 이 문서에선 다루지 않습니다 데이터 시각화 관련한 꿀팁은 카일 스쿨 2주차를 참고해주세요 :) 저는 Jupyter Notebook으로 레포트를 작성해, 레포트의 보는 사람이 누구인지에 따라 다르게 그래프를 그렸습니다 직접 그래프를 더 탐색해보고 싶은 목적이 있는 분들에게 보내는 그래프라면 동적인(Interactive) 그래프를 그렸습니다 반복적인 레포트는 정적인 그래프 기반으로 작성한 후, 추가적인 내용이 궁금하면 대시보드로 가도록 유도했습니다 이 문서는 맥북에서 작성되었으며, 부족한 내용이 있으면 연락주세요 :) 현재 프론트단의 문제로 자바스크립트 기반 그래프는 처음엔 보이지 않을 수 있습니다. 10초 뒤 새로고침하면 나올거에요-! plot.ly, pyecharts는 웹에서 정말 강력합니다. 꼭 직접 사용해보세요! 라이브러리 matplotlib seaborn plotnine folium plot.ly pyecharts Matplotlib Python에서 아마 가장 많이 쓰는 라이브러리입니다 pandas의 Dataframe을 바로 시각화할 때도 내부적으로 matplotlib을 사용합니다 설치 pip3 install matplotlib Matplotlib Tutorials 데이터 사이언스 스쿨 import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt print(&quot;Matplotlib version&quot;, matplotlib.__version__) %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; Matplotlib version 2.2.3 Figure&#51032; &#44396;&#49457; &#50836;&#49548; 처음 matplotlib을 사용해 그래프를 그릴 때, 그래프와 관련된 명칭을 (영어 표현으로) 몰라 애를 먹었습니다 여기 나온 표현을 숙지해두기만 해도 좋을 것 같습니다 Figure Figure는 그림이 그려지는 도화지라고 생각할 수 있습니다 우선 Figure를 그린 후, plt.subplots로 도화지를 분할해 각 부분에 그래프를 그리는 방식으로 진행합니다 plt.figure를 명시적으로 표현해주는 것이 좋으나, plot 함수에서 자동으로 figure를 생성하기 때문에 자주 사용하진 않습니다 그러나 현재 figure에 접근해야 할 필요성이 있다면, plt.gcf()로 접근할 수 있습니다 size를 조절하고 싶은 경우엔 fig.set_size_inches(18.5, 10.5) 또는 plt.figure(figsize=(10,5)) 또는 plt.rcParams['figure.figsize'] = (10,7) Axes Axes는 plot이 그려지는 공간입니다 Axis plot의 축입니다 fig = plt.figure() fig.suptitle(&#39;figure sample plots&#39;) fig, ax_lst = plt.subplots(2, 2, figsize=(8,5)) ax_lst[0][0].plot([1,2,3,4], &#39;ro-&#39;) ax_lst[0][1].plot(np.random.randn(4, 10), np.random.randn(4,10), &#39;bo--&#39;) ax_lst[1][0].plot(np.linspace(0.0, 5.0), np.cos(2 * np.pi * np.linspace(0.0, 5.0))) ax_lst[1][1].plot([3,5], [3,5], &#39;bo:&#39;) ax_lst[1][1].plot([3,7], [5,4], &#39;kx&#39;) plt.show() df = pd.DataFrame(np.random.randn(4,4)) df.plot(kind=&#39;barh&#39;) Out[4]: ggplot 스타일로 그리고 싶다면 아래 옵션 추가 plt.style.use('ggplot') pd.options.display.mpl_style = 'default' fig = plt.figure() fig.suptitle(&#39;ggplot style&#39;) fig, ax_lst = plt.subplots(2, 2, figsize=(8,5)) ax_lst[0][0].plot([1,2,3,4], &#39;ro-&#39;) ax_lst[0][1].plot(np.random.randn(4, 10), np.random.randn(4,10), &#39;bo--&#39;) ax_lst[1][0].plot(np.linspace(0.0, 5.0), np.cos(2 * np.pi * np.linspace(0.0, 5.0))) ax_lst[1][1].plot([3,5], [3,5], &#39;bo:&#39;) ax_lst[1][1].plot([3,7], [5,4], &#39;kx&#39;) plt.show() Seaborn seaborn은 matplotlib을 기반으로 다양한 색 테마, 차트 기능을 추가한 라이브러리입니다 matplotlib에 의존성을 가지고 있습니다 matplotlib에 없는 그래프(히트맵, 카운트플랏 등)을 가지고 있습니다 설치 pip3 install seaborn Seaborn Tutorials import seaborn as sns print(&quot;Seaborn version : &quot;, sns.__version__) sns.set() sns.set_style(&#39;whitegrid&#39;) sns.set_color_codes() Seaborn version : 0.9.0 current_palette = sns.color_palette() sns.palplot(current_palette) relplot tips = sns.load_dataset(&quot;tips&quot;) sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, hue=&quot;smoker&quot;, style=&quot;smoker&quot;, data=tips) Out[11]: df = pd.DataFrame(dict(time=np.arange(500), value=np.random.randn(500).cumsum())) g = sns.relplot(x=&quot;time&quot;, y=&quot;value&quot;, kind=&quot;line&quot;, data=df) g.fig.autofmt_xdate() Catplot sns.catplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, col=&quot;time&quot;, aspect=.6, kind=&quot;swarm&quot;, data=tips) Out[12]: titanic = sns.load_dataset(&quot;titanic&quot;) g = sns.catplot(x=&quot;fare&quot;, y=&quot;survived&quot;, row=&quot;class&quot;, kind=&quot;box&quot;, orient=&quot;h&quot;, height=1.5, aspect=4, data=titanic.query(&quot;fare &gt; 0&quot;)) g.set(xscale=&quot;log&quot;); Pairplot iris = sns.load_dataset(&quot;iris&quot;) sns.pairplot(iris) Out[16]: g = sns.PairGrid(iris) g.map_diag(sns.kdeplot) g.map_offdiag(sns.kdeplot, n_levels=6); Heatmap flights = sns.load_dataset(&quot;flights&quot;) flights = flights.pivot(&quot;month&quot;, &quot;year&quot;, &quot;passengers&quot;) plt.figure(figsize=(10, 10)) ax = sns.heatmap(flights, annot=True, fmt=&quot;d&quot;) Plotnine plotnine은 R의 ggplot2에 기반해 그래프를 그려주는 라이브러리입니다 R로 시각화하는 것이 익숙하신 분들에게 좋을 것 같습니다. 저는 사용해보진 않았습니다! 공식 문서 설치 pip3 install plotnine import plotnine from plotnine import * print(&quot;plontnine version :&quot;,plotnine.__version__) plontnine version : 0.4.0 n = 10 df = pd.DataFrame({&#39;x&#39;: np.arange(n), &#39;y&#39;: np.arange(n), &#39;yfit&#39;: np.arange(n) + np.tile([-.2, .2], n//2), &#39;cat&#39;: [&#39;a&#39;, &#39;b&#39;]*(n//2)}) (ggplot(df) + geom_col(aes(&#39;x&#39;, &#39;y&#39;, fill=&#39;cat&#39;)) + geom_point(aes(&#39;x&#39;, y=&#39;yfit&#39;, color=&#39;cat&#39;)) + geom_path(aes(&#39;x&#39;, y=&#39;yfit&#39;, color=&#39;cat&#39;)) ) Out[39]: df2 = pd.DataFrame({ &#39;letter&#39;: [&#39;Alpha&#39;, &#39;Beta&#39;, &#39;Delta&#39;, &#39;Gamma&#39;] * 2, &#39;pos&#39;: [1, 2, 3, 4] * 2, &#39;num_of_letters&#39;: [5, 4, 5, 5] * 2 }) (ggplot(df2) + geom_col(aes(x=&#39;letter&#39;,y=&#39;pos&#39;, fill=&#39;letter&#39;)) + geom_line(aes(x=&#39;letter&#39;, y=&#39;num_of_letters&#39;, color=&#39;letter&#39;), size=1) + scale_color_hue(l=0.45) # some contrast to make the lines stick out + ggtitle(&#39;Greek Letter Analysis&#39;) ) Out[41]: Folium folium은 지도 데이터(Open Street Map)에 leaflet.js를 이용해 위치정보를 시각화하는 라이브러리입니다 자바스크립트 기반이라 interactive하게 그래프를 그릴 수 있습니다 한국 GeoJSON 데이터는 southkorea-maps에서 확인할 수 있습니다 참고 자료 공식 문서 진현수님의 Github : 서울지역 범죄 데이터를 시각화한 노트북 파일입니다 PinkWink님의 블로그 그 외에도 pydeck, ipyleaflet 등으로 지도 시각화를 할 수 있습니다 설치 pip3 install folium import folium print(&quot;folium version is&quot;, folium.__version__) folium version is 0.6.0 m = folium.Map(location=[37.5502, 126.982], zoom_start=12) folium.Marker(location=[37.5502, 126.982], popup=&quot;Marker A&quot;, icon=folium.Icon(icon=&#39;cloud&#39;)).add_to(m) folium.Marker(location=[37.5411, 127.0107], popup=&quot;한남동&quot;, icon=folium.Icon(color=&#39;red&#39;)).add_to(m) m Out[76]: Plot.ly plotly는 Interactive 그래프를 그려주는 라이브러리입니다 Scala, R, Python, Javascript, MATLAB 등에서 사용할 수 있습니다 시각화를 위해 D3.js를 사용하고 있습니다 사용해보면 사용이 쉽고, 세련된 느낌을 받습니다 Online과 offline이 따로 존재합니다(온라인시 api key 필요) plotly cloud라는 유료 모델도 있습니다 설치 pip3 install plotly 참고 자료 plotly jupyter notebook tutorial Plotly를 이용한 캔들차트 import plotly print(&quot;plotly version :&quot;, plotly.__version__) plotly version : 3.1.1 plotly.offline.init_notebook_mode() plotly.offline.iplot({ &quot;data&quot;: [{ &quot;x&quot;: [1, 2, 3], &quot;y&quot;: [4, 2, 5] }], &quot;layout&quot;: { &quot;title&quot;: &quot;hello world&quot; } }) import plotly.figure_factory as ff import plotly.plotly as py import plotly.graph_objs as go df = pd.read_csv(&quot;https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv&quot;) table = ff.create_table(df) plotly.offline.iplot(table, filename=&#39;jupyter-table1&#39;) data = [go.Bar(x=df.School, y=df.Gap)] plotly.offline.iplot(data, filename=&#39;jupyter-basic_bar&#39;) data = [dict( visible = False, line=dict(color=&#39;#ff0000&#39;, width=6), name = &#39;𝜈 = &#39;+str(step), x = np.arange(0,10,0.01), y = np.sin(step*np.arange(0,10,0.01))) for step in np.arange(0,5,0.1)] data[10][&#39;visible&#39;] = True steps = [] for i in range(len(data)): step = dict( method = &#39;restyle&#39;, args = [&#39;visible&#39;, [False] * len(data)], ) step[&#39;args&#39;][1][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(step) sliders = [dict( active = 10, currentvalue = {&quot;prefix&quot;: &quot;Frequency: &quot;}, pad = {&quot;t&quot;: 50}, steps = steps )] layout = dict(sliders=sliders) fig = dict(data=data, layout=layout) plotly.offline.iplot(fig, filename=&#39;Sine Wave Slider&#39;) pyecharts Baidu에서 데이터 시각화를 위해 만든 Echarts.js의 파이썬 버전입니다 정말 다양한 그래프들이 내장되어 있어 레포트를 작성할 때 좋습니다! 자바스크립트 기반이기 때문에 Interactive한 그래프를 그려줍니다 공식 문서 설치 pip3 install pyecharts import pyecharts print(&quot;pyecharts version : &quot;, pyecharts.__version__) pyecharts version : 0.5.8 attr = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;] v1 = [2.0, 4.9, 7.0, 23.2, 25.6, 76.7, 135.6, 162.2, 32.6, 20.0, 6.4, 3.3] v2 = [2.6, 5.9, 9.0, 26.4, 28.7, 70.7, 175.6, 182.2, 48.7, 18.8, 6.0, 2.3] bar = pyecharts.Bar(&quot;Bar chart&quot;, &quot;precipitation and evaporation one year&quot;) bar.add(&quot;precipitation&quot;, attr, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;]) bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;]) bar.height = 500 bar.width = 800 bar Out[109]: attr = [&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;] v1 = [2.0, 4.9, 7.0, 23.2, 25.6, 76.7, 135.6, 162.2, 32.6, 20.0, 6.4, 3.3] v2 = [2.6, 5.9, 9.0, 26.4, 28.7, 70.7, 175.6, 182.2, 48.7, 18.8, 6.0, 2.3] bar = pyecharts.Bar(&quot;Bar chart&quot;, &quot;precipitation and evaporation one year&quot;) bar.use_theme(&quot;dark&quot;) bar.add(&quot;precipitation&quot;, attr, v1, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;]) bar.add(&quot;evaporation&quot;, attr, v2, mark_line=[&quot;average&quot;], mark_point=[&quot;max&quot;, &quot;min&quot;]) bar.height = 500 bar.width = 800 bar Out[110]: title = &quot;bar chart2&quot; index = pd.date_range(&quot;8/24/2018&quot;, periods=6, freq=&quot;M&quot;) df1 = pd.DataFrame(np.random.randn(6), index=index) df2 = pd.DataFrame(np.random.rand(6), index=index) dfvalue1 = [i[0] for i in df1.values] dfvalue2 = [i[0] for i in df2.values] _index = [i for i in df1.index.format()] bar = pyecharts.Bar(title, &quot;Profit and loss situation&quot;) bar.add(&quot;profit&quot;, _index, dfvalue1) bar.add(&quot;loss&quot;, _index, dfvalue2) bar.height = 500 bar.width = 800 bar Out[111]: from pyecharts import Bar, Line, Overlap attr = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;] v1 = [10, 20, 30, 40, 50, 60] v2 = [38, 28, 58, 48, 78, 68] bar = Bar(&quot;Line Bar&quot;) bar.add(&quot;bar&quot;, attr, v1) line = Line() line.add(&quot;line&quot;, attr, v2) overlap = Overlap() overlap.add(bar) overlap.add(line) overlap Out[112]: from pyecharts import Pie attr = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;] v1 = [10, 20, 30, 40, 50, 60] v2 = [38, 28, 58, 48, 78, 68] pie = Pie(&quot;pie chart&quot;, title_pos=&quot;center&quot;, width=600) pie.add(&quot;A&quot;, attr, v1, center=[25, 50], is_random=True, radius=[30, 75], rosetype=&#39;radius&#39;) pie.add(&quot;B&quot;, attr, v2, center=[75, 50], is_randome=True, radius=[30, 75], rosetype=&#39;area&#39;, is_legend_show=False, is_label_show=True) pie Out[113]: bar = Bar(&quot;가로 그래프&quot;) bar.add(&quot;A&quot;, attr, v1) bar.add(&quot;B&quot;, attr, v2, is_convert=True) bar.width=800 bar Out[114]: import random attr = [&quot;{}th&quot;.format(i) for i in range(30)] v1 = [random.randint(1, 30) for _ in range(30)] bar = Bar(&quot;Bar - datazoom - slider &quot;) bar.add(&quot;&quot;, attr, v1, is_label_show=True, is_datazoom_show=True) bar Out[115]: days = [&quot;{}th&quot;.format(i) for i in range(30)] days_v1 = [random.randint(1, 30) for _ in range(30)] bar = Bar(&quot;Bar - datazoom - xaxis/yaxis&quot;) bar.add( &quot;&quot;, days, days_v1, is_datazoom_show=True, datazoom_type=&quot;slider&quot;, datazoom_range=[10, 25], is_datazoom_extra_show=True, datazoom_extra_type=&quot;slider&quot;, datazoom_extra_range=[10, 25], is_toolbox_show=False, ) bar Out[116]: 3D from pyecharts import Bar3D bar3d = Bar3D(&quot;3D Graph&quot;, width=1200, height=600) x_axis = [ &quot;12a&quot;, &quot;1a&quot;, &quot;2a&quot;, &quot;3a&quot;, &quot;4a&quot;, &quot;5a&quot;, &quot;6a&quot;, &quot;7a&quot;, &quot;8a&quot;, &quot;9a&quot;, &quot;10a&quot;, &quot;11a&quot;, &quot;12p&quot;, &quot;1p&quot;, &quot;2p&quot;, &quot;3p&quot;, &quot;4p&quot;, &quot;5p&quot;, &quot;6p&quot;, &quot;7p&quot;, &quot;8p&quot;, &quot;9p&quot;, &quot;10p&quot;, &quot;11p&quot; ] y_axis = [ &quot;Saturday&quot;, &quot;Friday&quot;, &quot;Thursday&quot;, &quot;Wednesday&quot;, &quot;Tuesday&quot;, &quot;Monday&quot;, &quot;Sunday&quot; ] data = [ [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5], [1, 0, 7], [1, 1, 0], [1, 2, 0], [1, 3, 0], [1, 4, 0], [1, 5, 0], [1, 6, 0], [1, 7, 0], [1, 8, 0], [1, 9, 0], [1, 10, 5], [1, 11, 2], [1, 12, 2], [1, 13, 6], [1, 14, 9], [1, 15, 11], [1, 16, 6], [1, 17, 7], [1, 18, 8], [1, 19, 12], [1, 20, 5], [1, 21, 5], [1, 22, 7], [1, 23, 2], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 0], [2, 5, 0], [2, 6, 0], [2, 7, 0], [2, 8, 0], [2, 9, 0], [2, 10, 3], [2, 11, 2], [2, 12, 1], [2, 13, 9], [2, 14, 8], [2, 15, 10], [2, 16, 6], [2, 17, 5], [2, 18, 5], [2, 19, 5], [2, 20, 7], [2, 21, 4], [2, 22, 2], [2, 23, 4], [3, 0, 7], [3, 1, 3], [3, 2, 0], [3, 3, 0], [3, 4, 0], [3, 5, 0], [3, 6, 0], [3, 7, 0], [3, 8, 1], [3, 9, 0], [3, 10, 5], [3, 11, 4], [3, 12, 7], [3, 13, 14], [3, 14, 13], [3, 15, 12], [3, 16, 9], [3, 17, 5], [3, 18, 5], [3, 19, 10], [3, 20, 6], [3, 21, 4], [3, 22, 4], [3, 23, 1], [4, 0, 1], [4, 1, 3], [4, 2, 0], [4, 3, 0], [4, 4, 0], [4, 5, 1], [4, 6, 0], [4, 7, 0], [4, 8, 0], [4, 9, 2], [4, 10, 4], [4, 11, 4], [4, 12, 2], [4, 13, 4], [4, 14, 4], [4, 15, 14], [4, 16, 12], [4, 17, 1], [4, 18, 8], [4, 19, 5], [4, 20, 3], [4, 21, 7], [4, 22, 3], [4, 23, 0], [5, 0, 2], [5, 1, 1], [5, 2, 0], [5, 3, 3], [5, 4, 0], [5, 5, 0], [5, 6, 0], [5, 7, 0], [5, 8, 2], [5, 9, 0], [5, 10, 4], [5, 11, 1], [5, 12, 5], [5, 13, 10], [5, 14, 5], [5, 15, 7], [5, 16, 11], [5, 17, 6], [5, 18, 0], [5, 19, 5], [5, 20, 3], [5, 21, 4], [5, 22, 2], [5, 23, 0], [6, 0, 1], [6, 1, 0], [6, 2, 0], [6, 3, 0], [6, 4, 0], [6, 5, 0], [6, 6, 0], [6, 7, 0], [6, 8, 0], [6, 9, 0], [6, 10, 1], [6, 11, 0], [6, 12, 2], [6, 13, 1], [6, 14, 3], [6, 15, 4], [6, 16, 0], [6, 17, 0], [6, 18, 0], [6, 19, 0], [6, 20, 1], [6, 21, 2], [6, 22, 2], [6, 23, 6] ] range_color = [&#39;#313695&#39;, &#39;#4575b4&#39;, &#39;#74add1&#39;, &#39;#abd9e9&#39;, &#39;#e0f3f8&#39;, &#39;#ffffbf&#39;, &#39;#fee090&#39;, &#39;#fdae61&#39;, &#39;#f46d43&#39;, &#39;#d73027&#39;, &#39;#a50026&#39;] bar3d.add( &quot;&quot;, x_axis, y_axis, [[d[1], d[0], d[2]] for d in data], is_visualmap=True, visual_range=[0, 20], visual_range_color=range_color, grid3d_width=200, grid3d_depth=80, ) bar3d.width=700 bar3d.height=500 bar3d Out[117]: from pyecharts import Boxplot boxplot = Boxplot(&quot;Box plot&quot;) x_axis = [&#39;expr1&#39;, &#39;expr2&#39;, &#39;expr3&#39;, &#39;expr4&#39;, &#39;expr5&#39;] y_axis = [ [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960], [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800], [880, 880, 880, 860, 720, 720, 620, 860, 970, 950, 880, 910, 850, 870, 840, 840, 850, 840, 840, 840], [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780], [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870] ] _yaxis = boxplot.prepare_data(y_axis) boxplot.add(&quot;boxplot&quot;, x_axis, _yaxis) boxplot Out[118]: from pyecharts import Funnel attr = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;] value = [20, 40, 60, 80, 100, 120] funnel = Funnel(&quot;퍼널 그래프&quot;) funnel.add( &quot;퍼널&quot;, attr, value, is_label_show=True, label_pos=&quot;inside&quot;, label_text_color=&quot;#fff&quot;, ) funnel.width=700 funnel.height=500 funnel Out[119]: from pyecharts import Gauge gauge = Gauge(&quot;Gauge Graph&quot;) gauge.add(&quot;이용률&quot;, &quot;가운데&quot;, 66.66) gauge Out[120]:",
    "tags": "python development",
    "url": "/development/2018/08/24/data-visualization-in-python/"
  },{
    "title": "How to win a data science competition Week2 - EDA",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler Week2 EDA를 듣고 정리한 내용입니다 EDA : 탐색적 자료 분석, Exploratory data analysis EDA EDA가 주는 것 데이터를 더 잘 이해할 수 있음 인사이트를 형성 가능 가설 생성할 수 있음 직관력을 가질 수 있음 magic feature를 찾을 수 있음 Visualiations Visualization -&gt; Idea : Find pattern Idea -&gt; Visualization : Hypothesis testing 모델링에 바로 들어가기보다 반드시 EDA를 먼저 해라! Exploratory data analysis Get domain knowledge 대회 관련 도메인 지식 쌓기 위키피디아에 검색, 구글에서 기사 검색 등 Check if the data is intuitive 직감에 기반해 데이터 체크 나이에 336이 있으면 이건 오타인가? 33인가? 36인가? Understand how the data was generated Train과 Test data의 분포 비교 데이터를 잘 섞었는지? 적절한 validation이 셋팅되야 함 Exploring anonymized data Anonymized data 보안상의 문제로 encode type of feature를 알 수 없게 만듬 우리는 의미를 알 수 없음 decode 할 수 있지만, 거의 불가능 각각의 feature를 explore column의 의미지 추측 column의 타입 추측 Feature의 관계를 explore Feature간 관계 찾기 Feature groups 찾기 histogram을 그려보거나 value_counts()로 빈도 보기 Decoding 실습 feature_importance 보고 중요도 파악 mean, std 파악 같은 값이 반복되는 것이 보임 아마도 StandardScaler! Decode해보자 unique value를 뽑고 sort -&gt; 0.04332159가 반복 모든 값에 0.04332159를 나눈 후, 소수점을 정리 Visualizations Explore individual features Histograms : plt.hist() Plot (index vs value) : plt.plot(x, '.'), plt.scatter(range(len(x)), x, c=y), 데이터가 적절히 섞였는지 확인 Statistics : df.describe(), x.mean(), x.var() Other tools : x.value_counts(),x.isnull() Explore feature relations Pairs Scatter plots : plt.scatter(x1, x2), pd.scatter_matrix(df) Corrplot : df.corr(), plt.matshow(...) Groups Corrplot + clustering Plot (index vs feature statistics) : df.mean().plot(style='.'), df.srot_values().plot(style='.') Dataset cleaning and other things to check Duplicated columns 중복되는 column이라면 제거하는 것이 좋음(메모리 관점) traintest.T.drop_duplicates() Duplicated rows 같은 label을 가지는지 체크 왜 이 값이 중복인지 이해하기 Check if dataset is shuffled 만약 셔플되지 않았으면 data leakages를 찾을 수도 있음! rolling_mean과 mean 비교 Validation Validation and overfitting private 리더보드가 나오면 성적이 떨어지는 경우가 있습니다 2가지 원인 1) 경쟁자가 validation을 무시하고 public 리더보드에서 가장 좋은 제출물을 선택 2) 경쟁자가 public, private 데이터가 일치하지 않거나 private 리더보드에 데이터가 적은 경우 private 데이터에 맞게 제출하는 것이 목표! Validation Unseen data에도 잘 맞췄으면 좋겠음 잘 맞추지 못한다면 모델의 실수를 측정하고 싶음 이 성능은 Train(past) 데이터와 Test(future) 데이터에 따라 다름 학습한 후, 모델의 성능을 평가하기 위해 validation을 사용 Train(past), Validation(past), Test(future) Validation에서 성능이 잘 나온 것을 Best 모델로 하고 계속 튜닝을 하면 오버피팅이 될 수 있음(Test엔 잘 안맞는 모델) underfitting and overfitting 머신러닝의 오버피팅과 대회의 오버피팅은 살짝 다름 Overfitting in general != overfitting in competition General capturing noize capturing patterns which do not getneralize to test data Competition low models’ quality on test data, which was unexpected due to validation socres 모델의 복잡도가 높을 경우, 낮추면 당연히 더 좋을 것이라 예상하지만 아닐 경우 Validation strategies How many splits should we make What are the most foten methods to perform such splits Validation 유형 Holdout ngroups=1 sklearn.model_selection.ShuffleSplit Data를 Part A, B로 나눔 A로 Train, B로 Predict B의 예측을 토대로 model quality 측정하고 하이퍼파라미터 진행. B의 quality를 최대화 Use Case 충분한 데이터를 가지고 있을 때 유용 같은 모델을 다르게 split해서 성능을 보고싶을 경우 Split 방식이 성능 추정에 민감한 영향을 미침 K-fold ngroups=k sklearn.model_selection.kfold Train data를 K folds로 나눔 각 fold마다 iterate: 현재 fold를 제외한 모든 fold에 retrain한 후, 현재 fold로 predict prediction값을 사용해 각 fold별 quality를 계산, 하이퍼 파라미터를 찾고, 각 fold의 quality 최대화 loss의 mean과 variance를 측정해 개선을 파악할 수 있음 서로 다른 holdout을 k번 반복 모든 데이터를 training과 test에 쓸 수 있음 Score를 평균 일반화 성능을 만족시키는 최적의 하이퍼 파라미터를 구하기 위한 모델 튜닝에 사용 Strtified 방법을 사용해 샘플링하면 알고리즘이 더 개선되곤 함 Leave-one-out ngroups=len(train) sklearn.model_selection.LeaveOneOut 작은 데이터를 가지고 있을 때 유용 현재 샘플을 제외한 모든 샘플로 retrain, 현재 샘플로 predict 다른 알고리즘에 비해 실행 시간이 오래 걸림 Stratification 충분한 sample이 있을 경우 shuffling data로 random split 그러나 sample이 충분하지 않으면 random split은 실패할 수 있음 Train시 클래스 비율은 38%, 28%, 34%인데 Valid시 데이터셋은 24%, 44%, 32%라면 불균형 발생 각 클래스가 train set과 test set에 정확하게 분포되도록 데이터셋을 랜덤하게 나눔 유용한 상황 작은 데이터셋 불균형한 데이터셋 Multiclass classification Data splitting strategies 1) Random rows in validation 2) Time based split 3) Differend approached to validation 모델의 목적에 따라 적절한 split이 필요 파란색 선이 mean value 좌측의 경우 test보다 validation 때 더 나은 score 얻음 우측의 경우 test와 validation와 유사한 score 얻음 Summary 다음과 같은 상황에 전략이 다를 수 있음 생성된 feature 사용시 모델이 해당 feature에 의존할 경우 target leak일 경우 Splitting data into train and validation Random, rowwise Row들이 독립적일 때 유용 Row가 사람일 경우 독립적인 Case 가족이거나 같은 회사 동료일 경우, 가족이 하나의 카드를 사용할 경우는 의존적인 Case Timewise Time based 특정 일 이전의 데이터는 train, 이후의 데이터는 test Moving window validation By id Combined Date + Id, geographic Problems occurring during validation 1) Validation stage inconsistency for data example : 1월이 2월보다 명절이 더 있어 판매량이 증가되는 데이터가 있음. 이 경우 1월 데이터로 2월을 예측하면? score와 최적의 파라미터가 다른 이유 너무 적은 데이터 (Too little data) 너무 다양하고 불일치한 데이터 (Too diverse and inconsistent data) Extensive validation 다른 KFold에서 얻은 score를 평균 1 split으로 모델을 만든 후, 나머지로 score 평가 2) Submission stage 우리가 종종 보는 현상 LB score가 validation score보다 일관되게 상승/하락 LB score가 valdation score와 관련이 없음 Organizer가 split한 것을 분류하는 것은 매우 어려움 계속 제출해보며 기록 leader board 점수를 또다른 validation fold로 보기! Other reasons too little data in public leaderboard 부정확한 train/test split train and test data가 다른 분포 LB shuffle 리더보드에서 랭킹이 급상승 또는 급하락할 경우 Randomness Little amount of data Different public/private distributions Time-series 데이터에서 종종 발생 Conclusion validation stage가 크게 다를 경우 Average scores form different KFold splits Tune model on one split, evaluate score on the other Submission score가 local validation score와 일치하지 않을 경우 Public LB가 너무 작은지 check 오버피팅인지 check 올바른 split 전략을 선택했는지 check train/test의 분포가 다른지 check LB shuffle의 원인 Randomeness 적은 데이터 public/private 분포가 다름 Advices on validation in a competition Data leakages Basic data leaks Data leakage : 데이터 유출, 비현실적으로 좋은 결과를 내는 예상하지 못한 정보가 있는 경우 현실에선 사용하는 것이 말도 안되지만, 경진 대회에선 높은 점수가 목적이라 사용하기도 함 마감 전에 공개하면 대회가 흔들릴 수 있음 종류 Leaks in time series Future picking 현실에선 우린 미래의 정보를 알 수 없음 대회에선 train/public/private으로 나뉨 CTR tasks에서 유저 로그, 날씨 같은 경우 미래의 정보를 포함할 수 있음 Unexpected information Meta data 파일 생성일, 이미지 해상도 등 특정 카메라에서 찍은 것이 모두 고양이일 수 있음 Information in IDs 모델에 ID를 넣는 것은 이치에 맞지 않음(이미 target값과 연결되어 있으니) 그러나 언제나 그런 것은 아님 id는 hash의 결과일 수 있음 id에 대해 신중히 다루기 Row order Leaderboard probing and examples of rare data leaks ID와 밀접하게 연결된 카테고리는 LB probing에 취약함 id가 같은 것의 라벨을 넣고 제출해보고 나온 결과로 역추적 가능 Redhat and west nile competition Peculiar(이상한) examples Truly Native Data collection, Date proxies가 저장되어 있음. 추가적인 데이터를 수집해 feature 생성 Expedia Hotel recommendations 유저가 예약할 호텔 그룹 예측 distance feature에 data leak Reverse engineering으로 좌표를 추정 Falvours of physics signal이 인위적으로 시뮬레이션됨 Reverse engineering Pairwise tasks Data leakge in item frequencies Similarities from connectivity matrix Expedia challenge 강의 자료 강사님이 참여한 대회 중 가장 흥미로운 대회 Data leakage 썰 풀어줌 호텔 그룹으로 라벨링 되었다는 것은 실제 호텔의 특성임을 기억! 유저와 호텔의 거리를 통해 추측할 수 있는 것들이 있음! Train과 test에서 많이 매칭됨 user city와 destination distance pair 더 많은 match를 찾는 것 유저 도시, 호텔 국가, 호텔 도시 3개의 방정식으로 어느 그룹의 호텔이 몇 개나 있는지를 알 수 있음 리버스 엔지니어링을 하며 모든 도시의 좌표를 반복적으로 찾음 일부 도시는 바다 위에 있는것으로 보였는데, 알고리즘이 정확하지 않다는 것을 의미 3개의 방정식이 아닌 수백 수천개의 방정식과 수만개의 변수를 사용해 정확한 좌표를 얻음 모든 도시에 대해 그리드 셀을 남겨서 개수 count Out-of-fold로 feature generation. 2013&lt;-&gt;2014 Xgboost로 16시간 학습 (그냥 진짜 리버스 엔지니어링으로 철저하게 데이터를 얻음.. 변태처럼 해야 3등하는구나) Reference Coursera How to win a data science competition Competitive-data-science Github",
    "tags": "kaggle data",
    "url": "/data/2018/08/21/how-to-win-a-data-science-competition-week2/"
  },{
    "title": "파이콘(PyCon) 2018 - 2일차 메모",
    "text": "PyCon(파이콘) 2018 2일차 세션 메모입니다! 일정이 있어 오전 세션만 들었습니다 인공지능 슈퍼마리오의 거의 모든것 정원석님 발표 자료 동물이 학습하는 방법 동물은 학습 능력이 존재 머리철수반사 : 위험한 물체가 있을것이라 판단하고 하는 반사 행동 여러번 시도하면 후퇴하는 범위가 증가 Law of effect : 어떤 행동의 결과가 만족스러우면 다음에도 그 행동을 반복한다. 반대로 만족하지 않으면 그 행동을 하지 않는다. Reinforcement(강화) : 이전에 일어난 행동을 반복하게 만드는 자극 Punishment(처벌) : 이전에 일어난 행동을 피하게 만드는 자극 사람이 학습하는 방법 Environment에서 Experience를 통해(interaction) 배워나감 Reinforcement : 이전에 일어난 행동을 반복하게 만드는 자극 Punishment : 이전에 일어난 행동을 피하게 만드는 자극 Tap Ball을 통한 강화 학습 진행ㅋㅋㅋㅋㅋ Reinforcement : 성공의 쾌감, 근육의 성장 Punishment : 고통 강화 학습 상호 작용을 Computation, 수학적으로 표현 Reinforcement learning은 Reward(보상)을 최대화 하는 action(행동)을 선택 Learner(배우는자)는 여러 action을 해보며, reward를 가장 높게 받는 action을 찾음 선택된 action이 당장의 reward 뿐만 아닌, 다음의 상황 또는 다음 일어나게 될 reward에도 영향을 끼칠수도 있음 Exploitation : 가장 좋아보이는 상황 Exploration : 현재 가장 좋은 상황은 아니지만 먼 미래를 위해 탐험 풀이는 공에 관심이 없고(Exploration x) 셀이는 관심이 있음(Exploration) Reward : 간식 셀이는 탭볼을 칠 수 있음 State-value 어떤 상황을 value로 측정 State-Action Value Function 내가 할 수 있는 Action들의 가치를 측정 Optimal Policy가 목표 슈퍼마리오 원석님 Github 설치 pip install gym-super-mario-bros 코드 env = gym_super_mario_bros.make(‘SuperMarioBros-v0') env.reset() env.render() World &amp; Level env = gym_super_mario_bros.make('SuperMarioBros-&lt;world&gt;-&lt;level&gt;-v&lt;version&gt;') 버전별로 이미지를 전처리(버전4는 뭉그러트림) 깃발에 가까워지면 +, 목표에 도착하면 + 목표 달성을 못하면 -, 시간이 지나면 -, 깃발에서 멀어지면 - env.action_space.n : 256개인데 너무 많음. 필요한 것만 따로 처리 env.step은 액션을 받아서 마리오의 상황을 받음 info : 디버깅을 위한 정보 eps_max : 아무 행동이나 막 함 memory를 생성해서 사용 loss를 줄이기! supervised learning과 다르게 자신이 선택한 액션 중 가장 좋아보이는 것을 정답으로 둠 Double DQN 이미지 4장이 input 1장만 들어가면 전 상황을 알 수 없음 Q-Network를 통해 Action value 얻음 Exploitation, Exploration를 고민한 후, Action Replay Memory 5000 에피소드를 하니 4일 소요, 깃발에 도착함! 다른 환경 Deepmind Lab, Sonic, OpenAI, Starcraft, Minecraft, DQN, DDQN, Rainbow DQN, DDPG 추천 시스템을 위한 어플리케이션 서버 개발 후기 김광섭님, 카카오 발표 자료 추천 시스템 우리가 소비하는 데이터를 새로운 방법(경험)으로 추천 UX, UI 관점에서도 봐야하는 Task 복잡한 문제지만 2가지 맥락으로 보면 아이템 추천 개인화 추천 시나리오 바나나를 보고 있을 때, 어떤 것이 적합할까? 원하는 것이 뭘까? 맥락을 이해해서 유사도를 생성 상품의 상태(품절 유무) 선호도로 필터링해서 상품을 보여줌 필요한 정보 활동 정보 : 좋아요, 싫어요, 산 것/본 것 관계, 유사도, 거리 상품 정보 : 상품명, 재고량, 가격, 판매처 disliked한 것을 5개 제외하고 status okay인 20개를 추출! 직접 만들기! 어플리케이션 서버 요구 사항 분석 실시간 분석 실시간 개인화 추천 OLAP에 가까움 복잡한 필터링, 비지니스 로직, 온라인 기계학습 이게 가장 중요! 추천 퀄리티를 위해 미리 계산하는 것은 매우 비효율적 OLTP 관계형 분석 기능을 제공해 낮은 지연시간을 보장 데이터베이스 메모리에 적재할 순 없음 유사도/메타/이력데이터적재 NoSQL 수평 확장이 용이해야 하고 대량의 데이터에도 안정적 성능 유지 필요 솔루션 조사 ArangoDB : MongoDB의 차세대 Dgraph : 차세대.. 허나 버전이 이제 1점대 실시간 분석 어려운 로직 결과가 없으면 같은 카테고리내 다른 상품을 보여줬으면 좋겠어요 본 거는 빼주세요.그런데 3번 정도 봐야 빠졌으면 좋겠어요 대용량 개인화 추천, 온라인 기계학습 속도 및 성능 데이터베이스 작업을 단순화하고, 복잡한 로직을 더 효과적으로 구현 데이터베이스별로 쿼리 속도가 달라서 따로 만드는 것이 좋다 판단 확장성 및 안정성 대용량 데이터베이스의 특성 + 실시간 분석 2가지를 하나로 하기 어려움 완결성 API를 사용하는 입장에서 추가 작업없이 바로 사용할 수 있도록! 아키텍쳐 mysql에 있는 정의된 룰(스키마) 로드 새로운 서비스에 적용하려면 스키마만 추가하면 됨 개발 후기 만들면서 겪은 문제 Tornado vs Sanic 항상 legacy가 발목.. Sanic의 장점 코드 가독성 증가, LOC 감소 성능 : 2배 이상의 퍼포먼스 asyncio.Future 어떻게 사용하는지에 따라 퍼포먼스가 다름 위 코드는 비동기콜을 순차적으로 진행해서 context switching에선 효율적일 수 있찌만 개개의 속도는 비슷 아래 코드는 단일 리퀘스트를 여러개로 뿌려줘서 속도 상승 데이터베이스 프로파일링 거의 네트워크 부분(DB 조회)에서 시간이 더 소요 sanic-motor를 사용 MongoDB에서 $filter 연산을 사용 $group은 앱 서버에서 처리하는 것이 수배 빠름 DB를 모두 믿지는 말자! 꼼꼼한 캐싱 데이터베이스 조회 비용이 비싸니 최대한 캐싱을 꼼꼼히! Redis가 1개 로컬 아래에 앱이 붙어있는 구조 다양한 쿼리를 날릴 경우 중복 질의 존재 캐쉬를 세세하게 적용 프로세스 내의 임베디드 캐싱을 하면 프로세스간 중복 요청도 처리됨(1대만 쓰니까!) 슈퍼노드를 해결을 위해 인스턴스 내 캐시가 필요 DB가 샤딩으로 쪼개두고 모든 request가 골고루 퍼지길 기대 그러나 1번 샤딩만 죽는 케이스가 발생 쿼리에 특정 요청이 많이 들어오는 것은 아닌데? 이상하네 정말 Hot한 포도가 있음..! 가끔 방문한 유저는 정보가 없어서 유사한 사람을 찾아서 보여줌(콜백 사용) 그 콜백이 있던 곳이 1번이었음 샤딩 내 캐싱으로 해결 복잡한 필터링 제거하기는 쉽게 할 수 있음 뒤로 밀어내기 rolling 기능 구현 빌트인 프로시져로 처리 룰에 파이썬 코드를 심고 런타임 컴파일을 사용. @filter 설정 배포 한번 만들면 룰(스키마)만 추가함 버켓을 바꾸기 위해 마라톤 앱을 껐다 키면 리퀘스트 유실이 있을 수 있음 App을 순차적 재시작해도 리퀘스트 유실 가능 따라서 GUNICORN을 사용해 KILL SIGTERM, 안전하게 앱 재시작 퍼포먼스 마지막 파이썬으로도 가능!!! 실시간 어플리케이션 서버의 병목은 대부분 네트워크 IO CPU 연산은 수치 연산에 최적화된 라이브러리 잘 활용하기",
    "tags": "lecture etc",
    "url": "/etc/2018/08/19/pycon2018-2/"
  },{
    "title": "PyCon 2018 - 1일차 후기 및 메모",
    "text": "PyCon(파이콘) 2018 1일차 세션 메모 및 행사 후기입니다! Pycon 2018 후기 파이콘도 3년차, 처음 파이콘 왔을 땐 데이터 관련 내용만 들었는데 작년부턴 자주 접하지 못하는 분야의 세션 위주로 듣고 있습니다. 바운더리를 한정하지 않고 확장하기 위해서! 1년차 때 비해 지나가다 아는 사람들을 자주 만나네요. 더 많은 분들과 교류하고 싶어요! 내년에 발표자 신청하기. 개인 프로젝트 선정해보기 개발자 행사 중 규모가 꽤 있는 편! 다른 언어의 행사는 어떤지 궁금하네요. 종종 다른 언어 행사도 가봐야겠습니다 역시 개발자 행사 다녀오면 잠옷이 많이 생겨서 좋아요! 후원을 해준 다양한 회사들의 굿즈도 좋았고, 매년 상품 퀄리티도 좋아지고 다양한 이벤트가 진행되서 신나요!(코딩하는 문제들이 재미있었음!) 준비해주신 분들 고생하셨습니다. 내년 파이콘도 기대됩니다! 머신러닝으로 치매 정복하기 박세진님, 뷰노 한양대 컴퓨터비전 박사 과정ing 대상 머신러닝을 입문하고 자신만의 문제를 찾는 분 의료 영상 하지 않을 얘기 치매 정복은 못함ㅠㅠ 데이터에 대한 얘기 치매 연구의 목적, 방향 치매 여러 원인으로 인지능력이 저하되어 일상 생활에 장애 치료 불가하지만 진행을 늦출 수는 있음 알츠하이머에 집중 알츠하이머 치매의 가장 흔한 형태 조직병리학적 특징 : 뇌위축(Atroph) 진단 방법 뇌 및 신경계 질환 분석 MRI, CT, ET 촬영 상담 연구의 목적과 방향 치매를 조기에 발견해 치료 제안에 도움 MRI 영상에서 뇌위축에 대한 정보를 제공하는 SW 개발 해결 가능한 문제 정의 알츠하이머에 관련된 뇌영역을 영역 분할(Parcellation) 분할된 영역으로부터 특징 추출 정상인과 알츠하이머 환자 특징 DB 생성 특징 DB에서 특정 환자의 비정상 수치 산출 기존 뇌영역 분할 SW Neuroquant 분석 툴, 리포트, 시각화, 10분 Freesurfer 분석 시간 6시간 문제점 매우 느림 현실의 데이터 복잡도를 완전히 반영하진 않음 매우 복잡하고 단계가 많은 처리 방법 접근 방법 차별점 병원 진료 과정에서 사용 가능한 수행 속도 한국인의 정상인에서 비교 뇌영역 분할 수행결과를 시각화 획기적으로 빠르게 중간 단계를 최소화 현실의 다양한 데이터 복잡도를 반영 해결책 GPU End to End 지향 Brain MRI 전처리 과정 Biase field correction -&gt; Resample to 1mm lsotropic -&gt; AC-PC Alignment 엄청 복잡 CNN Atrous convolution ResNet Dice loss 전처리 사진 참고 파이썬 영상처리 opencv가 가장 유명하지만 Skimage도 좋음 PIL Numpy Object Detection Faster RCNN vs Deconvolutional network Pixel Classification Data Imbalance Positive, negative, class imbalance Categorical weight balancing : Label의 비율대로 class별 loss 비율 맞춤 Dice coefficient loss High ResNet No Pooling (Atrous Convolution이라!) 3D convolution 알츠하이머 예측 Volumne, ICV Percentile 기반 Biomarker 기반 임상 검증 다양한 장비, 환경에서 재현성, 일관성 검증 어떤 측도를 제시할 경우 현실적 정확도를 만족하는가 과정이나 인과를 설명 가능한가? 설명할 수 없다면 문제를 부분적으로 설명 A-B-C 인과 관계에서 B만이라도! 결론 왜 치매 연구를 하는가? 치매 예방을 위해 해결 방안 의료 데이터 불규칙성 극복 객체 인식 기법 3D 데이터 학습 Luigi로 Data Pipeline system 구현하기 TRUE SHORT : 주식 정보(공매도 전문) 제공 CTO, 송은우님 사내 데이터 파이프라인을 파이썬 스크립트와 크론탭으로 시작해서 Luigi로 업그레이드한 이야기 ETL Extract, Transform, Load 시작은 파이썬 스크립트와 크론탭으로 시작 처음엔 충분 기본 데이터 취합 데이터 분석/가공 데이터 저장 그러나 시스템의 규모가 커지고, 데이터의 종류가 다양해지며 복잡해짐 데이터가 준비되는 시점에 모두 틀림 실제 구조 데이터를 받아오는 스크립트를 각각 시점에 시간순으로 실행 다 받아질거라 예상되는 시점에 랭킹을 계산하는 스크립트 작성 그러나 데이터를 외부 소스에서 받아오기 때문에 다양한 문제가 있을 수 있음 Error 데이터가 준비되는 시간 지연 데이터가 준비되어 있는지 Dependency 체크 Depedency가 준비 안되었으면 Dependency 재실행 그 후 전체 로직 재실행 실제 로직보다 보일러플레이터 코드들이 많아짐 What I Need Workflow 관리 즁복 작업 최소화 부분 실패 최소화 Task 모니터링 Dependency 그래프 시각화 보일러플레이트 코드 최소화 Luigi luigi.Task를 Super Class로 사용 파라미터 정의 / 명확한 Depedency 정의 / 비즈니스 로직 / Output Luigi Github 기본적으로 Airflow와 유사한 듯 스케일 아웃도 쉬움 Luigi도 만능은 아님 No central task trigger system 루이지를 실행할 때, crontab 사용 분산 실행(Distributed Execution)은 우리 일 여러 Task를 동시에 돌리는 것은 Luigi에서 지원 동일한 Task를 워커에 나누어 작업하는 것은 미지원해서 직접 구현해야 함 설정은 많으나 문서가 충분하지 않음 Tips Dependencise들은 (가능하면) 최대한 심플하게 Task는 최소한의 단위로 Extraction, Transfer, Load 모두 각각의 Task로 구분 Noitification 또한 최소한 것만 설정 PyQt로 만드는 웹기반 데스크탑 어플리케이션 PyCon 2015, 2017 발표 예제로 배우는 PyQt PyQt : 상대적으로 나쁘지 않은 개발환경, 크로스 플랫폼 지원 But 지난 세대 디자인과 부족한 위젯 모듈 QWebEngine Web을 보여줄 수 있는 Qt Widget Qt4, Qt5 이후에 빠르게 변하고 있는 모듈 예제 코드 View만 띄운 거라 이벤트를 할 수 없음 Chrome을 통한 웹 디버깅 QtWebEngine은 크로미움 기반 실행시 옵션 추가 : sys.argv.append(\"--remote-debugging-port=8000\") 웹 View를 사용하는 대신 다 직접 만들어야 함 네비게이션바, 웹 표출단, 상태바 위젯을 따로 빼서 작성 웹 요청 처리 새 창, 탭 생성 요청 닫기 요청 다운로드 요청 프린터 사용 요청 Flash 사용 허가 경고창 변경 QWebChannel PyQt와 Web을 서로 이어주는 역할 데이터를 전달하고 기능을 호출 코드 동작 개요 이미지 웹을 불러온 이후 웹에 QWebChannel 삽입하기 웹브라우저 사용과 아닌 경우를 분리 코드 QWebChannel에서 사용되는 데이터 타입 QJsonValue, QJsonDocument 타입을 지키지 않으면 데이터를 주고 받을 수 없음 하이퍼커넥트에서 자동 광고 성과 측정 시스템 구축하기 박승호님(데이터 엔지니어), 정강식님(CTO) CTO님이 하이퍼커넥트 소개해주셨음 WebRPC 통화 글로벌 인프라 모바일 기기에서 딥러닝 가속 기술 사용 아자르를 제외한 다른 앱에선 대부분 파이썬 백엔드 사용(생산성을 위해) ML 관련도 파이썬 사용 데이터 기반 의사결정 시 파이썬 사용 아키텍쳐 자동 광고 성과 측정 시스템 디지털 마케팅 정량적 성과 평가 가능 광고에 얼마만큼 지불, 노출, 광고를 통해 얼마나 들어왔는가 등 슬랙 채널에 Report 알림을 붙여서 제공 ROAS 높으면 -&gt; 유사한 광고 생성, ROAS가 안좋으면 광고 중단 아키텍쳐 스케쥴링 cron : 디펜던시에 취약 Airflow Daily Report 작성 Ad Spending 페이스북, 애드워즈 API를 통해서 가져옴 페이스북 API를 사용해 가져오는 것 정말 쉬움! 매체마다 다른 마케팅 용어를 통일 Revenue(payment) 결제 데이터는 트랜잭션 데이터베이스(MySQL)에 쌓임 주기적으로 스냅샷을 찍어 테이블로 구성 Revenue(user behavior) 매출과 관련된 이벤트 로그도 측정 프리미엄 매치를 하고 싶은데, 매칭 상대방이 없다면? 돈을 쓰고 싶어도 돈을 쓸 수가 없는 것! 매출에 기여한 유저라 판단 서버 로그를 전처리 + 익명화 과정을 통해 원하는 테이블로 구성 실재 매출이 발생하지 않았는데 어떻게 추산할 수 있을까? 서비스마다 다르고 경험적인 내용이 들어가야 함 Page Rank 알고리즘 영향력 있는 페이지를 보여주는 알고리즘 영향력 있는 페이지가 인용할수록 해당 페이지의 PageRank가 올라감 높은 매출인 유저와 매치하면 상대방 유저의 PageRank가 올라감 Shapely Value 게임 이론 서비스마다 매출이 될 수 있는 행동이 존재하지만, 그에 따라 적절한 알고리즘을 선택 Attribution 유저가 어떤 광고를 통해 앱에 유입했는지 파악 각 매체에서 제공하는 SDK를 클라이언트에 추가 : 비용 감소, 개발 코스트 증가 Third Party Tool 사용 : TUNE, Appsflyer : 비용 증가, 개발 코스트 감소, 용량 감소 LTV 고객 생애 가치, 해당 고객이 평생동안 얼마만큼의 가치를 가질 것인가? 고객 획득 비용(마케팅 비용) + 고객 매출(구매) + 고객 유지 비율(리텐션) LTV 계산 방법 : BG/NBD 모델 가정1 : 고객은 일정한 비율로 구매하고, 고객마다 다른 비율을 가진다 가정2 : 고객은 구매 직후 일정한 확류로 이탈하고, 고객마다 다른 확률을 가진다 구매 빈도는 포아성 분포를 따르고, 고객간 이질성은 감마 분포를 따른다고 가정 Drouput Rate는 Geometric 분포, 고객간 이질성은 베타 분포 구매 정보를 사용해 다시 구매할 확률과 이탈할 확률을 계산 Gamma-Gamma 모델 : 가격 관련 모델링 가정 : 고객의 1회 구매 금액은 감마 분포를 따룬다. 고객간 이질성은 감마 분포를 따른다 파이썬에서 lifetimes 라이브러리 존재(다행) ToDo 광고 성과에 따른 액션(광고예산, 입찰 수준 변경)을 자동화! WIP : 광고 성과(Alert에 따른 마케터의 액션 수집중 광고 노출 최적화 서비스 인프라에 따른 광고 노출을 최적화 No-Pacing 광고와 Pacing 광고의 성과 비교 -&gt; 독자적인 알고리즘 생성 타겟팅 광고 자동화 광고 성과가 좋을 것으로 예상하는 유저를 자동으로 타겟팅",
    "tags": "lecture etc",
    "url": "/etc/2018/08/18/pycon2018-1/"
  },{
    "title": "Coursera Kaggle 강의(How to win a data science competition) 1주차",
    "text": "Coursera 강의인 How to Win a Data Science Competition: Learn from Top Kaggler Week1을 듣고 정리한 내용입니다 데이터 경진대회를 처음 접하는 분들에게 추천하고 싶은 강의입니다. 다양한 팁들이 존재하네요! Competiton mecahnics Competitions’ concepts Data csv, text, file, db dump, image 등으로 제공 description을 읽어보면 feature 추출시 유용 Model 거대하고 많은 component로 구성된 모델(stacking) Submission 제출해서 점수 확인 Evaluation 모델이 좋은지 측정 -&gt; score (Accuracy, Logistic loss, AUC, RMSE, MAE 등) Leaderboard 리더보드의 랭킹 확인 그러나 이 점수가 최종은 아님 대회중엔 Public Test을 사용하고, 최종 랭킹을 산정할 땐 Private Test를 사용 Real World Application vs Competitions 현실의 머신러닝 문제는 매우 복잡 비즈니스 문제를 이해 문제를 형식화(formalization) 데이터 수집 데이터 전처리 모델링 모델 평가할 방법 결정 Model 배포할 방법 결정 모델 퍼포먼스를 모니터링하고, 새로운 데이터로 재학습 경진 대회는 데이터 수집, 모델링에 초점(나머진 fix됨) 결론 현실의 문제가 더 복잡 Competition은 배우기 좋은 방법 그러나 문제 형식화, 배포, 테스팅을 하진 않음 Competition의 철학 알고리즘 자체가 다 해주진 않음 모든 사람들이 고전적인 접근 방식을 튜닝할 수 있음 이기기 위한 인사이트가 필요 가끔은 ML이 필요없음 휴리스틱한 방법, 메뉴얼된 데이터 분석도 OK 복잡한 방법, 심화된 feature engineering, 거대한 연산을 두려워하지 말자! 창의적으로 접근 존재하는 알고리즘을 바꾸거나 완전 새로운 것을 만들어보는 것도 좋음 다른 사람의 소스를 읽고 바꿔보는 것도 좋음 Enjoy! Do your best! Recap of main ML algorithms Linear Try to seperate objects with a plane which divides space into two parts 공간을 2부분으로 나누는 선 찾기 example : Logistic Regression, SVM spare high dimensional data일 때 좋음 비선형 문제는 풀지 못함 Tree-based 공간을 2부분으로 나누는 선 (1개의 class와 그 나머지) 찾기 그 후 나머지를 또 2부분으로 나누는 선 찾기 tabular 데이터(표 형태의)에서 좋은 default 방법 example : Decision Tree, Random Forest, GBDT, xgboost, LightGBM kNN 가장 근처에 있는 객체와 같은 라벨을 가짐 Missing value를 채울 때, kNN으로 가능 Neural Networks 이미지, 사운드, text, 시퀀스 데이터에 좋음 이 강의에선 다루지 않음 Conclusion Silver bullet 알고리즘은 없음(=전지전능한 알고리즘은 없다!!!) Linear model들은 2개의 subspace로 나눔 Tree-based 방법은 공간을 상자로 나눔 k-NN은 “closeness”를 어떻게 측정하는지에 따라 영향을 받음 NN은 non-linear한 decision boundary를 스무스하게 해줌 가장 강력한 방법은 Gradient Boosted Decision Trees와 Neural Networks 그러나 다른 방법을 간과하진 말자! 참고 자료 Explanation of Random Forest Explanation/Demonstration of Gradient Boosting Example of kNN Software/Hardware requirements 대부분의 competition을 풀 수 있는 사양(이미지 제외) CPU 4+ cores, 16+ gb ram 꽤 좋은 사양 CPU 6+ cores, 32+ gb ram RAM : 많을수록 좋음 Core : 많을수록 좋음 Storage : 작은 조각이 많은 큰 데이터 또는 이미지일 경우 SSD 중요 Cloud resource AWS : spot instance option! Azure GCP Software Language Python : Numpy, Pandas, Matplotlib, Scikit-learn IDE Jupyter Special packages XGBoost, Keras, Light GBM, Catboost External tools Vowpal Wabbit : 거대한 데이터셋을 핸들링할 때 유용 libfm, libffm : sparse한 CTR 데이터를 다룰 때 유용 fast_rgf : 또다른 tree-based 방법 Blog “datas-frame” (contains posts about effective Pandas usage) Feature preprocessing and generation with respect to models Overview 기존에 있는 feature를 토대로 새로운 feature 생성 Main topics Feature preprocessing Feature generation Their dependence on a model type Features : numeric, categorical, ordinal, datetime, coordinates Missing values Feature type별로 preprocessing과 generation을 다르게 진행 Numeric features Numeric feature : 수치로 표현한 자료 Non-tree Linear, kNN 모델은 scaling에 따라 결과값이 변함 gradient descent 방법은 적절한 scaling이 없으면 미칠 수 있음(Neural net도 동일) Scaling MinMaxScaler : Range가 규정된 경우 사용(RGB), Outlier가 없는 경우 사용 StandardScaler : PCA시 사용 Outliers lower bound, upper bound를 선정해서 1~99% 데이터만 사용(clip) Rank transformation Outliear가 있다면 MinAxScaler보다 좋은 옵션 scipy.stats.rankdata로 사용 Test시에도 같은 rank value 적용 Log transform : np.log(1+x), 큰 value를 평균에 가깝도록 만들기 때문에 유용 Raising to the power &lt;1 : np.sqrt(x +2/3) 하나의 scaling만 사용하지 않고 여러 scaling을 사용해 feature로 추가할 경우 좋을 수 있음 Feature generation 사전 지식과 EDA 기반으로 진행 만들기 쉬운 것부터 생성하고, feature를 더하고 곱하고 빼고 나누고 등등.. Binning Fixed binning Adaptive binning Tree scaling에 거의 영향을 받지 않음 Categorical and ordinal features Categorical feature : 수치로 측정이 불가능한 자료, 범주형 변수 Ordinal feature : 카테고리 데이터처럼 비 연속적이지만 숫자처럼 비교 가능할 경우 Label encoding Categorical data를 숫자로 변환 sklearn.preprocessing.LabelEncoder와 Pandas.factorize로 가능 전자는 알파벳 순 또는 정렬된 순서로 인코딩 후자는 있는 그대로 인코딩 데이터마다 다르지만 factorize가 약 2배정도 빠름 Frequence Encoding 빈도별 인코딩 Linear, Tree 모델 모두 도움이 될 수 있음 Value의 빈도가 Target과 연관이 있으면 유용 Encoding시 같은 빈도를 가지면 Feature로 추가하지 않아도 좋음 One-hot encoding Tree 모델에선 효율성이 떨어질 수 있음 컬럼이 많아져 학습이 힘들 수 있음(메모리 이슈) spare 매트릭스는 categorical data나 텍스트 데이터에서 종종 유용 pandas.get_dummies, sklearn.preprocessing.OneHotEncoder Datetime and coordinates Datetime Periodicity Datetime feature를 week, month, season, year, second, minute, hour, 주말/평일 등으로 나눠서 추가 Time since 독립적 기간 : since 1970 1/1 의존적 기간 : 다음 연휴까지 남은 일, 지난 연휴부터 지난 일 Difference between dates datetime_feature_1 - datetime_feature_2 Coordinates 위도, 경도 Tree 모델은 Numerical feautre가 선형성을 띄면 구분하기 힘들 수 있어서 회전을 주고 사용하기도 함 근접한 지역을 추가 feature로 생성 가능 클러스터링에 사용할 수 있음(Center of clusters를 찾음) 지역별 feature(Aggregated stats) Handling missing values 보통 missing value를 ‘’, -1, 99 등으로 채움 feature 별로 히스토그램을 그려서 주최측이 missing value를 어떻게 처리했는지 유추할 수 있음 Fillna approaches -999, -1, etc Tree model mean, median simple linear model, neural net에 유용 Reconstruct value “Isnull” feature missing value를 조심스럽게 다뤄야 함. 이 작업에 따라 성능이 달라질 수 있음 Reference Coursera How to win a data science competition Competitive-data-science Github",
    "tags": "kaggle data",
    "url": "/data/2018/08/16/how-to-win-a-data-science-competition-week1/"
  },{
    "title": "CS224n 1강. Natural Language Processing with Deep Learning",
    "text": "Stanfoard Stanfoard CS224n 2017를 요약한 포스팅입니다. 정리 목적이 강한 글입니다! :) 자연어 처리 전산 언어학과 동의어 컴퓨터 과학과 언어학과 인공 지능의 교차점 우리가 하려는 일 컴퓨터를 영리하게 하는 것 인간의 언어를 이해하고 인간처럼 인간 언어를 표현 따라서 자연어 처리는 인공지능의 일부로 간주 인공지능의 다른 중요한 부분 언어의 매우 독특한 특성 우리는 언어를 통해 세상에 대해 생각 우주의 많은 생물들 중 언어를 사용하는 것은 사람뿐 언어는 사고의 도구이자 의사소통의 도구 컴퓨터가 인간의 언어를 처리할 수 있는 방법을 배우는 것이 목표 거대한 IT 회사들이 시리, 구글 어시스턴트 등으로 사용자와 자연어를 사용해 소통 키보드는 입력하기 작고 문자 메세지를 입력하는 속도의 편차가 존재 한자는 엄청 더 어려움 이 수업에서 들을 수 있는 것은 인간 언어에 대한 감사와 언어에서 레벨이 의미하는 것이 무엇이고 어떻게 처리하는지 등 NLP Levels input이 speech 또는 text - 데이터 전처리 - 구문 분석으로 문장의 구조 이해 - 의미론적 이해 - 담화 처리 Speech recognition이 제일 처음 유용하다고 입증 동그라미 친 부분에 집중해서 강의할 예정 NLP Applications Spell checking, keyword search, finding synonyms 웹사이트에서 정보 추출 상품 가격, 날짜, 위치, 사람/회사 이름 Classifying : 긍정/부정 Machine transplation : 한국어 to 영어 Spoken dialog systems Complex qeustion answering NLP in industry Search (written and spoken) Online advertisement matching Automated/assited translation Sentiment analysis for marketing or finance/trading Speech recognition Chatbots/Dialog agents Automating customer support Controlling devices Ordering goods Special about human language 말하는 사람(글쓴 사람)의 의미를 전달하도록 제작 discrete/symbolic/categorical signaling system 기호를 통해 거리가 있어도 안정적으로 신호를 보냄 우리의 뇌 역시 지속적인 활성화 패턴을 가지고 있고, 언어를 사용할 때마다 계속 상징적으로 변함 large vocabulary, symbolic encoding of words가 머신러닝에서 문제 =&gt; Sparsity! Why NLP is difficult 인간 언어는 모호하고 프로그래밍 언어는 모호하지 않고 규칙을 가지고 있음 사람의 언어는 생략해도 이해할 수 있음 효율적으로 빠르게 의사소통하는 것이 목표 듣는 사람은 상식이나 상황에 대한 문맥 지식으로 이해하는데 이게 어려운 가장 큰 이유 Word 모두 벡터로 나타낼 수 있음(vectorizing) 소리, 단어의 부분, 문장 벡터를 재구성할 수 있음. 다음 강의에서 Vector에 대해 설명할 예정 Reference Stanfoard CS224n 2017",
    "tags": "cs224 data",
    "url": "/data/2018/08/16/cs224n-intro/"
  },{
    "title": "jupyter notebook(ipynb)로 jekyll 글쓰기",
    "text": "기존엔 Markdown 파일로만 글을 작성했는데, 코드와 함께 블로깅을 하고 싶은 생각에 방법을 찾아봤습니다 제 테마에 맞게 하나씩 삽질한 과정을 작성한 글입니다 이 글은 jupyter notebook(ipynb)에서 작성되었으며, 환경은 Mac OS입니다 Jekyll 플러그인을 만들었다가, Github Page에선 안되는 것을 알고 뒤늦게 방법을 수정했습니다 관련 공식 문서 쉘 스크립트 생성(jupyter nbconverter 사용) &#51652;&#54665; &#44284;&#51221; 1) jupyter nbconverter 사용하기 2) _ipynbs 폴더에 *.ipynb 작성 3) 쉘 스크립트를 사용해 *.ipynb을 *.html으로 변환 4) Github Push 1) jupyter nbconverter &#49324;&#50857;&#54616;&#44592; jupyter nbconverter는 Jupyter Notebook 파일을 (ipynb) 다른 형태로 변환시켜줍니다 PDF, HTML, LaTex, Reveal.js HTML slideshow, Markdown, notebook 등.. 저흰 여기서 HTML을 사용합니다! Markdown으로 변환해도 되긴 하는데, output의 이미지 주소를 연결하는 작업이 귀찮아서 HTML로 선택했습니다 &#49444;&#52824; nbconvert의 모든 기능을 사용하고 싶은 경우엔 Pandoc과 Tex도 설치해야 합니다! nbconvert 공식 문서 // linux sudo apt-get install pandoc sudo apt-get install texlive-xetex // mac brew install pandoc /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"&lt;code id=\"selectable\"&gt;&lt;/code&gt; brew install caskroom/cask/brew-cask brew cask install mactex pip(3) install nbconveter &#49324;&#50857;&#48277; jupyter nbconvert --to html your_ipynb_file.ipynb 다른 Format이나 설정은 nbconvert 공식 문서 참고 2) ipynb &#54028;&#51068; &#51089;&#49457; _ipynbs 폴더를 만든 후, 쥬피터 노트북 파일을 작성해주세요 저는 2018-08-15-jupyter-notebook-in-jekyll.ipynb로 만들었습니다! _posts 폴더에 2018-08-15-jupyter-notebook-in-jekyll.html 생성 아래와 같은 정보 추가 이 부분은 어떻게 자동화할지 고민해보기(jupyter notebook에서 아래 정보를 넣고 변환하면 깨짐) --- layout: post title: \"jupyter notebook(ipynb)로 jekyll 글쓰기\" subtitle: \"jupyter notebook(ipynb)로 jekyll 글쓰기\" categories: development tags: web comments: true --- 3) &#49752; &#49828;&#53356;&#47549;&#53944;&#47484; &#49324;&#50857;&#54644; &#54028;&#51068; &#48320;&#54872; 최상단 폴더에 ipython_to_html.sh 생성 사용 방법 : bash ipython_to_html.sh {file_name} file_name은 2018-08-15-jupyter-notebook-in-jekyll 이런식으로! 지금은 아래와 같은 방식을 사용하시만 추후 ipynb 상관없이 대응하도록 수정할 예정 현재는 &gt;&gt; (append) #!/bin/bash/ file_name=$1 echo ${file_name} function converter() { jupyter nbconvert --to html --template basic --stdout --TemplateExporter.exclude_input_prompt=True --HTMLExporter.anchor_link_text=' ' `pwd`/_ipynbs/${file_name}.ipynb &gt;&gt; `pwd`/_posts/${file_name}.html echo \"Success\" } converter 4) Github Push 위 스크립트로 _posts에 파일이 변환! 그 파일을 push하면 끝! 단, 이 방법은 직접 쉘 스크립트를 실행해줘야 하는 단점이 있음.. 더 자동화할 예정인데 아이디어 있는 분은 댓글을 남겨주세요 :) Test (##) Test (###) Test (####) 여기부턴 어떻게 ipynb 파일이 변하는지 테스트하기 위해 작성했습니다 목차1 목차2 세부 목차1 Markdown Table Process Max needs Current needs P0 10 5 P1 4 2 P2 9 2 Image img src=\"\" 사용 Code &#48512;&#48516; 디테일한 format은 추후 수정 예정! import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; # 걍 주석 plt.plot([1,2,3,4]) plt.ylabel(&#39;some numbers&#39;) plt.show() print(&quot;convert html test&quot;) convert html test def test_function(n): print(&quot;len : {}, &quot;.format(len(n)), &quot;word is &quot; + n) test_function(&quot;oh&quot;) len : 2, word is oh",
    "tags": "web development",
    "url": "/development/2018/08/15/jupyter-notebook-in-jekyll/"
  },{
    "title": "OS 디스크 스케쥴링 이해하기",
    "text": "디스크 스케쥴링에 대해 작성한 글입니다 디스크 스케쥴링 대표적인 보조 기억장치 : 하드 디스크 실린더의 헤드를 움직여야 함 Disk Scheduling 디스크 접근 시간 Seek time + rotational delay + transfer time 탐색시간 (seek time) 이 가장 크다. 다중 프로그래밍 환경 대부분의 프로그램은 디스크를 사용하기 때문에 디스크 큐(disk queue)에는 많은 요청(request)들이 쌓여있다 요청들을 어떻게 처리하면 탐색시간을 줄일 수 있을까? 디스크 스케쥴링 알고리즘 FCFS (First-Come First-Served) 음식점같은 방법 SSTF (Shortest-Seek-Time-First) SCAN FCFS Scheduling First-Come First-Served Simple and fair 예제 200 cylinder disk, 0 .. 199 Disk queue: 98 183 37 122 14 124 65 67 Head is currently at cylinder 53 53 -&gt; 98 -&gt; 183 -&gt; 37 … Total head movement = 640 cylinders Is FCFS efficient? No.. SSTF Scheduling Seek Time이 최소화되는 것부터 처리 Seek Time : 디스크 헤더를 움직이는 시간 Shortest-Seek-Time-First 현재 헤드 위치에서 seek time이 가장 작은 요청을 선택 예제 200 cylinder disk, 0 .. 199 Disk queue: 98 183 37 122 14 124 65 67 Head is currently at cylinder 53 Total head movement = 236 cylinders 문제점 Starvation Seek time이 많이 떨어진 프로세스는 계속 기다림.. 그러면서 외부에서 새로운 놈이 들어옴(그러나 Seek Time이 짧음) 그러면 기존 프로세스는… 안녕… Is SSTF optimal? No! (e.g., 53 → 37 → … = 208 cyl) SCAN Scheduling Scan disk 헤드가 디스크를 계속해서 앞뒤로 스캔 스캔하는 방향에 따라 결과가 달라짐 예제 200 cylinder disk, 0 .. 199 Disk queue: 98 183 37 122 14 124 65 67 Head is currently at cylinder 53 (moving toward 0) Total head movement = 53+183 cylinders (less time) = 236 토론 프로세스가 많으면 실린더에 대한 요청은 골고루 있다고 가정 Circular SCAN is necessary! 최고 안쪽은 최고 바깥이랑 연결된 C-SCAN SCAN Variants: C-SCAN, LOOK, C-LOOK C-SCAN Treats the cylinders as a cicular list that wraps around from the final cylinder to the first one LOOK The head goes only as far as the final request in each direction Look for a request before continuing to move in a given direction C-LOOK LOOK version of C-SCAN Elevator Algorithm 스캔 알고리즘을 엘레베이터라고 부름(기존 그래프를 90도 rotate하면 엘레베이터처럼 보임) The head behaves just like an elevator in a building, first servicing all the requests going up, and then reversing to service requests the other way Reference 양희재 교수님 운영체제 강의 introduction to Operating System",
    "tags": "os development",
    "url": "/development/2018/08/03/disk-scheduling/"
  },{
    "title": "BigQuery ML Beta 사용기",
    "text": "최근 발표되고 이슈가 되었던 Google Cloud BigQuery ML Beta를 사용하며 남긴 글입니다. Beta라서 많이 바뀔 것 같지만 그래도 남겨봅니다! 공식 문서를 보고 작성했습니다 BigQuery ML BigQuery ML에 대해 설명할 필요 없이 위 Gif를 참고하면 될 것 같습니다 이 서비스가 충격적이고 Hot한 이유는 쿼리문으로 쉽게 모델을 만들 수 있는 점과 AutoML을 보여주기 때문이라 생각합니다 BigQuery의 standard SQL 쿼리로 머신러닝 모델을 생성하고 실행할 수 있습니다 예측 Table을 만든 후, REST API처럼 사용할 수 있습니다 제공하는 모델 현재(18.8월) 베타 기준으로 2가지 모델이 제공됩니다 다음 달에 classification, recommendation 등이 지원될 예정이라 합니다 Linear regression numerical value를 예측할 때 사용 Binary Logistic regression 이메일 스팸 필터처럼 2개의 클래스를 분류할 때 사용 사용할 수 있는 곳 BigQuery web UI bq command-line tool BigQuery REST API Jupyter Notebook 같은 외부 툴 BigQuery ML의 장점 Python이나 Java를 몰라도 SQL문으로 Machine Learning 모델을 생성할 수 있음. 즉 쉽게 모델을 만들 수 있습니다 데이터웨어 하우스(BigQuery)에서 데이터를 Export하지 않고 모델링이 진행되기 때문에 속도가 빠릅니다 학습 파라미터, Feature, Weight 등의 정보도 제공합니다 가격 정책 BigQuery의 가격 정책을 그대로 따릅니다 데이터 저장비용 쿼리 비용(쿼리할 때 탐색하는 데이터양에 비례해 비용이 부과) 허나 아직 베타라서 이런 듯! 추후 가격 정책이 생길 것 같습니다 Getting Started with BigQuery ML Data Analyst와 Data Scientist를 위한 글로 나뉘어져 있는데, 전자는 BigQuery Console에서 진행하고 후자는 Datalab에서 진행합니다(데이터셋은 동일) 저는 BigQuery Console에서 Data Scientist에 나와있는 내용을 진행했습니다 Google Analytics 샘플 데이터를 사용해 website 방문자가 결제를 할지 예측하는 모델을 만듭니다 Method 정리 모델 생성할 때 : CREATE MODEL Evalute할 때 : ML.EVALUATE Predcit할 때 : ML.PREDICT 1. Dataset 생성 BigQuery Console에서 리소스 - 프로젝트 아이디 클릭해주세요 데이터세트 만들기 클릭! : id는 bqml_tutorial 2. 분류 모델 생성하기 우리가 다룰 데이터는 다음과 같습니다 label은 0, 1로 transcation이 있으면 1 없으면 0 os, mobile 유무, 국가, pageviews가 feature 모델 생성은 아래와 같은 쿼리로 수행합니다 CREATE OR REPLACE MODEL `bqml_tutorial.sample_model` OPTIONS(model_type='logistic_reg') AS SELECT IF(totals.transactions IS NULL, 0, 1) AS label, IFNULL(device.operatingSystem, \"\") AS os, device.isMobile AS is_mobile, IFNULL(geoNetwork.country, \"\") AS country, IFNULL(totals.pageviews, 0) AS pageviews FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` WHERE _TABLE_SUFFIX BETWEEN '20160801' AND '20170630' label이란 컬럼이 꼭 필요합니다(label2라고 하면 에러 뿜음) 20160801부터 20170630까지 약 11개월치 데이터를 학습했는데 약 8분 소요 Test로 20170601부터 20170630까지 1개월 데이터를 학습했는데 99초 소요 (UI가 다른 이유는 이 테스트는 과거 BigQuery Console에서 진행했기 때문입니다! 과거 UI에서도 정상 작동함) 3. 학습 모델 정보 보기 생성된 Table에서 정보 확인 모델 유형, loss 유형, 학습 옵션도 보임!!! Early Stopping이 적용된 듯 위 방법 이외에도 모델의 정보를 보고 싶은 경우엔 ML.TRAINING_INFO 를 사용하면 됩니다 SELECT * FROM ML.TRAINING_INFO(MODEL `bqml_tutorial.sample_model`) training_run은 새로 생성된 모델은 값이 0이고 warm_start 옵션을 사옹할 경우, 다시 학습시 값이 증가합니다 Feature의 정보를 보고싶을 경우 SELECT * FROM ML.FEATURE_INFO(MODEL `bqml_tutorial.sample_model`) Weights의 정보를 보고싶을 경우 SELECT * FROM ML.WEIGHTS(MODEL `bqml_tutorial.sample_model`) 이런 식으로 weight도 나옵니다! 충격적… 4. 모델 평가하기 ML.EVALUATE를 사용해 모델 평가합니다 학습할 때 사용하지 않은 20170701 ~ 20170801 데이터로 평가했습니다 SELECT * FROM ML.EVALUATE(MODEL `bqml_tutorial.sample_model`, ( SELECT IF(totals.transactions IS NULL, 0, 1) AS label, IFNULL(device.operatingSystem, \"\") AS os, device.isMobile AS is_mobile, IFNULL(geoNetwork.country, \"\") AS country, IFNULL(totals.pageviews, 0) AS pageviews FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` WHERE _TABLE_SUFFIX BETWEEN '20170701' AND '20170801')) 1초가 지난 후 아래와 같은 결과가 나왔습니다 5. 모델로 예측하기 ML.PREDICT을 사용해 예측한 후, 국가별 총 예측 구입량을 계산했습니다 SELECT country, SUM(predicted_label) as total_predicted_purchases FROM ML.PREDICT(MODEL `bqml_tutorial.sample_model`, ( SELECT IFNULL(device.operatingSystem, \"\") AS os, device.isMobile AS is_mobile, IFNULL(totals.pageviews, 0) AS pageviews, IFNULL(geoNetwork.country, \"\") AS country FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` WHERE _TABLE_SUFFIX BETWEEN '20170701' AND '20170801')) GROUP BY country ORDER BY total_predicted_purchases DESC LIMIT 10 역시 1초 뒤 아래와 같은 결과가 나왔습니다 BigQuery ML 문법 CREATE MODEL : 링크 l1_reg, l2_reg, max_iterations, learn_rate_strategy, learn_rate, early_stop, min_rel_progress, data_split_method, warm_start 등을 설정할 수 있음 ML.EVALUATE : 링크 logistic regression은 precision, recall, accuracy, f1_score, log_loss, roc_auc로 결과가 나옴 linear regression은 mae, mse, msle, mae, r2_score, explained_variance로 결과가 나옴 ML.PREDICT : 링크 ML.TRAINING_INFO : 링크 ML.FEATURE_INFO : 링크 ML.WEIGHTS : 링크 Tutorial Using BigQuery ML to Predict Basketball Outcomes Using BigQuery ML to Predict Birth Weight 사용 후기 원래 BigQuery에 대한 만족이 높았는데, 이번 BigQuery ML로 더욱 만족도가 높아졌습니다 Structured된 데이터라면 더 쉽게 모델을 만들 수 있을 것 같습니다 단, BigQuery 특성상 딥러닝 모델까지 나올진 의문이 듭니다 BigQuery는 쿼리할 때 탐색하는 데이터의 양에 따라 비용이 부과되는데 비정형 데이터는 데이터가 크기 때문에 비용도 들고.. 비효율성이 나타날 것 같음 BigQuery에 데이터를 적재하기도 어려운 상황 아마 구조화된 데이터는 BigQuery ML로 비정형 데이터는 AutoML로 커버하지 않을까 생각됩니다 그래도 갓구글님들이 이런 제품을 내놓아서 리스펙.. 자꾸 내놓으면 저같은 데이터쟁이는 어떻게 되나요(..) Reference BigQuery ML - Machine Learning using SQL in BigQuery Tech crunch",
    "tags": "BigQuery gcp",
    "url": "/gcp/2018/08/03/bigqueryml/"
  },{
    "title": "OS 파일 할당 이해하기",
    "text": "파일 할당에 대해 작성한 글입니다 파일 할당 파일 시스템 중 일부인 파일 할당에 대해 배웁니다 컴퓨터 시스템 자원 관리 CPU: 프로세스 관리 (CPU 스케쥴링, 프로세스 동기화) 주기억장치: 메인 메모리 관리 (페이징, 가상 메모리) 보조기억장치: 파일 시스템 보조기억장치 (하드 디스크) 하드디스크: track (cylinder), sector Sector size = 512 bytes, cf. Block size Sector를 모아둔 것이 Block 블록 단위의 읽기/쓰기 (block device) &lt;-&gt; character device : 글자 단위 디스크 = pool of free blocks 각각의 파일에 대해 free block 을 어떻게 할당해줄까? ☞ 파일 할당 파일 할당 (File Allocation) 연속 할당 (Contiguous Allocation) 연결 할당 (Linked Allocation) 색인 할당 (Indexed Allocation) 연속 할당 리스트처럼 할당 Contiguous Allocation 우리가 가장 쉽게 생각할 수 있는 할당 방법 각 파일에 대해 디스크 상의 연속된 블록을 할당 장점: 디스크 헤더의 이동 최소화 = 빠른 i/o 성능 옛날 IBM VM/CMS 에서 사용 동영상, 음악, VOD 등에 적합 순서적으로 읽을 수도 있고 (sequential access 순차접근) 특정 부분을 바로 읽을 수도 있다 (direct access 직접접근) 문제점 파일이 삭제되면 hole 생성 파일 생성/삭제 반복되면 곳곳에 흩어지는 holes 새로운 파일을 어느 hole 에 둘 것인가? ☞ 외부 단편화! (메모리에서 발생했던 문제) First-, Best-, Worst-fit 단점: 외부 단편화로 인한 디스크 공간 낭비 Compaction 할 수 있지만 시간 오래 걸린다 (초창기 MS-DOS) 디스크의 1/3을 날림 또 다른 문제점 파일 생성 당시엔 이 파일의 크기를 알 수 없다 ☞ 파일을 어느 hole 에 넣어야 할까? 파일의 크기가 계속 증가할 수 있다 (log file) ☞ 기존의 hole 배치로는 불가! 어떻게 해결할 수 있을까? 연결 할당 연결 리스트처럼 할당 Linked Allocation 파일 = linked list of data blocks 파일 디렉토리(directory)는 제일 처음 블록 가리킨다. 각 블록은 포인터 저장을 위한 4바이트 또는 이상 소모 새로운 파일 만들기 비어있는 임의의 블록을 첫 블록으로 파일이 커지면 다른 블록을 할당 받고 연결 외부 단편화 없음! 외부 단편화는 없어 제일 중요한 디스크 낭비는 없음 문제점 순서대로 읽기 = sequential access Direct access 불가 포인터 저장 위해 4바이트 이상 손실 낮은 신뢰성: 포인터 끊어지면 이하 접근 불가 연속 할당은 포인터가 끊어져도 그 다음 것을 알 수 있음 느린 속도: 헤더의 움직임 개선: FAT 파일 시스템 File Allocation Table 파일 시스템 MS-DOS, OS/2, Windows 등에서 사용 포인터들만 모은 테이블 (FAT) 을 별도 블록에 저장 FAT 손실 시 복구 위해 이중 저장 Direct access 도 가능! FAT는 일반적으로 메모리 캐싱 몇 bit를 할당하면 좋을까? 32비트 -&gt; FAT32, 64비트 -&gt; FAT62 색인 할당 Indexed Allocation 파일 당 (한 개의) 인덱스 블록 (데이터 블록 외에) 인덱스 블록은 포인터의 모음 디렉토리는 인덱스 블록을 가리킨다. Unix/Linux 등에서 사용 장점 Sequential accecss 가능 Direct access 가능 외부 단편화 없음 문제점 인덱스 블록 할당에 따른 저장공간 손실 예: 1바이트 파일 위해 데이터 1블록 + 인덱스 1블록 바이트가 작기 때문에 큰 파일을 만들 수 없음 =&gt; 여러 인덱스로 구성(Linked) 파일의 최대 크기 예제: 1블록 = 512바이트 = 4바이트 × 128개 인덱스, 128 × 512바이트 = 64KB 예제: 1블록 = 1KB = 4바이트 × 256개 인덱스, 256 × 1KB = 256KB 해결 방법: Linked, Multilevel index, Combined 등 Linked : 여러 인덱스로 계층적 구성 Multilevel : 여러 인덱스로 넓게 구성 Combined : Linked + Multilevel index Reference 양희재 교수님 운영체제 강의 introduction to Operating System",
    "tags": "os development",
    "url": "/development/2018/08/02/file-allocation/"
  },{
    "title": "BigQuery와 Datalab을 사용해 데이터 분석하기",
    "text": "Google Cloud Platform 제품인 BigQuery와 Datalab을 사용해 Structed Data를 분석하는 방법에 대해 설명한 글입니다. 데이터는 빅콘테스트의 NC 데이터를 예시로 들었습니다. 혹시 문제가 있다고 생각하면 제게 연락주세요! 쉘 스크립트와 쿼리 코드는 2018-bigcontest-nc Repository에 있습니다 :) 대용량 데이터 다루기 캐글이나 공모전에서 약간 큰(사실은 절대 크진 않은) 데이터를 가지고 작업할 경우 Python Pandas에서 pd.read_csv시 dtype을 np.int32 등으로 설정해 최대한 메모리를 절약하고 gc.collect()로 할당된 메모리를 해지 또는 Spark를 사용해 분산으로 작업을 진행 또는 좋은 컴퓨터(CPU, RAM이 좋은)를 구입 좋은 컴퓨터도 메모리가 낮을 수 있으니 클라우드 서비스를 사용 이정도 생각해볼 수 있음 그러나 좋은 메모리를 사용해도 “퍼포먼스” 부분에선 한계가 존재(Python의 자체 한계나 코드적인 부분에서) 그래서 저는 캐글할 때 BigQuery에 데이터를 적재해서 사용 BigQuery는 데이터 웨어하우스로 데이터 분석용 클라우드 서비스 별도의 설치나 운영이 필요 없고, 데이터만 넣어주면 됨 쿼리 수행시 탐색하는 데이터의 양에 비례해 요금이 부과됨(1TB당 $5) 월 1TB는 무료고 GCP 처음 가입시 $300 제공 많이 사용하는 라이브러리는 거의 다 설치되어 있으며, 심지어 xgboost도 기본 설치됨 BigQuery와 로컬 컴퓨터의 Python을 사용하면 IO 이슈가 생김(쿼리하는 것은 빠르나 쿼리한 결과물의 로우 수가 많다면, 로컬로 넘어오는 시간이 오래 걸림) 따라서 Google Cloud Platform에서 제공하는 datalab(jupyter notebook이랑 유사)를 사용해 IO를 최소화 목차 Google Cloud Platform 가입 : 링크 참고 데이터 다운로드 : 각자 다운로드 Google Storage에 데이터 업로드 BigQuery에서 테이블 생성 Datalab 생성 Datalab에서 BigQuery 연동 Google Storage에 데이터 업로드 GCP Console에서 Storage 클릭 “버킷 생성” 클릭 이름 : upload-bigquery 저장소 등급 : Reginal 위치 : ASIA-EAST1 “파일 업로드”를 눌러서 파일을 업로드 하거나 파일을 드래그앤드롭 이제 업로드한 파일의 주소는 gs://upload-bigquery/{file_name}.csv BigQuery에서 테이블 생성 BigQuery Console로 접속 만약 파란색 배경의 “베타” 콘솔 화면이 뜬다면 Go to Classic UI를 클릭해 클래식 UI로 접속 세모 버튼을 클릭한 후, Create new dataset 클릭 Dataset ID : nc_new nc_new를 선택한 후 + 버튼 클릭 위 설정으로 한 후, Create Table을 하면 몇분 뒤 테이블 생성 {file_name}은 저장된 파일 이름을 지정 단, nc 데이터는 총 11개라 귀찮음. 자동화 작업 쉘 스크립트 작성 gcloud를 설치하면 command창에서 명령어 입력 가능 gcloud init을 입력 후, 계정 설정 bq ls 입력해 datasetId 출력되나 확인 $ bq ls &gt;&gt;&gt; datasetId ----------- nc_new 다음과 같은 명령어로 로컬에서 BigQuery Table을 생성할 수 있습니다 bq --location=US load --autodetect --skip_leading_rows=1 --source_format=CSV nc_new.test_guild gs://upload-bigquery/test_guild.csv 이걸 한번에 작업하기 위해 쉘 스크립트로 작성 vi load-data-bigquery.sh load-data-bigquery.sh BUCKET_NAME=\"upload-bigquery\" TABLE_NAMES=\"test_activity test_guild test_party test_trade test_payment train_activity train_guild train_label train_party train_payment train_trade\" for TABLE in $TABLE_NAMES; do bq --location=US load --autodetect --skip_leading_rows=1 --source_format=CSV \"nc_new.${TABLE}\" \"gs://${BUCKET_NAME}/${TABLE}.csv\" done ESC+:wq로 저장한 후 권한을 755로 준 후, 실행 chmod 755 load-data-bigquery.sh ./load-data-bigquery.sh 약 10분 후 작업 완료(명령어 순서는 아래 사진을 따라하지 말고 위에서 작성한대로 진행) 이제 다시 BigQuery Console로 돌아가 테이블이 생성되었나 확인 Details를 선택하면 테이블 정보를 볼 수 있고, Preview를 누르면 데이터를 볼 수 있음 Datalab 생성 링크로 가서 API 사용 설정 링크에서 사용 확인이 뜨지 않는다면 구글클라우드 콘솔에서 API 및 서비스 - API 및 서비스 사용 설정 - Google Compute Engine, Cloud Source Repositories APIs 2개 사용설정 터미널에서 아래 명령어 입력 datalab create --machine-type n1-standard-8 datalab-instance zone은 BigQuery의 위치인 US(us-east1-b) 선택 머신 유형은 링크 참고 n1-standard-8의 성능은 cpu 8개 RAM 30기가로 사용할 때만 키고 사용하지 않으면 끄는 것을 추천!!! datalab-instance 대신 원하는 이름 설정 가능 약 5분 후 Compute Engine에 인스턴스가 생성된 것을 볼 수 있음 첫 생성시 localhost:8081로 자동 연결 추후 접속하고 싶을 경우 터미널에서 datalab connect datalab-instance 실행 Datalab에서 BigQuery 연동 docs-BigQuery에 가면 예제 파일이 있음 BigQuery에 대해 궁금하면 BigQuery Tutorial 참고(Star해주시면 좋아합니다) BigQuery에서 쿼리로 EDA Feature Engineering를 빅쿼리에서 진행하고 Join 등의 작업을 BigQuery에서 진행한 후, Datalab에서 모델링 진행 추천 Feature Engineering 이제 가설을 잡고 모델링 가설 1) 결제한 유저들은 게임을 더 오래할 것이다 생성 feature : payment_cnt, payment_total 2) 문파에 가입한 유저들은 게임을 더 오래할 것이다(단, 길드의 멤버수도 영향을 끼칠 것이다) 생성 feature : guild_tf, guild_member_cnt guild는 9963개.. 우선은 guild 가입 유무 변수만 추가 3) 접속하는 빈도를 통해 속도를 측정할 수 있을 것이다(소수점 둘째자리에서 round) 생성 feature : login_speed : sum(cnt_dt)/count(wk)*7, last_week : max(wk) 1주차 7일, 2주차 7일, 3주차 7일 =&gt; feature 1. 접속 속도 1, max(접속주차) =&gt; feature2. 마지막 주차 1주차 7일, 2주차 5일 = 12/14 = 0.85 1주차 7일, 3주차 3일 = 10/21 = 0.47 sum(cnt_dt)/count(wk)*7 4) Baseline에는 activity에서 다음만 사용 : wk, cnt_dt, play_time, normal_chat, cnt_use_buffitem 위 Feature들을 Join하는 쿼리 작성 후 진행 쿼리문은 Github에 올려둘 예정 이후 xgboost로 거의 default 옵션가지고 모델링하고 제출해보니 0.65점 나옴 Feature 추가하고 이것저것 해보면 더 올라가지 않을까 생각됨 예전에 하던 일이 이런 일들이라 이제 대회를 참여하지 않을 예정",
    "tags": "BigQuery gcp",
    "url": "/gcp/2018/08/02/bigquery-and-datalab/"
  },{
    "title": "가상 메모리 이해하기",
    "text": "가상 메모리에 대해 작성한 글입니다 가상 메모리 Virtual Memory 물리 메모리 크기 한계 극복 물리 메모리보다 큰 프로세스를 실행? e.g. 100MB 메인 메모리에서 200MB 크기의 프로세스 실행 어떻게? 프로세스 이미지를 모두 메모리에 올릴 필요는 없다. 현재 실행에 필요한 부분만 메모리에 올린다! 오류 처리 제외, 배열 일부 제외, 워드프로세스에서 정렬, 표 기능 제외 ☞ 동적 적재(dynamic loading)와 비슷한 개념 요구 페이징 Demand Paging 가상 메모리는 거의 Demand Paging을 사용(다른 방법도 있지만..) 프로세스 이미지는 backing store에 저장 프로세스는 페이지의 집합 지금 필요한 페이지만 메모리에 올린다(load) ☞ 요구되는 (demand) 페이지만 메모리에 올린다 하드웨어 지원 valid 비트 추가된 페이지 테이블 valid 비트 : 메모리에 없으면 0, 있으면 1 backing store (= swap device) 페이지 결함(Page Fault) 접근하려는 페이지가 메모리에 없다 (invalid) = 페이지 부재 Backing store에서 해당 페이지를 가져온다. Steps in handling a page fault 용어 pure demand paging : 처음 실행할 때 0개의 페이지로 시작, 속도가 느리지만 메모리가 절약됨 prepaging : 미리 페이지를 가지고 옴 비교: swapping vs demand paging backing 스토어와 메모리를 왔다갔다 하는 것은 유사 swapping : 프로세스 단위 demang paging : 페이지 단위 유효 접근 시간 Effective Access Time 메인 메모리에 올라와있는지에 따라 어떤 것은 빠르게 어떤 것은 느리게 읽혀짐. 평균적으론 얼마일까?를 나타내는 값 p: probability of a page fault = page fault rate T_{eff} = (1-p)T_{m} + pT_{p} 예제 T_{m} = 200 nsec (DRAM) T_{p} = 8 msec (seek time + rotational delay + transfer time) T_{eff} = (1-p)200 + p8,000,000 = 200 + 7,999,800p p = 1/1,000 ☞ T_{eff} = 8.2usec (40배 느림) p = 1/399,990 ☞ T_{eff} = 220nsec (10% 느림) 지역성의 원리 Locality of reference local : 지역에 모여있다 메모리 접근은 시간적, 공간적 지역성을 가진다! 컴퓨터는 반복문이 많아 근처를 읽을 확률이 큼 실제 페이지 부재 확률은 매우 낮다 공간적 지역성 : 1000번째를 읽은 후 근접 부분을 읽을 확률이 큼 다른 방법 HDD 는 접근 시간이 너무 길다 ☞ swap device 로 부적합 SSD 또는 느린 저가 DRAM 사용 페이지 교체 Page Replacement 요구되어지는 페이지만 backing store 에서 가져온다 프로그램 실행 계속에 따라 요구 페이지가 늘어나고, 언젠가는 메모리가 가득 차게 된다 ☞ Memory full! 메모리가 가득 차면 추가로 페이지를 가져오기 위해 어떤 페이지는 backing store로 몰아내고 (page-out) 그 빈 공간으로 페이지를 가져온다 (page-in) 용어: victim page Victim Page 쫓겨난 페이지. 희생양 어느 페이지를 몰아낼 것인가? i/o 시간 절약을 위해 기왕이면 modify 되지 않은 페이지를 victim으로 선택 방법: modified bit(= dirty bit) 수정되었으면 1 여러 페이지 중에서 무엇을 victim으로? Random : 성능도 랜덤 First-In First-Out (FIFO) 그외 용어: 페이지 교체 알고리즘 (page replacement algorithms) 페이지 참조 스트링 (Page reference string) 페이지 참조 열. 몇 번째 페이지를 읽으려고 하는가? CPU가 내는 주소: 100 101 102 432 612 103 104 611 612 Page size = 100 바이트라면 페이지 번호 = 1 1 1 4 6 1 1 6 6 연속된 페이지에선 page faults가 일어나지 않음 따라서 이어지는 숫자는 스킵하는 것이 Page reference string Page reference string = 1 4 6 1 6 Page Replacement Algorithms FIFO (First-In First-Out) OPT (Optimal) LRU (Least-Recently-Used) (1) First-In First-Out (FIFO) Simplest Idea: 초기화 코드는 더 이상 사용되지 않을 것 예제 페이지 참조 스트링 = 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 7 0 1 -&gt; 2 0 1 -&gt; 2 3 1 -&gt; 2 3 0 -&gt; … number of frames = 3 15 page faults Belady’s Anomaly 프레임 수 (= 메모리 용량) 증가에 Page Fault 회수 증가 상식에 어긋나는 일 (2) Optimal (OPT) Rule: Replace the page that will not be used for the longest period of time 최적 알고리즘 (제일 좋음) 앞으로 가장 오랜 기간동안 사용이 안될 것을 victim으로! 예제 페이지 참조 스트링 = 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 7 0 1 -&gt; 2 0 1 -&gt; 2 0 3 -&gt; 2 4 3 -&gt; … number of frames = 3 9 page faults Unrealistic 미래는 알 수 없다! cf. SJF CPU scheduling algorithm 레디큐 안에 프로세스가 여러 개 있을 때, 작업 시간이 제일 작은 것을 선택 작업 시간이 제일 작은 것은 실제로 알 수 없음 (3) Least-Recently-Used (LRU) Rule: Replace the page that has not been used for the longest period of time 과거를 보고 미래를 짐작하는 알고리즘 최근에 가장 적게 사용된 것을 victim으로!(근사화) 대부분 LRU를 사용 Idea: 최근에 사용되지 않으면 나중에도 사용되지 않을 것 저번 학기에 공부를 잘한 사람은 이번 학기에 공부를 잘할 확률이 높다..? 예제 페이지 참조 스트링 = 7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1 7 0 1 -&gt; 2 0 1 -&gt; 2 0 3 -&gt; 4 0 3 -&gt; … number of frames = 3 12 page faults Global replacement 메모리 상의 모든 프로세스 페이지에 대해 교체 Local replacement 메모리 상의 자기 프로세스 페이지에 대해 교체 성능 비교 Global replacement가 더 효율적일 수 있다. 프레임 할당 Allocation of Frames 프레임 : 물리 메모리를 일정한 크기로 나눈 블록, 메모리 단편화 문제를 해결하기 위해 고안된 기법 CPU utilization vs Degree of multiprogramming Degree of multiprogramming : 메인 메모리에 올라온 프로세스의 개수 프로세스 개수 증가 ☞ CPU 이용률 증가 일정 범위를 넘어서면 CPU 이용률 감소 이유: 너무 많은 프로세스로 서로 메모리가 부족 -&gt; 빈번한 page in/out 쓰레싱 Thrashing: 디스크 i/o 시간 증가 때문 쓰레싱 극복 Global replacement 보다는 local replacement 프로세스당 충분한/적절한 수의 메모리(프레임) 할당 적절한 수를 어떻게 알 수 있을까? 정적 할당 동적 할당 정적 할당 (static allocation) 프로세스에 맞게 할당 균등 할당 (Equal allocation) 비례 할당 (Proportional allocation) : 사이즈에 비례해 할당 동적 할당 (dynamic allocation) Working set model Page fault frequency etc Working set model Locality(모여져 있음) vs working set(과거를 참고) Working set window Working set 크기 만큼의 프레임 할당 Page-Fault Frequency (PFF) Page fault 발생 비율의 상한/하한선 상한선 초과 프로세스에 더 많은 프레임 할당 하한선 이하 프로세스의 프레임은 회수 페이지 크기 Page size 일반적 크기: 4KB ~ 4MB 점차 커지는 경향을 보임 페이지 크기 영향 내부 단편화 : 작아야 좋음 프로세스가 35 byte 1 페이지는 10 byte일 경우, 이 프로세스는 4개의 페이지가 필요. 마지막 5 byte를 버려야 함 Page-in, page-out 시간 : 커야 좋음 페이지 테이블 크기 : 커야 좋음 Memory resolution : 정밀도, 작아야 좋음(크면 필요없는 것도 가져올 것) Page fault 발생 확률 : 커야 좋음 페이지 테이블 원래는 별도의 chip (TLB 캐시) 기술 발달에 따라 캐시 메모리는 on-chip 형태로 TLB 역시 on-chip 내장 Reference 양희재 교수님 운영체제 강의 introduction to Operating System",
    "tags": "os development",
    "url": "/development/2018/08/01/virtual-memory/"
  },{
    "title": "메모리 이해하기",
    "text": "주기억장치인 메모리에 대해 작성한 글입니다 주기억장치 관리 Main Memory Management 메모리 메모리 역사 메모리 종류 Core memory : 반지 모양의 철심에 자석 물질을 바르고 전기를 흘러 파장 생성 진공관 메모리 트랜지스터 메모리 집적회로 메모리: SRAM, DRAM 메모리 용량 1970년대: 8-bit PC, 64KB 1980년: 16-bit IBM-PC, 640KB → 1MB → 4MB 1990년: 수MB → 수십 MB 2000년~현재: 수백 MB → 수 GB 언제나 부족한 메모리 메모리 관리가 issue. 메모리가 계속 커져도 메모리가 부족함(마치 우리의 연봉같이.. 계속….ㅎㅎㅎ) 기계어/어셈블리어 → C언어 → 자바, 객체지향형 언어 숫자 처리 → 문자 처리 → 멀티미디어 처리 → Big Data 메모리 용량 증가 vs 프로그램 크기 증가 언제나 부족한 메모리 어떻게 메모리를 효과적으로 사용할 수 있을까? 메모리 낭비 없애기 가상 메모리 (virtual memory) 메모리 주소 주소(Address) + 데이터(Data) 입력 : 주소(address), 입출력 : 데이터 프로그램을 메모리에 올리기 프로그램 개발 원천파일 (Source file): 고수준 언어 또는 어셈블리 언어, main.c 목적파일 (Object file): 컴파일 또는 어셈블 결과, main.o 실행파일 (Executable file): 링크 결과 개발 도구 컴파일러 compiler : Source file -&gt; Object file 어셈블러 assembler 링커 linker : Object 파일과 라이브러리 연결 로더 loader : 메인 메모리로 올라감 프로그램 실행 code(기계어) + data(변수) + stack(함수 호출할 때 돌아오는 주소 저장, 지역변수 저장) 실행파일을 메모리에 올리기 메모리 몇 번지에? OS가 없으면 우리가 고민해야 함. 다행히 OS의 로더가 고민해줌 다중 프로그래밍 환경에서는? MMU 사용 CPU와 메모리 사이에 존재 재배치 레지스터 (Relocation register) addresss translation CPU가 만든 값에 relocation register에 있는 값을 주소에 더해서 실제 메모리 주소값을 도출. 주소로 갈 때 베이즈 레지스터에 대해 다시 바인딩 주소 구분 논리주소 (logical address) vs 물리주소 (physical address) 메모리 낭비 방지 Dynamic Loading Dynamic Linking Swapping (1) 동적 적재 (Dynamic Loading) 현재 시스템은 Dynamic Loading 프로그램 실행에 반드시 필요한 루틴/데이터만 적재 모든 루틴(routine)(함수)이 다 사용되는 것은 아니다 (예: 오류처리) 모든 데이터(data)가 다 사용되는 것은 아니다 (예: 배열) 자바: 모든 클래스가 다 사용되는 것은 아니다 실행 시 필요하면 그때 해당 부분을 메모리에 올린다 cf. 정적 적재 (Static loading) (2) 동적 연결 (Dynamic Linking) 여러 프로그램에 공통 사용되는 라이브러리 공통 라이브러리 루틴(library routine)를 메모리에 중복으로 올리는 것은 낭비 라이브러리 루틴 연결을 실행 시까지 미룬다. 오직 하나의 라이브러리 루틴만 메모리에 적재되고, 다른 애플리케이션 실행시 이 루틴과 연결(link)된다 원래 exe 파일 만들기 전에 링크가 발생하는데(정적 연결), 링크를 메모리에 올리고 실행시 연결 =&gt; 하나의 라이브러리만 메모리에 적재 cf. 정적 연결 (Static linking) 공유 라이브러리 (shared library) - Linux 동적 연결 라이브러리 (Dynamic Linking Library, DLL) - Windows (3) 스와핑 (Swapping) 메모리에 적재되어 있으나 현재 사용되지 않고 있는 프로세스 이미지 프로세스 이미지를 하드디스크의 특정 부분으로 몰아냄 메모리 활용도 높이기 위해 Backing store (= swap device)로 몰아내기 하드디스크(file system) : banking store, swap device, 메인 메모리 크기정도면 ok swap-out vs. swap-in Relocation register 사용으로 적재 위치는 무관 프로세스 크기가 크면 backing store 입출력에 따른 부담 크다(하드디스크는 느림) 연속 메모리 할당 Contiguous Memory Allocation 다중 프로그래밍 환경 부팅 직후 메모리 상태: O/S + big single hole, 메모리가 비어있음 프로세스 생성 &amp; 종료 반복 ☞ scattered holes 메모리 단편화 (Memory fragmentation) Hole들이 불연속하게 흩어져 있기 때문에 프로세스 적재 불가 외부 단편화 (external fragmentation) 발생 메모리 관리자 입장에선 매우 억울 외부 단편화를 최소화 하려면? 연속 메모리 할당 방식 First-fit (최초 적합) : 순차적으로 찾아 처음으로 발견되는 곳에 할당 Best-fit (최적 적합) : 들어갈 수 있는 Hole 중 가장 유사한 사이즈로 할당 Worst-fit (최악 적합) : 크기가 제일 안 맞는 곳에 할당 예제 Hole: 100 /500 / 600 / 300 / 200 KB 프로세스: 212 417 112 426 KB 할당 방식 성능 비교: 속도 및 메모리 이용률 속도: first-fit 이용률: first-fit, best-fit 외부 단편화로 인한 메모리 낭비 1/3 수준 (사용 불가) Compaction : 최적 알고리즘 없음, 고부담. hole을 한 곳으로 모음 다른 방법은? 페이징 Paging 프로세스를 연속적으로 해야된다고 생각했는데, 발상을 돌려서 일정한 단위로 잘라서 메모리로 올림 프로세스를 일정 크기(=페이지)로 잘라서 메모리에! 프로세스는 페이지(page)의 집합 메모리는 프레임(frame)의 집합 페이지를 프레임에 할당 MMU 내의 재배치 레지스터 값을 바꿈으로서 CPU는 프로세스가 연속된 메모리 공간에 위치한다고 착각(CPU를 속이자) MMU는 페이지 테이블 (page table)이 된다 논리주소 (Logical address) CPU가 내는 주소는 2진수로 표현 (전체 m 비트) 하위 n 비트는 오프셋(offset) 또는 변위(displacement) 상위 m-n 비트는 페이지 번호 물리주소 메모리를 접근할 때 사용되는 주소, 기억장치의 주소 레지스터에 적재 주소변환 (Address translation) 논리주소 → 물리주소 (Physical address) 페이지 번호(p)는 페이지 테이블 인덱스 값 p 에 해당되는 테이블 내용이 프레임 번호(f) 변위(d)는 변하지 않음 요약 : 10진수 값을 2진수로 변환한 후, 페이지 사이즈에 따라 p와 d를 정한 후, p를 p에 해당하는 테이블 내용 프레임 번호(f)로 변경하면 됨 예제 (1) Page size = 4 bytes Page Table: 5 6 1 2 (인덱스 0 1 2 3) 논리주소 13 번지는 물리주소 몇 번지? 13 = 1101(2) 4바이트(2^2)니까 페이지 번호(p) : 11, displacement(d) : 01 p는 3이니까 index가 3인 페이지 테이블은 2 1001 =&gt; 2번째 프레임에서 1만큼 떨어짐 예제 (2) Page Size = 1KB = 1024 Page Table: 1 2 5 4 8 3 0 6 논리주소 3000번지는 물리주소 몇 번지? 3000 = 101110111000(2) : 10칸이 d p : 10 (2) =&gt; 5, d : 1110111000 1011110111000(2) 물리주소 0x1A53 번지는 논리주소 몇 번지? 0x1A53는 16진수 =&gt; 1A53을 2진수로 p : 110 (6), d : 1001010011 1111001010011(2) 내부 단편화 (Internal Fragmentation) 프로세스 크기가 페이지 크기의 배수가 아니라면, 마지막 페이지는 한 프레임을 다 채울 수 없다 남은 공간 = 메모리 낭비 내부 단편화로 인한 메모리 낭비는 미미함 페이지 테이블 만들기 MMU를 어디에 넣을까! CPU 레지스터로 장점 : 주소 변환이 빠름 단점 : 많이 못들어감 메인 메모리로 장점 : 많이 들어감 단점 : 주소 변환이 느림 TLB (Translation Look-aside Buffer)로 요즘 방식 SRAM 사용 CPU와 메모리의 중간 단계 척도: 테이블 엔트리 개수 vs 변환 속도 연습: TLB 사용 시 유효 메모리 접근 시간 Tm(총 걸린 시간) = 100ns, Tb(읽는 시간) = 20ns, hit ratio = 80% Teff = hTb + (1-h)(Tb+Tm) = 140ns 보호 (Protection): 해킹 등 방지 모든 주소는 페이지 테이블을 경유하므로, 페이지 테이블 엔트리마다 r(reader), w(writer), x(executor) 비트를 두고 해당 페이지에 대한 접근 제어 가능 공유 (Sharing): 메모리 낭비 방지 같은 프로그램을 쓰는 복수 개의 프로세스가 있다면, Code + data + stack 에서 code는 공유 가능 (단, 코드는 내용이 달라지면 안됨. non-selfmodifying code = reentrant code = pure code 인 경우) 프로세스의 페이지 테이블 코드 영역이 같은 곳을 가리키도록 설정 세그멘테이션 Segmentation 페이징이 더 보편적으로 사용 페이징에선 일정 크기(페이지)로 잘랐음 세그멘테이션은 프로세스를 논리적 내용(=세그멘트)으로 잘라서 메모리에 배치! 프로세스는 세그멘트(segment)의 집합 세그멘트의 크기는 일반적으로 같지 않다 세그멘트를 메모리에 할당 MMU 내의 재배치 레지스터 값을 바꿈으로서 CPU는 프로세스가 연속된 메모리 공간에 위치한다고 착각 MMU는 세그멘트 테이블(segment table)이 된다 논리주소 (Logical address) CPU 가 내는 주소는 segment 번호(s) + 변위(d) 주소변환 논리주소 → 물리주소 (Physical address) 세그멘트 테이블 내용: base + limit 세그멘트 번호(s)는 세그멘트 테이블 인덱스 값 s에 해당되는 테이블 내용으로 시작 위치 및 한계값 파악 한계(limit)를 넘어서면 segment violation 예외 상황 처리 물리주소 = base[s] + d 예제 Limit Base 1000 1400 400 6300 400 4300 1100 3200 1000 4700 논리주소 (2, 100)는 물리주소 무엇인가? index=2 =&gt; 4300 + 100 논리주소 (1, 500)은 물리주소? 해당 주소 없음. limit을 넘어섬(400) 보호 (Protection): 해킹 등 방지 모든 주소는 세그멘트 테이블을 경유하므로, 세그멘트 테이블 엔트리마다 r, w, x 비트 두어 해당 세그멘트에 대한 접근 제어 가능 페이징보다 우월! 프로세스는 일정 크기로 자르기 때문에 code, data, stack이 혼재할 수 있음 공유 (Sharing): 메모리 낭비 방지 같은 프로그램을 쓰는 복수 개의 프로세스가 있다면, Code + data + stack 에서 code 는 공유 가능 (단, non-selfmodifying code = reentrant code = pure code 인 경우) 프로세스의 세그멘트 테이블 코드 영역이 같은 곳을 가리키게 페이징보다 우월! 외부 단편화 (External Fragmentation) 세그멘트 크기는 고정이 아니라 가변적 크기가 다른 각 세그멘트를 메모리에 두려면 = 동적 메모리 할당 First-, best-, worst-fit, compaction 등 문제 세그멘테이션 + 페이징 세그멘테이션은 보호와 공유면에서 효과적 페이징은 외부 단편화 문제를 해결 따라서 세그멘트를 페이징하자! ☞ Paged segmentation 예: Intel 80x86 Reference 양희재 교수님 운영체제 강의 introduction to Operating System 프로그래머가 알아야 하는 메모리 관리 기법 Operating-System 8장 정리",
    "tags": "os development",
    "url": "/development/2018/07/31/memory/"
  },{
    "title": "프로세스 동기화 이해하기",
    "text": "프로세스 동기화에 대해 작성한 글입니다 프로세스 동기화 Process Synchronization 사실 요새 OS는 Thread synchronization Cooperating Processes Processes Independent vs. Cooperating Cooperating process: one that can affect or be affected by other processes executed in the system. 다른 프로세스에 영향을 주거나 받음 프로세스간 통신: 전자우편, 파일 전송 프로세스간 자원 공유: 메모리 상의 자료들, 데이터베이스 등 명절 기차표 예약, 대학 온라인 수강신청, 실시간 주식거래 Process Synchronization: Why? Concurrent access to shared data may result in data inconsistency Orderly execution of cooperating processes so that data consistency is maintained. 순서를 지켜서 이상한 상태(수강 인원을 초과해서 신청된다거나 한명이 동시에 같은 시간에 2과목 등록 등)를 피해 데이터 일관성을 유지 Example: BankAccount Problem (은행계좌문제) 부모님은 은행계좌에 입금; 자녀는 출금 입금(deposit)과 출금(withdraw)은 독립적으로 일어난다. 입출금 동작 알기 위해 “+”, “-“ 출력하기 입출금 동작에 시간 지연 추가 void deposit(int amount) { // 변경된 값을 임시변수에 저장하고 int temp = balance + amount; System.out.print(\"+\");// 시간 지연 후 balance = temp;// 잔액 업데이트 } void withdraw(int amount) { // 변경된 값을 임시변수에 저장하고 int temp = balance - amount; System.out.print(\"-\"); // 시간 지연 후 balance = temp;// 잔액 업데이트 } 잘못된 결과값 이유: 공통변수(common variable)에 대한 동시 업데이트 (concurrent update), 하이레벨 언어로는 1줄이지만 로우한 언어에선 여러 줄. 도중에 스위칭 발생 해결: 한 번에 한 쓰레드만 업데이트하도록(Atomic) → 임계구역 문제 class Test { static final int MAX = 100; // 입출금 회수 public static void main(String[] args) throws InterruptedException { // 은행계좌를 만들고 BankAccount b = new BankAccount(); // 부모 쓰레드와 Parent p = new Parent(b, MAX); // 자식 쓰레드를 만든 후 Child c = new Child(b, MAX); // 각각 실행시킨다. p.start(); c.start(); p.join();// 부모와 자식 쓰레드가 c.join();// 각각 종료하기를 기다린다. System.out.println(\"Final balance = \" + b.getBalance());// 최종 잔액 출력 } } class BankAccount { int balance; void deposit(int amount) { balance = balance + amount; } void withdraw(int amount) { balance = balance - amount; } int getBalance() { return balance; } } class Parent extends Thread { BankAccount b; int count; Parent(BankAccount b, int count) { this.b = b; this.count = count; } public void run() { for (int i=0; i&lt;count; i++) b.deposit(1); } } class Child extends Thread { BankAccount b; int count; Child(BankAccount b, int count) { this.b = b; this.count = count; } public void run() { for (int i=0; i&lt;count; i++) b.withdraw(1); } } 임계구역 문제 The Critical-Section Problem, 치명적인 오류가 일어날 수 있는 공간 Critical section A system consisting of multiple threads(process) Each thread has a segment of code, called critical section, in which the thread may be changing common variables, updating a table, writing a file, and so on. 같이 사용하는 것들을 write, update, change하는 경우 Solution Mutual exclusion (상호배타): 많아야 한 쓰레드만 진입 Progress (진행): 진입 결정은 유한 시간 내(누가 들어갈지 유한 시간 내에 결정) Bounded waiting (유한대기): 어느 쓰레드라도 유한 시간 내 진입(유한 시간 내에 Critical section에 진입해야 함) 프로세스/쓰레드 동기화 임계구역 문제 해결 (틀린 답이 나오지 않도록) 프로세스 실행 순서 제어 (원하는 대로) Busy wait 등 비효율성 제거 잠시 정리 OS의 역할 Process management CPU scheduling Synchronization 임계구역 문제를 해결해야 하고 프로세스 실행 순서를 제어할 수 있어야 함 비효율성 제거 동기화 하기 위해 만든 도구 : 세마포, 모니터 Memory management File System IO … 세마포 Synchronization Tools (동기화 도구) Semaphores Monitors …… Semaphores (세마포) n. (철도의) 까치발 신호기, 시그널; U (군대의) 수기(手旗) 신호 동기화 문제 해결을 위한 소프트웨어 도구 1960년대 네덜란드의 Edsger Dijkstra 가 제안 구조: 정수형 변수 + 두 개의 동작 (P, V) 동작 마치 스택(stack)과 같이 …: push() &amp; pop() P: Proberen (test) → acquire() V: Verhogen (increment) → release() 전체 구조 내부적으로는 프로세스(쓰레드)가 대기하는 큐(queue/list)가 포함되어있다. class Semaphore { private int value; // number of permits Semaphore(int value) { ... } void acquire() { // P ... } void release() { // V ... } } 세부 동작 If the semaphore value is negative, its magnitude is the number of processes waiting on that semaphore. void acquire() { value--; if (value &lt; 0) { add this process/thread to list; block; } } void release() { value++; if (value &lt;= 0) { remove a process P from list; wakeup P; } } 일반적 사용 (1): Mutual exclusion sem.value = 1; // Number(#) of permit sem.acquire(); Critical-Section sem.release(); 예제: BankAccount Problem import java.util.concurrent.Semaphore; class BankAccount { int balance; Semaphore sem; BankAccount() { sem = new Semaphore(1);// 초기값 = 1 } void deposit(int amount) { // 입금 try { sem.acquire(); // 진입 전: acquire() } catch (InterruptedException e) {} int temp = balance + amount; System.out.print(\"+\"); balance = temp; sem.release(); // 나온 후: release() } void withdraw(int amount) { // 출금 try { sem.acquire(); // 진입 전: acquire() } catch (InterruptedException e) {} int temp = balance - amount; System.out.print(\"-\"); balance = temp; sem.release(); // 나온 후: release() } int getBalance() { return balance; } } 일반적 사용 (2): Ordering sem.value = 0; // # of permit P1을 먼저 실행했으면 OK, P2가 먼저 실행했으면 sem.acquire()에 의해 -1이 되서 밑으로 못 내려감. Context Switch으로 S1이 진행한 후, sem.release()로 0이 되서 S2가 실행 P1 P2   sem.acquire(); S1 S2 sem.release();   예제: BankAccount Problem (1) 항상 입금 먼저 (= Parent 먼저) 상호배타를 위한 sem 세마포어 외에 실행순서 조정을 위한 sem2 세마포어를 추가 프로그램이 시작되면 부모 쓰레드는 그대로 실행되게 하고, 자식 쓰레드는 초기 값이 0인 sem2 세마포어에 대해 acquire() 를 호출하게 하도록 deposit(), withdraw() 메소드를 각각 수정 즉 자식 쓰레드가 먼저 실행되면 세마포어 sem2에 의해 블록되고, 블록된 자식 쓰레드는 나중에 부모 쓰레드가 깨워주게 한다. Parent Child   sem.acquire(); deposit withdraw sem.release();   (2) 항상 출금 먼저 (= Child 먼저) ☞ 위와 유사 (3) 입출금 교대로 (P-C-P-C-P-C- …) 블록된 부모 쓰레드는 자식 쓰레드가 깨워주고, 블록된 자식 쓰레드는 부모 쓰레드가 각각 깨워주도록 한다 상호배타를 위한 sem 세마포어 외에 부모 쓰레드의 블록을 위해 dsem 세마포어를, 자식 쓰레드의 블록을 위해 wsem 세마포어를 각각 사용한다 Parent Child   wsem.acquire(); deposit withdraw wsem.release(); dsem.release(); dsem.acquire();   (4) 잔액이 항상 0 이상 출금하려는 액수보다 잔액이 작으면 자식 쓰레드가 블록되도록 하며 이후 부모 쓰레드가 깨워주게 한다 상호배타를 위한 sem 세마포어 외에 sem2 세마포어를 사용하여 잔액 부족시 자식 쓰레드가 블록되도록 한다 전통적 동기화 예제 Classical Synchronization Problems (1) Producer and Consumer Problem 생산자-소비자 문제 유한버퍼 문제 (Bounded Buffer Problem) (2) Readers-Writers Problem 공유 데이터베이스 접근 (3) Dining Philosopher Problem 식사하는 철학자 문제 생산자-소비자 문제 생산자가 데이터를 생산하면 소비자는 그것을 소비 예: 컴파일러 &gt; 어셈블러, 파일 서버 &gt; 클라이언트, 웹 서버 &gt; 웹 클라이언트 유한 버퍼 (bounded buffer) producer가 생성하고, 저장 창고(buffer)에 저장하고 consumer가 사용(마치 메세지 큐 시스템 펍섭 같음) 생산된 데이터는 버퍼에 일단 저장 (속도 차이 등) 현실 세계에서 버퍼 크기는 유한 생산자는 버퍼가 가득 차면 더 넣을 수 없다. 소비자는 버퍼가 비면 뺄 수 없다 class Test { public static void main(String[] arg) { Buffer b = new Buffer(100); Producer p = new Producer(b, 10000); Consumer c = new Consumer(b, 10000); p.start(); c.start(); try { p.join(); c.join(); } catch (InterruptedException e) {} System.out.println( \"Number of items in the buf is \" + b.count); } } class Buffer { int[] buf; int size, count, in, out; Buffer(int size) { buf = new int[size]; this.size = size; count = in = out = 0; } void insert(int item) { /* check if buf is full */ while (count == size) ; /* buf is not full */ buf[in] = item; in = (in+1)%size; count++; } int remove() { /* check if buf is empty */ while (count == 0) ; /* buf is not empty */ int item = buf[out]; out = (out+1)%size; count--; return item; } } class Producer extends Thread { Buffer b; int n; Producer(Buffer b, int n) { this.b = b; this.n = n; } public void run() { for (int i=0; i&lt;n; i++) b.insert(i); } } class Consumer extends Thread { Buffer b; int n; Consumer(Buffer b, int n) { this.b = b; this.n = n; } public void run() { int item; for (int i=0; i&lt;n; i++) item = b.remove(); } } 잘못된 결과 실행 불가, 또는 count ≠ 0 (생산된 항목 숫자 ≠ 소비된 항목 숫자) 최종적으로 버퍼 내에는 0 개의 항목이 있어야 이유 공통변수 count, buf[] 에 대한 동시 업데이트 공통변수 업데이트 구간(= 임계구역)에 대한 동시 진입 해결법 임계구역에 대한 동시 접근 방지 (상호배타) 세마포를 사용한 상호배타 (mutual exclusion) 세마포: mutex.value = 1 (# of permit) import java.util.concurrent.Semaphore; class Buffer { int[] buf; int size, count, in, out; Semaphore mutex; Buffer(int size) { buf = new int[size]; this.size = size; count = in = out = 0; mutex = new Semaphore(1); } void insert(int item) { while (count == size); try { mutex.acquire(); } catch (InterruptedException e) {} buf[in] = item; in = (in+1)%size; count++; // increase item count mutex.release(); } int remove() { while (count == 0); try { mutex.acquire(); int item = buf[out]; out = (out+1)%size; count--; mutex.release(); return item; } catch (InterruptedException e) { return -1; // dummy } } } Busy-wait 생산자: 버퍼가 가득 차면 기다려야 = 빈(empty) 공간이 있어야 소비자: 버퍼가 비면 기다려야 = 찬(full) 공간이 있어야 세마포를 사용해 버퍼가 다 찼으면 프로듀서는 세마포에 들어가서 대기 그리고 빈 공간이 생기면 들어감 세마포를 사용한 busy-wait 회피 생산자: empty.acquire() // # of permit = BUF_SIZE empty.acquire(); PRODUCE; full.release(); 소비자: full.acquire() // # of permit = 0 full.acquire(); CONSUME; empty.release(); import java.util.concurrent.Semaphore; class Buffer { int[] buf; int size, count, in, out; Semaphore mutex, full, empty; Buffer(int size) { buf = new int[size]; this.size = size; count = in = out = 0; mutex = new Semaphore(1); full = new Semaphore(0); empty = new Semaphore(size); } void insert(int item) { try { empty.acquire(); // while (count == size); mutex.acquire(); count++; buf[in] = item; in = (in+1)%size; mutex.release(); full.release(); } catch (InterruptedException e) { } } int remove() { try { full.acquire(); // while (count == 0); mutex.acquire(); count--; int item = buf[out]; out = (out+1)%size; mutex.release(); return item; } catch (InterruptedException e) { return -1; // dummy } } } Readers-Writers Problem 공통 데이터베이스 Readers: read data, never modify it Writers: read data and modifiy it 상호배타: 한 번에 한 개의 프로세스만 접근 ☞ 비효율적 Database : Critical Section, 최대 한놈만 접근하도록 하면 비효율적임. 한 reader가 db에 들어가있는 경우 다른 reader가 들어와도 상관이 없음(읽기만 하니) 그러나 writer는 상호배제. reader가 들어갔으면 writer는 금지 효율성 제고 Each read or write of the shared data must happen within a critical section Guarantee mutual exclusion for writers Allow multiple readers to execute in the critical section at once 변종 The first R/W problem (readers-preference) The second R/W problem (writers-preference) The Third R/W problem Dining Philosopher Problem 5명의 철학자, 5개의 젓가락, 생각 → 식사 → 생각 → 식사 … 식사하려면 2개의 젓가락 필요 젓가락을 들면 주변 사람이 사용 불가 프로그래밍 ☞ 코드-6 젓가락: 세마포 (# of permit = 1) 젓가락과 세마포에 일련번호: 0 ~ 4 왼쪽 젓가락 → 오른쪽 젓가락 import java.util.concurrent.Semaphore; class Philosopher extends Thread { int id; // philosopher id Semaphore lstick, rstick; // left, right chopsticks Philosopher(int id, Semaphore lstick, Semaphore rstick) { this.id = id; this.lstick = lstick; this.rstick = rstick; } public void run() { try { while (true) { lstick.acquire(); rstick.acquire(); eating(); lstick.release(); rstick.release(); thinking(); } }catch (InterruptedException e) { } } void eating() { System.out.println(\"[\" + id + \"] eating\"); } void thinking() { System.out.println(\"[\" + id + \"] thinking\"); } } class Test { static final int num = 5; // number of philosphers &amp; chopsticks public static void main(String[] args) { int i; /* chopsticks */ Semaphore[] stick = new Semaphore[num]; for (i=0; i&lt;num; i++) stick[i] = new Semaphore(1); /* philosophers */ Philosopher[] phil = new Philosopher[num]; for (i=0; i&lt;num; i++) phil[i] = new Philosopher(i, stick[i], stick[(i+1)%num]); /* let philosophers eat and think */ for (i=0; i&lt;num; i++) phil[i].start(); } } 잘못된 결과: starvation 모든 철학자가 식사를 하지 못해 굶어 죽는 상황 모두 왼쪽 젓가락을 들면 아무도 식사를 할 수 없음 이유 = 교착상태 (deadlock) 교착상태 Deadlock 프로세스는 실행을 위해 여러 자원을 필요로 한다. CPU, 메모리, 파일, 프린터, …… 어떤 자원은 갖고 있으나 다른 자원은 갖지 못할 때 (e.g., 다른 프로세스가 사용 중) 대기해야 다른 프로세스 역시 다른 자원을 가지려고 대기할 때 교착상태 가능성! 꼬리물기 교착상태 필요 조건 (Necessary Conditions) Mutual exclusion (상호배타) Hold and wait (보유 및 대기) No Preemption (비선점) Circular wait (환형대기) 자원 (Resources) 동일 형식 (type) 자원이 여러 개 있을 수 있다 (instance) 예: 동일 CPU 2개, 동일 프린터 3개 등 자원의 사용 요청 (request) → 사용 (use) → 반납 (release) 자원 할당도 (Resource Allocation Graph) 어떤 자원이 어떤 프로세스에게 할당되었는가? 어떤 프로세스가 어떤 자원을 할당 받으려고 기다리고 있는가? 자원: 사각형, 프로세스: 원, 할당: 화살표 교착상태 필요조건 자원 할당도 상에 원이 만들어져야 (환형대기) 충분조건은 아님! 예제: 식사하는 철학자 문제 원이 만들어지지 않게 하려면? 짝수번 사람은 왼쪽부터 홀수번 사람은 오른쪽부터 교착상태 처리 4가지 방법 (Handling deadlocks) 교착상태 방지 (Deadlock Prevention) 교착상태 회피 (Deadlock Avoidance) 교착상태 검출 및 복구 (Deadlock Detection &amp; Recovery) 교착상태 무시 (Don’t Care) (1) 교착상태 방지 Deadlock Prevention 교착상태 4가지 필요조건 중 한 가지 이상 결여되게 만듬 상호배타 (Mutual exclusion) 보유 및 대기 (Hold and wait) 비선점 (No preemption) 환형 대기 (Circular wait) 상호배타 (Mutual exclusion) 자원을 공유 가능하게 원천적으로 불가할 확률이 큼 따라서 교착 상태를 방지하기엔 부적절할 수 있음 보유 및 대기 (Hold &amp; Wait) 동시에 모두 잡도록 설정. 동시에 잡을 수 없으면 아예 잡지 않음 자원을 가지고 있으면서 다른 자원을 기다리지 않게 예: 자원이 없는 상태에서 모든 자원 대기; 일부 자원만 가용하면 보유 자원을 모두 놓아 주기 단점: 자원 활용률 저하, 기아 (starvation) 비선점 (No preemption) 자원을 뺏어올 수 있도록 자원을 선점 가능하게 원천적 불가할 수도 (예: 프린터) CPU는 강제로 context switch로 가능하지만… 환형대기 (Circular wait) 예: 자원에 번호부여; 번호 오름차순으로 자원 요청 단점: 자원 활용률 저하 (2) 교착상태 회피 Deadlock Avoidance 교착상태 = 자원 요청에 대한 잘못된 승인 (≒ 은행 파산) 데드락은 자원 요청에 잘못 응답해서 발생한 것이라고 생각 예제 12개의 magnetic tape 및 3개의 process 안전한 할당 (Safe allocation) Process Max needs Current needs P0 10 5 P1 4 2 P2 9 2 불안전한 할당 (Unsafe allocation) Process Max needs Current needs P0 10 5 P1 4 2 P2 9 3 운영체제는 자원을 할당할 때 불안전 할당 되지 않도록 불안전 할당 → 교착상태 대출전문 은행과 유사: Banker’s Algorithm (3) 교착상태 검출 및 복구 Deadlock Detection &amp; Recovery 필요한 자원을 모두 나눠줌 교착상태가 일어나는 것을 허용!! 주기적 검사 교착상태 발생 시 복구!! 검출 검사(detection)에 따른 추가 부담 (overhead): 그 전의 상태를 기억해야 함 =&gt; 계산, 메모리 사용 복구(recovery) 프로세스 일부 강제 종료 자원 선점하여 일부 프로세스에게 할당 (4) 교착상태 무시 교착상태는 실제로 잘 일어나지 않는다! 4가지 필요조건 모두 만족해도 … 교착상태 발생 시 재시동 (PC 등 가능) 모니터(Monitor) 요즘은 세마포보다 모니터를 더 많이 사용 자바에서 사용 세마포 이후 프로세스 동기화 도구 세마포 보다 고수준 개념 구조 공유자원 + 공유자원 접근함수 2개의 queues: 배타동기 + 조건동기 배타동기 : 한 쓰레드만 접근할 수 있음 wait()를 부르면 조건동기 큐로 이동 공유자원 접근함수에는 최대 1개의 쓰레드만 진입 진입 쓰레드가 조건동기로 블록되면 새 쓰레드 진입가능 새 쓰레드는 조건동기로 블록된 쓰레드를 깨울 수 있다(notify()) 깨워진 쓰레드는 현재 쓰레드가 나가면 재진입할 수 있다 자바의 모든 객체는 모니터가 될 수 있다. 배타동기: synchronized 키워드 사용하여 지정 조건동기: wait(), notify(), notifyAll() 메소드 사용 class C { private int value, …; synchronized void f() { ... } synchronized void g() { ... } void h() { ... } } 일반적 사용 (1): Mutual exclusion synchronized { Critical-Section } 예제: BankAccount Problem class BankAccount { int balance; synchronized void deposit(int amount) { int temp = balance + amount; System.out.print(\"+\"); balance = temp; notify();// 자식 쓰레드를 깨워준다. } synchronized void withdraw(int amount) { // 잔액이 부족하면 블록된다. while (balance &lt; amount) try { wait(); } catch (InterruptedException e) {} int temp = balance - amount; System.out.print(\"-\"); balance = temp; } int getBalance() { return balance; } } 일반적 사용 (2): Ordering P1 P2 wait()   S1 S2   notify() 예제: BankAccount Problem 항상 입금 먼저 (= Parent 먼저) 항상 출금 먼저 (= Child 먼저) 입출금 교대로 (P-C-P-C-P-C- …) 예제: 생산자-소비자 문제 class Buffer { int[] buf; int size, count, in, out; Buffer(int size) { buf = new int[size]; this.size = size; count = in = out = 0; } synchronized void insert(int item) { while (count == size) try { wait(); } catch (InterruptedException e) {} buf[in] = item; in = (in+1)%size; count++; notify(); } synchronized int remove() { while (count == 0) try { wait(); } catch (InterruptedException e) {} int item = buf[out]; out = (out+1)%size; count--; notify(); return item; } } 예제: 식사하는 철학자 문제 class Chopstick { private boolean inUse = false; synchronized void acquire() throws InterruptedException { while (inUse) wait(); inUse = true; } synchronized void release() { inUse = false; notify(); } } Reference 양희재 교수님 운영체제 강의 introduction to Operating System",
    "tags": "os development",
    "url": "/development/2018/07/30/process-synchronization/"
  },{
    "title": "CPU Scheduling, Process 이해하기",
    "text": "CPU Scheduling, Process에 대해 작성한 글입니다 OS의 역할 Process management CPU scheduling Synchronization Memory management File System IO etc CPU 스케쥴링 Preemptive vs Non-preemptive (선점 (先占) : 비선점(非先占)) Preemptive : CPU가 어느 프로세스를 실행하고 있는데(아직 끝나지도 않았고 IO를 만난 것도 아닌데) 강제로 쫓아내고 새로운 것이 들어갈 수 있는 스케쥴링(응급실) Non-preemptive : 프로세스가 끝나거나 IO를 만나기 전엔 안됨(은행) Scheduling criteria : 스케쥴링 척도 CPU Utilization (CPU 이용률) : CPU가 얼마나 놀지않고 부지런히 일하는가 Throughput (처리율) : 시간당 몇 개의 작업을 처리하는가 Turnaround time (반환시간) : 작업이 레디큐에 들어가서 나오는 시간의 차이(병원에서 진료 받을 때..대기하고 CT 찍고, … 나오는 시간 차) 짧아야 좋음 Waiting time (대기시간) : CPU가 서비스를 받기 위해 Ready Queue에서 얼마나 기다렸는가 Response time (응답시간) : Interactive system에서 중요. 클릭-답, 타이핑-답. 첫 응답이 나올 때 까지 걸리는 시간 CPU Scheduling Algorithms First-Come, First-Served (FCFS) : 먼저 온 놈 먼저 처리 Shortest-Job-First (SJF) : 작업 시간이 짧은 놈 먼저 처리 Shortest-Remaining-Time-First Priority : 우선 순위가 높은 놈부터 Round-Robin (RR) : 빙빙 돌면서 순서대로 Multilevel Queue : 큐를 여러개 돌림 Multilevel Feedback Queue First-Come, First-Served (FCFS) Scheduling 먼저 온 놈 먼저 서비스. 세상에서 많이 사용 Simple &amp; Fair 꼭 좋은 성능을 내는 것은 아님 Example: Find Average Waiting Time P1, P2, P3순일 경우 (0+24+27)/3 = 17 P3, P2, P1순일 경우 (6+3+0)/3 = 3 Gantt Chart Process Burst Time P1 24 P2 3 P3 3 Convoy Effect (호위효과) : 왕 뒤에 시중들이 따라다님. 다른 프로세스들이 시중들같이 따라다님 Nonpreemptive scheduling : 앞의 프로세스가 끝나야 뒤 프로세스가 진행 Shortest-Job-First (SJF) Scheduling 실행 시간이 짧은 놈을 먼저 실행 대기 시간을 줄이는 관점에선 SJF가 제일 좋음 Example: Process Burst Time P1 6 P2 8 P3 7 P4 3 AWT = (3+16+9+0)/4 = 7 msec cf. 10.25 msec (FCFS) Provably optimal Not realistic; prediction may be needed. 비현실적임. 예측을 해야 함 Preemptive or Nonpreemtive cf. Shortest-Remaining-Time-First (최소잔여시간 우선) Example Process Arrival Time Burst Time P1 0 8 P2 1 4 P3 2 9 P4 5 5 Preemptive: AWT = (9+0+15+2)/4 = 26/4 = 6.5 msec Nonpreemptive: (0+7+15+9)/4 = 7.75 msec Priority Scheduling Priority (우선순위): typically an integer number Low number represents high priority in general (Unix/Linux) Example Process Burst Time Priority P1 10 3 P2 1 1 P3 2 4 P4 1 5 P5 5 2 AWT = P2, P5, P1, P3, P4 = (6+0+16+18+1)/5 = 8.2 Priority Internal(내부적 요소): time limit, memory requirement, i/o to CPU burst, … External: amount of funds being paid(유료 컴퓨터일 경우), political factors, … Preemptive or Nonpreemptive Problem Indefinite blocking: starvation (기아) 외부에서 새로운 프로세스가 들어오는데, 그 프로세스가 우선 순위가 높으면 기다리던 프로세스보다 먼저 작업 Solution: aging. 오래 기다릴수록 우선 순위를 올림 Round-Robin (RR) Scheduling 수건 돌리기하듯 돌아가며 진행 시간을 쪼개서 프로세스 진행 쪼갠 동일한 시간을 Time Quantum, Time Slice라 부름 Time quantum 시간양자 = time slice (10 ~ 100msec) Time-sharing system (시분할/시공유 시스템) Preemptive scheduling : 끝나지 않아도 타임 퀀텀이 지나가면 다른 프로세스가 실행 Example Process Burst Time P1 24 P2 3 P3 3 Time Quantum = 4msec AWT = (6+4+7)/3 = 17/3 = 5.66 msec Time Quantum이 1이면 AWT이 변함! 퀀텀을 얼마나 잡아야 성능이 좋을까? Performance depends on the size of the time quantum (Δ), 타임 퀀텀에 의존적 Δ → ∞는 FCFS와 동일 Δ → 0는 Processor sharing : 스위칭이 빈번하게 돌아서 같이 도는 것처럼 보임(* context switching overhead가 빈번하게 발생) Example: Average turnaround time (ATT) Process Burst Time P1 6 P2 3 P3 1 P4 7 Average Turnaround Time ATT = (15+9+3+17)/4 = 11.0 msec (Δ = 1) ATT = (15+8+9+17)/4 = 12.25 msec (Δ = 5) 데이터에 따라 성능이 다름 Multilevel Queue Scheduling Process groups 프로세스를 그룹화. 은행에서 간단한 업무 / 대출 업무 구분해서 받는 것과 유사 System processes : OS에서 작업. 가상 메모리, IO, 통신. 가장 빨리 처리해야 함 Interactive processes : 사용자랑 대화하는 프로그램. 게임(&lt;-&gt; 컴파일하는 것은 대화하지 않는 프로그램임) Interactive editing processes : 편집하는 프로그램 Batch processes : 대화형이 아닌 프로세스. 꾸러미로 일괄적으로 처리 Student processes Single ready queue → Several separate queues 각각의 Queue에 절대적 우선순위 존재 또는 CPU time을 각 Queue 에 차등배분 그룹에 따라 다르게 스케줄링 각 Queue 는 독립된 scheduling 정책 Multilevel Feedback Queue Scheduling 복수 개의 Queue 다른 Queue로의 점진적 이동 모든 프로세스는 하나의 입구로 진입 너무 많은 CPU time 사용 시 다른 Queue로 기아 상태 우려 시 우선순위 높은 Queue로 한 Queue에만 있지 않고 옮겨가는 방식 프로세스 생성과 종료 프로세스는 프로세스에 의해 만들어진다! 부모 프로세스 (Parent process) 자식 프로세스 (Child process) 제일 첫 프로세스는? 부팅하고 OS가 첫 프로세스(init)를 생성 cf. Sibling processes : 부모가 같은 형제 프로세스 프로세스 트리 (process tree) Process Identifier (PID) Typically an integer number cf. PPID : Parent PID 프로세스 생성 (creation) fork() system call = 부모 프로세스 복사 exec() = 실행파일을 메모리로 가져오기 예제: Windows 7 프로세스 나열 : ctrl + alt + delete 예제: Ubuntu Linux 프로세스 나열 : ps 프로세스 종료 (termination) exit() system call 해당 프로세스가 가졌던 모든 자원은 O/S에게 반환 (메모리, 파일, 입출력장치 등) 쓰레드(Thread) 프로세스 내에서 실행되는 세부 작업의 단위 프로세스 내부의 흐름, 맥 class Test { public static void main(String[] args) { int n = 0; int m = 6; System.out.println(n+m); while (n &lt; m) n++; System.out.println(\"Bye\"); } } 다중 쓰레드 (Multithreads) 한 프로그램에 2개 이상의 맥 맥이 빠른 시간 간격으로 스위칭 된다 ⇒ 여러 맥이 동시에 실행되는 것처럼 보인다 concurrent(공존하는) vs simultaneous(동시에) 예: Web browser 화면 출력하는 쓰레드 + 데이터 읽어오는 쓰레드 예: Word processor 화면 출력하는 쓰레드 + 키보드 입력 받는 쓰레드 + 철자/문법 오류 확인 쓰레드 예: 음악 연주기, 동영상 플레이어, Eclipse IDE, 요즘 사용하는 대부분의 프로그램들 요즘 프로그램들은 Context 변화하는 단위가 프로세스가 아닌 쓰레드 Thread vs Process 한 프로세스에는 기본 1개의 쓰레드 단일 쓰레드 (single thread) 프로그램 한 프로세스에 여러 개의 쓰레드 다중 쓰레드 (multi-thread) 프로그램 쓰레드 구조 프로세스의 메모리 공간 공유 (code, data) 프로세스의 자원 공유 (file, i/o, …) 비공유: 개별적인 PC, SP, registers, stack 프로세스의 스위칭 vs 쓰레드의 스위칭 예제: 자바 쓰레드 java.lang.Thread 주요 메소드 public void run() // 새로운 맥이 흐르는 곳 (치환) void start() // 쓰레드 시작 요청 void join() // 쓰레드가 마치기를 기다림 static void sleep() // 쓰레드 잠자기 Thread.run() 쓰레드가 시작되면 run() 메소드가 실행된다 ⇒ run() 메소드를 치환한다 class MyThread extends Thread { public void run() { // 치환 (override) // 코드 } } 예제: 글자 A 와 B 를 동시에 화면에 출력하기 모든 프로그램은 처음부터 1개의 쓰레드는 갖고 있다 (main) 2개의 쓰레드: main + MyThread class Test { public static void main(String[] arg) { MyThread th = new MyThread(); th.start(); for (int i=0; i&lt;1000; i++) System.out.print(\"A\"); } } class MyThread extends Thread { public void run() { for (int i=0; i&lt;1000; i++) System.out.print(\"B\"); } Reference 양희재 교수님 운영체제 강의 introduction to Operating System 위키백과",
    "tags": "os development",
    "url": "/development/2018/07/29/cpu-scheduling-and-process/"
  },{
    "title": "신입사원을 위한 웹서비스 확장 전략",
    "text": "강대명님의 신입사원을 위한 웹서비스 확장 전략 발표 자료를 보며 정리 및 모르는 내용을 작성한 글입니다 주제 웹서비스를 확장하기 위해 알아야할 방법들 돈 많으면 서버를 빵빵하게 사용하면 끝ㅋ 그러나 항상.. 회사에선 비용 절감을 하려고 함 대규모 서비스 개발 노하우의 필요성 대규모 서비스 사례 2011년 bit.ly : 초당 1216 clicks, 하루에 1억 5백만 클릭 2011년 Netflix : 1년 사이에 트래픽 20배 증가 배틀 그라운드 : 3개월만에 5배 이런 서비스 트래픽 증가를 버틸려면? 첫번째 고려 사항 이 정도의 서비스 증가가 일어나면 어떻게 해야할까? 한동안 서비스가 안된다면 어떤 일이 벌어질까? 대규모 서비스의 특징 Elastic Resiliency Elastic 트래픽이나 상황에 따라서 서버의 추가/제거가 쉬워야 함 Resiliency 특정 장비의 장애 등은 자동으로 복구가 되어야 함 서버가 복구되는 건 아님 해당 장비의 장애로 인해 다른 쪽이 영향을 받지 않아야 함 Scale Up &amp; Scale Out 초당 1000 TPS 상황이라면 Scale Up : 3배 처리 가능한 서버를 투입 Scale Out : 서버의 개수를 늘려서 처리 SPOF : Single Point Of Failure 시스템의 구성 요소 중, 동작하지 않으면 전체 시스템이 중단되는 요소 LB : Load Balancer 여러 Server에서 Traffic을 분산시켜줌 NAT(Netwrok Address Translation IP 주소를 공인 IP로 바꿀 때 사용하는 주소 변조 기능 Tunneling DSR(Dynamic Source Routing Protocol) 로드 밸런서(Load Balancer)란? 참고 사례로 보는 확장 아이디어 다시 돌아와서 어디를 확장해야 할까? API 서버 API 서버에만 부하가 몰리는 작업은? 파일 IO가 많은 정적 파일 서빙 웹 스크래핑 독립적인 작업이 가능한데, CPU나 다른 작업이 많이 필요한 MO 게임서버 이미지의 영상 인식, 전처리 DB 서버 카톡방의 대화는? 페이스북의 글, 댓글, 친구 관계는? 유튜브 등에 올라가는 비디오나 댓글 우리가 아는 대부분이 여기에 부하를 줌! Stateless한 서버는 API 서버는 비즈니스 로직만 가짐 크고 안정적인 디비가 필요 장점 추가/삭제가 간단 사용하는 쪽에서 주소만 추가하거나 제거해서 사용 Or 로드밸런서에 추가하거나 제거하면 끝 단점 결국 데이터의 저장이 필요하므로, 뒤에 책임을 떠넘기는 구조 개별 성능은 Stateful한 경우보다 떨어질 수 있음 이 방식을 사용한 이유 stateless한 서버에 신경쓰지 말고 중요한 DB(Storage)에 집중하는 것이 더 좋다고 판단 두 마리 토끼를 쫓지 말고 한 놈만 팬다 Shard Nothing 일반적인 DB 서버는 Read에 부하가 많음 읽기 분배 : Master에서만 Write, Slave에선 Only Read 각 DB 서버를 2대로 늘림 =&gt; Slave 장비를 추가해도 계속 성능이 증가하진 않음 Database Partitioning Vertical Horizontal Sharding Horizontal Partitioning Database의 샤딩(Sharding)이란? 참고 특정 Key를 저장하는 방법 Range : 특정 범위대역으로 나누기 (User 1~100은 Server #1) =&gt; 데이터의 이동이 심해짐 Modular : 서버 대수로 나누기 (0%2=0, Server #1, 1%2=1면 Server #2, 3대가 되면 2가 아니라 3으로 나눔) Inedxed : 특정 데이터의 위치를 가리키는 서버가 존재 특정 Key를 찾는 방법 consistent hashing",
    "tags": "web development",
    "url": "/development/2018/07/28/webservice-scaling/"
  },{
    "title": "데이터 엔지니어 관련 영상 메모",
    "text": "Youtube에서 데이터 엔지니어로 검색해서 나온 몇 영상들을 보고 정리한 글입니다. 추후 영어 비디오도 정리할 예정입니다 (지속적으로 업데이트) 데이터 지능 팟캐스트. E11 데이터 엔지니어링편 넷플릭스 배재현님, 쿠팡 글로벌 최현식님(2018년 3월) 데이터 지능 팟캐스트 넷플릭스 배재현님 서치 백엔드 경력으로 시작해서 이렇게 넷플릭스, 우버, 넷플릭스 넷플릭스에서 실시간 파이프라인 (카프카) 우버에선 시장데이터 분석 및 최적화 카프카 메세지 큐 링크드인 고전적 데이터 파이프라인은 지점 A에서 B로 보내면 B가 다른 쪽으로 보내고, 그리고 저장하는 방식 카프카는 브로커 컨슈머 프로듀서. 브로커가 저장하고 프로듀서가 보내고 컨슈머가 받고. 브로커가 디스크에 저장하니 메세지 큐라고 생각 카프카가 생기고 개런티드 딜리버리란 말을 쉽게 사용하게 됨 컨슈머가 데이터를 가져가는 과정에서 락이 없음 자바, 씨쁠쁠에선 락을 걸지만 카프카는 토픽. 내부에 파티션이 존재해서 락이 없어도 됨 실시간 처리한 이유는 드루이드 프로젝트 파일럿. 드루이드에서 카프카를 사용하고 있어서 사용 쿠팡 최현식님 그루터에서 컨트리뷰터 (타조?), 쿠팡 미국지사 타조 : 기능적으론 Hive와 동일 ETL 데이터 파이프라인 설계, 구현, 유지보수, 머신러닝 파이프라인 일부 하이브 시퀄, 스팤sql, sql 최적화 airflow 관리, r 서버 관리 등 S3에서 Hive로, Hive에서 오로라로 동기화 오로라 : 데이터 웨어하우스 보통 관계형 DB/OLTP : 히스토리가 아닌 현재 값만 저장 Operation DB : 트랜잭션에 최적화. 분석을 하면 서비스에 영향을 줄 수 있어서 분석엔 적합하지 않음 Change data capture : mysql의 데이터를 dump떠서 hive로 load db의 오버헤드가 크고 실시간이나 잦은 주기로 덤프를 뜨면 db에 오버헤드가 생김 김진영님이 생각한 데이터 엔지니어링 : 로우하게 하둡 관리, 파이프라인 관리, 메트릭 관리 등으로 나눌 수 있다고 생각 데이터 과학자들과 어떻게 일을 하는지? 데이터 사이언스 : 문제 정의 - 데이터 검증 - 실험 데이터 엔지니어링 : 데이터를 질의 가능하게 처리. 데이터 과학자가 할 수 있을정도로 만들고, 데이터 엔지니어링이 그 밑단 데이터 준비 엔지니어링과 사이언스 사이에 구분이 애매함. 데이터 사이언스에서 시작한 엔지니어링은 지속 가능하기 쉽지 않음 서로의 언어를 이해할 수 있어야 함 쿠팡은 workflow가 백여개가 있는데 job이 오래 걸리거나 문제가 생기는 경우 데이터 사이언티스트가 최적화를 해달라고 요청. 이럴 때 시퀄, 스팤시스템 최적화 등을 진행 타조 시스템을 통해 분산 시스템을 이해하고 있기에 가능함 비즈니스 로직부터 이해하면 필요없는 구문이나 조인을 찾을 수 있음 데이터 엔지니어링의 역사 데이터를 질의 가능하게 만든다는 관점으로 보면, 랭킹 시스템/검색 시스템도 데이터 엔지니어링이라 볼 수 있음 맵리듀스 논문. 프로토콜 버퍼. 직렬화/역직렬화 논문을 읽어보길 하둡, 하둡을 쉽게 사용하기 위한 하이브, 하둡 2.0 생기고 스케쥴러 yarn이 별도로 나옴 지금은 batch 일을 하다보니 하둡 자체는 잡 스케쥴러 yarn과 자체 데이터센터인 hdfs 외에는 크게 커다란 비중을 차지하고 있진 않은 듯 하둡의 IO는 디스크에서 발생하고 스파크는 메모리! 스파크가 강세 denormalization, schemaless한 데이터를 다루고, nosql 데이터 엔지니어의 스킬셋 Q) 스킬셋이 최근에 바뀐 것 같은데 어떤가요? A) 클라우드 서비스가 점점 나와서 데이터 엔지니어들이 했던 일들이 점점 대체되고 있는 듯 하둡 : 프러덕션 / 분석 2가지로 사용됨. 프러덕션에는 여전히 중요함. 추천 엔진이나 검색 로직은 최적화가 필요해서 여전히 엔지니어링이 많이 필요함. 반면 분석은 점점 엔지니어링 스킬이 없어도 점점 가능. 인프라가 고도화되며 분석을 위한 도구를 만들던 데이터 엔지니어는 아리송 그러나 데이터 엔지니어는 계속 수요가 있음 비즈니스 어플리케이션을 만들 수 있는 사람이 되야 함 모든 것이 자동화되는 것은 먼 미래의 일이기에 수요가 있지만, 커리어의 그로스가 있을 분야인가 걱정함. 너무 빠르게 변화중(이건 딥러닝도 동일) 분석을 위한 데이터 엔지니어링은 어느정도 성숙기 (내 생각 : 딥러닝/머신러닝을 위한 데이터 엔지니어링은 아직 성장기인듯! 분산 딥러닝 같은 것과 Serving) 데이터 분석은 90%정도는 SQL로도 커버 가능하다고 생각. 그래서 이 직군의 성장에 대한 고민을 하는듯 진영님 Q) IOT같은 곳에서 데이터 처리가 더 중요하고, 요즘은 센서에서 음성 데이터를 모집할테니 이것도 발전이 필요하지 않을까요? A) 리얼타임은 아직 발전이 많이되진 않음. 로우 레이턴시와 하이 아이피에스는 항상 이슈! AB Test를 위해 셋업하고 그걸 보기 위해 이것저것 만듬. 사이언티스트도 ETL을 점점 관리하는듯 자체 데이터센터 구축한 사례 : 데이터 엔지니어링을 작게 시작하면 벤더에 주문하는 방식. 장비 셋팅까지 너무 오래걸리고 안정화까지 오래 걸림. 배포나 어플리케이션들 다 직접 개발. 페이스북 급 아니면 클라우드가 정답일듯 클라우드가 보안 관련이 취약적이다? 그렇지 않음 하드에 있다고 보안이 되는건 아님. 차라리 클라우드가 일관적으로 커버 가능하고 복구하는 기능이 존재. 전통적인 기업에서 걱정하는데, 딱히? 진영님 Q) 어떤 종류의 데이터는 클라우드에 올라가면 안된다 이런거 있지 않나? A) 개인 정보는 안됨! 법무팀과 이야기해서 처리했음 데이터 엔지니어로서 보람 및 단점 엔지니어링 문제를 해결하는 과정. 즐거움은 어려운 문제를 해결했을 때 느끼는 자기만족과 남들이 인정할 때(구현한 시스템이 비즈니스에 임팩트) 행복 데이터 과학자들이 제가 구축한 시스템을 사용하며 기분이 좋아지지만 몸은 힘들어지는 과정. 제 이름을 몰라도 시스템의 이름을 알면 기분이 좋아짐 어려운 문제를 해결해야 되는 중압감과 시스템 장애를 대처하는 책임감은 힘들고 고통받았음. 시스템이 유명해질수록 시스템의 한계도 빨리 오는 편. 오픈소스에 의존하는데 오픈소스 내부까지 이해하지 않으면 디버깅하기 힘듬. 소스코드를 읽어보고 커뮤니티에서 커버하지 못하면 자신이 패치하거나 해결해야 함 단순히 오픈소스를 조합해 파이프라인을 만드는 것은 누구나 할 수 있음. 진짜 어려운 문제를 해결하는 것부터 본인의 역량이 중요 데이터 엔지니어링을 시작하려는 분들에게 조언 굉장히 매력적인 분야. 수업에서 배운 내용을 그대로 쓸 수 있음(컴퓨터 사이언스 지식) 아무리 툴이 고도화되고 데이터 사이언스와 데이터 엔지니어가 점점 구분이 흐려지고 있지만, 본질은 변하지 않음. 분산 시스템과 데이터 구조, 알고리즘 공부를 많이 하길! 스파크, 배치 스트리밍 등을 다룰 수 있는 것도 기본기가 있었기 때문에 가능하다고 생각 제네럴리스트는 점점 가치를 잃고있는 것 같으니 본질에 집중하며 자신의 관심사에 대해 깊게 공부하는 것을 추천 데이터베이스! (쿼리 프로세스 체크는 하이브 이해할 때 좋음) 데이터 웨어하우스 챕터를 잘 이해하면 데이터레이크 이해할 때 도움이 될 것 자신이 잘 다루는 오픈소스! 잘 정리된 자료가 거의 없으니 직접 사용해보고 소스를 까보는 것을 추천 학생들에게 실습을 추천한다면 방법은? 데이터 과학은 캐글이 있는데 데이터 엔지니어링은? 실습은 보통 오픈소스 홈페이지가서 튜토리얼 따라하는 정도뿐이고 사실 공부하기 쉽지 않음. 데이터 엔지니어링의 실력은 scalability과 트러블슈팅을 어떻게 하는지가 중요한데 대용량 환경을 경험해봐야 함 AWS를 사용해서 서버 구축정도는 경험해볼 순 있지만 의미있는 데이터를 경험하긴 힘듬 ㅠㅠㅠ 사실 여전히 어떻게 할지는 어려움 데이터 엔지니어 주니어를 거의 뽑지 않음. 어플리케이션과 연결되있으면 순수 데이터 엔지니어보단 프러덕 개발에 가깝게 신입을 뽑긴하지만 고전적 데이터 엔지니어는 주니어를 잘 뽑지 않음 [실리콘밸리의 한국인 2018] 세상에 임팩트를 주는 엔지니어 김형진 우버 데이터 엔지니어(2018년 4월) 영상 엔지니어가 원하는 것은 무엇인가? 돈? 워라밸? 복지? 세상을 바꾸는 임팩트를 주는 엔지니어가 되고 싶다라는 엔지니어가 꽤 있음 Hacker News : 주말동안 못 만드는 이유는 효율성과 확장성! 우버의 마켓플레이스 팀 드라이버 포지셔닝 다이나믹 프라이싱 플랫폼, 데이터 매칭 Uber X 합석 40억회 매칭 매칭팀의 효율성 프로젝트(Uber X) 예약 매칭(Forward Dispatch) 현재 여정이 곧 끝날 예정이라면, 다음 승객을 미리 연결 여정 교환(Trip Swap) 더 나은 운전자/승객 매칭이 가능해지면 교환을 통해 모두 win-win 지속적 최적화(Continuous Optimization) 지역의 상황 변화에 따라 계속적으로 매칭을 최적화 프로젝트가 가져온 임팩트 여정 교환을 통한 시간 절약 : 1주일당 10년 이상 예약 매칭을 통한 시간 절약 : 1주일간 40년 이상 우버 익스프레스 풀 추가적으로 기다리기 지정돠니 장소로 걸어가기 더 저렴한 가격 2017년 5월 기준 총 50억회의 여정 달성 2017년 한 해 동안 총 40억회 여정 달성 만약 우버에서 일어나는 모든 여정의 예상 도착시간을 1분씩 절감한다면? 매년 76명의 삶에 해당하는 시간을 절약! 효율성 X 확장성 = 임팩트! 구글 엔지니어가 들려주는 디자인, 클라우드, 빅데이터 이야기 구글 클라우드 엔지니어, 홍기락(2017년 9월) 영상 앞부분만 들었음. 뒷부분은 디자이너가 개발하고 싶다면? 부분은 메모하지 않음 조지아텍 컴퓨터 사이언스 박사 빅데이터 관련 툴을 만드는 중! 클라우드가 인기있는 이유 기술적으론 예전부터 다 존재하던 것 데이터가 많아지며 컴퓨터 100대, 1000대를 직접 운영하기 어려워서 선호 데이터가 왜 많아졌나? 데이터가 점점 쌓임. 병원 같은 곳도 데이터를 쌓는 중 데이터가 쌓여서 우리가 못할 것이라고 했던 것들을 할 수 있게됨 맵리듀스 분산시스템 분산 시스템이 좋아서 기락님은 시작 구글이 시작한 것은 이유가 있음 맵리듀스 논문을 공개 하둡이 생겨 비즈니스(그러나 구글이 하진 않음) 구글도 이제 이런 것을 구현해 비즈니스를 하자! 클라우드 예시 가상으로 머신 제공 크롬북. 네트워크만 되면 모든 것이 될 수 있도록 설정(다른 불필요한 프로그램 없이) 고객 데이터를 클라우드에 올려서 분석(하이레벨 API 제공) 구글 드라이브, 구글 포토",
    "tags": "engineering data",
    "url": "/data/2018/07/28/data-engineer-video/"
  },{
    "title": "운영체제 이해하기",
    "text": "운영체제(Operation System)의 개론 공부를 하고 작성한 글입니다 PC를 구입하면 Windows XP, Linux, MS-DOS, Mac OSX 등등의 운영 체제(Operating System)가 설치되어 있음 운영체제가 없는 컴퓨터는 어떤 상황일까? 컴퓨터는 프로세서와 메모리로 구성되어 있음 메모리에 임의의 값이 들어가있어 process가 아무것도 할 수 없음. 야생마! 프로그램을 실행하려면? 하드디스크 안에 실행 파일이 존재 =&gt; 메모리에 올려야 함 =&gt; 올리는 역할을 OS가 함. OS가 없는 컴퓨터라면 올릴 수 없음 여러 개의 프로그램을 동시에 실행시키려면? 프로세스는 1개인데 동시에 여러개를 실행하는 것도 OS가 해줌 프린터에 인쇄 명령을 내리려면? 하드 디스크에 저장하려면? 운영체제 컴퓨터 하드웨어를 잘 관리해 : 프로세서, 메모리, 디스크, 키보드, 마우스, … 성능을 높이고 : Performance 사용자에게 편의성 제공 : Convenience 컴퓨터 하드웨어를 관리하는 프로그램 Control program for computer 부팅(Booting) 컴퓨터 구조 프로세서, 메모리(ROM, RAM), 하드디스크 POST(Power-On Self Test) : 전기를 키면 환경 설정이 잘 되어 있는지 Test. ROM에 있음 부트로더(Boot Loader) : 하드디스크에 있는 OS 정보를 찾아 메인 메모리의 램 영역으로 올림. 이 작업을 부팅이라 함 OS는 전기를 끄면 사라짐 운영체제 관리(Management) 프로그램 프로세서, 메모리, 디스크, 입출력장치 드라이브 메모리에 Resident(상주) 커널(kernel) vs 명령 해석기(shell, command interpreter) 커널 : 하드웨어를 제어하고 관리하는 부분 쉘 : OS의 껍질. 명령어를 실행하는 부분 운영체제는 정부와 비슷 자원 관리자 (resource manager) 자원 할당자 (resource allocator) 주어진 자원을 어떻게 잘 활용할까? (국토, 인력, 예산) 정부가 직접 일하진 않고 민간 기업이 일함 정부 부서가 존재 : 행정자치부, 교육부, 국방부 … 운영 체제 부서 : 프로세스 관리, 메모리 관리, 파일 관리, 입출력장치 관리 … 운영체제 역사 컴퓨터의 역사 : 1940년대 말에 만들어짐 하드웨어 발전 vs 운영체제 기술 발전 Batch Processing : 일괄 처리, 묶어서 처리. resident monitor. 최초의 OS Multiprogramming system : 다중프로그래밍 40년대, 50년대 컴퓨터는 매우 비싼 자원 빠른 CPU(계산), 느린 IO =&gt; 메모리에 여러 개의 job CPU scheduling, 메모리 관리, 보호 Time Sharing system : 시공유 시스템 강제 절환, interactive system (대화형) 빠른 스위칭 기술발전: 가상 메모리, 프로세스간 통신, 동기화 하드디스크를 메모리처럼 사용하는 기술 동시에 여러가지 일을 진행, 요즘 나오는 OS 대부분은 이 시스템 컴퓨터 규모별 분류 Supercomputer &gt; Mainframe &gt; Mini &gt; Micro Supercomputer &gt; Server &gt; Workstation &gt; PC &gt; Handheld &gt; Embedded 고성능컴퓨터의 OS 기술이 Handheld/Embedded 까지 Batch processing Multiprogramming Timesharing 고등 운영체제 1) 다중 프로세서 시스템 (Multiprocessor system) 메모리는 1개인데 CPU(프로세스)가 여러 개 병렬 시스템 (parallel system) 강결합 시스템 (tightly-coupled system) : 하나의 메모리에 CPU가 결합 3가지 장점: performance, cost, reliability ☞ 다중 프로세서 운영체제 (Multiprocessor OS) 2) 분산 시스템 (Distributed system) CPU-Memory 쌍이 LAN으로 구성 하둡 다중 컴퓨터 시스템 (multi-computer system) 3가지 장점: performance, cost, reliability 소결합 시스템 (loosely-coupled system) ☞ 분산 운영체제 (Distributed OS) 3) 실시간 시스템 (Real-time system) 시간 제약: Deadline 공장 자동화 (FA), 군사, 항공, 우주 ☞ 실시간 운영체제 (Real-time OS = RTOS) 인터럽트 기반 시스템 Interrupt-Based System 현대 운영체제는 인터럽트 기반 시스템! 부팅시 ROM에 있는 부트로더 실행 OS 정보를 메인 메모리로 가져옴 부팅이 끝나면? 운영체제는 메모리에 상주 (resident) 사건(event)을 기다리며 대기: 키보드, 마우스, … 하드웨어 인터럽트 (Hardware interrupt) 인터럽트 결과 운영체제 내의 특정 코드 (ISR) 실행 Interrupt Service Routine 종료 후 다시 대기 소프트웨어 인터럽트 (Software interrupt) 사용자 프로그램이 실행되면서 소프트웨어 인터럽트 (운영체제 서비스 이용 위해) 인터럽트 결과 운영체제 내의 특정 코드 실행 (ISR) ISR 종료 후 다시 사용자 프로그램으로 인터럽트 기반 운영체제 운영체제는 평소엔 대기 상태를 유지 하드웨어 인터럽트에 의해 운영체제 코드 (ISR) 실행 소프트웨어 인터럽트에 의해 운영체제 코드 실행 내부 인터럽트(Interrnal interrupt)에 의해 운영체제 코드 실행 : 코드에서 0으로 나누는 것이 있으면? 계산할 수 없으니 강제 종료시켜야 함. 이런 경우 내부 인터럽트 ISR 종료되면 원래의 대기상태 또는 사용자 프로그램으로 복귀 이중모드 한 컴퓨터를 여러 사람이 동시에 사용하는 환경 또는 한 사람이 여러 개의 프로그램을 동시에 사용 한 사람의 고의/실수 프로그램이 전체 영향 STOP, HALT, RESET 등 이런 명령어들은 관리자만 내릴 수 있도록 해야겠다 =&gt; 이중 모드 사용자 프로그램은 STOP 등 치명적 명령 사용 불가하게! 사용자 (user) 모드 vs 관리자 (supervisor) 모드 이중 모드 (dual mode) 관리자 모드 = 시스템 모드 = 모니터 모드 = 특권 모드 Supervisor, system, monitor, priviliged mode 특권 명령 (privileged instructions) STOP, HALT, RESET, SET_TIMER, SET_HW, … 이중 모드 (dual mode) 레지스터에 모드를 나타내는 플래그(flag) flag : overflow, carry, negative, zero, 모니터 비트로 모드 확인 운영체제 서비스 실행될 때는 관리자 모드 사용자 프로그램 실행될 때는 사용자 모드 하드웨어/소프트웨어 인터럽트 발생하면 관리자 모드 운영체제 서비스가 끝나면 다시 사용자 모드 일반 유저가 특권 명령을 실행하려고 하면 CPU가 모니터 비트를 확인하고, 관리자 모드가 아님을 인지. 내부 인터럽트 발생해서 서비스 루틴으로 점프해 강제 종료 일반적 프로그램의 실행 프로그램 적재 (on memory) 프로그램이 하드디스크에 있는 저장하고 싶은 경우 os에게 인터럽트 발생해 요청 (python에서 import os를 생각해보면..!) user mode → (키보드, 마우스) → system mode (ISR) → user mode → (모니터, 디스크, 프린터) → system mode (ISR) → user mode → …… 하드웨어 보호 입출력장치 보호 (Input/output device protection) 메모리 보호 (Memory protection) CPU 보호 (CPU protectiopn) 입출력장치 보호 사용자의 잘못된 입출력 명령 다른 사용자의 입출력, 정보 등에 방해 예: 프린트 혼선, 리셋, 다른 사람의 파일 읽고 쓰기 (하드디스크) IN, OUT 명령을 아무나 못 하도록 해결법 입출력 명령을 특권명령으로: IN, OUT 입출력을 하려면 운영체제에게 요청하고 (system mode 전환), 운영체제가 입출력 대행; 마친 후 다시 user mode 복귀 올바른 요청이 아니면 운영체제가 거부 사용자가 입출력 명령을 직접 내린 경우? Privileged instruction violation : 특권 명령을 침범 메모리 보호 다른 사용자 메모리 또는 운영체제 영역 메모리 접근 우연히 또는 고의로 다른 사용자 정보/프로그램에 대한 해킹 운영체제 해킹 해결법 문지기(MMU)를 두어 다른 메모리 영역 침범 감시하도록 (Memory Management Unit) MMU 설정은 특권명령: 운영체제만 바꿀 수 있다 다른 사용자 또는 운영체제 영역 메모리 접근 시도? Segment violation : 영역 침범 CPU 보호 한 사용자가 실수 또는 고의로 CPU 시간 독점 예: while (n = 1) … 다른 사용자의 프로그램 실행 불가 해결법 CPU Protection Timer를 두어 일정 시간 경과 시 타이머 인터럽트 인터럽트 → 운영체제 → 다른 프로그램으로 강제 전환 운영체제 서비스 프로세스 관리 주기억장치 관리 파일 관리 보조기억장치 관리 입출력 장치 관리 네트워킹 보호 그외 프로세스 관리 Process management 프로세스 (process) 메모리에서 실행 중인 프로그램 (program in execution) 주요기능 프로세스의 생성, 소멸 (creation, deletion) 프로세스 활동 일시 중지, 활동 재개 (suspend, resume) 프로세스간 통신 (interprocess communication: IPC) 프로세스간 동기화 (synchronization) 교착상태 처리 (deadlock handling) 주기억장치 관리 Main memory management 주요기능 프로세스에게 메모리 공간 할당 (allocation) 메모리의 어느 부분이 어느 프로세스에게 할당되었는가 추적 및 감시 프로세스 종료 시 메모리 회수 (deallocation) 메모리의 효과적 사용 가상 메모리: 물리적 실제 메모리보다 큰 용량 갖도록 파일 관리 File management Track/sector 로 구성된 디스크를 파일이라는 논리적 관점으로 보게 주요기능 파일의 생성과 삭제 (file creation &amp; deletion) 디렉토리(directory)의 생성과 삭제 (또는 폴더 folder) 기본동작지원: open, close, read, write, create, delete Track/sector 와 file 간의 매핑(mapping) 백업(backup) 보조기억장치 관리 Secondary storage management 하드 디스크, 플래시 메모리 등 주요기능 빈 공간 관리 (free space management) 블락이 점점 사용된 공간, 비어있는 공간으로 나뉘는데 이런 공간 관리 저장공간 할당 (storage allocation) 디스크 스케쥴링 (disk scheduling) 헤드를 적게 움직이며 원하는 트랙을 읽을 수 있을까 입출력 장치 관리 I/O device management 주요기능 장치 드라이브 (Device drivers) 입출력 장치의 성능향상: buffering, caching, spooling 시스템 콜 System calls 프로세서가 운영체제 서비스를 받기 위해 호출(요청) 주요 시스템 콜 Process: end, abort, load, execute, create, terminate, get/set attributes, wait event, signal event Memory: allocate, free File: create, delete, open, close, read, write, get/set attributes Device: request, release, read, write, get/set attributes, attach/detache devices Information: get/set time, get/set system data Communication: socket, send, receive 프로세스 관리 CPU 자원을 효율적으로 관리 프로그램 vs 프로세스 (program vs process) process, task, job … 프로세스 : 실행중인 프로그램(program in execution): text + data + stack, pc, sp, registers, … 메모리 상에서 실행되는 작업 단위 일반적으로 CPU는 한번에 하나의 프로세스만 관리할 수 있음 프로그램 : 실행 되기 전의 명령어와 데이터의 묶음 무덤 속 프로그램, 살아 움직이는 프로세스 프로세스 상태 new, ready, running, waiting, terminated new : 메인 메모리에 올라온 상태 ready : 실행할 준비가 되어있는 상태 running : CPU가 실제 실행한 상태 waiting : CPU가 다른 작업을 진행하면 기존 작업은 waiting terminated : 종료 PCB: Process Control Block Task Control Block (TCB) 프로세서 제어 블록, 프로세스에 대한 모든 정보 보유 process state (running, ready, waiting, …), PC, registers, MMU info (base, limit), CPU time, process id, list of open files, … 사람과 비유? : 정부는 내 정보를 모두 기록하고 있음. PCB와 주민등록 등본과 유사 프로세스 대기열 (queue) Job Queue Job scheduler Long-term scheduler Ready Queue CPU scheduler Short-term scheduler Device Queue Device scheduler Multiprogramming Degree of multiprogramming i/o-bound vs CPU-bound process Medium-term scheduler Swapping 메모리에 올라와 있지만 아무 작업도 안하는 경우 디스크에 해당 메모리를 저장한 후(swap out), 사용하던 메모리를 다른 프로세서에게 분배 다시 작업이 진행될 경우 다시 메모리를 가지고 옴(swap in) 이런 작업들을 통칭 swapping이라고 함 컴퓨터의 성능 향상을 위해 진행 용어 Context switching (문맥전환) 프로세스가 메모리에 여러개 있어도 어느 순간엔 1개만 진행할 수 있음 p1-&gt;p2, p2-&gt;p3 Scheduler 지금 프로세스가 끝나고 어떤 프로세스를 실행할 것인가를 결정 Dispatcher 실제로 스케쥴러가 선택한 프로세스를 실행하도록 상태, 레지스터의 값을 바꿔줌 어셈블리 같은 로우 레벨 코드로 짜야됨 준비상태의 프로세스들 중에서 우선순위가 가장 높은 프로세스에게 CPU를 할당 Context switching overhead 하나의 프로세스가 CPU를 사용 중인 상태에서 다른 프로세스가 CPU를 사용하도록 하기 위해, 이전의 프로세스의 상태(문맥)를 보관하고 새로운 프로세스의 상태를 적재하는 작업 Running 상태의 Task가 사용하던 Context를 메모리 특정 영역에 저장한 후 새로이 수행 될 Task의 Context를 TCB또는 Stack에서 CPU의 레지스터 영역으로 복사하여 새로운 Task가 수행되도록 하는 일련의 작업 Reference 양희재 교수님 운영체제 강의 introduction to Operating System 스레드와 프로세스, 멀티프로그래밍,멀티태스킹,멀티스레딩,멀티프로세싱",
    "tags": "os development",
    "url": "/development/2018/07/27/os-intro/"
  },{
    "title": "Apache Kafka 운영 가이드",
    "text": "카프카, 데이터 플랫폼의 최강자를 읽고 Apache Kafka 기본 개념과 운영 가이드에 대해 정리한 글입니다. Reference 카프카, 데이터 플랫폼의 최강자",
    "tags": "engineering data",
    "url": "/data/2018/07/25/apache-kafka-guide/"
  },{
    "title": "Apache Kafka Install on Linux",
    "text": "카프카, 데이터 플랫폼의 최강자를 읽고 Apache Kafka 설치에 대해 정리한 글이며 linux 16.04에서 진행했습니다. 이 문서에선 직접 밑단부터 설치하는 내용을 다루지만, Confluent에서 다운받아 사용하거나 Docker를 사용해도 좋습니다 설치해야 하는 리스트 아파치 주키퍼 아파치 카프카 아파치 주키퍼 개요 분산 애플리케이션 관리를 위한 안정적 코디네이션 애플리케이션 각 애플리케이션의 정보를 중앙에 집중하고 구성 관리, 그룹 관리 네이밍, 동기화 등의 서비스 제공 직접 개발하기보다 안정적이라고 검증된 주키퍼를 많이 사용 카프카, 스톰, hbase, Nifi 등에서 사용됨 znode : 데이터를 저장하기 위한 공간 이름, 폴더 개념 주키퍼 데이터는 메모리에 저장되어 처리량이 매우 크고 속도 또한 빠름 앙상블(클러스터)라는 호스트 세트를 구성해 살아있는 노드 수가 과반수 이상 유지되면 지속적 서비스가 가능 과반수 방식으로 운영되어 홀수로 서버를 구성 3대 : 최대 초당 약 80,000 request 처리 5대 : 최대 초당 약 140,000 request 처리 로그 로그는 별도의 디렉토리에 저장 znode에 변경사항이 발생하면 트랜잭션 로그에 추가됨 로그가 어느정도 커지면 모든 znode의 상태 스냅샷이 파일시스템에 저장 myid 주키퍼 노드를 구분하기 위한 ID 각 클러스터에 다른 값 설정 환경설정 zoo.cfg 공식 문서 참고 설치 Cluster Setup 참고 jdk 설치 : sudo add-apt-repository ppa:openjdk-r/ppa sudo apt-get update sudo apt-get install openjdk-8-jdk java heap 사이즈 설정 zookeeper 설치 config 파일 설정 주키퍼 홈페이지로 이동해 mirrors 클릭 후 URL 복사 다운로드 후 압축 풀기 sudo passwd su // 암호 입력. root로 변경 cd /usr/local/ wget http://mirror.navercorp.com/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz tar zxf zookeeper-3.4.13.tar.gz 심볼릭 링크 설정 : 주키퍼 버전을 올릴 경우 심볼릭 링크가 없으면 모두 변경해야 함 ln -s zookeeper-3.4.13 zookeeper // 확인하고 싶은 경우 ls -la zookeeper &gt;&gt;&gt; lrwxrwxrwx 1 byeon byeon 16 Jul 24 05:11 zookeeper -&gt; zookeeper-3.4.13 데이터 디렉토리 생성 mkdir data myid 설정 cd data vi myid // 내용은 1 // 다른 클러스터(서버)에도 data/myid에 2, 3 작성 후 저장 zoo.cfg 설정 cd /usr/local/zookeeper/conf vi zoo.cfg zoo.cfg 파일 분산일 경우 server.1 밑부분을 작성하고 standalone일 경우엔 비움 // zoo.cfg tickTime=2000 initLimit=10 syncLimit=5 dataDir=/usr/local/data clientPort=2181 server.1=0.0.0.0:2888:3888 server.2={머신2의 ip}:2888:3888 server.3={머신3의 ip}:2888:3888 실행 /usr/local/zookeeper/bin/zkServer.sh start 중지 /usr/local/zookeeper/bin/zkServer.sh stop systemd service 파일 작성 관리를 효율적으로 하기 위해 등록 예기치 않게 서버의 오작동으로 리부팅된 경우 특정 프로세스는 자동으로 시작, 특정 프로세스는 수동 시작일 경우가 있음 vi /etc/systemd/system/zookeeper-server.service [Unit] Description=zookeeper-server After=network.target [Service] Type=forking User=root Group=root SyslogIdentifier=zookeeper-server WorkingDirectory=/usr/local/zookeeper Restart=on-failure RestartSec=0s ExecStart=/usr/local/zookeeper/bin/zkServer.sh start ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop systemd service 등록 및 실행 systemctl daemon-reload systemctl enable zookeeper-server.service systemctl start zookeeper-server.service // 중지는 // systemctl stop zookeeper-server.service // 상태 확인은 // systemctl status zookeeper-server.service Cli 모드로 접속 /usr/local/zookeeper/bin/zkCli.sh -server localhost:2181 아파치 카프카 설치 cd /usr/local/ wget http://apache.mirror.cdnetworks.com/kafka/1.0.0/kafka_2.11-1.0.0.tgz tar zxf kafka_2.11-1.0.0.tgz 심볼릭 링크 설정 ln -s kafka_2.11-1.0.0 kafka 저장 디렉토리 준비 컨슈머가 메세지를 가져가더라도 저장된 데이터를 임시로 보관 디렉토리를 하나만 구성하거나 여러 디렉토리로 구성 가능 디스크가 여러개인 서버는 디스크 수만큼 디렉토리 만들어야 디스크별로 IO 분산 가능 mkdir kafka-data1 mkdir kafka-data2 카프카 브로커 서버들과 주키퍼 서버와 통신 가능 유무 확인 nc -v IP주소 Port 번호 // Connected to 1.1.1.1:2181이 뜨면 성공 // 포트가 막혀있으면 포트 설정 환경 설정 vi /usr/local/kafka/config/server.properties broker.id=1 log.dirs=/usr/local/kafka-data1, /usr/local/kafka-data2 // zookeeper.connect={호스트1:2181, 호스트2:2181, 호스트3:2181}/zzsza-kafka // 여기선 단일로 진행할거라 설정 그대로 사용 zookeeper.connect=localhost:2181/zzsza-kafka 카프카 실행 /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties systemd service 파일 작성 vi /etc/systemd/system/kafka-server.service [Unit] Description=kafka-server After=network.target [Service] Type=simple User=root Group=root SyslogIdentifier=kafka-server WorkingDirectory=/usr/local/kafka Restart=no RestartSec=0s ExecStart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh systemd service 등록 및 실행 systemctl daemon-reload systemctl enable kafka-server.service systemctl start kafka-server.service // 중지는 // systemctl stop kafka-server.service // 상태 확인은 // systemctl status kafka-server.service 카프카 상태 확인 TCP 포트 확인 주키퍼 netstat -ntlp | grep 2181 &gt;&gt;&gt; tcp6 0 0 :::2181 :::* LISTEN 1954/java 카프카 netstat -ntlp | grep 9092 &gt;&gt;&gt; tcp6 0 0 :::9092 :::* LISTEN 2205/java zookeeper znode를 이용한 카프카 정보 확인 /usr/local/zookeeper/bin/zkCli.sh ls / &gt;&gt;&gt; [zookeeper, zzsza-kafka] ls /zzsza-kafka/brokers/ids &gt;&gt;&gt; [1] 카프카 로그 확인 cat /usr/local/kafka/logs/server.log 카프카 Topic 생성 /usr/local/kafka/bin/kafka-topics.sh --zookeeper localhost:2181/zzsza-kafka --replication-factor 1 --partitions 1 --topic hi-topic --create 메세지 생성 /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic hi-topic &gt; This is a message &gt; This is anotehr message // ctrl + C 메세지 가져오기 /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hi-topic --from-beginning Reference 카프카, 데이터 플랫폼의 최강자 systemd unit 등록 관련 옵션 정리 Apache Zookeeper Server 설치 Ubuntu 16.04 system service 등록하",
    "tags": "engineering data",
    "url": "/data/2018/07/24/apache-kafka-install/"
  },{
    "title": "VIM 자주 사용하는 명령어 및 Tip",
    "text": "VIM에서 자주 사용하는 명령어 및 Tip에 대해 작성한 글입니다. 페이스북 생활코딩의 정기욱님 글을 토대로 작성했습니다 명령 모드 ESC 누른 상태가 명령 모드 :set nu : 라인 출력 i : insert 모드로 변경 :{line number} : 해당 라인으로 이동 :wq : 저장하고 종료(ZZ도 동일) :q! : 강제 종료 w, b : 단어 단위 이동 dd : 줄 삭제 v : 블럭 지정 ?{검색할 단어} : 윗부분에서 검색 /{검색할 단어} : 아랫부분에서 검색 ^, $ : 라인 처음, 끝으로 이동 o, O : 커서 밑, 위에 빈 행 추가하며 입력 r : 현재 커서에 있는 글자 바꾸기 s : 현재 커서 글자 바꾸고 입력모드 yy : 라인 복사 p : 라인 복사 후 붙여넣기 H, L : 현재 스크린에서 출력된 첫 라인, 마지막 라인으로 이동 control + v : 컬럼 블록 shift + v : 줄 단위 블록 G : 마지막 행으로 가기 %s/{Old 단어}/{New 단어}/gc : Old 단어를 New 단어로 변경(바꾸기 전에 물어봄) /g는 글로벌 옵션 /c는 컨펌 옵션 u : 이전으로 되돌리기(undo) control + r : 되돌리기한 것을 다시 실행(redo) :v/관심패턴/d : 관심 패턴 라인만 남겨서 볼 경우 로그 분석시 사용 u 눌러서 복구 실행시 vi -d A파일 B파일 : 다른 부분들 하이라이트 CheatSheet 문서로 된 것은 링크 참고",
    "tags": "linux development",
    "url": "/development/2018/07/20/vim-tips/"
  },{
    "title": "리눅스 top 정리 및 설명",
    "text": "Linux 리눅스 top에 대해 정리한 글입니다 top을 통해 살펴보는 프로세스 정보들 top 시스템의 상태를 전반적으로 가장 빠르게 파악 가능(CPU, Memory, Process) 옵션 없이 입력하면 interval 간격(기본 3초)으로 화면을 갱신하며 정보를 보여줌 top 실행 전 옵션 순간의 정보를 확인하려면 -b 옵션 추가(batch 모드) -n : top 실행 주기 설정(반복 횟수) top 실행 후 명령어 shift + p : CPU 사용률 내림차순 shit + m : 메모리 사용률 내림차순 shift + t : 프로세스가 돌아가고 있는 시간 순 k : kill. k 입력 후 PID 번호 작성. signal은 9 f : sort field 선택 화면 -&gt; q 누르면 RES순으로 정렬 a : 메모리 사용량에 따라 정렬 b : Batch 모드로 작동 1 : CPU Core별로 사용량 보여줌 ps와 top의 차이점 ps는 ps한 시점에 proc에서 검색한 cpu 사용량 top은 proc에서 일정 주기로 합산해 cpu 사용율 출력 top -b -n 1 3:58 : 3시간 58분 전에 서버가 구동 load average : 현재 시스템이 얼마나 일을 하는지를 나타냄. 3개의 숫자는 1분, 5분, 15분 간의 평균 실행/대기 중인 프로세스의 수. CPU 코어수 보다 적으면 문제 없음 Tasks : 프로세스 개수 KiB Mem, Swap : 각 메모리의 사용량 PR : 실행 우선순위 VIRT, RES, SHR : 메모리 사용량 =&gt; 누수 check 가능 S : 프로세스 상태(작업중, I/O 대기, 유휴 상태 등) VIRT, RES, SHR 현재 프로세스가 사용하고 있는 메모리 VIRT 프로세스가 사용하고 있는 virtual memory의 전체 용량 프로세스에 할당된 가상 메모리 전체 SWAP + RES RES 현재 프로세스가 사용하고 있는 물리 메모리의 양 실제로 메모리에 올려서 사용하고 있는 물리 메모리 실제로 메모리를 쓰고 있는 RES가 핵심! SHR 다른 프로세스와 공유하고 있는 shared memory의 양 예시로 라이브러리를 들 수 있음. 대부분의 리눅스 프로세스는 glibc라는 라이브러리를 참고하기에 이런 라이브러리를 공유 메모리에 올려서 사용 Memory Commit 프로세스가 커널에게 필요한 메모리를 요청하면 커널은 프로세스에 메모리 영역을 주고 실제로 할당은 하지 않지만 해당 영역을 프로세스에게 주었다는 것을 저장해둠 이런 과정을 Memory commit이라 부름 왜 커널은 프로세스의 메모리 요청에 따라 즉시 할당하지 않고 Memory Commit과 같은 기술을 사용해 요청을 지연시킬까? fork()와 같은 새로운 프로세스를 만들기 위한 콜을 처리해야 하기 때문 fork() 시스템 콜을 사용하면 커널은 실행중인 프로세스와 똑같은 프로세스를 하나 더 만들고, exec() 시스템 콜을 통해 다른 프로세스로 변함. 이 때 확보한 메모리가 쓸모 없어질 수 있음 COW(Copy-On-Write) 기법을 통해 복사된 메모리 영역에 실제 쓰기 작업이 발생한 후 실질적인 메모리 할당을 진행 프로세스 상태 참고글 : Load Average에 관하여 SHR 옆에 있는 S 항목으로 볼 수 있음 D : Uninterruptiable sleep. 디스크 혹은 네트워크 I/O를 대기 R : 실행 중(CPU 자원을 소모) S : Sleeping 상태, 요청한 리소스를 즉시 사용 가능 T : Traced or Stopped. 보통의 시스템에서 자주 볼 수 없는 상태 Z : zombie. 부모 프로세스가 죽은 자식 프로세스",
    "tags": "linux development",
    "url": "/development/2018/07/18/linux-top/"
  },{
    "title": "리눅스 시스템 정보 확인하기",
    "text": "리눅스 시스템 정보(CPU, 메모리, 디스크, 네트워크 카드 등)를 확인하는 명령어에 대해 작성한 글입니다 커널 정보 확인하기 uname -a &gt;&gt;&gt; Linux instance-1 4.13.0-1019-gcp #23-Ubuntu SMP Thu May 31 16:13:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 위 서버의 커널은 4.13.0-1019-gc 버전을 사용하며 x86 계열의 64비트 운영 체제를 사용하고 있으며, 이름은 instance-1입니다 dmesg 커널의 디버그 메세지, 커널이 부팅할 떄 나오믐 메세지와 운영 중에 발생하는 메세지 출력 커널이 메모리를 인식하는 과정, 하드웨어를 인식하고 드라이버 올리는 과정 등을 알 수 있음 CPU 정보 확인하기 dmidecode -t bios 특정 BIOS 버전에 문제가 있다는 경우, 버전을 확인하기 위해 사용 dmidecode -t system : 시스템 모델명 제공 dmidecode -t processor cat /proc/cpuinfo lscpu : NUMA 정보도 제공 dmesg | grep CPU 메모리 정보 확인하기 dmidecode -t memory Memory Device가 실제로 시스템에 꽂혀있는 메모리 cat /proc/meminfo dmesg | grep momory 디스크 정보 확인하기 df -h &gt;&gt;&gt; Filesystem Size Used Avail Use% Mounted on udev 835M 0 835M 0% /dev tmpfs 169M 2.7M 167M 2% /run /dev/sda1 9.7G 1.2G 8.6G 12% / tmpfs 845M 0 845M 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 845M 0 845M 0% /sys/fs/cgroup tmpfs 169M 0 169M 0% /run/user/1001 각 파티션은 /, /dev, /run, /dev/shm, /run/lock, /sys/fs/cgroup, /run/user/1001로 마운트되어 있음 네트워크 정보 확인하기 lspci | grep -i ether ethtool -g eth0 -g 옵션은 Ring Buffer 크기를 확인 -G 옵션은 값을 설정할 때 사용 -i 옵션은 커널 드라이버 정보 표시",
    "tags": "linux development",
    "url": "/development/2018/07/18/linux-system-information/"
  },{
    "title": "Tensorflow Serving Tutorial",
    "text": "딥러닝 모델을 만들면, 만들고 끝!이 아닌 Product에 모델을 배포해야 합니다. 이 모델을 추가하는 과정을 어떻게 할 수 있을까요? 그리고 직접 API를 만드는 것과 Tensorflow Serving API을 사용하는 것의 차이점은 무엇일까요? 이런 궁금증을 해결하기 위해 다양한 자료를 보고 정리한 글입니다. 시리즈물로 정리할 예정이며 이번 글에선 Tensorflow Serving Tutorial에 대해서만 작성했습니다. 잘못된 내용이 있으면 말씀해 주세요!!! 수정 사항 2019.09.01 글을 작성할 당시엔 estimator가 많이 사용되지 않아 그 예제를 작성하진 않았습니다. 요샌 estimator도 많이 사용되는데, 관련해서 flask-tensorflow estimator로 serving하는 Repo를 발견해 공유드립니다 차금강님의 flask-tensorflow Github 깔끔하게 코드를 작성해주셔서 매우 유용합니다 :) 관련 글 Tensorflow Serving Tutorial(현재 글) Tensorflow Serving My Model Serving with Google CloudML Serving with Flask Model Serving Model Serving한다는 것은 inference를 의미. 대표적인 3가지 방법 Tensorflow Serving Python : tensorflow-serving-api 사용 다른 언어 : bazel 빌드 Serving시 Python 사용하면 퍼포먼스가 상대적으로 좋지 않음 어려운 이유 C++ code. Tensorflow 사용하는 사람들은 대부분 Python만 익숙 Kubernetes, gRPC, bazel? 모르는 용어 투성이 Compile 필요하고 오래 걸림 Google Cloud CloudML 장점 : 쉬움! CloudML이 다 해줌 단점 : 비용 발생 Flask를 사용한 API 장점 : 빠르게 Prototype 만들 때 사용 가능 단점 : 초당 API Request가 많은 대용량 서비스라면 사용하기 힘듬(퍼포먼스 이슈) 정리하면 Tensorflow Serving보다 다른 방법(CloudML, Flask)은 상대적으로 쉬운 편이고, CloudML은 노드 시간당 비용이 발생하고, Flask 사용한 방법은 대용량 Request에 버티기 힘듬 결국 주어진 상황에 따라 선택하면 될 듯(유저에게 사용하는 API가 아니고, 사내 자동화를 위해서라면 초당 Request가 적을테니 Flask 사용해도 OK) Architecture 파란색 : Client 초록색 : Server 연두색 : 여러 Model들(v1, v2 …) gRPC RPC의 일종 GET / POST만 존재 Component Servables 클라이언트가 계산을 수행하는데 사용하는 기본 object(perform computation) model 저장 4개의 component의 중심 Loaders servable(model)의 life cycle을 관리 manager를 위한 임시 저장소(temporary storage for the manager) Sources contain servables gateway loader로 올림 모델의 다른 버전을 track Managers full lifecycle 관리 Install 1) Bazel 2) Tensorflow-serving-api (공통) Ubuntu(Docker) docker ubuntu 16.04 이미지를 통해 컨테이너 실행 Dockerfile FROM ubuntu:16.04 RUN apt-get update &amp;&amp; apt-get install -y software-properties-common &amp;&amp; add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \\ apt-get update &amp;&amp; apt-get install -y python3.6 python3.6-dev python3-pip git RUN ln -sfn /usr/bin/python3.6 /usr/bin/python3 &amp;&amp; ln -sfn /usr/bin/python3 /usr/bin/python &amp;&amp; ln -sfn /usr/bin/pip3 /usr/bin/pip build docker build -t docker-ubuntu16-python3.6 . docker run docker run -it docker-ubuntu16-python3.6 bash 1) Bazel 소스코드 직접 빌드시 사용 Bazel 0.5.4 이상 version 필요한 패키지 설치 sudo apt-get install pkg-config zip g++ zlib1g-dev unzip Bazel 설치 : Github wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-installer-linux-x86_64.sh Installer 실행 chmod +x bazel-0.15.0-installer-linux-x86_64.sh ./bazel-0.15.0-installer-linux-x86_64.sh --user Environment 설정 export PATH=\"$PATH:$HOME/bin\" gRPC dependencies 설치 sudo apt-get update &amp;&amp; sudo apt-get install -y \\ automake \\ build-essential \\ curl \\ libcurl3-dev \\ git \\ libtool \\ libfreetype6-dev \\ libpng12-dev \\ libzmq3-dev \\ pkg-config \\ python-dev \\ python-numpy \\ python-pip \\ software-properties-common \\ swig \\ zip \\ zlib1g-dev gRPC 설치 pip3 install grpcio tensorflow serving clone git clone --recursive https://github.com/tensorflow/serving bazel build(뒤에 … 꼭 포함해야 함) cd serving bazel build -c opt --local_resources 5000,1.0,1.0 tensorflow_serving/… 2) Tensorflow-serving-api Bazel을 설치하지 않고(=빌드하지 않고) 그냥 Python에서 사용하고 싶은 경우 Tensorflow Serving 배포 URI를 패키지 소스로 추가 echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add - TensorFlow ModelServer 설치 sudo apt-get update &amp;&amp; sudo apt-get install tensorflow-model-server 공식적으론 Python2만 지원, 비공식적으로 Python3 존재 참고 Issue 설치(Python2) pip3 install tensorflow-serving-api 설치(Python3) pip3 install tensorflow-serving-api-python3 Test serving에서 제공하는 sample 모델 사용 tensorflow_model_server를 띄운 상태에서 Client가 Request 설치와 마찬가지로 1) Bazel 2) tensorflow-serving-api로 나눠서 설명 공통 Sample Model 다운(serving folder에서 실행) python tensorflow_serving/example/mnist_saved_model.py /tmp/mnist_model model_base_path에 model.pb 파일과 variables 폴더를 저장하면 됨. saved_model github 참고해서 구현 1) Bazel Tensorflow server 실행 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/ Client Request bazel-bin/tensorflow_serving/example/mnist_client --num_tests=1000 --server=localhost:9000 2) Tensorflow-serving-api python을 사용했지만 3점대(python3)도 몇가지 문법만 수정하면 정상 작동함! Tensorflow server 실행 tensorflow_model_server --port=9000 --model_name=mnist --model_base_path=/tmp/mnist_model/ Client Request python tensorflow_serving/example/mnist_client.py --num_tests=1000 --server=localhost:9000 동시에 여러 모델을 추가하고 싶을 경우 config file 생성하고 실행 시 model_config_file path 지정 model_config_list: { config: { name: \"Model1\", base_path: \"/path/to/model1\", model_platform: \"tensorflow\" }, config: { name: \"Model2\", base_path: \"/path/to/model1\", model_platform: \"tensorflow\" }, } 실행 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_config_file=&lt;path_to_your_config_on_disk&gt; Reference Tensorflow Serving Document",
    "tags": "dl data",
    "url": "/data/2018/07/12/tensorflow-serving-tutorial/"
  },{
    "title": "Python Scheduler 만들기(APScheduler)",
    "text": "종종 스케쥴러를 만들어야할 때가 있습니다. 스케쥴러를 만드는 방법은 분산 작업큐를 담당하는 Celery, crontab, Airflow, APScheduler 등 다양하게 존재합니다. Airflow 내용은 링크를 참고해주세요! 이번 글에선 APScheduler로 스케쥴러를 만들겠습니다. 이 라이브러리는 다른 라이브러리에 비해 간단히 구현이 가능합니다 APScheduler Advanced Python Scheduler의 약자로 Daemon이나 Service가 아님 이미 존재하는 어플리케이션 내에서 실행 코드를 보면 쉽게 이해할 수 있음 SQLAlchemy, MongoDB, Redis 등의 백엔드와 같이 사용할 수 있음 (default는 Memory) asyncio, gevent, Tornado, Twisted, Qt 등의 프레임워크와 같이 쓸 수 있음 실행 방식 Cron : Cron 표현으로 실행 Interval : 일정 주기로 실행 Date : 특정 날짜로 실행(사실상 Cron과 동일) 스케쥴러 종류 BlockingScheduler : 단일 스케쥴러 BackgroundScheduler : 다중 스케쥴러 AsyncIOScheduler GeventScheduler TornadoScheduler TwistedScheduler QtScheduler 설치 pip3 install apscheduler 코드 from apscheduler.jobstores.base import JobLookupError from apscheduler.schedulers.background import BackgroundScheduler import time class Scheduler: def __init__(self): self.sched = BackgroundScheduler() self.sched.start() self.job_id = '' def __del__(self): self.shutdown() def shutdown(self): self.sched.shutdown() def kill_scheduler(self, job_id): try: self.sched.remove_job(job_id) except JobLookupError as err: print(\"fail to stop Scheduler: {err}\".format(err=err)) return def hello(self, type, job_id): print(\"%s Scheduler process_id[%s] : %d\" % (type, job_id, time.localtime().tm_sec)) def scheduler(self, type, job_id): print(\"{type} Scheduler Start\".format(type=type)) if type == 'interval': self.sched.add_job(self.hello, type, seconds=10, id=job_id, args=(type, job_id)) elif type == 'cron': self.sched.add_job(self.hello, type, day_of_week='mon-fri', hour='0-23', second='*/2', id=job_id, args=(type, job_id)) if __name__ == '__main__': scheduler = Scheduler() scheduler.scheduler('cron', \"1\") scheduler.scheduler('interval', \"2\") count = 0 while True: ''' count 제한할 경우 아래와 같이 사용 ''' print(\"Running main process\") time.sleep(1) count += 1 if count == 10: scheduler.kill_scheduler(\"1\") print(\"Kill cron Scheduler\") elif count == 15: scheduler.kill_scheduler(\"2\") print(\"Kill interval Scheduler\") Reference 공식 Document",
    "tags": "python development",
    "url": "/development/2018/07/07/python-scheduler/"
  },{
    "title": "Apache Spark RDD, Dataframe을 DB(MySQL, PostgreSQL)에 저장하기",
    "text": "Apaceh Spark RDD, Dataframe을 MySQL, PostgreSQL에 저장하는 방법에 대해 작성한 글입니다. Mac 환경에서 작업했으며, Spark Version은 2.3.0입니다 작업 순서 RDD 생성 RDD to Dataframe Dataframe to DB(MySQL, PostgreSQL) 필요한 Driver 설정 build.sbt에 libraryDependencies에 추가 \"mysql\" % \"mysql-connector-java\" % \"5.1.12\", \"postgresql\" % \"postgresql\" % \"9.1-901.jdbc4\" RDD to DB import org.apache.spark.SparkConf import org.apache.spark.sql.{SaveMode, SparkSession} import java.util.Properties import org.apache.spark.sql._ import org.apache.spark.sql.types._ val appName = \"rddSaveDatabase\" val master = \"local[*]\" val conf = new SparkConf().setAppName(appName).setMaster(master) val sc = SparkSession.builder .master(master) .appName(appName) .config(\"spark.some.config.option\", \"config-value\") .getOrCreate() val values = List(\"20180705\", 1.0) // Row 생성 val row = Row.fromSeq(values) // SparkSession은 2.x 이후 엔트리 포인트로, 내부에 sparkContext를 가지고 있음 val rdd = sc.sparkContext.makeRDD(List(row)) val fields = List( StructField(\"First Column\", StringType, nullable = false), StructField(\"Second Column\", DoubleType, nullable = false) ) val dataFrame = sc.createDataFrame(rdd, StructType(fields)) val properties = new Properties() properties.put(\"user\", \"mysql_username\") properties.put(\"password\", \"your_mysql_password\") // to MySQL dataFrame.write.mode(SaveMode.Append).jdbc(\"jdbc:mysql://localhost:3306/dbtest\", \"test\", properties) // to PostgreSQL dataFrame.write.mode(SaveMode.Append).jdbc(\"jdbc:postgresql://localhost:5432/dbtest\", \"test\", properties) println(\"RDD Save to DB!\") SaveMode.Append : DB에 추가 SaveMode.Overwrite : DB에 덮어쓰기 Dataframe to DB import org.apache.spark.SparkConf import org.apache.spark.sql.{SaveMode, SparkSession} import java.util.Properties import org.apache.spark.sql._ import org.apache.spark.sql.types._ val appName = \"dataframeSaveDatabase\" val master = \"local[*]\" val conf = new SparkConf().setAppName(appName).setMaster(master) val sc = SparkSession.builder .master(master) .appName(appName) .config(\"spark.some.config.option\", \"config-value\") .getOrCreate() import sc.implicits._ val values = List((\"zzsza\", \"2018-07-05\", \"2017-07-06\")) val dataFrame = values.toDF(\"user_id\", \"join_date\", \"event_date\") val properties = new Properties() properties.put(\"user\", \"database_username\") properties.put(\"password\", \"your_database_password\") // to MySQL dataFrame.write.mode(SaveMode.Append).jdbc(\"jdbc:mysql://localhost:3306/dbtest\", \"test\", properties) // to PostgreSQL dataFrame.write.mode(SaveMode.Append).jdbc(\"jdbc:postgresql://localhost:5432/dbtest\", \"test\", properties) println(\"Dataframe Save to DB!\") Reference Spark Dataframe을 MySQL에 저장하는 방법 Spark Dataframe From List[Any] Spark for Data Analyst",
    "tags": "engineering data",
    "url": "/data/2018/07/05/rdd-dataframe-save-to-rdb/"
  },{
    "title": "Xgboost 논문 리뷰 및 코드",
    "text": "Xgboost 논문 리뷰 및 코드를 작성한 내용입니다. 논문의 전체를 리뷰하진 않고 특정 부분만 했습니다 부스팅 라운드가 지날수록 모델의 에러를 줄이는 과정 어떤 모델이 유효한지, 적절한지를 찾아내는 과정 약한 예측 모형들을 결합해 강한 예측 모형을 만드는 알고리즘 배깅과 유사하게 초기 샘플 데이터로 다수의 분류기를 만들지만 배깅과 다르게 순차적 무작위성이 없으며 강력한 사전 가지치기 사용 XGBoost Paper Abstract Tree boosting은 매우 효과적이고 머신러닝 방법에서 많이 사용됨 유연하고 end-to-end tree bootsting 시스템인 XGBoost sparse한 data를 위한novel sparsity-aware algorithm과 approximate tree learning를 위한 weighted quantile sketch를 제안 cache access 패턴, data compression, sharding에 대한 인사이트 제공 Keywords Large-scale Machine Learning 1. Introduction 성공적인 어플리케이션의 2가지 중요한 요인 효과적인 통계적인 모델 사용 유연한 learning 시스템 XGBoost의 가장 중요한 요인 : 모든 시나리오에 대한 확장성 단일 시스템에서 사용한 기존 일반적 솔루션보다 10배 이상 빠르게 실행 분산 또는 메모리가 제한된 상황에서도 수십억 가지의 예제로 확장 XGBoost의 확장성은 여러 중요한 시스템과 알고리즘 최적화로 구성됩니다 Sparse data를 handling하기 위한 novel tree learning algorithms qunatitle sketch procedure를 사용해 approximate tree learning에서 인스턴스 가중치를 처리할 수 있음 병렬 및 분산 컴퓨팅으로 학습 속도가 빨라져 model ex-ploration이 빨라짐 out-of-core(core를 벗어난? 넘어선?) 계산을 이용하고 데이터 과학자가 데스크탑에서 수억개의 예시를 처리할 수 있게함 본 논문의 핵심 We design and build a highly scalable end-to-end tree boosting system : 확장성이 뛰어닌 end-to-end 부스팅 시스템 설계 We propose a theoretically justified weighted quantile sketch for efficient proposal calculation : 효율적인 proposal 계산을 위한 weighted quantitle sketch 제안 We introduce a novel sparsity-aware algorithm for parallel tree learning : 병렬 트리 학습을 위한 novel sparsity-aware 알고리즘 소개 We propose an effective cache-aware block structure for out-of-core tree learning : 코어를 넘어선 트리를 학습하기 위한 효율적 캐시 인식 블록 구조 제안 2. Tree Boosting in a nutshell review gradient tree boosting algorithms 2.1 Regularized Learning Objective Leaf 노드 하나에 대해서만 Decision Value를 갖는 Decision tree와 달리, 모든 regression tree는 연속적인 점수를 포함 이 방식은 CART 방식으로, 모든 리프들이 모델의 최종 스코어에 연관되어있다는 뜻 CART는 모델끼리 모델의 우위를 비교할 수 있음 i번째 잎의 점수를 w_{i}로 표현 L(\\emptyset) = \\sum_{i}l(\\hat{y_{i}}, y_{i}) + \\sum_{k}\\Omega(f_{k}) where\\quad \\Omega(f_{k})=\\gamma T+\\frac{1}{2}\\lambda ||w||^{2} l : differentiable convex loss function \\Omega : 모델의 복잡성 패널티 계수 최종 학습된 가중치를 부드럽게할 때 도움 RGF(Regulized greedy forest) 모델에서도 비슷한 정규화 방법 사용 \\gamma T : Number of leaves \\gamma T+\\frac{1}{2}\\lambda \\lVert w\\rVert^{2} : L2 norm of leaf scores 리프 스코어와 리프 갯수가 모델의 복잡도를 결정 2.2 Gradient Tree Boosting t번째 iteration에서 i번째 인스턴스의 예측을 \\hat{y_{i}}^{(t)}라 하고 f_{t}를 추가해 다음과 같은 목적 함수를 최소화 L^{t} \\simeq \\sum_{i}^{n}[l(y_{i},\\hat{y}^{(t-1)}) + g_{i}f_{t}(x_{i})+\\frac{1}{2}h_{i}f_{t}^2(x_{i})] + \\Omega(f_{t}) 목적 함수를 단순히 하기 위해 상수항을 제거 \\tilde{L}^{(t)} = \\sum_{i=1}^{n}[g_{i}f_{t}(x_{i})+\\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\\Omega(f_{t})\\qquad(3) \\Omega를 풀면 \\tilde{L}^{(t)} = \\sum_{i=1}^{n}[g_{i}f_{t}(x_{i})+\\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\\gamma T+\\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_{j}^{2} = \\sum_{j=1}^{T}[(\\sum_{i \\in I_{j}}g_{i})w_{j}+\\frac{1}{2}(\\sum_{i\\in I_{j}} h_{i}+\\lambda)w_{j}^{2}]+\\gamma T \\qquad(4) q(x)가 fixed structure라서 leaf j의 optimal weight w_{j}^*를 계산할 수 있음 w_{j}^{*}=-\\frac{\\Sigma_{i\\in I_{j}}g_{i}}{\\Sigma_{i \\in I_{j}} h_{i}+\\lambda} + \\gamma T \\quad(5) \\tilde{L^{(t)}}(q)=-\\frac{1}{2}\\sum_{j=1}^{T} \\frac{(\\Sigma_{i\\in I_{j}}g_{i})^{2}}{\\Sigma_{i \\in I_{j}} h_{i}+\\lambda} + \\gamma T \\quad(6) (6)은 tree 구조 q의 scoring function으로 사용될 수 있음 2.3 Shinkage and Column Subsampling 오버피팅을 방지하기 위해 사용된 기술 1) Shrinkage : tree boosting 후 각 step후에 새로 추가된 weights 2) Column(feature) subsampling : column subsampling을 통해 오버피팅 방지하고 병렬적으로 학습 속도를 높일 때 사용됨 (전통적인 subsampling은 row 기반이었음) 3. Split finding algorithms 3.1 Basic Exact Greedy Algorithm Exact Greedy Algorithm : 모든 feature의 가능한 split point를 나눠서 탐색 =&gt; 계산 오래 걸림 3.2 Approximate Algorithm feature 분포의 percentile에 기반해 bucket으로 매핑 인터넷을 통해 공부한 내용 Boosting 기존 모델이 잘 못맞춘 데이터를 간단한 추가 모델을 이용해 보완. 이런 과정으로 모델의 표현력을 높임 weight에 기반하면 Adaboost(못 맞추는 것에 대한 weight), error를 미분한 gradient에 기반하면 gradient boost가 됨 Feature Importance : tree 계열의 알고리즘에서 각 attribute가 얼마나 유용한지를 판단하는 feature importance를 계산할 수 있음. 하나의 decision tree에서 각각의 attribute로 이루어진 split point에 의해 얼마나 트리의 성능이 올라가는지를 계산함. instance가 split point에 의해 영향을 받는지 숫자만큼 가중치를 두어 성능 향상치를 계산함. 앙상블 모델(RF)은 모든 decision tree에 대해 구하고 평균을 냄. 성능 측정은 지니계수 또는 information gain을 사용 욕심쟁이 알고리즘을 사용해 분류 모델 M, G, H를 발견하고 분산처리를 통해 적합한 비중 파라미터를 찾음! 분류기는 Regression Score를 사용해 정확도 스코어를 측정하고 각 순서에 따라 강한 분류기부터 약한 분류기까지 랜덤하게 생성! =&gt; 분류기가 트리임 xgboost를 tree를 만들 때 CART라 불리는 앙상블 모델을 사용. 모든 리프들이 모델의 최종 스코어와 연관되어 있어서 같은 분류 결과를 갖는 모델끼리도 모델의 우위를 비교할 수 있음 트리 가지를 나누는 시점에서 information gain을 순차적으로 계산한 후, 점수가 마이너스일 때 가지를 잘라냄 xgboost가 빠른 이유는 폭 방향으로 성장하기 때문. 각 레벨에서 한번만 정렬, traverse 가능 또한 sorted features는 cache 파라미터 2-1. 일반 파라미터 booster : 어떤 부스터 구조를 쓸지. gbtree, gblinear, dart n_jobs : 몇 개의 쓰레드를 동시에 처리. default는 많이 num_feature : feature 차원의 숫자를 정해야 할 경우. default는 많이 2-2. 부스팅 파라미터 eta : learning rate로 boosting step마다 weight를 줘서 부스팅 과정에 오버피팅이 일어나지 않도록 함 gamma : infromation gain에서 r로 표현한 값. 이 값이 커지면 트리 깊이가 줄어들어 보수적인 모델이 됨 max_depth : 한 트리의 maximum depth. 커질수록 모델의 복잡도가 커짐. default는 6이고 이 경우 리프 노드의 개수는 최대 2^6 lambda(l2 reg-form) : l2의 weight로 숫자가 클수록 보수적인 모델 alpha(l1 reg-form) : l1의 weight로 숫자가 클수록 보수적인 모델 2-3. 학습 과정 파라미터 objective : 목적함수로 reg:linear, binary:logistic, count:poisson 등 다양함 eval_metric : 모델의 평가 함수, rmse, logloss, map 등 seed : 시드값 고정 2-4. 커맨드 라인 파라미터 num_rounds : boosting 라운드를 결정. 이 수가 적당히 큰게 좋음. epoch 옵션과 동일 XGBoost 설치 Mac brew install gcc git clone --recursive https://github.com/dmlc/xgboost.git cd xgboost; cp make/config.mk ./config.mk; vi config.mk // 아래 2개 주석 처리된 것 풀기 export CC = gcc-7 export CXX = g++-7 Esc:+q make -j4 sudo pip3 install -e python-package Test import xgboost as xgb import numpy as np data = np.random.rand(5, 10) label = np.random.randint(2, size=5) dtrain = xgb.DMatrix(data, label=label) model = xgb.XGBClassifier(n_estimators=5, nthread=-1, seed=8888) Reference 공식 문서 XGBoost Paper XGBoost 사용하기 Introduction to Boosted Trees(한국어) BoostedTree",
    "tags": "ml data",
    "url": "/data/2018/07/03/xgboost/"
  },{
    "title": "Up-sampling with Transposed Convolution 번역",
    "text": "Naoki Shubuya님의 Up-sampling with Transposed Convolution을 허락받고 번역한 글입니다. 번역이 어색한 경우엔 영어 표현을 그대로 사용했으며, 의역이 존재할 수 있습니다. 피드백 언제나 환영합니다! Up-sampling with Transposed Convolution 이 글은 Transposed convolution에 대해 들었본 적이 있지만 실제로 무엇을 의미하는지 모르는 사람을 위한 글입니다. 이 글에서 다루는 컨텐츠는 다음과 같습니다 Up-sampling의 필요성 왜 Transposed Convolution인가? Convolution 연산 Going Backward Convolution Matrix Transposed Convolution Matrix Summary 노트북은 Github에서 확인할 수 있습니다 Up-sampling의 필요성 Neural networks를 사용해 이미지를 생성할 때, 일반적으로 저해상도에서 고해상도로 Up-sampling합니다 up-sampling 연산을 수행하는 다양한 방법이 있습니다 Nearest neighbor interpolation Bi-linear interpolation Bi-cubic interpolation 역자 : CS231 2017 11강에선 Upsampling하는 방법으로 Unpooling, Transpose convolution을 말합니다 이런 방법들은 네트워크 아키텍처를 결정할 때 보간 방법을 필요로 합니다. 이것은 수동적인 feature engineering이며 network가 알 수 없습니다. 왜 Transposed Convolution인가? Up-sampling을 최적으로 하려면 Transposed convolution를 사용하면 됩니다. Transposed convolution은 미리 정의된 보간 방법을 사용하지 않으며 학습 가능한 parameter들이 있습니다. 다음과 같은 중요한 논문 및 프로젝트에서 사용되는 Transposed convolution을 이해하면 유용합니다 DCGAN의 generator는 무작위로 샘플링된 값을 사용해 full-size 이미지를 생성합니다 Semantic segmentation은 convolutional 레이어를 사용해 encoder에서 feature를 추출한 다음, original 이미지의 모든 픽셀을 분류하기 위해 decoder에서 original 이미지 크기를 복원합니다. 참고로 Transposed convolution은 아래와 같이 불리기도 합니다 Fractionally-strided convolution Deconvolution 역자 : Upconvolution, Backward strided convolution라고도 불립니다 이 글에선 transposed convolution이란 단어만 사용하지만 다른 글에선 위처럼 쓰일 수 있습니다 Convolution 연산 Convolution 연산이 어떻게 진행되는지 설명하기 위해 간단한 예시를 들겠습니다. 4x4 matrix와 convolution 연산(3x3 kernel, no padding, stride 1)이 있다고 가정하겠습니다. 아래 이미지에서 볼 수 있듯, output은 2x2 matrix입니다. Convolution 연산은 input matrix와 kernel matrix간 element-wise 곱의 합으로 계산합니다. 패딩이 없고 stride가 1이라 4번의 연산만 할 수 있습니다. 따라서 output matrix는 2x2입니다. 이런 convolution 연산의 한 가지 중요한 점은 input matrix와 output matrix 사이에 위치 연결성(positional connectivity)이 존재하는 것입니다 예를 들어, input matrix의 상단 왼쪽의 값은 output matrix의 상단 왼쪽값에 영향을 줍니다 조금 더 구체적으로, 3x3 커널은 input matrix의 9개의 값을 output matrix 1개의 값에 연결할 때 사용됩니다. convolution 연산은 many-to-one 관계를 형성합니다. 추후 이 개념이 필요하므로 꼭 기억해주세요. Going Backward 이제 다른 것을 해보겠습니다. matrix의 값 1개를 다른 matrix의 값 9개와 연결하려고 합니다. 이는 one-to-many 관계입니다. convolution 연산을 반대로 하는 것과 같으며, transposed convolution의 핵심 개념입니다. 예를 들어, 2x2 matrix 4x4 matrix로 up-sampling합니다. 이 연산은 1-to-9 관계를 형성합니다. 그러나 이런 연산을 어떻게 수행할까요? 설명을 위해, convolution matrix와 transposed convolution matrix를 정의하겠습니다. Convolution matrix Matrix를 사용해 convolution 연산을 표현할 수 있습니다. convolution 연산을 수행하고 matrix 곱을 위해 kernel matrix를 재배치합니다 3x3 kernel을 4x16 matrix로 재배치하겠습니다: Convolution matrix입니다. 각 row는 1개의 convolution 연산을 뜻합니다. 이것을 처음 본다면 아래 다이어그램이 도움이 될 수 있습니다. convolution matrix는 그냥 zero padding을 포함해 재배치한 kernel matrix라고 생각하면 됩니디 이 개념을 사용해, input matrix(4x4)를 column vector(16x1)로 펴보겠습니다(flatten) 4x16 convolution matrix와 1x16 input matrix를 곱할 수 있습니다 (16 차원의 column vector) output 4x1 matrix는 전과 같은 결과를 가지는 2x2 matrix로 reshape할 수 있습니다 역자 : Convolution 연산의 이미지를 비교해보면 쉽게 이해될 거에요 :) 요약하면, convolution matrix는 kernel weights의 재배치일 뿐이고 convolution 연산은 convolution matrix를 사용해 표현할 수 있습니다. 그래서? 핵심은 convolution matrix는 4x16이기 때문에 16(4x4)에서 4(2x2)로 갈 수 있습니다. 그리고 16x4 matrix가 있는 경우 4(2x2)에서 16(4x4)으로 갈 수 있습니다 헷갈리신가요? 계속 읽어주세요 Transposed Convolution Matrix 4(2x2)에서 16(4x4)로 가고 싶습니다. 따라서 우리는 16x4 matrix를 사용합니다. 또한 1 to 9의 관계를 유지하길 원합니다. Convolution matrix C(4x16)를 C.T(16x4)로 Transpose 했다고 가정하겠습니다. C.T(16x4)와 column vector(4x1)를 행렬 곱해서 output matrix(16x1)를 구할 수 있습니다. Transposed matrix는 1개의 값을 9개의 값들과 연결합니다. output은 4x4로 reshape할 수 있습니다 작은 matrix(2x2)를 더 큰 matrix(4x4)로 up-sampling했습니다. Transposed Convolution은 가중치를 배치하는 방식 때문에 1-9 관계를 유지합니다 주의 : matrix의 실제 가중치는 기존의 convolution matrix로 계산되지 않아도 됩니다. 중요한 것은 가중치 배치가 convolution matrix의 transpose에 의해 바뀌는 것입니다. Summary Transposed convolution 연산은 일반적인 convolution 연산과 동일한 연결성을 형성하지만 반대 방향으로 연결됩니다. Transposed convolution을 up-sampling시 사용할 수 있습니다. 또한 Transposed convolution의 가중치들은 학습 가능하기 때문에 미리 정의된 보간 방법이 필요하지 않습니다. Transposed convolution이라고 부르지만, 기존에 있던 convolution matrix를 Transpose해서 사용하는 것을 의미하진 않습니다. 핵심은 input과 output간의 연관성이 일반 convolution matrix와 비교할 때 역방향으로 처리되는 것입니다(many-to-one이 일반 convolution matrix, one-to-many가 transposed convolution) 따라서 Transposed convolution은 convolution이 아닙니다. 그러나 convolution을 사용해 transposed convolution을 모방할 수 있습니다. Transposed convolution과 동일한 효과를 내는 직접적인 convolution을 만들기 위해 input matrix에 0을 추가해 input을 up-sampling합니다. 이런 방식으로 Transposed convolution를 설명하는 글을 볼 수 있습니다. 그러나 Convolution 연산 전에, up-sampling을 위해 input matrix에 0을 추가하는 작업은 효율성이 떨어집니다 1가지 주의점 : Transposed convolution는 이미지 생성시 checkerboard artifacts를 만듭니다. 이 글에선 이런 문제를 줄이기 위해 convolution 연산을 뒤따르는 up-sampling(즉, 보간법)을 권장합니다. 주된 목적이 그런 artifacts이 없는 이미지를 생성하는 것이라면 해당 글을 읽을 가치가 있습니다 역자 : Transposed convolution 과정에서 네모 모양이 남습니다. 그걸 checkerboard artifacts라고 부릅니다 References [1] A guide to convolution arithmetic for deep learning Vincent Dumoulin, Francesco Visin https://arxiv.org/abs/1603.07285 [2] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks Alec Radford, Luke Metz, Soumith Chintala https://arxiv.org/pdf/1511.06434v2.pdf [3] Fully Convolutional Networks for Semantic Segmentation Jonathan Long, Evan Shelhamer, Trevor Darrell https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf [4] Deconvolution and Checkerboard Artifacts Augustus Odena, Vincent Dumoulin, Chris Olah https://distill.pub/2016/deconv-checkerboard/",
    "tags": "dl data",
    "url": "/data/2018/06/25/upsampling-with-transposed-convolution/"
  },{
    "title": "Apache Kafka(아파치 카프카) Intro",
    "text": "카프카, 데이터 플랫폼의 최강자를 읽고 Apache Kafka에 대해 정리한 글입니다! Kafka Intro 시대가 발전하며 점점 느슨한 결합(loosely coupled)로 이루어진 컴퓨팅 아키텍처를 사용하고 있습니다. 구체적인 예를 들면 시스템에서 사용하던 리소스가 사라질 수도 있고, Auto-Scale로 리소스가 몇십배 이상 확장될 수도 있습니다. 또한 요즘엔 서버와 클라이언트가 직접 통신하기보다 비동기 메세징 프레임워크를 기반으로 데이터를 주고받는 형태를 취하고 있습니다. 메세징 프레임워크 중 카프카에 대해 설명하겠습니다 카프카는 링크드인에서 처음 출발한 기술로, 링크드인에서 발생하는 이슈를 해결하기 위해 탄생했습니다. 카프카가 개발되기 전의 링크드인 아키텍쳐는 다음과 같습니다 기존 아키텍쳐 Monitoring : 앱/서비스에서 일어나는 미터링(사용량, 응답 시간, 에러 카운트)를 저장할 시계열 데이터 처리용 시스템 Splunk : 앱/서비스에서 발생하는 로그를 저장하고 실시간 또는 배치로 분석할 수 있도록 저장하는 시스템 Relational Database : 컨텐츠, 고객 정보 데이터를 저장하는 메인 데이터 시스템. OLTP 쿼리를 실행 Key-value store : 추천이나 장바구니와 같이 트랜잭션 처리까진 필요없지만 실시간으로 처리해줘야 하는 내용들 저장 Relational Data Warehouse : 데이터를 모아 일간/주간/월간/연간 데이터를 제공하는 데이터 마켓, 배치 분석을 하는 데이터 웨어하우스. 각종 데이터 시스템에서 이곳으로 데이터를 보냄 Hadoop : 빅데이터를 저장/처리하기 위한 하둡. ETL 작업을 통해 데이터 웨어하우스로 보냄 문제점 실시간 트랜잭션(OLTP) 처리와 비동기 처리가 동시에 이루어지지만 통합된 전송 영역의 부재로 복잡도가 증가 파이프라인 관리가 어려움. 특정 부분을 수정해야할 때, 앞단부터 다 수정해야 할 수 있음 새로운 시스템의 목적 문제점을 해결하기 위해 새로운 시스템 구축!!!! Producer와 Consumer의 분리 메시지 시스템과 같이 영구 메세지 데이터를 여러 Consumer에게 허용 높은 처리량을 위한 메세지 최적화 데이터가 증가함에 따라 스케일아웃이 가능한 시스템 새로운 아키텍쳐 기존엔 데이터 스토어 백엔드 관리와 백엔드에 따른 포맷, 별도의 앱 개발을 해야했는데 이젠 카프카에만 데이터를 전달하면 필요한 곳에서 각자 가져갈 수 있도록 변경되었습니다 카프카가 제공하는 표준 포맷으로 연결되어 데이터를 주고받는 데 부담이 없어졌습니다 카프카의 지향점 카프카의 용도 메세지 처리 사용자의 웹 사이트 활동 추적 파이프라인 애플리케이션의 통계 집계 시간순으로 발생하는 이벤트를 저장해 필요한 곳으로 보냄 카프카의 동작 방식과 원리 기본적으로 메시징 서버로 동작 메세징 시스템 Producer, publisher : 데이터 단위를 보내는 부분 Consumer, subscriber : 토픽이라는 메시지 저장소에 저장된 데이터를 가져가는 부분 중앙에 메세지 시스템 서버를 두고 메세지를 보내고(publish) 받는(subscribe) 형태의 통신을 펍/섭(Pub/Sub) 모델이라고 합니다 펍/섭(Pub/Sub) 발신자의 메세지엔 수신자가 정해져있지 않은 상태로 발행(publish) 구독(subscribe)을 신청한 수신자만 메세지를 받을 수 있음 일반적인 형태의 통신은 통신에 참여하는 개체끼리 모두 연결해야해서 확장성이 좋지 않습니다 펍섭은 확장성 Good, 데이터 유실 확률이 적음 그러나 메세징 시스템이 중간에 있어서 전달 속도가 빠르지 않음 전화기의 교환대의 원리와 살짝 유사한 느낌 Kafka 전달 속도가 느린 단점을 극복하기 위해 메세지 전달의 신뢰성 관리를 프로듀서와 컨슈머에게 넘기고, 교환기 기능을 컨슈머가 만들 수 있게 했습니다 카프카 말고도 RabbitMQ 같은 메세징 프로그램도 있습니다 카프카의 특징 Producer와 Consumer의 분리 멀티 Producer와 멀티 Consumer 디스크에 메세지 저장 일반적인 메시징 시스템들은 Consumer가 메세지를 읽어가면 큐에서 바로 메세지를 삭제합니다 카프카는 보관 주기동안 디스크에 메세지를 저장 확장성 3대의 브로커로 시작해 수십대의 브로커로 확장 가능 브로커 : 카프카 애플리케이션이 설치되어 있는 서버 무중단 확장 가능 높은 성능 내부적으로 분산 처리, 배치 처리 기법 사용 파이프라인 사례 Netflix Uber Reference 카프카, 데이터 플랫폼의 최강자 Putting Apache Kafka To Use: A Practical Guide to Building a Streaming Platform (Part 1) Apache Kafka - Cluster Architecture Evolution of the Netflix Data Pipeline uReplicator: Uber Engineering’s Robust Kafka Replicator",
    "tags": "engineering data",
    "url": "/data/2018/06/15/apache-kafka-intro/"
  },{
    "title": "Google Cloud Platform에서 Tensorflow, Jupyter Notebook 설치 및 startup script 설정",
    "text": "이 글은 Google Cloud Platform의 Compute Engine에서 Tensorflow, Pytorch, Keras, Jupyter Notebook를 설치하는 내용을 다룹니다. 그리고 인스턴스 시작할 때마다 jupyter notebook을 자동으로 켜주는 startup script을 사용하는 방법도 포함되어 있습니다. 목차 Google Cloud Platform 가입하기 Quota 요청 Instance 생성 Instance 접속 CUDA 설치 cuDNN 설치 Tensorflow, Pytorch, Keras 설치 Jupyter 및 기타 라이브러리 설치 방화벽 Port 열기 Jupyter notebook 띄우기 startup-script Google Cloud Platform 가입하기 [링크] 참고 Quota 요청 이제 막 GCP에 가입했으면 GPU Quota(할당량)가 없습니다. 별도로 신청한 후, 승인받아야 GPU를 사용할 수 있습니다 GCP 콘솔(https://console.cloud.google.com/)에서 IAM 및 관리자 - 할당량을 선택해주세요 그 후, 아래와 같이 선택해주세요 서비스 : Compute Engine API 측정항목 : NVDIA K80 GPUs, NVDIA P100 GPUs 위치 : asia-east1 이렇게 설정하면 모든 region별로 GPU 할당량을 볼 수 있습니다 (저는 예전에 신청해서 Quota가 2로 되어있습니다) K80 GPU 왼쪽 체크박스에 체크한 후, 할당량 수정을 눌러주세요 그리고 신상정보를 입력한 후, 요청 설명을 작성합니다 저는 “딥러닝을 공부하고 있고, 데이터를 활용해 훈련 모델을 만들고 싶다”라고 영어로 작성했습니다! 24시간 이내로 메일로 할당량 증가 요청이 허가되었다는 메일이 옵니다! Instance 생성 Compute Engine - VM 인스턴스 - 인스턴스 만들기 클릭 지역 : asia-east1, 영역 : asia-east1-a 머신 유형 : 사용하고 싶은 유형을 선택해주세요. 저는 cpu 8, ram 30, K80 GPU를 선택했습니다 부팅 디스크 : Ubuntu 16.04 LTS 방화벽에 HTTP/HTTPS 트래픽 허용 체크 Instance 접속 ssh를 이용해도 되지만 여기선 gcloud를 사용해보겠습니다(편한 기능이 많습니다!!) gcloud가 설치되어 있다면, 터미널에서 아래와 같은 명령어를 입력해주세요! gcloud compute --project &lt;project_id&gt; ssh --zone &lt;region&gt; &lt;instance name&gt; // 예시 gcloud compute --project \"project_101\" ssh --zone \"asia-east1-a\" \"gpu\" 접속 후 locale 설정 sudo apt-get install language-pack-ko sudo locale-gen ko_KR.UTF-8 pip3 설치 sudo apt-get update sudo apt-get install python3-pip -y CUDA 설치 curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo dpkg -i ./cuda-repo-ubuntu1604_8.0.61-1_amd64.deb sudo apt-get update sudo apt-get install cuda-9-0 nvidia-smi를 입력해 출력이 정상적으로 되는지 확인해주세요! cat /usr/local/cuda/version.txt하면 cuda의 version이 출력됩니다 cuDNN 설치 sudo sh -c 'echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /\" &gt;&gt; /etc/apt/sources.list.d/cuda.list' cat /etc/apt/sources.list.d/cuda.list &gt;&gt;&gt; deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 / &gt;&gt;&gt; deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 / sudo apt-get update sudo apt-get install libcudnn7-dev Tensorflow, Pytorch, Keras 설치 Tensorflow sudo pip3 install tensorflow-gpu Pytorch sudo pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp35-cp35m-linux_x86_64.whl sudo pip3 install torchvision Keras sudo pip3 install keras Jupyter 및 기타 라이브러리 설치 아래와 같은 라이브러리를 설치합니다! sudo pip3 install jupyter sklearn matplotlib seaborn pandas jupyter config를 생성합니다 jupyter notebook --generate-config &gt;&gt;&gt; Writing default config to: /home/~~~/.jupyter/jupyter_notebook_config.py notebook에서 사용할 비밀번호를 생성하기 위해 터미널에서 ipython 실행 from IPython.lib import passwd; passwd() 위 명령어 이후에 비밀번호를 입력합니다. 그리고 Out[1]에 나오는 해시값을 복사합니다. 이제 아까 생성된 jupyter config 파일을 수정하겠습니다 vi /home/byeon/.jupyter/jupyter_notebook_config.py // 아래와 같은 내용 추가 c = get_config() c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False c.NotebookApp.password = '&lt;해시값&gt;' 방화벽 Port 열기 메뉴 - VPC 네트워크 - 방화벽 규칙 - 방화벽 규칙 만들기 선택 네트워크 태그에 gpu를 가지고있는 인스턴스의 8888 포트만 엽니다 이제 다시 Compute Engine - 아까 생성한 인스턴스를 클릭해주세요! (인스턴스가 켜있으면 종료하신 후) 인스턴스 수정 - 네트워크 태그에 gpu를 추가해주세요! + VPC 네트워크 - 외부 IP 주소 - 임시를 고정으로 설정 Jupyter notebook 띄우기 nohup mkdir notebooks cd notebooks nohup jupyter notebook &gt; ~/notebook.log &amp; screen screen -dmS jupyter screen -r jupyter jupyter notebook // CTRL+A,D startup-script 저는 원래 screen으로 띄우는 방식을 선호했는데, 인스턴스 시작 -&gt; 인스턴스 접속 -&gt; jupyter notebook을 매번 입력하는 것이 귀찮네요. 인스턴스가 시작될 때마다 특정 스크립트가 실행되도록 하겠습니다 Service로 실행하는 방법, cron을 사용하는 방법 등 다양하게 있지만, GCP에서 제공하는 startup-script를 사용해보겠습니다 우선 VM 인스턴스를 클릭해주세요. 만약 인스턴스가 실행되고 있다면 종료를 해주세요!(GPU 머신은 종료를 해야 설정값을 수정할 수 있습니다) VM 인스턴스 세부정보에서 “수정”을 클릭해주세요 하단에 맞춤 메타데이터 아래에 다음과 같은 값을 넣어주세요! byeon을 여러분들이 사용하는 유저 이름으로 바꿔주세요 key : startup-script value : #! /bin/bash echo \"start jupyter notebook\" cd /home/byeon/workspace nohup jupyter notebook --config /home/byeon/.jupyter/jupyter_notebook_config.py --allow-root &gt;&gt; ./logs.txt &amp; startup-script는 root 권한으로 실행되기 때문에 --allow-root를 추가했고, nohup으로 노트북을 실행했습니다 저장을 눌러주시고 인스턴스를 실행해주세요! (startup-script가 root로 실행되서 이런 방식을 사용했습니다. 위에서 sudo pip3 install library를 한 이유도 root로 실행되기 때문입니다. 유저 이름으로 스크립트를 실행하는 방법을 아는 분은 댓글 부탁드립니다! 추후 이 부분은 업데이트할 예정입니다!) 만약 인스턴스 실행한 후, 노트북으로 연결이 안된다면 아래와 같은 방법으로 디버깅할 수 있습니다 인스턴스에 접속한 후, 아래와 같은 명령어 입력 sudo google_metadata_script_runner --script-type startup sudo journalctl -u google-startup-scripts.service 이제 인스턴스를 키면 자동으로 노트북을 실행해줍니다! Reference Using a GPU &amp; TensorFlow on Google Cloud Platform Set up Anaconda + IPython + Tensorflow + Julia on a Google Compute Engine VM Google Document : startup-script CUDA 설치 우분투 환경",
    "tags": "basic gcp",
    "url": "/gcp/2018/06/14/install-tensorflow-jupyter-in-gcp/"
  },{
    "title": "Apache Spark Streaming",
    "text": "Apache Spark Streaming에 대한 글입니다 Spark Streaming 다양한 소스로부터 실시간 스트리밍 데이터 처리 Spark RDD와 사용 방법이 유사하며 lambda 아키텍쳐를 만들기 좋음 Structured Streaming라는 것도 최근 추가됨 : 공식 문서 스트림 데이터를 일정 단위로 쪼개어 batch 처리 DStream (discreatized stream 불연속적 스트림) 데이터를 끊어서 연속된 RDD로 만들어 처리 데이터를 아주 짧은 주기로 처리 (ex: 1초마다 처리) SparkConf 여러가지 설정을 저장 AppName과 master 주소 StreamingContext 소스 (DStream, RDD) 생성과 스트리밍 처리 시작, 종료 등을 수행 Input DStream Input data를 표현하는 DStream, Receiver와 연동 Receiver 건별로 들어오는 데이터를 모아서 처리할 수 있도록 처리하는 친구 데이터를 받아서 Spark 메모리에 저장해놓음 DStream Operations Transformations RDD와 거의 유사 Window Operations 최근 1분(주기)의 평균을 1초마다 refresh하고 싶을 경우! Time Window 개념을 제공 window length Output Operations print foreachRDD : 자유도가 높음 saveAsTextFile, saveAsObjectFile, saveAsHadoopFile 등 Example 공식 문서 import org.apache.spark._ import org.apache.spark.streaming._ import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Create a local StreamingContext with two working thread and batch interval of 1 second. // The master requires 2 cores to prevent from a starvation scenario. val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\") val ssc = new StreamingContext(conf, Seconds(1)) // Seconds(1) : 1초 주기 // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream(\"localhost\", 9999) // Split each line into words val words = lines.flatMap(_.split(\" \")) import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation // start를 해야 시작! ssc.awaitTermination() // Wait for the computation to terminate Streaming Twitter API Twitter 가입 전화번호 인증 https://apps.twitter.com/ 에서 앱 만들기 Keys and Access Tokens 클릭 Your Access Token에서 키 생성 다음 값 챙기기 Consumer Key (API Key) Consumer Secret (API Secret) Access Token Access Token Secret build.sbt 설정 lazy val root = (project in file(\".\")). settings( inThisBuild(List( organization := \"com.example\", scalaVersion := \"2.11.7\", version := \"0.1.0-SNAPSHOT\" )), name := \"Hello\", libraryDependencies ++= List( \"org.twitter4j\" % \"twitter4j-core\" % \"4.0.6\", \"org.apache.bahir\" % \"spark-streaming-twitter_2.11\" % \"2.1.0\", \"org.apache.spark\" % \"spark-core_2.11\" % \"2.2.0\", \"org.apache.spark\" % \"spark-streaming_2.11\" % \"2.2.0\" ), retrieveManaged := true ) twitterStreaming File import org.apache.log4j.{Level, Logger} import org.apache.spark.SparkConf import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.streaming.twitter.TwitterUtils object twitterStreaming extends App{ println(\"hello\") val appName = \"spark_course\" val master = \"local[*]\" val conf = new SparkConf().setAppName(appName).setMaster(master) val ssc = new StreamingContext(conf, Seconds(10)) val rootLogger = Logger.getRootLogger() rootLogger.setLevel(Level.ERROR) val consumerKey = \"****\" val consumerSecret = \"****\" val accessToken = \"****\" val accessTokenSecret = \"****\" System.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey) System.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret) System.setProperty(\"twitter4j.oauth.accessToken\", accessToken) System.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret) val stream = TwitterUtils.createStream(ssc, None) // stream.print() // val text = stream.map(status =&gt; status.getText) // 아래처럼 리팩토링 val text = stream.map(_.getText) val hashTags = text.flatMap(_.split(\" \")).filter(_.startsWith(\"#\")) // val hashTagCounts = hashTags.map(h =&gt; (h, 1)).reduceByKey(_+_) val hashTagCounts = hashTags.map(h =&gt; (h, 1)).reduceByKeyAndWindow(_+_, Seconds(60)) // Window 사용 : 60초의 데이터를 모음 // text.print() // hashTags.print() // hashTagCounts.print() // 순서대로 정렬은 안됨..! foreachRDD를 사용하자 hashTagCounts.foreachRDD { rdd =&gt; println(\"=====\") rdd.sortBy(_._2, false).take(10).foreach(println) } ssc.start() ssc.awaitTermination() } TwitterUtils 살펴보기! TwitterInputDStream, getReceiver 함수들 확인해보기 TwitterReceiver의 onstart()할 때 store(status)도 확인해보기! Streaming같은 경우 처리를 여러 컴퓨터로 하면 날라갈 수 있음! 정확하게 1번만 처리하는 것이 어려운 이슈 실시간 처리할 때 정확도를 조금 포기하는편 그래서 정확한 수치가 필요한 것은 배치로 처리! Kafka가 Spark로 쏘고, S3로 쏘기도 함 (람다 아키텍쳐) Performance Tuning 스트리밍은 정해진 batch 주기 이내에 데이터 처리가 모두 끝나야해서 성능이 중요 병렬처리, 데이터 Serialization 방법, 메모리, GC 튜닝 등 수많은 요소를 점검하고 손봐야 합니다 Tuning Guide 및 Streaming 문서 참고 https://spark.apache.org/docs/latest/tuning.html https://spark.apache.org/docs/latest/streaming-programmingguide.html",
    "tags": "engineering data",
    "url": "/data/2018/06/12/apache-spark-streaming/"
  },{
    "title": "CS231n 13강. Generative Models",
    "text": "Stanfoard CS231n 2017 13강을 요약한 글입니다. 정리 목적이라 자세하게 작성하지 않은 부분도 있습니다. CS231n의 나머지 14강~16강은 작성하지 않을 예정입니다! Overview Unsupervised Learning Generative Models PixelRNN and PixelCNN Variational Autoencoders (VAE) Generative Adversarial Networks (GAN) Supervised vs Unsupervised Learning Supervised Learning Data : (x, y) x is data, y is label Goal : Learn a function to maxp x -&gt; y Examples : Classification, regression, object detection, semantic segmentation, image captioning, etc Unsupervised Learning Data : x just data, no labels Training data is cheap Goal : Learn som underlying hidden structure of the data Holy grail : Solve unsupervised learning =&gt; understand structure of visual world Examples : Clustering, dimensionality reduction, feature learning(autoencoders), density estimation Generative Models Train 데이터가 주어지면, 이 데이터의 분포와 동일한 새로운 samples을 생성 p_{model}(x)와 p_{data}(x)가 유사하도록 학습 Density estimation Several flavors Explicit density estimation : p_{model}(x)을 확실히 정의하고 estimation (MLE) Implicit density estimation : 확실히 정의하지 않고 p_{model}(x)의 샘플 생성 Why Generative Models? Data로부터 실제와 같은 샘플을 얻을 수 있음(artwork, super-resolution, colorization 등) time-series data의 generative model들은 simulation과 planning할 때 사용 가능(강화학습!) Generative model을 학습하는 것은 일반적인 특징을 찾을때 유용한 latent representation(잠재적인 representation을 추론할 수 있음 High dimensional prob, distribution을 추출해서 다룰 수 있음 Semi-supervised learning에서 활용될 수 있음 Taxonomy of Generative Models Generative Model의 분류 이 수업에선 3가지만 다룰 예정입니다 PixelRNN and PixelCNN Fully visible belief network Explicit density model 1-d 차원의 이미지 x의 likelihood를 decompose하기 위해 chain rule 사용 그 후, training data의 likelihood를 최대화 n-dim vector의 확률을 n개의 확률곱으로 나타냄. WaveNet 샘플 요소들을 하나하나 차례로 생성해야하기 때문에 매우 느림 pixel value들의 복잡한 분포 =&gt; Neural Network를 사용해 표현! previous pixels의 순서를 정의해야 함 PixelRNN corner부터 시작해 이미지 픽셀 생성 RNN/LSTM을 사용해 이전 픽셀에 dependency 그러나, sequential generation은 느림!! PixelCNN corner부터 시작해 이미지 픽셀 생성 CNN over context region을 사용해 이전 픽셀에 dependency Training : maximize likelihood of training images PixelRNN보단 빠름 Training 이미지의 context region values로 convolution을 병렬화 그러나 sequentially하기 때문에 여전히 느림 Generations Samples Summary Variational Autoencoders (VAE) VAE는 latent z를 가진 intractable(다루기 힘든) 밀도 함수를 정의 직접적으로 최적화할 수 없고, likelihood에 대한 lower bound(하한)으로 유도하고 최적화 background first: Autoencoders unlabeled training data로 저차원의 feature representation을 만드는 unsupervised 접근 Encoder는 계속 변하고 있음 차원 축소를 하는 이유는? 의미있는 데이터 변동 factor를 찾을 수 있는 Feature를 얻기 위해 How to learn this feature representation? original data를 재설계할 수 있는 feature를 학습 Autoencoding은 encoding 그 자체! Decoder도 Encoder처럼 계속 변하고 있음 Encoder는 supervised model을 initialize하기 위해 사용할 수 있음 그렇다면 autoencoder로 새로운 이미지를 생성할 수 있을까? =&gt; VAE Variational Autoencoders true parameters \\theta^*를 estimate하고 싶음 How should we represent this model? prior p(z)는 간단하게 선택. (ex. 가우시안) conditional p(x\\mid z)는 복잡 (이미지 생성) Neural network로 표현 How to train the model? learn model parameters to maximize likelihood of training data p_{\\theta}(x) = \\int p_{\\theta}(z)p_{\\theta}(x\\mid z)dz What is the problem with this? Intractable! Intractabiltiy decoder network 모델링에 추가하기 위해 encoder network를 정의(p_{\\theta}(z\\mid x)를 근사할) Generating Data Summary Generative Adversarial Networks (GAN) PixcelCNN은 다루기 쉬운 밀도 함수를 정의하고, training data의 likelihood를 optimize했습니다 VAEs는 대조적으로 잠재변수 z를 가진 다루기 힘든 함수를 정의합니다. z는 좋은 속성을(property) 많이 가지고 있습니다 직접 optimize하지 못하고, likelihood의 lower bound를 optimize하도록 유도합니다 명시적으로 밀도를 모델링하는 것을 포기하고, sample을 얻으면 어떨까요? GANs : 명시적인 밀도 함수를 적용하지 않고 게임 이론으로 접근합니다 2 player game의 training 분포를 사용해 생성하는 것을 학습 Problem 복잡하고 고차원의 샘플을 원함 이것을 직접적으로 할 방법이 없음 Solution 간단한 분포의 sample을 얻음(예를 들어 random noise) 간단한 분포로부터 변형을 학습 Q) What can we use to represent this complex transformation? A) Neural Network! Input : Random noise Output : Sample from training distribution Training GANs: Two-player game Generator network : 실제와 같은 이미지를 생성해 discriminator를 속입니다 Discriminator network : 실제와 가짜 이미지를 구별합니다 Generator가 진짜같은 이미지를 생성해낸다면!! Discriminator는 objective를 최대화. D(x)는 1에 가깝고 D(G(z))는 0에 가까워야 합니다 Generator는 objective를 최소화 D(G(z))는 1에 가까워야 함 generator에서 graident ascent 사용 GAN training algorithm training 후, 새 이미지를 생성하기 위해 generator 네트워크를 사용합니다 Generated samples Convolutional Architectures Gan이 더 잘 생산할 수 있도록 도와주는 아키텍쳐 이런 작업들도 진행 가능 Vector의 연산이 가능 (Word2Vec처럼) 2017: Year of the GAN 다양한 종류의 GAN이 연구되고 있습니다 The GAN Zoo : GAN 관련 논문들을 정리한 repo ganhacks : GAN들을 학습할 때 트릭과 팁을 주는 repo Summary Recap Reference Stanfoard CS231n 2017 엄태웅님 포스팅 GAN tutorial 2016 정리(1) 알기쉬운 Variational AutoEncoder What is variational autoencoder? GAN(Generative Adversarial Network) 기초",
    "tags": "cs231 data",
    "url": "/data/2018/06/11/cs231n-generative-models/"
  },{
    "title": "Apache Spark Cluster on Google Cloud Platform",
    "text": "구글 클라우드 플랫폼(Google Cloud Platform, GCP)을 사용해 Apache Spark Cluster를 띄우는 방법을 작성한 글입니다. 1) Compute Engine에서 클러스터를 띄우는 방법과 2) Dataproc을 사용하는 방법 2가지를 설명합니다. Spark 공식 문서 참고 1) Compute Engine Instance에서 직접 설치 Spark 설치 GCP 가입 : 포스팅 참고 Gcloud 로컬에 설치 : 포스팅 Compute Engine 클릭한 후, Create Instance CPU 1개, 메모리 3.75 선택, Ubuntu 18.04 gcloud compute --project \"&lt;프로젝트 ID&gt;\" ssh --zone \"&lt;지역&gt;\" \"&lt;인스턴스 이름&gt;\"로 접속 참고) 프로젝트 ID는 my-sample-project-191923 이런식으로 작성되어 있습니다 ssh localhost 입력 에러 발생시 키젠 등록 ssh-keygen cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys chmod og-wx ~/.ssh/authorized_keys Spark 설치 및 실행 wget http://mirror.navercorp.com/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz tar -xvf spark-2.3.0-bin-hadoop2.7.tgz cd spark-2.3.0-bin-hadoop2.7/ ./bin/spark-shell ./bin/spark-shell을 사용할 경우는 Spark Local Mode로 띄운 것 Java 설치(JAVA_HOME is not set라고 나오는 경우) apt-get install software-properties-common add-apt-repository ppa:webupd8team/java apt-get update apt-get install oracle-java8-installer // \"Configuring oracle-java8-installer\" --- [Ok] --- [Yes] java -version // 1.8.0 Spark Cluster Cluster Manager Driver 노드 ( Master, 8080 port, ./bin/spark-shell ) SparkContext(APP)이 구동 로컬의 Spark Shell, 제플린 Worker 노드 ( Slave, 808n port,./sbin/start-slave(s).sh ) 컴퓨터, 실제적인 일을 수행, Executor 구동 1개의 Executor가 여러 Task를 수행 1) Slave가 될 컴퓨터에 접속해서 Slave에서 설정 : ./sbin/start-slave.sh 2) Master에서 Slave에 접속해 띄우기 : ./sbin/start-slaves.sh Spark Cluster 실습 1 우선 1대의 컴퓨터에서 각각의 요소를 모두 띄워보겠습니다 단, 메모리 설정은 하지 않았음 Spark Master 실행 ./bin/spark-shell --master=&lt;스파크 마스터 주소&gt; ./sbin/start-master.sh &gt;&gt;&gt; logging to /home/byeon/spark-2.3.0-bin-hadoop2.7/logs/spark-byeon-org.apache.spark.deploy.master.Master-1-spark-cluster.out Spark Master를 실행한 후, Log를 확인해보겠습니다 vi /home/byeon/spark-2.3.0-bin-hadoop2.7/logs/spark-byeon-org.apache.spark.deploy.master.Master-1-spark-cluster.out 확인해보면, 아래와 같은 메세지를 발견할 수 있습니다 Master:54 - Starting Spark master at spark://spark-cluster.어쩌구저쩌구.internal:7077 : 스파크 마스터 주소!!! spark://spark-cluster.어쩌구저쩌구.internal:7077를 복사해주세요! 이제 master를 지정해서 실행하겠습니다 ./bin/spark-shell --master=&lt;스파크 마스터 주소&gt; sc.makeRDD(List(1,2,3)).count 마지막 sc.makeRDD에서 에러 발생 이유는 현재 Master만 켰기 때문! Cluster도 띄워야 합니다 방화벽 설정 Cluster를 띄우기 전에, 방화벽 설정을 하겠습니다 방화벽 규칙을 클릭한 후, 방화벽 규칙 만들기 ssh 터널링을 사용해서 특정 포트만 열어주는 것을 추천하지만 이번엔 실습을 위해서 전체를 열어두겠습니다(반드시 다시 설정해주세요!!!!) 소스 IP 범위 : 0.0.0.0/0 입력 프로토콜 및 포트 : 모두 허용 :8080 접속 되는지 확인! Spark Master UI Spark Master를 띄우면 8080 포트로 접속 가능 Spark Worker 실행 1) Slave가 될 컴퓨터에 접속해서 Slave에서 설정 ./sbin/start-slave.sh &lt;master 주소&gt; 2) Master에서 Slave에 접속해 띄우기 ./sbin/start-slaves.sh Conf 설정 cd &lt;스파크 메인 디렉토리&gt;/conf cp slaves.template slaves // 기본 설정은 localhost 다시 클러스터 실행 ./sbin/start-slaves.sh Spark UI에서 보면 Workers가 추가됨 다시 Master 실행 ./bin/spark-shell --master=&lt;스파크 마스터 주소&gt; Spark UI에서 보면 Running Applications가 추가됨 sc.makeRDD(List(1,2,3)).count 입력하면 작업이 수행됨 Spark Worker UI :8081 : 워커 UI 워커를 하나 더 띄우면 8082에 뜸 Spark Application :4040 Description 아래를 클릭하면 자세한 정보들이 나오며 어디가 느린지 확인할 수 있음 storage는 캐싱할 경우 나옴 Spark Cluster 실습 2 1대의 Master, 2개의 Cluster 생성 스냅샷 만들기 스냅샷 이동 스냅샷 만들기 클릭 소스 디스크 : 위에서 만든 디스크 선택 인스턴스 만들기 - 부팅디스크 - 스냅샷 선택해서 클러스터 2개 인스턴스 생성 conf 설정 vi /conf/slaves 클러스터 인스턴스의 내부 IP 입력후 Esc :q Master, Slave 모두 실행 ./sbin/start-all.sh 큰 사이즈의 데이터로 시뮬레이션 spark-shell에서 def df = spark.sparkContext.makeRDD(1 to 1000000000) val bigDf = (0 to 10).map(_ =&gt; df).reduce(_ union _) bigDf.count :4040으로 접속하면!! 클러스터 2개를 제외하고 재실행 vi &lt;spark 주소&gt;/conf/slaves // 기존 2개의 클러스터 주석처리 2개의 Worker가 죽어있음 spark-shell에서 def df = spark.sparkContext.makeRDD(1 to 1000000000) val bigDf = (0 to 10).map(_ =&gt; df).reduce(_ union _) bigDf.count :4040으로 접속하면!! 시간이 더 오래걸렸습니다 2) Dataproc 실행 Dataproc 클릭 클러스터 만들기 클릭 각종 설정한 후 만들기 생성된 클러스터 이름을 클릭 - vm 인스턴스 클릭하면 마스터와 워커가 보입니다 마스터를 ssh로 접속한 후, spark-shell 입력하면 스파크가 실행됩니다 추후 Dataproc을 사용해 데이터를 처리하는 내용을 포스팅할 예정입니다 사실 실무에서는 Dataproc을 사용합니다! 박정운님 글 읽어보기!",
    "tags": "engineering data",
    "url": "/data/2018/06/09/apache-spark-cluster/"
  },{
    "title": "Apache Spark RDD NotSerializableException",
    "text": "Apache Spark RDD NotSerializableException 오류에 대한 포스팅입니다! 예제 MySQL에 Insert하는 사례 class Mysql() { def createConnection() {} def insert(n: Int) {} } val mysql = new Mysql() mysql.createConnection val rdd = sc.makeRDD(List(1,2,3,4)) rdd.foreach { n =&gt; mysql.insert(n) } rdd.foreach는 워커에서 실행되는 작업! rdd.map(f)와 같은 경우 f라는 함수 코드가 클러스터에 전송되서 사용됩니다 따라서 f는 다른 클러스터에 전송 가능한 형태여야 사용 가능 이것이 안될 경우 NotSerializableException 발생 해법 rdd.foreachPartition { iter =&gt; val mysql = new Mysql() mysql.createConnection iter.foreach { n =&gt; mysql.insert(n) } } foreachPartition을 사용해 해결!",
    "tags": "engineering data",
    "url": "/data/2018/06/09/apache-spark-NotSerializableException/"
  },{
    "title": "Apache SparkSQL과 Dataframe",
    "text": "SparkSQL과 Dataframe에 대한 포스팅입니다! SparkSQL SQL을 원하는 이유 익숙한 언어인 SQL로 데이터를 분석하고 싶어서 맵리듀스보다 빠르고 편함 정형화된 데이터의 경우 높은 최적화를 구현 가능 Hive의 경쟁자 Spark 2.0은 Spark SQL을 위한 업데이트 더 많은 쿼리와 파일포맷 지원 강화 현재 Low-level API인 RDD와 공존, 앞으로 Dataset API쪽으로도 무게가 실릴 수도! 머신러닝은 정형화된 데이터셋을 주로 다루기 때문에 Dataframe API로 다시 쓰여짐 SparkSQL Programming Guide Spark Dataset, Dataframe API SparkSQL은 구조화된 데이터가 필요 Dataset[Row] = Dataframe Json 파일에서 스키마를 읽어 Dataframe을 만듬 RDD -&gt; DS, DF로 변환 가능 MLlib에서도 구조화된 데이터를 다루고 언어별 통일성 등의 장점을 취하기 위해 Dataframe 사용 Dataset(Dataframe) Document Functions : 들어가서 함수 Check! agg 안에 넣을 수 있는 함수들입니다 Dataset은 여러 개의 row를 가질 수 있는 가상의 리스트 Dataset to represent a DataFrame Dataframe 실습 val df = spark.read.option(\"header\", \"true\").csv(\"character-deaths.csv\") df.show(10, true) // true : 내용이 길면 ...화 되는 것! false로 하면 풀네임이 나옴 // API 문서찾을 때 Dataset을 찾아보면 됩니다!! df.count // TempView 생성 df.createOrReplaceTempView(\"game_of_throne\") %sql select * from game_of_throne %sql select Allegiances, count(1) from game_of_throne group by Allegiances // sql로 표현한 것을 df에서 진행하면 아래와 같음 df.groupBy(\"Allegiances\").count.show df.groupBy(\"Allegiances\").agg(count(\"Name\")).show df.groupBy(\"Allegiances\").agg(count(\"Name\"), collect_list(\"Name\")).show(false) df.groupBy(\"Allegiances\").agg(count($\"Name\"), sum(\"Gender\"), sum(\"Nobility\")).show(false) %sql select * from game_of_throne // %sql은 제플린에서만 지원되는 기능 // 제플린이 아니라면 spark.sql(\"SELECT * FROM game_of_throne\") // 바차트를 선택한 후, settings를 클릭해서 key와 그룹을 조절할 수 있음 // error 발생 df.filter(\"Allegiances\" == \"Baratheon\").show // ===를 사용, 앞의 $\"Allegiances\" : column name이 됨 df.filter($\"Allegiances\" === \"Baratheon\").show // Death Year 역순 정렬 df.filter($\"Allegiances\" === \"Baratheon\").orderBy($\"Death Year\".desc).show // 특정 가문의 성별 count df.filter($\"Allegiances\" === \"Baratheon\").groupBy(\"Gender\").count.show // select df.filter($\"Allegiances\" === \"Baratheon\").select($\"Name\", $\"Death Year\").show // 파일 쓰기 val df2 = df.filter($\"Allegiances\" === \"Baratheon\").select($\"Name\", $\"Death Year\") df2.write.csv(\"df2\") // 하둡에 저장하는 것을 기본으로 해서, 읽기 쓰기는 쉽지만 삭제 및 수정이 어려움 // mode(\"overwrite\")로 가능 df2.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"df2\") df2.write.mode(\"overwrite\").option(\"header\", \"true\").json(\"df2\") // colum 2개를 삭제하고 Gender2를 복사, Gender가 1이냐 아니냐로 나눔 df.drop(\"GoT\", \"CoK\").withColumn(\"Gender2\", $\"Gender\" ===1).show // 성별을 남녀로 하고싶다면 UDF를 사용해야 합니다 // UDF 정의 // SQL용 sqlContext.udf.register(\"genderString\", (gender: int =&gt; if (gender ==1) \"Male\" else \"Female\") // Dataframe용 : 2개가 나뉨.. 과도기라서 둘 다 알아야 합니다 val genderString = udf((gender: Int) =&gt; if (gender ==1) \"Male\" else \"Female\") // UDF를 사용해서 성별을 Male, Female로 나옴 df.drop(\"GoT\", \"CoK\").withColumn(\"Gender2\", genderString($\"Gender\")).show // join // https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.Dataset 참고 val character_deaths = spark.read.option(\"header\", \"true\").csv(\"/Users/byeon/Downloads/character-deaths.csv\") val character_predictions = spark.read.option(\"header\", \"true\").csv(\"/Users/byeon/Downloads/character-predictions.csv\") character_deaths.join(character_predictions, \"name\").show RDD 복습 val rdd = sc.textFile(\"/Users/byeon/Downloads/character-deaths.csv\").map(t =&gt; t.split(\",\")).map { a =&gt; (a(0), a(1), a(2)) } rdd.take(5).foreach(println) // Tuple로 관리하는 것은 조금 번거로워서 case class를 만듬 case class Character(name: String, allegiances: String, deathYear: Option[Int], isDeath: Boolean) val rdd = sc.textFile(\"/Users/byeon/Downloads/character-deaths.csv\").map(t =&gt; t.split(\",\")).filter(a =&gt; a(0) != \"Name\").map { a =&gt; val deathYear = if (a(2) !=\"\") Some(a(2).toInt) else None val isDeath = !deathYear.isEmpty Character (a(0), a(1), deathYear, isDeath) } // rdd를 사용하려면 파싱을 해줘야 함. dataframe은 부가작업이 존재하지 않음 rdd.filter(_.allegiances ==\"Lannister\").take(10).foreach(println) // RDD To Dataset Dataframe rdd.toDS rdd.toDF(\"Name\", \"Allegiances\", \"DeathYear\", \"IsDeath\") Dataset 실습 Dataframe과 RDD의 중간 느낌 case class Character2(Name: String, Allegiances: String) val ds = character_deaths.as[Character2] // character_deaths의 character2를 가지고 있는 Dataset // dataframe에선 filter 하려면 // untyped api ds.filter($\"Name\" === \"Addam Marbrand\").show // dataset에선 // typed api ds.filter(_.Name == \"Addam Marbrand\").show // 에러 발생(Character2에서 정의를 안했기 때문에) ds.filter(_.GoT == \"Addam Marbrand\").show // 이런 방식으로 사용 가능 ds.filter(ds(\"Name\") === \"Addam Marbrand\").show // join을 했을 때 어느 컬럼인지 헷갈릴 수 있음. 이 경우 사용! // Dataset to RDD ds.rdd.map { _.Name.length }.collect",
    "tags": "engineering data",
    "url": "/data/2018/06/05/apache-spark-sql-and-dataframe/"
  },{
    "title": "CS231n 12강. Visualizing and Understanding",
    "text": "Stanfoard CS231n 2017 12강을 요약한 글입니다. Question What’s really going on inside convolutional networks? How did they do the things that they do? What kinds of features are they looking for? What are all these other layers in the middle doing? What are the intermediate features looking for? How ConvNets are working? What types of things in the image they are looking for? ConvNet 안에있는 레이어에서 어떤 일이 일어나고 있을까? 블랙박스라고 불리는 딥러닝 모델을 사람이 조금 더 해석할 수 있도록 도와주는 시각화 방법들에 대해 배웁니다 First Layer : Visualize Filters Filter로 구성됩니다. AlexNet은 여러개의 Convolution Filter가 존재합니다 Get slide input image -&gt; inner product Weight of filter and the pixel of the image Weight of filter를 시각화 oriented edges를 볼 수 있음(light bar and dark bar) 다양한 각도에서 다양한 모습, 반대 색상 등 강의 초반부에 말했듯, 사람의 뇌는 oriented edges를 찾음 Intermediate layer 생각보다 흥미롭지 않음. 해석하기 힘듬 Input 이미지와 직접적으로 연결되어 있지 않음 그러나 두번째 레이어는 첫번째 레이어와 연결되어 있기 때문에 첫번째 이후의 Activation 패턴을 알 수 있음. 그러나 해석이 힘들기 때문에 다른 방법을 찾아야 합니다 Last Layer AlexNet의 경우 4096차원의 feature vector ConvNet의 마지막 layer는 어떤 일을 하고 있을까? 알아보기 위해 많은 이미지를 네트워크에 돌려보고 마지막 레이어를 시각화 단순히 생각하면 Nearest Neighbors를 떠올릴 수 있음 2강에서 배운 것처럼 test image와 pixel space에서 이웃인 것을 찾습니다(좌측) ConvNet는 Semantic content가 유사하게 나옵니다. feature space에서 이웃인 것을 찾습니다 코끼리의 좌측면이 Test image였는데 우측면이 이웃으로 나온 것은 pixel 관점에선 매우 다르지만 feature 관점에선 유사합니다! 차원 축소의 효과도 있습니다 4096 차원을 2 차원으로 압축 PCA, t-SNE도 같은 효과 t-SNE natural clustering karpathy의 CNN 코드 참고 김홍배님 Slideshare 참고 Visualizing Activation 중간 레이어를 가중치를 시각화하는 것은 해석하기 힘들었지만, Activation을 시각화하는 것은 어떤 경우 해석 가능합니다 데이터를 네트워크에 넣고 어떤 이미지에서 특정 뉴런이 최대 활성화를 가지는지 봅니다. 그러면 입력 이미지와 연결된 영역을 볼 수 있습니다 왼쪽 아래 사진은 사람의 얼굴 feature로 보임 요약 Feature Map은 계층적인 응답을 보여줍니다. 예를 들어 눈 2개 + 혀 + 코 + 귀 =&gt; 개 얼굴 각각의 Feature Map은 강하게 그룹화합니다 높은 layer(layer 5)에서 변화가 큽니다 이미지 내에서 다양한 위치의 영상을 선택할 수 있습니다 Maximally Activating Patches Visualizing intermediate feature의 다른 방법 Input image의 특정 patch를 시각화 conv5는 128 \\times 13 \\times 13, 17번 채널(총 128 중)을 선택 네트워크에 이미지를 많이 돌린 후, 선택된 채널의 value를 기록 그 후, 최대 activation에 해당하는 이미지 패치를 시각화합니다 Occlusion Experiments Occlusion : 폐쇄 input 이미지의 어떤 부분이 classification에 영향을 많이 미치는지 찾아보는 방법 cnn에 넣기 전에, 이미지의 일부를 마스크하고 마스크 위치에서 확률의 히트맵(heatmap of probability)를 그립니다 이 실험은 이미지의 특정 부분을 block하면 network score가 극단적으로 변할것이고, 그러면 특정 부분은 classification decision에 매우 중요할 것이라는 아이디어에서 나왔습니다 빨간 색은 low probability고 하얀색/노란색은 high probability입니다. go-kart 부분을 block out하면 go-kart class 확률은 꽤 내려갈 것입니다 이 작업의 목적은 성능을 향상시키는 것이 아니라 사람이 조금 더 잘 이해할 수 있도록 돕는 것입니다 Saliency Maps saliency : 중요한 이미지 픽셀에 대해 class score의 gradient를 계산하고, 절대값을 취하기 + RGB 채널을 최대로 취하는 방식(이미지를 흔든다고 표현했음) Input에서 어떤 이미지가 classification에 중요한지 파악하는 방법!(Occlusion Experiments처럼) Semantic Segmentation을 수행하기 위해 해당 작업을 하기도 함 이미지 내의 Object를 추출할 수 있어서 정말 좋음! 그러나 다루기 어렵고 효과를 보려면 엄청 많은 접근을 해야합니다. 따라서 이게 실용적인진 모르겠음 그러나 supervision과 함께 훈련된 것보다 훨씬 효과적일듯! fixed input image 위키피디아 참고 Intermediate Features via (guided) backprop 이해가 잘 안됨.. DeConv를 이용해 뉴런의 Gradient를 시각화하는 과정에서 사용하는 Back Propagation의 일종 일반적인 Back Propagation과 달리 Positive influence만 반영해 관심있는 뉴런의 Gradient를 선명하게 시각화 가능 이미지의 어떤 부분이 최고로 영향을 미치는지 확인할 수 있는 방법 fixed input image Grad-CAM: 대선주자 얼굴 위치 추적기 참고 Gradient Ascent 참고 자료 : Grad Cam을 이용한 딥러닝 모형 해석, 엄태웅님 글 일부 Input image에서 의존성을 제거합니다 Gradient Ascent를 실행해 이미지를 합성 Weight는 고정한 후, Input image의 픽셀을 변경 “하늘”이라는 초기 이미지를 주고 “새”의 속성을 갖도록 gradient ascent로 점점 변형 =&gt; 딥드림, 딥러닝 아트의 원리 Regularization Term 특정 뉴런의 값을 Maximize 그러나 자연스러운 이미지로 보이기 gradient ascent를 이미지 자체의 픽셀에 적용 Simple Regularizer : 생성된 이미지의 L2 norm을 penalize 이미지를 생성하기 시작 가우시안 블러 이미지를 추가하고, 작은 value를 가진 픽셀을 0으로 바꾸고, 작은 그라디언트를 가지는 픽셀을 0으로 바꿈 위 결과로 조금 더 나은 시각화를 보여주고 있습니다 Pixel space 대신 FC6 latent space를 최적화 궁금하면 논문을 꼭 읽어볼 것 해당 강의에서 깊게 들어가진 않겠음 Fooling Images / Adversarial Examples 이미지를 속이는 것! 임의의 이미지를 선택한 후, 다른 이미지의 점수를 최대화 코끼리의 사진에 코알라의 점수를 최대화 네트워크는 코끼리 사진을 코알라로 분류합니다 (1) Start from an arbitrary image (2) Pick an arbitrary class (3) Modify the image to maximize the class (4) Repeat until network is fooled 위와 같은 행동이 어떻게 가능한지는 이안 굿펠로우의 강의를 통해서 알아볼 예정입니다 DeepDream : Amplify existing features 참고 자료 : 텐서플로우를 이용해서 딥드림(DeepDream) 알고리즘 구현해보기 amplify : 증폭시키다 image와 layer를 선택한 후, 아래 작업을 반복 Forward 선택된 레이어에서 Activation 값과 같은 Gradient를 설정 Backward Update Image 한마디로 하면 Neural Networks의 Feature를 시각화하고 이를 Input 이미지와 결합해 환각적인 이미지를 만들어내는 알고리즘 하나의 뉴런이 아니라 레이어로 확장 코드에서 사용한 몇가지 트릭 Jitter : 이미지를 두 픽셀씩 옮김 L1 Normalize Clip pixel values Feature Inversion 이미지에 대한 feature vector가 주어지면, 다음과 같은 이미지를 찾습니다 주어진 feature vector와 일치한 이미지 자연스러운 이미지 relu2_2가 완벽히 재현되어 있는 것을 보면, 해당 레이어는 버리면 안된다는 뜻입니다 relu5_3의 시각화는 이미지의 공간 구조는 유지한 채, 점점 색상 및 질감이 달라지고 있습니다 네트워크가 깊어지며 점점 Feature를 잃어버립니다 Texture Synthesis 어떤 texture의 샘플 patch가 주어졌을 때, 같은 texture의 더 큰 이미지를 생성할 수 있을까요? 컴퓨터 그래픽에서 오래된 문제 동일한 texture를 가지는 두 이미지가 있다면, 두 이미지의 spatial statistics는 같다는 가정에서 진행 현재 픽셀 주변의 이웃을 계산한 후, 입력 이미지에서 한 픽셀을 복사 고전적인 알고리즘도 많이 있다는 것을 알려주고 싶었음. 자세히 몰라도 됨 Neural Texture Synthesis Neural Style Transfer [코드] 문제점 : many forward / backward가 필요해서 매우 느림 해결책 : 다른 neural network를 사용 =&gt; Fast Style Transfer Fast Style Transfer [코드] Summary Reference Stanfoard CS231n 2017 김홍배님 Slideshare Grad-CAM: 대선주자 얼굴 위치 추적기",
    "tags": "cs231 data",
    "url": "/data/2018/06/04/cs231n-visualizing-and-understanding/"
  },{
    "title": "Apache Zeppelin(아파치 제플린)",
    "text": "Apache Zeppelin 장점 Apache Spark와 궁합이 잘맞음 Interactive한 레포트 작성 가능 Apache Zeppelin Install 홈페이지 접속 후 다운로드 용량이 큰 것은 빅쿼리, 카산드라 등이 모두 빌드된 것이고 작은 것은 스파크만 빌드 압축 풀기 cd /folder : 압축을 푼 폴더로 들어가기 빌드 : ./bin/zeppelin.sh 빌드 후 실행 : ./bin/zeppelin-daemon.sh start 종료 : ./bin/zeppelin-daemon.sh stop localhost:8080로 접속 종료해도 죽지 않는다면(2번 이상 실행해서) ps -al로 Zepplin PID를 찾은 후, kill -9 &lt;PID&gt;로 죽이기 Apache Zeppelin 자동완성 기능 : control + . 또는 tab(0.8.0 version에서 추가됨!) 각종 설정을 변경하고 싶을 경우 /$ZEPPELIN_FOLDER/conf/에 있는 *.template 파일의 이름에서 .template를 제거한 후 설정하면 됩니다 ex) 기본 포트인 8080을 변경하고 싶은 경우 /conf/zeppelin-site.xml.template을 /conf/zeppelin-site.xml 로 수정한 후, zeppelin.server.port라고 작성된 곳의 value에 포트를 변경해주면 됩니다 Zeppelin의 Notebook 파일은 $ZEPPELIN_HOME/notebook 폴더에 저장됩니다! Jupyter Notebook과 다르게 json 파일로 저장됩니다 default라고 되어있는 버튼을 클릭하면 Report 형태(Code 숨김)로 볼 수 있습니다 우측 최상단 Anonymous를 클릭하면 하단에 메뉴가 나옵니다. 다른 부분은 직접 클릭해보면 알 수 있고, Interpreter를 눌러보겠습니다 각종 Interpreter 설정을 할 수 있는 곳입니다. 위 사진은 Spark 부분의 옵션값 페이지입니다 Tutorial Zeppelin Tutorial을 해봤습니다 이런 식으로 코드를 작성해서 사용합니다! 빠른 데이터 시각화가 가능합니다 settings를 클릭하면 더 자세한 설정 가능 bank.toDF().registerTempTable(\"bank\")를 통해 bank라는 TempTable 생성했습니다! Dynamic Form Dynamic Form은 사용자가 클릭만으로 쉽게 조작할 수 있도록 도와줍니다! 코딩을 할 줄 모르는 사람에게 유용 Select form은 ${formName=defaultValue,option1|option2...} 이런 방식으로 생성 Checkbox form은 ${checkbox:formName=defaultValue1|defaultValue2...,option1|option2...} 이런 방식으로 생성 Text input도 있습니다! Zeppelin 공식문서 참고해주세요!",
    "tags": "engineering data",
    "url": "/data/2018/06/02/apache-zeppelin/"
  },{
    "title": "Apache Spark(아파치 스파크) RDD API",
    "text": "Apache Spark RDD API(Scala)에 대한 설명 및 예제 코드입니다. 원본 글 이 글은 완성되지 않았습니다! 계속 업데이트할 예정입니다 참고 자료 : RDD Programming guide RDD Resilient Distributed Dataset의 약자 클러스터에서 구동시 여러 데이터 파티션은 여러 노드에 위치할 수 있습니다. RDD를 사용하면 모든 파티션의 데이터에 접근해 Computation 및 transformation을 수행할 수 있습니다 RDD의 일부가 손실되면 lineage information을 사용해 손실된 파티션의 데이터를 재구성할 수 있습니다 Dataframe에 더 관심이 증가하고 있는 중! 4개의 Extensions DoubleRDDFunctions 숫자값 aggregation의 다양한 방법 포함 Data가 implicitly double 타입으로 변환가능할 때 사용합니다 PairRDDFunctions Tuple 구조가 있을 때 사용합니다 OrderedRDDFunctions Tuple 구조이며 Key가 implicitly 정렬 가능할 경우 사용합니다 SequenceFiledRDDFunctions 하둡의 Sequence 파일을 만들 수 있는 방법 포함 PairRDDFunctions처럼 Tuple 구조 필요 Tuple을 쓰기 가능한 유형으로 변환할 수 있도록 추가 요구사항 존재 Working with Key-Value Pairs 공식 문서 PairRDDFunction에서 Source로 가면 만들 때 (self: RDD[(K, V)])로 되어있음!! 제네릭 타입 Transformations mapPartitions : 엔지니어링시 필요할 수도..! distinct : unique한 set repartition MySQL에 넣을 때 파티션이 너무 작게 나뉘어져 있어서 적당한 사이즈로 올려야겠다고 생각할 때 사용! RDD 파티션이 1000개라면 파일이 1000개로 생성됨. 이럴 경우에도 리파티션으로 10개로 줄임 but 비싼 연산 Actions reduce reduceByKey는 Transformation! collect : to array, 작을 때만 사용! count take(n) takeOrdered saveAsTextFile SequenceFile, ObjectFile은 사실 몰라도.. 될 듯 countByKey foreach : RDD에서 foreach(println) 하면 워커 로그에 남음! WordCount 예제 // RDD 복습 : wordCount // 제플린 실행 후, sc.textFile(\"../README.md\").take(10) // take(10) : 10줄 val text = sc.textFile(\"../README.md\") text.take(10).foreach(println) // WordCount val split = text.map(s =&gt; s.split(\" \")) split.take(10).foreach(println) // 이상하게 java.lang.String 이렇게 나옴 val split = text.map(s =&gt; s.split(\" \").mkString(\"//\")) // mkString을 활용해 잘 나오는지 봅시다 split.take(10).foreach(println) &gt;&gt;&gt; #//Apache//Zeppelin ... 처럼 나옴 // flat하게 하려고 flatMap사용 val split = text.flatMap(s =&gt; s.split(\" \")) split.take(10).foreach(println) // 이제 같은 단어끼리 묶어서 Count하면 됨 // RDD가 groupby할 수 있도록 Tuple로 만듬 split.map(w =&gt; (w, 1)).take(10).foreach(println) // reduceByKey로 슝! // reduce는 키 기반으로 합침 split.map(w =&gt; (w, 1)).reduceByKey(_+_).take(10).foreach(println) // 이해 안될까봐 위 코드를 풀어보면 split.map(w =&gt; (w, 1)).reduceByKey((a,b) =&gt; a+b).take(10).foreach(println) // 갯수가 많이 나오는 것으로 내림차순을 하고싶다면 val wordCount = split.map(w =&gt; (w, 1)).reduceByKey(_+_) wordCount.sortBy(_._2, false).take(10).foreach(println) // false : 역순정렬 API 소개 aggregate 2개의 다른 reduce 함수를 aggregate RDD 데이터 타입과 action 결과 타입이 다를 경우 사용 파티션 단위의 연산과 각 연산의 결과를 합침 def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U // seqOp: (U, T) =&gt; U : RDD 데이터 타입 T를 연산 결과 데이터 타입 U로 변경 및 연산을 수행 // 노드 로컬 파티션에서 수행 // zeroValue : 파티션에서 누적해야 될 값의 시작값 // comOp(U, U) =&gt; U : 각 파티션에서 seqOp의 작업이 끝났을 때 합치는 함수 val z = sc.parallelize(List(1,2,3,4,5,6), 2) // List를 2개로 패래럴라이즈 val z1 = sc.parallelize(List(1,2,3,4,5,6), 3) // List를 3개로 패래럴라이즈 // 함수 정의 def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = { iter.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\") } z.mapPartitionsWithIndex(myfunc).collect &gt;&gt;&gt; Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6]) z1.mapPartitionsWithIndex(myfunc).collect &gt;&gt;&gt; Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:1, val: 3], [partID:1, val: 4], [partID:2, val: 5], [partID:2, val: 6]) z.aggregate(0)(math.max(_, _), _ + _) // partition0은 1,2,3이 있음. max(0,1,2,3) =&gt; 3 // partition1은 4,5,6이 있음. max(0,4,5,6) =&gt; 6 // 0 + 3 + 6 = 9 z1.aggregate(0)(math.max(_, _), _ + _) // partition0은 max(0,1,2) =&gt; 2 // partition1은 max(0,3,4) =&gt; 4 // partition2은 max(0,5,6) =&gt; 6 // 0 + 2 + 4 + 6 = 12 z.aggregate(5)(math.max(_, _), _ + _) &gt;&gt;&gt; Int = 16 val z2 = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"),2) def myfunc(index: Int, iter: Iterator[(String)]) : Iterator[String] = { iter.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\") } z2.mapPartitionsWithIndex(myfunc).collect &gt;&gt;&gt; Array[String] = Array([partID:0, val: a], [partID:0, val: b], [partID:0, val: c], [partID:1, val: d], [partID:1, val: e], [partID:1, val: f]) z2.aggregate(\"\")(_ + _, _+_) &gt;&gt;&gt; String = abcdef z2.aggregate(\"X\")(_ + _, _+_) &gt;&gt;&gt; String = XXabcXdef val z3 = sc.parallelize(List(\"12\",\"23\",\"345\",\"4567\"),2) z3.aggregate(\"\")((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y) &gt;&gt;&gt; String = 42 z3.aggregate(\"\")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) &gt;&gt;&gt; String = 11 val z4 = sc.parallelize(List(\"12\",\"23\",\"345\",\"\"),2) z4.aggregate(\"\")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) &gt;&gt;&gt; String = 10 val z5 = sc.parallelize(List(\"12\",\"23\",\"\",\"345\"),2) z5.aggregate(\"\")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) &gt;&gt;&gt; String = 11 aggregateByKey 같은 key가 있는 값에 적용 초기 값은 두번째 reduce에 적용되지 않음 def aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)] def aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)] def aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)] val pairRDD = sc.parallelize(List((\"cat\", 2), (\"cat\", 5), (\"mouse\", 4), (\"cat\", 12), (\"dog\", 12), (\"mouse\", 2)), 2) pairRDD.collect &gt;&gt;&gt; Array[(String, Int)] = Array((cat,2), (cat,5), (mouse,4), (cat,12), (dog,12), (mouse,2)) def myfunc(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = { iter.map(x =&gt; \"[partID:\" + index + \", val: \" + x + \"]\") } pairRDD.mapPartitionsWithIndex(myfunc).collect &gt;&gt;&gt; Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)]) pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect &gt;&gt;&gt; Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6)) // partition0는 max(cat : 0, 2, 5), max(mouse : 0, 4) // partition1는 max(cat : 0, 12), max(dog : 0, 12), max(mouse : 0, 2) // cat : 5, mouse : 4 // cat : 12, dog : 12, mouse : 2 // 합침! dog : 12, cat : 17, mouse : 6 pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect &gt;&gt;&gt; Array[(String, Int)] = Array((dog,100), (cat,200), (mouse,200)) cartesian 2개의 RDD에서 cartesian product 새로운 RDD 생성 메모리 때문에 문제가 생길 수 있으므로 사용할 때 주의하기 def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] val x = sc.parallelize(List(1,2,3,4,5)) val y = sc.parallelize(List(6,7,8,9,10)) x.cartesian(y).collect &gt;&gt;&gt; Array[(Int, Int)] = Array((1,6), (1,7), (1,8), (1,9), (1,10), (2,6), (2,7), (2,8), (2,9), (2,10), (3,6), (3,7), (3,8), (3,9), (3,10), (4,6), (5,6), (4,7), (5,7), (4,8), (5,8), (4,9), (4,10), (5,9), (5,10)) checkpoint RDD가 계산될 때 체크포인트를 생성합니다. checkpoint 폴더 안에 binary file로 저장되며 Spark Context를 사용해 지정할 수 있습니다 “my_directory_name”이 모든 slave에 존재해야 합니다. 대안으로 HDFS 디렉토리 URL을 사용할 수 있습니다 def checkpoint() sc.setCheckpointDir(\"my_directory_name\") val a = sc.parallelize(1 to 4) a.checkpoint a.count coalesce, repartition 관련 데이터를 주어진 수의 파티션으로 통합합니다. repartition(numPartitions)은 coalesce(numPartitions, shuffle=true)의 약어입니다 def coalesce ( numPartitions : Int , shuffle : Boolean = false ): RDD [T] def repartition ( numPartitions : Int ): RDD [T] val y = sc.parallelize(1 to 10, 10) val z = y.coalesce(2, false) y.partitions.length &gt;&gt;&gt; 10 z.partitions.length &gt;&gt;&gt; Int = 2 cogroup [Pair], groupWith [Pair] 최대 3개의 key-value RDD를 그룹화할 수 있는 함수 def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] def cogroup[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W]))] def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W]))] def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] def groupWith[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] def groupWith[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], IterableW1], Iterable[W2]))] val a = sc.parallelize(List(1, 2, 1, 3), 1) val b = a.map((_, \"b\")) val c = a.map((_, \"c\")) b.collect &gt;&gt;&gt; Array[(Int, String)] = Array((1,b), (2,b), (1,b), (3,b)) b.cogroup(c).collect &gt;&gt;&gt; Array[(Int, (Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b),CompactBuffer(c))), (2,(CompactBuffer(b),CompactBuffer(c)))) // 문서엔 ArrayBuffer인데 CompactBuffer 차이는? // 연산이 이해가 안되는 상황 val d = a.map((_, \"d\")) b.cogroup(c, d).collect &gt;&gt;&gt; Array[(Int, (Iterable[String], Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c),CompactBuffer(d, d))), (3,(CompactBuffer(b),CompactBuffer(c),CompactBuffer(d))), (2,(CompactBuffer(b),CompactBuffer(c),CompactBuffer(d)))) val x = sc.parallelize(List((1, \"apple\"), (2, \"banana\"), (3, \"orange\"), (4, \"kiwi\")), 2) val y = sc.parallelize(List((5, \"computer\"), (1, \"laptop\"), (1, \"desktop\"), (4, \"iPad\")), 2) x.cogroup(y).collect &gt;&gt;&gt; Array[(Int, (Iterable[String], Iterable[String]))] = Array((4,(CompactBuffer(kiwi),CompactBuffer(iPad))), (2,(CompactBuffer(banana),CompactBuffer())), (1,(CompactBuffer(apple),CompactBuffer(laptop, desktop))), (3,(CompactBu // 유저별 이벤트 count할 때 사용할 수 있을듯? collect, toArray RDD를 Scala array로 변환 RDD의 값을 보고싶을 경우 사용 def collect(): Array[T] def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] def toArray(): Array[T] val c = sc.parallelize(List(\"Gnu\", \"Cat\", \"Rat\", \"Dog\", \"Gnu\", \"Rat\"), 2) c.collect &gt;&gt;&gt; Array[String] = Array(Gnu, Cat, Rat, Dog, Gnu, Rat) collectAsMap[Pair] collect와 유사하나 key-value RDD를 생성하고 이를 Scala Map으로 변환 def collectAsMap(): Map[K, V] val a = sc.parallelize(List(1, 2, 1, 3), 1) val b = a.zip(a) b.collectAsMap &gt;&gt;&gt; scala.collection.Map[Int,Int] = Map(2 -&gt; 2, 1 -&gt; 1, 3 -&gt; 3) combineByKey[Pair] Tuple로 구성된 RDD의 값을 결합하는 함수 def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)] def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)] def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializerClass: String = null): RDD[(K, C)] val a = sc.parallelize(List(\"dog\",\"cat\",\"gnu\",\"salmon\",\"rabbit\",\"turkey\",\"wolf\",\"bear\",\"bee\"), 3) val b = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3) val c = b.zip(a) c.collect &gt;&gt;&gt; Array[(Int, String)] = Array((1,dog), (1,cat), (2,gnu), (2,salmon), (2,rabbit), (1,turkey), (2,wolf), (2,bear), (2,bee)) val d = c.combineByKey(List(_), (x:List[String], y:String) =&gt; y :: x, (x:List[String], y:List[String]) =&gt; x ::: y) d.collect &gt;&gt;&gt; Array[(Int, List[String])] = Array((1,List(cat, dog, turkey)), (2,List(gnu, rabbit, salmon, bee, bear, wolf))) compute RDD 연산을 실행 사용자가 직접 실행해서는 안됨 def compute(split: Partition, context: TaskContext): Iterator[T] context, sparkContext RDD를 생성하기 위해 사용된 SparkContext를 리턴 SparkContext Spark를 동작시키는 모체 Spark의 실행 환경을 handling SC간에 RDD는 공유되지 않음 def compute(split: Partition, context: TaskContext): Iterator[T] val c = sc.parallelize(List(\"Gnu\", \"Cat\", \"Rat\", \"Dog\"), 2) c.context &gt;&gt;&gt; org.apache.spark.SparkContext = org.apache.spark.SparkContext@7c226095 count RDD에 저장된 item의 수를 return def count(): Long val c = sc.parallelize(List(\"Gnu\", \"Cat\", \"Rat\", \"Dog\"), 2) c.count &gt;&gt;&gt; Long = 4 countApproxDistinct 대략적인 고유값 수를 count합니다 큰 RDD의 경우 빠르게 실행 가능 relativeSD 파라미터가 정확도를 컨트롤합니다 def countApproxDistinct(relativeSD: Double = 0.05): Long val a = sc.parallelize(1 to 10000, 20) val b = a++a++a++a++a b.countApproxDistinct(0.1) &gt;&gt;&gt; Long = 8224 b.countApproxDistinct(0.05) &gt;&gt;&gt; Long = 9750 b.countApproxDistinct(0.01) &gt;&gt;&gt; Long = 9947 b.countApproxDistinct(0.001) &gt;&gt;&gt; Long = 10000 countApproxDistinctByKey [Pair] countApproxDistinct와 유사 고유한 Key에 대해 value의 대략적인 수를 count Tuple def countApproxDistinctByKey(relativeSD: Double = 0.05): RDD[(K, Long)] def countApproxDistinctByKey(relativeSD: Double, numPartitions: Int): RDD[(K, Long)] def countApproxDistinctByKey(relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] val a = sc.parallelize(List(\"Gnu\", \"Cat\", \"Rat\", \"Dog\"), 2) val b = sc.parallelize(a.takeSample(true, 10000, 0), 20) val c = sc.parallelize(1 to b.count().toInt, 20) val d = b.zip(c) d.countApproxDistinctByKey(0.1).collect &gt;&gt;&gt; Array[(String, Long)] = Array((Rat,2567), (Cat,3357), (Dog,2414), (Gnu,2494)) d.countApproxDistinctByKey(0.01).collect &gt;&gt;&gt; Array[(String, Long)] = Array((Rat,2555), (Cat,2455), (Dog,2425), (Gnu,2513)) d.countApproxDistinctByKey(0.001).collect &gt;&gt;&gt; Array[(String, Long)] = Array((Rat,2562), (Cat,2464), (Dog,2451), (Gnu,2521)) countByKey [pair] count와 유사하지만 개별 키에 대해 RDD의 value를 count(?) 이해가 잘 안됨. Map을 이해 덜해서 그런가? def countByKey(): Map[K, Long] val c = sc.parallelize(List((3, \"Gnu\"), (3, \"Yak\"), (5, \"Mouse\"), (3, \"Dog\")), 2) c.countByKey &gt;&gt;&gt; scala.collection.Map[Int,Long] = Map(3 -&gt; 3, 5 -&gt; 1)",
    "tags": "engineering data",
    "url": "/data/2018/05/31/apache-spark-rdd-api/"
  },{
    "title": "CS231n 11강. Detection and Segmentation",
    "text": "Stanfoard CS231n 2017 11강을 요약한 글입니다. Object Detection, Segmentation, Localization, Classification 등의 개념에 대해 나옵니다 Computer Vision Tasks Semantic Segmentation No objects, just pixels Input : Image Output : decision of a category for every pixel(픽셀별로 어떤 카테고리에 속하는지 알려줌) 픽셀이 어떤 것을 나타내는지 알려주지만, 개별에 대해선 분류할 수 없음(2개 이상의 물체를 같은 것으로 인식) 추후 instance segmentation에서 이 문제를 해결할 예정입니다 Semantic Segmentation은 classification을 통해 진행될 수 있습니다. Sliding Window Approach 잘려진 patch마다 어떤 class인지 유추 이 방법은 computation이 비싼 방법이고 중복되는 patch 사이에서 공유된 feature를 재사용하지 않습니다 Fully Convolutional 3x3 filter를 사용해 이미지 크기를 유지하며 convolution에 넣음 한번에 모든 픽셀을 예측할 수 있도록 설계 Output : C \\times H \\times W의 Tensor 그러나 원본 이미지를 그대로 convolution하는 것은 비싼 연산 Fully Convolutional with downsampling and upsampling max pooling 또는 strided convolution을 통해 downsampling unpooling을 통해 upsampling Upsampling 1) Unpooling pooling의 반대 작업으로, unpooling 지역의 receptive field의 값을 복제 we duplicate receptive filed of unpooling region 하지만 우측처럼 나머지 값이 0일 경우는 Bed of Nails(무척 괴롭다) max pooling을 할 때 max인 값을 기억했다가 Max Unpooling할 때 사용. 해당 위치 말고는 모두 0으로 채워넣음 fixed function 2) Transpose Convolution 딥러닝에서 사용되는 여러 유형의 Convolution 소개 참고 Up-sampling with Transposed Convolution 이 글도 좋습니다 Convolution Operation은 input values와 output values 사이에 공간 연결성을 가지고 있습니다. 3x3 kernel을 사용한다면, 9개의 values가(kernel) 1개의 value(output, 문지른 후의 결과물)와 연결됩니다. 따라서 many to one 관계라고 볼 수 있습니다 Transposed Convolution은 1개의 value를 9개의 values로 변경합니다. 이것은 one to many 관계라고 볼 수 있습니다 필터의 값 사이에 0을 넣은 매트릭스를 Transpose한 후, 곱해줍니다 Others names Deconvolution (bad) Upconvolution Fractionally strided convolution Backward strided convolution Classification + Localization Single Object what the category is, where is that object in the image? Output : Bounding Box(around the region of label), label localization은 regression 문제! Multi Task Loss를 계산 처음부터 학습하기 어려울 수 있으니, ImageNet의 pretrain 모델을 사용하기도 합니다(Transfer Learning) Aside: Human Pose Estimation Pose Estimation에도 활용할 수 있음. Image를 넣으면 14개의 join position이 나옴 Object Detection Computer Vision의 핵심 Task Multiple Object Output : Bounding Box(around the region of label), label 딥러닝을 활용한 이후부터 점점 성능이 좋아지고 있음 Localization과의 차이점은 동일한 종류의 물체가 여러 개 있다면 Object Detection은 모두 잡음(Localization은 1개로 취급) Sliding Window Approach 다른 모양의 crop에서 물체인지 배경인지 분류 거대하고 많은 crop이 필요한데, 이게 비싼 연산 Region Proposals Selective Search 방법으로 물체가 있을만한 Region을 1000~2000개 생성 CPU에서 연산 R-CNN 문제점 Fast R-CNN ConvNet을 통해 나온 feature map에서 RoIs를 찾음 RoI Pooling Fully Connected Layer는 Fixed size input이 필요한데, RoI Pooling이 작업을 수행 Fast R-CNN은 여전히 Bottleneck을 보유 Faster R-CNN RPN(Region Proposal Network)을 추가해서 성능 개선 Output : Object 유무, Object Proposal Faster R-CNN 논문 리뷰 참고 Detection without Proposals: YOLO / SSD 해당 논문을 꼭 보는 것이 좋을듯! CS231 설명은 너무 빈약 박진우님 YOLO 분석 Aside: Object Detection + Captioning = Dense Captioning region에 대한 예측보다 region에 대한 캡션을 작성하려고 함 Faster R-CNN처럼 보임(region proposal stage, bounding box, processing per region) 그러나 실제론 각각의 region에 대한 캡션을 예측하는 RNN 모델 특정 문제로부터 배운 것을 다른 Task에 적용할 수 있음(Multi Task) Instance Segmentation Object Detection에서 Bounding Box 대신 Segmentation Mask R-CNN Reference Stanfoard CS231n 2017 이진원님의 PR-012: Faster R-CNN 딥러닝에서 사용되는 여러 유형의 Convolution 소개 Up-sampling with Transposed Convolution Faster R-CNN 논문 리뷰",
    "tags": "cs231 data",
    "url": "/data/2018/05/30/cs231n-detection-and-segmentation/"
  },{
    "title": "Apache Spark(아파치 스파크) Intro",
    "text": "아파치 스파크에 대한 입문 내용입니다 (주로 RDD) Apache Spark RDD, Dataframe 2개의 개념을 알아야 함 RDD(Resilient Distributed Dataset) 탄력적이며 분산된 데이터셋 오류 자동복구 기능이 포함된 가상의 리스트 클러스터에서 데이터를 처리 다양한 계산(map, reduce, count, filter, join )을 수행할 수 있으며, 메모리에 저장 작업을 병렬적으로 처리 여러 작업을 설정한 후, 결과를 얻을 때 lazy하게 계산 Lineage : 클러스터 중 일부 고장으로 작업이 실패해도 리니지를 통해 데이터를 복구 하둡 맵리듀스는 각종의 task가 있고, 실패하면 개별 task를 다시 실행! 스파크는 계산하는 과정에서 뻑나면 처음부터 다시 실행 데이터 IO는 비싸고 Computation은 싸기 때문에 스파크가 유리 리스트와 차이점 : 리스트는 1부터 100억을 저장하려고 하면 Memory Error가 발생. 컴퓨터 한대에서 처리할 수 있는만큼만 처리 가능. 반면 RDD는 만들어도 데이터를 실제로 메모리에 올리진 않고, 정보만 가지고 있음. 실제로 count, max, min 같은 operator가 실행될 때 진행 RDD Operation 1) Transformations : RDD의 데이터를 다른 형태로 변환 실제로 데이터가 변환되는 것이 아닌, 어떻게 바꾸는지에 대한 정보를 기록 실제 변환은 Action이 수행되는 시점에서 진행 map, filter, flatMap, mapPartitions, sample, union, intersection, distinct, groupByKey, reduceByKey, join, repartition 2) Actions : Transformations이 담긴 RDD의 정보를 계산 reduce, collect, count, first, take, saveAsTextFile, countByKey, foreach 등 Spark Programming Guide 참고 // word count val file = spark.textFile(\"hdfs://...\") val counts = file.flatMap(line =&gt; line.split(\" \")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) counts.saveAsTextFile(\"hdfs://...\") Apache Spark 확장 프로그램 GraphX를 제외하고 많이 쓰임 Spark SQL : SQL로 데이터 분석 Spark Streaming : 실시간 분석 MLlib : 머신러닝 라이브러리 GraphX : 그래프 분석 Apache Spark Install 홈페이지 접속 Download Spark: spark-2.3.0-bin-hadoop2.7.tgz 클릭한 후, http://mirror.apache-kr.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz 클릭 version명은 다를 수 있음 압축 풀기 : tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz 폴더 구조 bin : spark-shell, spark-submit 등 spark 를 실행해 볼 수 있는 실행 파일을 포함 sbin : spark process를 구동(start-all.sh)하는 파일 포함 conf : spark 설정 파일 포함 spark-env.sh spark-default.properties log4j.propreties example : spark 예제 실행 ./bin/spark-shell 로그가 너무 많다면 log4j.properties.template 파일을 복사해서 설정하면 됨 log4j.rootCategory=WARN, console sc.version 입력하고 버전이 출력되면 성공! Python용 PySpark는 pip3 install pyspark로 설치 RDD Caching 반복 계산의 성능 향상을 위해 RDD의 내용을 캐시 가능 Caching 했는데 왜 더 오래걸리지? 이런 경우가 있는데 이건 Serialize 관련 문제일 수도 있음 API 옵션 rdd.persist() or rdd.cache() rdd.unpersist() rdd.persist(MEMORY_ONLY) Persistence Level MEMORY_ONLY_SER 옵션을 반드시 Check 전송/저장할 때는 object 그대로 저장하면 바이트 상태로 저장되기 때문에 사람이 알아볼 수 있는 형태로 포맷을 저장해서 전송할 필요가 있습니다. 이런 작업을 Serialized한다고 표현 object를 serialize할 경우 json, binary 등으로 가능! serialize/deserialize 작업은 꽤 비싼 작업이기 때문에 옵션이 세세하게 있습니다 자바 직렬화, 그것이 알고싶다. 훑어보기편 참고 RDD 내부 4가지 파트 1) Partition : 데이터를 나누는 단위 2) Dependency : RDD의 파티션이 어디에서 파생되었는지를 기술하는 모델 3) Function : 부모 RDD에서 어떻게 파생되었는지를 계산하는 함수 4) Metadata : 데이터 위치, 파티션 정보 보유 Narrow &amp; Wide Dependency map, filter 등을 할 때 RDD의 종속 관계(화살표 관계)를 Dependency라고 함 Narrow Dependency 효율적 하나의 파티션이 하나의 파티션에서만 사용 map같은 연산은 데이터를 교환할 필요가 없음. 그냥 슝슝 co-partition이 되어있는 join은 빠름(파티션을 나눔) Wide Dependency 비효율적 하나의 파티션이 여러 파티션에서 쓰임 groupByKey나 join하면 서로를 참고해야 함. 따라서 1의 데이터가 2로 전송되어야 하고, 2의 데이터가 1로 되어야 함 네트워크 속도 및 메모리 문제가 생길 수 있음 디버깅 및 튜닝 rdd.dependencies(), rdd.toDebugString() 등을 활용 Narrow dependency OneToOneDependency PruneDependency RangeDependency Wide dependency ShuffleDependency Apache Spark 실습 sc.을 한 후, tab을 누르면 명령어들이 나옴 sc.makeRDD val a = sc.makeRDD(List(1,2,3,4,5)) &gt;&gt;&gt; org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:2 a.count &gt;&gt;&gt; LONG = 5 a.max &gt;&gt;&gt; Int = 5 a.min &gt;&gt;&gt; Int = 1 list와의 차이, lazy val b = List(1, 2,3, 4, 5) b &gt;&gt;&gt; List[Int] = List(1, 2, 3, 4, 5) // b는 list라 값이 바로 나옴 a.collect &gt;&gt;&gt; Array[Int] = Array(1, 2, 3, 4, 5) a.filter(_ &lt; 3) &gt;&gt;&gt; org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at filter at &lt;console&gt;:26 // 다른 RDD가 나온 것! 결과를 실제로 보고 싶다면 collect 같은 action을 취해야 함 // filter는 transformation a.filter(_ &lt; 3).collect &gt;&gt;&gt; Array[Int] = Array(1, 2) val c = sc.makeRDD(0 to 1000000000) // c를 생성할 때는 거의 시간이 걸리지 않음. 메타 정보만 가지고 있음 c.count &gt;&gt;&gt; Long = 1000000001 // 시간이 오래 걸림. 실제 연산 Narrow &amp; Wide Dependency // Narrow Dependency val a = sc.makeRDD(1 to 1000000).map(x =&gt; (x, x)) a.filter(_._2 &lt; 100).count // Wide Dependency val a = sc.makeRDD(1 to 1000000).map(x =&gt; (x, x)) val b = sc.makeRDD(1 to 10000).map(x =&gt; (x, x)) a.join(b).count // Join with partition (Narrow Dependency) val p = new org.apache.spark.Partitioner() { def getPartition(key: Any) = key.asInstanceOf[Int] % 10 def numPartitions = 10 } val a = sc.makeRDD(1 to 1000000).map(x =&gt; (x, x)).partitionBy(p) val b = sc.makeRDD(1 to 10000).map(x =&gt; (x, x)).partitionBy(p) a.join(b).count Caching val a = sc.makeRDD(1 to 20000000) a.count // 꽤나 시간이 걸림 val b = a.filter(_ &lt; 100000) b.count // 하나의 연산을 더 하기 때문에 더 오래걸림 b.count // 같은 연산을 또 하는거라 위에서 걸린 시간과 동일하게 걸림 // 반복 작업을 캐싱하면 사용 시간을 극적으로 줄일 수 있음 b.persist // b.cache도 동일한 명령어 // persist도 lazy하게 진행 b.count // 이번엔 메모리에 계산 결과를 올려둠. 처음보다 살짝 더 걸릴 수 있음(메모리에 올리는 시간) b.count // 캐시되어 있기에 바로 나옴 b.unpersist() // b는 메모리에서 내려감 // 계산을 한번만 하는거면 사실 캐시를 사용할 필요는 없음 Spark shell 우리가 사용하는 쉘은 Master-Slave가 단일로 되어있는 상태 클러스터를 제대로 구성하려면 할 것들이 더 많음 Spark Cluster 하둡에서 사용하는 Master-Slave 구조 Master 서버 Driver program Slave (Worker) Executor(들) Task 수행 데이터들을 캐싱 Lineage &amp; Fault Tolerance Lineage 데이터가 어디에서 왔는지에 대한 족보 문제가 생긴 데이터를 처음부터 다시 재구성해서 장애 복구 join 다음에 망가지면 계산을 다시해야 함. 그 이전 연산이 비싸다면 checkpoint나 파일로 저장해둘 수 있음",
    "tags": "engineering data",
    "url": "/data/2018/05/29/apache-spark-intro/"
  },{
    "title": "CS231n 10강. Recurrent Neural Networks",
    "text": "Stanfoard CS231n 2017 10강을 요약한 글입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) RNN과 LSTM을 이해해보자! 글과 Pytorch를 활용한 RNN 글을 함께 보면 좋을 것 같습니다! Recurrent Neural Networks Intro Process Sequences RNN : handling variable sized sequence data that allow us to pretty naturally capture all of these different types of setups in our models 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조며, 필요에 따라 다양하고 유연하게 구조를 만들 수 있음 히든 노드가 방향을 가진 엣지로 연결되어 순환 구조를 이루는 인공신경망의 한 종류 fixed size input과 fixed size output을 가지는 몇 문제점이 있지만, RNN은 그래도 유용합니다 Sequential Processing of Non-Sequence Data Classify images by taking a series of “glimpses” glimpses : 언뜻 보다, 주어진 이미지의 주변부를 보고 판단 Generate images one piece at a time! RNN Notice : function과 parameter들은 모든 time step마다 동일하게 사용됩니다 W_{hh}는 previous hidden state에서 나오는 Weight Matrix 활성 함수로 tanh를 사용한 이유는? Tensorflow KR 게시글 왼쪽부터 순차적으로 진행 y_{t} : class scores L_{t} : loss L : sum of individual losses final hidden state는 전체 sequence의 모든 context를 요약한 정보를 가지고 있습니다 Seq2Seq Model Machine Translation Many to one과 one to Many의 조합 Encoder + Decoder Example : Character-level Language Model 텐서플로우(TensorFlow)를 이용해서 글자 생성(Text Generation) 해보기 코드 참고 Language model 단어 시퀀스에 대한 확률 분포 m개의 단어가 주어졌을 때 m개의 단어 시퀀스가 나타날 확률, P(w_1, w_2,...,w_m)을 할당 언어모델(Language Model) 참고 Train 과정 Test 모든 Timestep에서 다음에 올 단어를 예측하고 오차를 Cross Entropy로 구함 RNN은 매우 느리고 converge되지 않습니다. 많은 memory를 사용하는 것이 RNN의 단점 느린 속도를 극복하기 위해 배치별로 나눠서 진행 karpathy Code Image Captioning caption : variably length sequence CNN에서 나온 것을 hidden layer의 초기값으로 설정 Image Captioning with Attention 이미지의 특정 부분에 기반해 단어를 유추했다고 보여줌 Attention, RNN, LSTM에 대한 내용은 CS231에선 많이 다루고 있지 않습니다. CS224를 참고하면 좋습니다 Visual Question Answering RNN과 Computer Vision의 조합은 Cool RNN에선 2~3개의 layer를 쌓는 것이 일반적입니다 Vainilla RNN Gradient Flow Vanilla RNN의 형태는 Exploding gradients/Vanishing gradients 문제를 가지고 있습니다 Exploding gradients의 경우 Gradient clipping을 사용 Gradient의 L2 norm이 기준 값을 초과할 때 (threshold/L2 norm)을 곱해주는 것 Long Short Term Memory (LSTM) forget gate가 매번 달라지기 때문에 vanishing gradient 문제가 발생하지 않음 Other RNN Varinats Summary Reference Stanfoard CS231n 2017 RNN과 LSTM을 이해해보자! 언어모델(Language Model) 텐서플로우(TensorFlow)를 이용해서 글자 생성(Text Generation) 해보기",
    "tags": "cs231 data",
    "url": "/data/2018/05/27/cs231n-rnn/"
  },{
    "title": "대용량 데이터 처리 기술(GFS, HDFS, MapReduce, Spark)",
    "text": "대용량 데이터 처리 기술에 대해 작성한 글입니다 실제 대용량 데이터 처리하는 방법이 궁금하신 분은 BigQuery와 Datalab을 사용해 데이터 분석하기를 참고하시면 좋을 것 같습니다 빅데이터 : 기존 데이터베이스 관리도구의 능력을 넘어서는 대량 의 정형 또는 심지어 데이터베이스 형태가 아닌 비정형의 데이터 집합조차 포함한 데이터로부터 가치를 추출하고 결과를 분석하는 기술(by 위키피디아) 기존에 데이터가 커졌을 때 사용하던 방식 큐잉 샤딩 : 데이터의 키를 Hash해서 여러 DB로 분산 but 시스템의 복잡도가 증가되며 유지보수 힘듬 이를 극복하기 위해 스스로 데이터를 분산시키고 오류가 발생하면 데이터를 복구하는 기능을 가진 시스템이 생김 GFS (Google File System) Google File System Paper 하둡의 원조 Failuer Tolerance, 물리적으로 서버 중 하나가 고장나도 정지하지 않고 잘 돌아가는 시스템 박준영님 블로그 위키피디아 MapReduce Google MapReduce Paper Map과 Reduce 함수를 조합해 분산 환경에서 다양한 계산 Shuffle (SQL의 GroupBy와 유사한 연산) Worker에서 Map 작업 수행한 후, Reduce를 수행한 워커가 결과물을 저장 Hadoop GFS와 MapReduce 논문을 보고 구현해서 오픈소스로 공개(2006년) name node가 데이터의 위치를 알려주고(Master 역할), data note가 실제 데이터를 조회(Slave) Hadoop HDFS HDFS paper GFS 논문의 오픈소스화 Block 단위로(64MB) 여러 노드에(3 Replica) 파일 보관 늘릴수록 디스크 용량을 많이 차지하고, 안정성을 가질 수 있음 여러 옵션 보유 중복 저장과 장애복구 기능을 가지고 있음 그러나 현실에선 많은 장애.. Hadoop Hive SQL로 분석 쿼리를 실행하면 MapReduce 코드로 자동으로 변환 SQL이 불가능한 Task 존재 분산 데이터베이스 (NoSQL) Not Only SQL 스케일 가능한 고성능 데이터베이스 HBase, Cassandra, MongoDB, Couchbase, Redis Apache HBase 구글의 BigTable 구현 빠른 속도 / 편리한 사용성 / 높은 내구성 컬럼 기반 데이터베이스 (Column Oriented) SQL은 Row 기반! 컬럼을 정해두고 Row를 추가하는 방식(=자유롭지 않음)인 반면 컬럼 기반은 스키마가 자유롭고 Sparse한 거대한 테이블에 적합 Master와 Slave가 존재 Apache Spark 메모리에서 처리해서 효율이 좋음 반복 계산이 많은 경우 특히 성능이 좋음 스트리밍 기술들 Apache Storm Spark Streaming 작은 Time window로 배치 프로세스를 계속 실행해 실시간처럼 보임 리소스 관리 플랫폼 이미 존재하는 컴퓨팅 자원을 share해서 필요에 맞게 리소스를 관리해주는 플랫폼 Yarn(자주 쓰임), Mesos 데이터 수집기 여러 로그를 취합해서 로그를 남김 크롤링할 경우, 소셜 데이터를 모을 경우에도 유사한 구조 Flume, Kafka, Logstash, 아마존 Kinesis 등 Flume 단순한 구조, 몇가지 설정만으로 구동 가능 push 방식 collector를 제공 Kafka 고성능 메세징 큐 다양한 입력과 출력을 복잡하게 라우팅 pub/sub 방식 메세지를 디스크에 저장해 유실없이 처리 가능 Flume으로 collect하고 Kafka에서 불러오는 구조도 있음 기타 Presto(추후 Check) Facebook이 리드 HIVE와 유사하지만 압도적 성능 다른 DB의 데이터를 가지고와서 Join해서 사용 가능 Elastic Search 스케일 가능한 검색, 인덱스 엔진 ELK Stack으로 유명 Architecture 데이터 수집 - 데이터 저장 - 데이터 가공 - 시각화 - 머신러닝 Kafka - HDFS - Spark/Spark SQL - Zeppelin - Spark MLLib Reference 빅데이터를 위한 플랫폼들",
    "tags": "engineering data",
    "url": "/data/2018/05/26/bigdata-preprocessing-skills/"
  },{
    "title": "CS231n 9강. CNN Architectures",
    "text": "Stanfoard CS231n 2017 9강을 요약한 글입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) Today Case Study AlexNet VGG GoogLeNet ResNet Also NiN (Network in Network) DenseNet Wide ResNet FractalNet ResNeXT SqueezeNet Stochastic Depth LeNet ConvNet의 최초 도입 우편 번호(zip code), 숫자(digit)에 사용 AlexNet First large scale ConvNet ILSVR’12 winner Input : 227x227x3 images First layer(CONV1) 96 11x11 filters applied at stride 4 output size? 96 55x55 total number of parameters? (11x11x3)x96 = 35K Second layer(POOL1) 3x3 filters applied at stride 2 output size? 96 27x27 total number of paramters? no parameter parameters are the weights that we’re trying to learn. and so convolutional layers have weights that we learn but pooling all we do is have a rule, we look at the pooling region, we take max. so there’s no parameters that are learned CONV/FC는 parameter가 있고 RELU/POOL 등은 parameter가 없음 Details first use of ReLU used Norm layers (not common anymore) heavy data augmentation (flipping, jittering, cropping, color normalization …) dropout 0.5 batch size 128 SGD Momentum 0.9 Learning rate : 1e-2, reduced by 10 manually when val accuracy plateaus L2 weight decay : 5e-4 7 CNN ensemble: 18.2% -&gt; 15.4% 앞 부분 ConvNet이 2개로 나뉜 이유 GTX 580으로 학습해서 2개의 GPU를 사용했었음 ImageNet winners ZFNet Improved hyperparameters VGGNet much deeper networks, much smaller filters 레이어의 수를 16~19개까지 늘림(기존 AlexNet은 8개) 3x3 CONV stride 1, pad 1 2x2 MAX POOL stride 2 Q1) Why use smaller filters? (3x3 conv) A1) 3x3 conv(stride 1) layer는 7x7 conv layer와 같은 effective receptive field를 가짐 Q2) What is the effective receptive field of three 3x3 conv (stride 1) layers? 작은 필터를 사용하면 파라미터수를 줄일 수가 있고, 여러번 겹쳐서 사용하면 더 큰 필터가 표현하는 영역(Receptive Field)을 표현할 수 있게 됩니다 3x3 2번을 겹치면 5x5 영역에 대한 특징을 뽑을 수 있고, 3번 겹쳐서 사용하면 7x7 영역에 대한 특징을 뽑아낼 수 있음 3x(3x3xC(인풋의 채널 수)) = 27C 7x7xC=49C 파라미터 수는 더 줄이고 망은 더 깊어지는 효과(More nonlinearity) Details ILSVRC’14 2nd in classification, 1st in localization localization : 어디에 물체가 있는지(Bounding Box) + Classification Similar training procedure as Krizhevsky 2012 No Local Response Normalisation (LRN) Use VGG16 or VGG19 (VGG19 only slightly better, more memory) Use ensembles for best results FC7 features generalize well to other tasks Featrue Representation! depth의 2가지 depth : width x height x depth할 때의 depth depth : total number of layers GoogLeNet much deeper networks with computational efficiency (22 layers) 효율적인 Inception Module No FC layers 5 million parameters(12x less than AlexNet) Inception Module good local network typology(network within a network), stack each other 여러 filter 연산을 parallel하게 진행한 후 concat(depth wise) Q) What is the problem with this? expensive compute Solution : “bottleneck” layers that use 1x1 convolutions to reduce feature depth 1x1 conv로 depth를 줄임 Google Inception Model 참고. v1이 GoogLeNet. v4까지 정리되어 있습니다 ResNet very deep networks using residual connections 152 layers classification / detetection What happens when we continue stacking deeper layers on a “plain” convolutional neural network? 가설 : problem은 optimization 문제! deeper model이 optimize되긴 어려움 Deep한 모델이 shallower한 모델보다 성능이 좋아야 함 A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping. Residual : 이전 몇 단계 전 레이어의 결과를 현재 레이어의 결과와 합쳐 내보내는 것 Full ResNet architecture bottleneck layer 효율성 증대를 위해 사용 GoogLeNet과 유사 Training ResNet in practice Batch Normalization after every CONV layer Xavier/2 initialization from He et al. SGD + Momentum (0.9) Learning rate: 0.1, divided by 10 when validation error plateaus Mini-batch size 256 Weight decay of 1e-5 No dropout used Result 사람보다 나은 performance를 보여줌 Comparing complexity inception-v4 : ResNet + Inception VGG : Highest memory, most operations GoogLeNet : most efficient AlexNet : Smaller compute, still memory heavy, lower accuracy ResNet : Moderate efficieny depending on model, highest accuracy Time and power consumption Other architectures Network in Network (NiN) Identity Mappings in Deep Residual Networks Improving ResNets Wide Residual Networks Improving ResNets ResNeXt(Aggregated Residual Transformations for Deep Neural Networks) Improving ResNets Deep Networks with Stochastic Depth Improving ResNets FractalNet: Ultra-Deep Neural Networks without Residuals Beyond ResNet Densely Connected Convolutional Networks Beyond ResNet SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model Size Efficient networks Summary Reference Stanfoard CS231n 2017 Google Inception Model",
    "tags": "cs231 data",
    "url": "/data/2018/05/25/cs231n-cnn-architectures/"
  },{
    "title": "Stanford CS231n 7강. Training Neural Networks 2",
    "text": "Stanfoard CS231n 2017를 요약한 포스팅입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) Today Fancier optimization Regularization Transfer Learning less data일 때 사용할 수 있는 방법 SGD while True: dx = compute_gradient(x) x -= learning_rate * dx Optimization loss function tell us how good or bad is that value of the weights doing on our problem gives us some nice landscape over the weights Problems with SGD Q) loss가 한 방향으론 빠르게, 다른 방향으로 천천히 변화하면 어떻게 될까? Gradient Descent의 역할은? A) shallow dimension에서 매우 느리고 jitter 모양 step마다 지그재그로 왔다갔다 해서 매우 느린 속도로 학습이 진행됨 고차원에선 이런 문제가 많이 발생 (위 그림은 2차원!) x : value of parameter, y : loss local minima SGD get will stuck == local minima, gradient is zero, 빠져나오지 못함 100 million dimension일 경우 모든 방향에서 loss가 증가함. 실제로 local minima는 매우 드물게 나타납니다 Saddle(안장) points엔 더 내려가야하는데 gradient 엄청 작은 값이라 정말 느리게 진행됨 Saddle points much more common in high dimension 100 million dimension에선 한 방향으론 loss가 증가하고, 다른 방향으론 loss가 내려감. 이런 경우는 정말 흔하게 나타납니다 near the saddle point도 slope가 평평해 gradient가 0에 가까워집니다 local minima보다 saddle point가 더 문제가 됩니다 local minima는 non-convex에서 gradient 음수(극대점), convex에선 gradient가 양수(극소점) vs saddle point는 방향에 따라 gradient가 다름 다크 프로그래머님 글 참고 mini batch를 이용해 gradient에 대한 true information이 아닌 noisy estimate를 얻음. noise가 mess up gradient SGD의 대안책 vx = 0 while True: dx = compute_gradient(x) vx = rho * vx + dx x -= learning_rate * vx 1) SGD + Momentum 위 코드에서는 x +=로 되어있는데 x -=가 맞지 않을까 생각됨 maintain velocity add our gradient estimates to the velocity 원래 가던 방향으로 가고자하는 관성을 이용하는 방법 saddle point에서 gradient가 0이어도 velocity가 유지되서 saddle point를 지나갈 수 있음 minima로 가는데 필요한 step이 줄어듬 friction을 보통 0.9 또는 0.99로 줌 velocity = running mean of gradients (1) Momentum update overcome some noise in our gradient estimate 현재 위치에서 gradient를 구한 후, 그 값에서 Velocity만큼 옮김 (2) Nesterov Momentum (NAG, Nesterov Accelerated Gradient) Velocity로 옮긴 후 gradient를 구함 convex optimization 관점에서 유용 cs231 정리 AdaGrad grad_sqaured = 0 while True: dx = compute_gradient(x) grad_squared += dx * dx x -= learning_rate * dx/(np.sqart(grad_squared) + 1e-7) velocity 대신 gradient의 제곱(sqaured_term)을 사용 1e-7 : 0으로 나누지 않기 위해 추가한 상수 high condition number 문제는 많이 바뀌는 gradient는 큰 수로 나누고, 잘 변하지 않는 gradient는 작은 수로 나누어 속도를 더 빠르게함 Adagrad, the steps actually smaller and smaller because we just continue updating this estimate of the squared gradients over time, so this estimate just grows and grows monotonically over the course of training. Now this causes our step size to get smaller and smaller over time saddle point : non-convex에선 stuck not so common RMSProp grad_squared = 0 while True: dx = compute_Gradient(x) grad_sqaured = decay_rate * grad_sqaured + (1-decay_rate) * dx * dx x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7) sqaured gradient를 축적만 하지 않고 decay rate를 도입 decay rate = 0.9 or 0.99 강화학습에선 RMSProp을 많이 사용 Adam # almost first_moment = 0 second_moment = 0 while True: dx = compute_gradient(x) first_moment = beta1 * first_moment + (1-beta1)*dx second_moment = beta2 * second_moment + (1-beta2) * dx * dx x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)) almost use velocity와 squared_gradient를 모두 사용 first moment : Momentum second moment : AdaGrad/RMSProp(RMSProp에 더 근접한 듯! decay rate가 있음) 첫 second_moment는 0!(beta2는 0.9 또는 0.99) 첫 timestep은 very very large step 위 문제를 해결하기 위해 Bias correction을 추가 (면접 질문) Adam이 왜 잘될까요? 2가지 이유는? # full from first_moment = 0 second_moment = 0 while True: dx = compute_gradient(x) first_moment = beta1 * first_moment + (1-beta1)*dx second_moment = beta2 * second_moment + (1-beta2) * dx * dx first_unbias = first_moment / (1-beta1**t) second_unbias = second_moment / (1-beta2**t) x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)) bias correction이 first, second_moment를 0으로 시작할 수 있도록 만들어줌 beta1 = 0.9, beta2 = 0.999, learning_rate = 1e-3 or 5e-4 Learning rate lr을 천천히 decay시켜 minima에 도달할 수 있도록 설정 처음엔 no decay로 시도해보고 직접 눈으로 보길! decay 방법은 exponential decay, 1/t decay 등이 있음 비교 밑 그림을 보면 SGD가 Converge가 가장 느림 왜 Adam은 없지.. Adadelta가 제일 좋아보이지만 문제 상황에 따라 다름. 문제 상황에 맞게 선택 Rmsprop과 Adagrad가 좋아 보임. saddle point 문제를 극복할 수 있기에 이 옵티마이저들이 좋음 First-Order(1차) optimization 위에서 말한 것들이 모두 First-Order optimization (1) Use gradient form linear approximation (2) Step to minimize the approximation Second-Order(2차) optimization 함수로 근사 (1) Use gradient and Hessian to form quadratic approximation (2) Step to the minima of the approximation 뉴턴 = 내츄럴 learning rate가 없음 딥러닝에 사용하기엔 계산량이 너무 많음(Hessian은 O(N^2), inverting은 O(N^3)) 최적화(Optimization) 기초와 포트폴리오 선택 참고 L-BFGS less stochastic, less parameter In practice Adam is a good default choice in most cases full batch update를 해야하면 L-BFGS train / val error의 gap을 줄이기 위해 어떻게 해야할까? Model Ensembles 1) Train multiple independent models 2) At test time average their results Enjoy 2% extra performance Tips and Tricks Instead of training independent models, use multiple snapshots of a single model during training! Cyclic learning rate schedules can make this work even better Instead of using actual parameter vector, keep a moving average of the parameter vector and use that at test time (Polyak averaging) (but not common) How to improve single-model performance? Regularization 1) Add term to loss L2는 neural network에서 사용하지 않음(?) 2) Dropout randomly하게 뉴런을 activation 확률을 0으로 바꿔서 끊음 왜 좋은건가? (1) Prevent co-adaptation(상호 작용) of features (2) kind of like large ensemble models(that share parameters) Test time Common pattern Batch normalization(during training) one data point might appear in difference mini batches with difference other data points. BN을 사용할 때는 Dropout을 사용하지 않음 3) Data Augmentation 데이터 뿔리기 Horizontal Flips Random crops and scales Training : sample ranom crops / scales Testing : average a fixed set of crops Color Jitter Apply PCA to all RGB Common pattern Transfer Learning 이미 많은 양의 data로 train된 모델을 사용 1) 적은 양의 data 마지막 layer를 reinitialize하고 train Use Linear Classifier on top layer 2) 많은 양의 data 마지막 Max Pooling layer 이후 모든 FC를 train(fine tuning, lr=0.1쯤?) finetune a few layer dataset이 많이 다른데 많은 양의 data를 보유하면 finetune a large number of layers Transfer learning을 하면 FC 이전의 layer들은 generic하고 FC layer들은 specific! Summary Reference Stanfoard CS231n 2017 다크 프로그래머님 글(local minima, saddle point) cs231 정리 최적화(Optimization) 기초와 포트폴리오 선택",
    "tags": "cs231 data",
    "url": "/data/2018/05/20/cs231n-training_neural_networks_2/"
  },{
    "title": "Stanford CS231n 6강. Training Neural Networks-1",
    "text": "Stanfoard CS231n 2017를 요약한 포스팅입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) Today Overview 1) One time setup activation functions, preprocessing, weight initialization, regularization, gradient checking 2) Training dynamics babysitting the learning process, parameter updates, hyperparameter optimization 3) Evaluations model ensembles Activation functions Sigmoid \\sigma (x) = 1/(1+e^{-x}) Squashes numbers to range [0, 1] Historically popular since they have nice interpretation(해석) as a saturating “firing rate” of a neuron saturated : activation value가 극단적 값만 가지게 되는 경우 3가지 문제점 1) Saturated neurons “kill” the gradients(Vanish Gradient) x가 -10, 10일 경우엔 gradients가 0 Chain Rule에 의해 gradient를 구할 때 “곱” 연산을 지속적으로 하면 gradient는 점점 0이 됩니다 2) Sigmoid outputs are not zero-centered input은 항상 positive(x&gt;0) output도 positive. 이 경우 w의 gradients는? Q) If all of X is positive? A) Always all positive or all negative w에 대한 gradient를 좌표평면에 표현하면 gradient 벡터는 1,3사분면으로 나옵니다. 이상적인 움직임은 파란색이지만, 원하는 곳으로 가기 위해선 지그재그로(빨간색) 움직여야 합니다. 이 경우 수렴속도가 늦어지는 비효율을 낳게 됩니다 3) exp() is a bit compute expensive Tanh Squashes numbers to range [-1, 1] zero centered (nice) still kills gradients when saturated(bad) ReLU Rectified Linear Unit Computes f(x) = max(0, x) Does not saturate (in +region) Very computationally efficient(exp이 없어서) Converges(수렴하다) much faster than sigmoid/tanh in practice(eg 6x) Actually more biologically plausible than sigmoid(sigmoid보다 뉴런의 작용을 잘 반영) 2012, AlexNet에서 처음 사용 문제점 Not zero-centered output An annoyance(0보다 작은 부분의 gradient는 0이 됨. 10~20%가 dead ReLU) people like to initialize ReLU neurons with slightly positive biases(e.g. 0.01) 0일때 그라디언트 체크 Leaky ReLU f(x) = max(0.01x, x) x가 음수면 gradient가 무조건 0이 되는 단점을 극복하기 위해 고안 장점 Does not saturate (in +region) Very computationally efficient(exp이 없어서) Converges(모여들다) much faster than sigmoid/tanh in practice(eg 6x) will not die PReLU(Parametric ReLU) f(x) = max(\\alpha x, x) backprop into alpha(parameter) little bit more flextibility ELU(Exponential Linear Units) f(n) = \\begin{cases} x, &amp; \\text{if } x &gt; 0 \\\\ \\alpha(exp(x)-1) &amp; \\text{if } x &lt; 0 \\end{cases} 장점 ReLU의 모든 장점 Closer to zero mean outputs Negative saturation regime compared with Leaky ReLU adds some robustness to noise 단점 exp() 연산 사용 SeLU 강의에선 다루지 않았으나, 일단 적어둠 추후 내용 추가하기 Maxout “Neuron” Does not have the basic form of dot product -&gt; nonlinearity nonlinearity : f(x+y) = f(x) + f(y), cf(x), f(cx) 만족 선형을 쓰면 레이어를 쌓는 의미가 없음(단지 이동일 뿐임) 연결된 두 개의 뉴런 값 중 큰 값을 취하고 비선형성을 확보. 단, 활성화 함수를 적용하기 위해 필요한 연산량이 많음 장점 Generalizes ReLU and Leaky ReLU Linear Regime! Does not saturate! Does not die! max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2}) 문제점 doubles the number of parameters/neuron In practice Use ReLU. Be careful with your learning rates Try out Leaky ReLU / Maxout / ELU Try out tanh but don’t expect much Don’t use sigmoid Data Preprocessing cs 231 번역 참고 zero-centered data Feature에 대해 평균값만큼 차감하는 방법 normalized data 각 차원의 데이터가 동일한 범위내의 값을 갖도록 하는 방법 이미지 처리에선 보통 하지 않음(scale이 달라지면 다른 feature) 머신러닝 관점에선 PCA Whitening input의 feature들을 uncorrelated하게 만들고, 각각의 variance를 1로 만들어줌 위키피디아 Summary 이미지에선 일반적으로 zero mean preprocessing(center)만 진행 Subtract the mean image(AlexNet) Subtract per-channel mean(VGGNet) : RGB Not common to normalize variance, to do PCA or whitening Train에서 진행한 작업을 Test에서도 진행 Weight Initialization Q) What happens when W=0 init is used? A) They will all do the same thing -&gt; same gradient -&gt; 모든 파라미터는 동일한 값으로 업데이트 -&gt; 찾고자 하는 값을 찾기 힘듬 따라서 Weight Initialization이 중요 아이디어 1) Small random numbers gaussian with zero mean and 1e-2 standard deviation small network는 okay, deep network에선 문제 발생 Q1) weight가 0.01일 경우 gradients의 look like는? A1) 점점 smaller gradient가 되서 update하지 못하게 됨 아이디어 2) Big random numbers Q2) weight가 1.0일 경우 gradient는? A2) all neurons completely saturated, either -1 and 1. Gradients will be all zero Vanishing gradient 발생 Xavier/He initialization Reasonable initialization. (Mathematical derivation assumes linear activations) input : fan_in, output : fan_out ( 차원의 수 ) (Xavier) W = np.random.randn(fan_in,fan_out) / np.sqrt(fan_in) (He) W = np.random.randn(fan_in,fan_out) / np.sqrt(fan_in/2) 레이어의 출력값들이 적절한 분포를 보임 그러나 ReLU를 사용하면 nonlinearity가 깨짐 출력값들이 점점 0이 되어버림 He는 ReLU과 잘 맞음 Batch Normalization(2015) keep activations in a gaussian range that we want consider a batch of activations at some layer. To make each dimension unit gaussian, apply (각 layer의 출력값을 비슷한 분포로 생성(Unit Gaussian)) instead of with weight initialization 기본적으로 Gradient Vanishing 이 일어나지 않도록 하는 아이디어 중 하나입니다. 여태는 이 문제를 Activation 함수의 변화, Careful Initialization, small learning rate 등으로 해결했지만, 이런 간접적인 방법보다 training하는 과정 자체를 안정화해서 학습 속도를 가속시킬 근본적인 방법을 찾았습니다 각 layer에 들어가는 input을 normalize해서 layer의 학습 속도를 가속, 각 mini-batch의 mean, variance를 구해 normalize N : training examples in current batch D : Each batch’s dimension CNN에서는 activation map마다 하나씩 BN 1) dimension마다 mean, variance를 구해서 계산 2) Normalize Fully Connected or Convolutional layer 뒤 또는 nonlinearity 전에 위치 장점 네트워크의 Gradient flow를 향상시킴(가우시안 유닛 벡터가 0 근처라서 vanishing이 일어나지 않음. flow 유지) 높은 Learning rate를 사용해도 안정적인 학습. weight 초기화의 의존성을 줄임 Regularization 기능도 하기 때문에 dropout의 필요성을 감소시킴 (dropout을 사용하면 학습속도가 느려짐) Test할 땐 미니배치의 평균과 표준편차를 구할 수 없으니 Training하면서 구한 평균의 이동평균을 이용해 고정된 Mean과 Std를 사용 Paper 꼭 보기! Batch Normalization 설명 및 구현 블로그 추천 Babysitting the Learning Process How to monitoring training How do we adjust hyperparameters as we go 1) Preprocess the data 2) Choose the architecture loss가 적절한지 확인하기 regularization term을 없앴다가 만든다. regularization term을 넣어서 loss가 올라가면 잘된거임! loss가 줄어들지 않고 그대로라면? learning rate가 너무 낮을 수 있으니 올려보자 loss가 폭발적이라면 learning rate가 너무 높은 것일수도 있으니 내려보자 cross validating 할 때 1e-3, 1e-5면 적당한 편 Hyperparameter Optimization cross-validation strategy (coarse -&gt; fine) how well do this hyperparameter 먼저 coarse하게 대충 파라미터들이 어떻게 작동하는지 파악 이후 fine하게 parameter 찾기 Tip : cost가 original cost보다 3배가 넘으면 explosion.. break out early Random Search vs Grid Search 랜덤 Search가 Grid Search보다 좋음 Hyperparameters to play with network architecture learning rate, its decay schedule, update type regularization(L2/Dropout strength) Monitor and visualize the loss curve Summary Reference Stanfoard CS231n 2017 딥러닝 학습 기술들 Batch Normalization 설명 및 구현 Gradient Descent Optimization Algorithms 정리",
    "tags": "cs231 data",
    "url": "/data/2018/05/18/cs231n-training_neural_networks/"
  },{
    "title": "Tiny SSD 논문 리뷰",
    "text": "Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection을 정리한 글입니다! 2018.2월에 나온 논문 Abstract 최근 embeded 기기에 적합한 object detection architecture에 대해 관심이 많아졌습니다 Tiny YOLO, SqueezeDet SqueezeNet에서 나온 Fire microarchitecture + SSD에서 나온 single-shot detection macroarchitecture model size : 2.3MB (~26X smaller than Tiny YOLO) maP : 61.3% on VOC 2007 (~4.2% higher than Tiny YOLO) 1. Introduction Object Detection은 combination of object classi-fication과 object localization 문제를 동시에 고려하는 문제입니다 Small Deep Neural Network에 대한 관심 증가 YOLO, YOLOv2 (1) 모델 사이즈는 753MB, 193MB (2) 임베디드 칩에서 실행될 때 object detection 속도가 급격히 떨어짐 Tiny YOLO 네트워크 구조를 축소해 모델 크기를 줄임(60MB) 부동 소수점 연산수를 대폭 줄이고 객체 감지 정확도를 희생 SqueezeDet SqueezeNet의 Fire microarchitecture를 활용한 Fully Convolutional neural network 성능이 좋고 자율 주행에 들어갈만큼 작은 사이즈로 만듬 그러나 object categories를 3개만 탐지 가능 실시간 Object Detection(on embeded)에 적합하며, 다양한 카테고리에서 높은 정확도를 달성하는 네트워크를 계속 만드려고 도전중 Tiny SSD : Fire microarchitecture introduced in SqueezeNet + single-shot detection macroarchitecture introduced in SSD non-uniform fire sub-network를 stack check : non-uniform의 의미 찾아보기 2개의 main sub-network를 stack 1) non-uniform Fire sub-network stack 2) non-uniform sub-network stack of highly optimized SSD-based auxiliary convolutional feature layers 2. Optimized Fire Sub-Network Stack SqueezeNet 1) reduce the number of 3 \\times 3 filters as much as possible 2) reduce the number of input channels to 3 \\times 3 filters where possible 3) perform downsampling at a later stage in the network Fire microarchitecture Squeeze convolutional layer of 1x1 filters 3 \\times 3 필터에 대한 input channels을 줄이는 2)를 실행 Expand convolutional layer comprised of both 1 \\times 1 filters, 3 \\times 3 filters 3 \\times 3 필터의 수를 줄이는 1)를 실행 First sub-network stack Fire modules로 최적화된 standard convolutional layer로 구성 Key Challange : Fire module의 갯수와 아키텍쳐를 결정하는 것 물체 감지 성능과 모델 크기, 추론 속도의 균형 10 Fire modules ( 경험적으로 선택 ) Key design parameters : (1 \\times 1 filters, 3 \\times 3 filters) 필터의 수 SqueezeNet 네트워크 아키텍처와 Fire 모듈의 microarchitecture는 거의 동일 3. Optimized Sub-network stack of SSD based Convolutional Feature layers 널리 사용되고 효과적인 SSD macroarchituecture base feature extraction network architecture convolutional feature layers + convolutional predictors auxiliary convolutional feature layers에서 얻을 수 있는 것 1) a confidence score for a object category 2) shape offset relative to default bounding box coordinates Key Challenge : auxiliary convolution feature layer와 convolutional predictor를 결정하는 것 Key Design parameter : the number of filters 5. Experimental results and discussion Training Setup iteration : 220,000 batch size : 24 RMSprop, learning rate = 0.00001 \\gamma = 0.5 Reference Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection Tiny SSD Reading Note",
    "tags": "paper data",
    "url": "/data/2018/05/17/tiny-ssd-review/"
  },{
    "title": "CS231n 5강. Convolutional Neural Networks",
    "text": "Stanfoard CS231n 2017를 요약한 포스팅입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) Convolutional Neural Networks History 1957, Perceptron 1960, Adaline, Madaline 1986, First time back-propgation became popular 2006, Reinvigorated(활력을 되찾은) research in Deep Learning 2012, Imagenet clssification with deep convolutional neural networks Hubel &amp; Wiesel의 실험 Visual Cortex가 어떻게 동작하는지 밝혀내려고 했습니다 고양이 머리에 전극을 부착하소 서로 다른 시각적 자극을 주었을 때 어떻게 반응하는지 관찰합니다 찾아낸 결론 하나의 영상(시야)엔 Visual Cortext 내의 여러 세포들이 각자 부분을 담당해 인지합니다 중요한 Region은 붉은 색 세포에, 덜 중요하고 주변적인 Region은 파란색 세포에 매핑 인지 과정이 계층적으로 일어납니다(Hierarchical organization) Neurocognition Hubel &amp; Wiesel의 실험에 영감을 받아 Simple cell과 Complex cell의 구조를 본떠서 만들었습니다 하지만 Backpropagation 방법론 등장 전..! 최초로 CNN 구조를 Backpropgation을 사용해 학습! zip code 그러나 데이터가 단순하고 스케일이 작음 AlexNet 1000개의 클래스를 분류하는 ImageNet 대회에서 CNN 모델로는 최초로 1등!! LeNet에 비해 Larger &amp; Deeper 데이터의 양에 비례해 Scalable하며 GPU를 이용한 병렬 연산 Convolutional Neural Networks 제가 작성했던 Introduction Convolution도 꽤 도움이 됩니다! Fully Connected Layer Fully Connected Layer는 이전 Layer의 모든 노드와 연결되어 있습니다. 따라서 Spatial Structure(공간적 구조)를 보존하기에 좋은 구조는 아닙니다 Convolution Layer Spatial Structure(공간적 구조)를 보존해줍니다 작은 필터로 이미지를 슬라이드하며 dot product를 실행합니다 Filter가 슬라이딩한 것의 결과 : Acitvation Map Convolution Network는 a sequence of Convolutional Layers, interspersed with activation functions 각 레이어의 Activation map들을 시각화해보니 Heirarchical하게 Feature를 추출하고 있습니다 A Closer look at spatial dimensions Q) Stride를 3을 주면 output의 size는 얼마나 될까? A) Doesn’t fit! : cannot apply 3x3 filter on 7x7 input with stride 3 Output size : (N-F) / stride + 1 Zero Padding output의 size는 7x7! zero padding을 1 추가하면 3x3 filter 사용 가능! zero padding = (F-1)/2 F = 3 =&gt; zero pad with 1 F = 5 =&gt; zero pad with 2 F = 7 =&gt; zero pad with 3 사용하는 이유 1) 원본 이미지 사이즈를 유지하기 위해 2) 가장자리 데이터를 사용하기 위해 Examples Convolution Layer Summary 1x1 Convolution Filter의 갯수에 따라 output의 dimension(depth)은 달라지지만, 기존 이미지 가로 세로의 사이즈는 그대로 유지됩니다 Filter의 갯수를 input의 dimension보다 작게하면, dimension reduction의 효과가 납니다 Image에서 Convolution layer는 Spatial Relation을 고려했다면, 1x1 Convolution layer는 한 픽셀만 고려하기 때문에 dimension reduction(차원 축소) 역할을 위해 사용합니다 The brain/neuron view of CONV layer 차이점 : local connectivity entire input 대신 local region만 봄(모든 뉴런들을 연결하는 것이 비실용적이기 때문) Spatial structure를 보존 Activtion map is 28x28 sheet of neuron outputs Each is connected to a small region in the input All of them share parameters “5x5 filter” =&gt; “5x5 receptive field for each neuron” Pooling layer Activation map마다 독립적으로 크기를 downsampling 특정 부분의 주변값을 대표하는 하나의 값을 뽑습니다 Output dimension이 작아지며 Parameter수를 줄입니다 그 결과 더 작고 다루기 쉽게 만들 수 있습니다 MAX Pooling (2x2 filter를 사용하는 경우) 전체 데이터의 75%를 버리고 25%만 선택 Computational Complexity 감소 Filter 내에서 가장 큰 값을 선택 Average Pooling은 평균값을 선택. Spatial Structure를 보존하되 이미지가 smooth해짐. Max Pooling은 더 강한 특징만 남기는 방식 Depth를 줄이지 않고 Spatially하게만 줄임(Height &amp; Width) Q) stride와 pooling 모두 downsampling인데 어떤 것을 사용해야 하나요? A) 최근 CNN 아키텍쳐는 stride를 사용하는 편이 많습니다. stride 추천합니다 힌튼 교수님이 추후에 캡슐넷에서 맥스 풀링의 단점을 이야기했었음! check Fully Connected Layer (FC layer) Contains neurons that connect to the entire input volume, as in ordinary Neural Networks Summary ConvNets stack CONV, POOL, FC layers Trend towards smaller filters and deeper architectures Trend towards getting rid of POOL/FC layers (just CONV) Typical architectures look lik [(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX WHERE N is usually up to ~5, M is large, 0&lt;=k&lt;=2 but recent advanced such as ResNet/GoogLeNet challenge this paradigm Reference Stanfoard CS231n 2017",
    "tags": "cs231 data",
    "url": "/data/2018/05/14/cs231n-cnn/"
  },{
    "title": "Stanford CS231n 4강. Backpropagation and Neural Networks",
    "text": "Stanfoard CS231n 2017를 요약한 포스팅입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) 이번 강의는 복잡한 함수의 Analytics Gradient를 계산하는 방법에 대해 이야기할 예정입니다 Numerical gradient : slow, approximate, easy to write Analytics gradient : fast, exact, error-prone(오류가 생기기 쉬움) Analytics 계산 후, Numerical gradient로 Check! Computational Graphs 이점 Back Propagation 사용 가능 Complex Function을 할 때 유용 CNN, Neural Turing Machine 등 모든 변수에 대한 Gradient를 계산하기 위해 Chain Rule을 재귀적으로 사용합니다 which is going to recursively use the Chain Rule in order to compute the gradient with respect to every variable in the computation graph 동그라미는 Node Graph Steps of Computation L \\Sigma (regularization term + data term) Example 1 z 부분은 \\frac{\\partial{f}}{\\partial{z}} = -12/-4 = 3 q 부분은 \\frac{\\partial{f}}{\\partial{q}} = -12/3 = -4 y 부분은 \\frac{\\partial{f}}{\\partial{y}} = \\frac{\\partial{f}}{\\partial{q}} \\frac{\\partial{q}}{\\partial{y}} = -4 * 1 = -4 x 부분은 \\frac{\\partial{f}}{\\partial{x}} = \\frac{\\partial{f}}{\\partial{q}} \\frac{\\partial{q}}{\\partial{x}} = -4 * 1 = -4 구성 Local Gradient : 우리가 구하려던 것은 아님 Gradients : loss 대비 우리가 구하려던 것 각각의 노드는 주변 환경을 알고 있습니다(immediate surronding) 국소적 계산(Local Computation) 전체에서 어떤 일이 벌어지든 상관없이, 자신과 관계된 정보만으로 결과를 출력 가능 일종의 조립라인의 분업처럼, 복잡한 계산도 나누면 단순한 계산이 가능! Example 2 local gradient를 구하면 x = 1.37 0.37*-0.53 웅원님의 이야기 : 손으로 꼭 해보세요~! sigmoid gate를 더 쉽게 계산할 수 있습니다! Patterns in backward flow Add gate : gradient distrubutor Q) What is a max gate? Q) What is a mul gate? max gate : gradient router mul gate : gradient switcher 바꿔준다의 swith Gradients for vertorized code Vector일 경우 Gradient를 구하는 방법에 대해 알아보겠습니다 \\frac{\\partial{z}}{\\partial{x}}는 Jacobian matrix입니다 derivative of each element of z w.r.t each element of x (w.r.t = with respect to, ~에 대해) 자세한 설명 대신 동훈님의 공돌이의 수학정리노트에 있는 자코비안 행렬 링크를 첨부합니다! 내용 다 좋아요 :) 유튜브도 좋아요! 위키피디아 자코비안 행렬 Vectorized operations Q) What is the size of the Jacobian matrix? A) 4096 * 4096 만약 미니배치를 100으로 진행하고 있었다면 409,600 x 409,600 Q) what does it(Jacobian matrix) look like? A) Diagonal This is element-wise, each element of the input only affect that corresponding element in the output A Vectorized Example The gradient of a vector is always going to be the same size as the original vector, and each element of this gradient is going to it means how much of this particular element affects our final output of the function Always check The gradient with respect to a variable should have the same shape as the variable ~왜 Transpose를 해주나 이해가 안되서 찾아본 링크 링크를 보고 다시 생각해보니 납득~ 원하는 식을 얻기 위해 식을 변형 W_{k,i} (Transpose) Backpropagation Summary Neural Nets은 굉장히 거대함 모든 파라미터를 직접 그라디언트 계산하기 힘듭니다 Backpropagation recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates Graph structure 구현 forward compute result of an operation and save any intermediates needed for gradient computation in memory backward apply the chain rule to compute the gradient of the loss function with respect to the inputs Neural networks class of function that are stacked on top of each other and we stack them in a hierarchical way in order to make up a more complex non-linear function h : intemediate variable(layer) 3 layers 구현(2-layer) Full implementation of training a 2-layer Neural Network needs ~20 lines import numpy as np from numpy.random import randn N, D_in, H, D_out = 64, 1000, 100, 10 x, y = randn(N, D_in), randn(N, D_out) w1, w2 = randn(D_in, H), randn(H, D_out) for t in range(2000): h = 1/(1+np.exp(-x.dot(w1))) y_pred = h.dot(w2) loss = np.square(y_pred - y).sum() print(t, loss) grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h.T.dot(grad_y_pred) grad_h = grad_y_pred.dot(w2.T) grad_w1 = x.T.dot(grad_h * h * (1-h)) w1 -= 1e-4 * grad_w1 w2 -= 1e-4 * grad_w1 Neural Network 이름의 유래 뉴런의 자극 처리 메커니즘과 neural network의 메커니즘이 유사합니다 input을 자극으로 생각하고 데이터를 처리하는 함수가 cell body! activation function을 통해 결과값이 나오는 것도 유사하게 볼 수 있음(역치를 떠올리면..) 하지만 정확히는 뉴런의 메커니즘과 같지는 않으니 참고만! Activation functions 추후 하나 하나 설명할 예정! Neural networks: Architectures 아키텍쳐도 추후 설명할 예정입니다 Neural networks Summary We arrange neurons into fully-connected layers The abstraction of a layer has the nice property that is allows us to use efficient vectorized code(ex. matrix multiplies) (check) Neural networks are not really neural Next time : Convolutional Neural Networks Reference Stanfoard CS231n 2017 공돌이의 수학정리노트 위키피디아 자코비안 행렬",
    "tags": "cs231 data",
    "url": "/data/2018/05/13/cs231n-backpropagation-and-neural-networks/"
  },{
    "title": "Faster R-CNN 논문 리뷰",
    "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks을 정리한 글입니다! Introduction Faster R-CNN 논문은 Fast R-CNN을 보완한 논문입니다 기존에 사용되던 Region Proposal 방법인 Selective Search는 CPU에서 계산 CNN 외부에서 진행 GPU의 이점을 최대한 활용하고 CNN 내부에서 진행하기 위해 Region Proposal Network(RPN)을 도입 RPN은 각 위치의 object bounds와 objectness score를 동시에 예측하는 fully convolutional network입니다 Architecture 2개의 Network Region Proposal Network 위에서 나오는 proposed regions을 사용해 object 감지하는 Detector Input Height \\times Width \\times Depth RGB Color를 갖는 이미지 Feature Extraction pretraind model을 사용해 Feature Map 생성 input : 이미지 output : object의 Feature Maps Region Proposal Network input : Feature Extraction에서 뽑은 Feature Maps output layer classification layer : Object 유무 reg layer : Object Proposal Feature Maps 위에 n \\times n spartial window(보통 3\\times 3)를 슬라이드 sliding-window가 찍은 지점마다 여러 Region Proposal(Anchor) 예측 Anchor sliding window의 각 위치에서 Bounding Box의 후보로 사용되는 상자 k로 표현 3개의 크기(128, 256, 512), 3개의 비율(2:1, 1:1, 1:2) = 9개의 조합 Classification layer 모든 anchor마다 foreground, background 분류 Anchor가 ground truth box와 IoU가 가장 크고 0.7 이상이면 foreground, 적으면 background ground truth box : 실제 box의 좌표 IoU : Intersection over Union foreground는 positive anchor background는 non-positive anchor Regression layer Bounding box regression t는 4개의 좌표값을 가지고 있으며 ground-truth t^*도 4개의 좌표값을 가지고 있습니다 Region of Interest Pooling RPN을 지나면 서로 다른 크기의 proposed region이 나옵니다. 서로 다른 크기의 region을 동일한 크기로 맞추기 위해 RoI Pooling을 사용합니다 Fixed-size resize RoI Pooling 대신 object detection을 구현할 떄 많이 쓰이는 방법으로, feature map을 crop시킨 후, 고정된 크기로 보간해 resize 그 이후 2 \\times 2 kernel을 사용해 7\\times 7 \\times depth로 max pooling Training Loss Function i : anchor의 index p_{i} : anchor i가 객체인지 배경인지 예측값 p_{i}^{*} : ground-truth label, 1은 객체(positive)를 뜻하며 0은 배경(negative) L_{cls} : 객체인지 배경인지의 log loss N_{cls} : normalization 값, mini-batch값 t_{i} : 4개의 bounding box 좌표 t_{i}^{*} : ground-truth box L_{reg} : 객체가 있을 경우 loss function. smmoth l1 loss 사용 N_{reg} : normalization, anchor locations의 갯수 \\lambda : 기본값 10 Training RPN 한 이미지에서 random으로 mini-batch만큼 anchors를 샘플링 이 때, positive anchor와 non-positive anchor를 1:1 비율로 사용 보통 negative anchors가 더 많기 때문에 비율을 조정하지 않으면 학습이 한쪽으로 편향됨 하지만 positive anchor가 128개보다 적으면 zero-padding을 시켜주거나 아예 positive가 없으면 IoU값이 높은 값을 사용 weight는 랜덤하게 초기화 ImageNet classification으로 fine-tuning Learning Rate : 0.001(60k mini batch), 0.0001(20k mini batch) Momentum : 0.9 Weidght decay : 0.0005 Results   R-CNN Fast R-CNN Faster R-CNN Test time per Image (with proposal) 50s 2s 0.2s 상대적 Speed 1x 25x 250x mAP (VOC 2007) 66.0 66.9 69.9 Experiments Table 2. RPN을 사용했을 때 mAP가 조금 더 좋음 Table 8 : 3 scales, 3 ratios를 사용할 때 성능이 가장 좋았음 Table 9 : lambda값이 10일 때 성능이 가장 좋았음 Conclusion Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates 각종 방법론 Reference PR12: 이진원님의 PR-012 박진우님 블로그 논문으로 시작하는 딥러닝 Edwith",
    "tags": "paper data",
    "url": "/data/2018/05/09/Faster-RCNN-review/"
  },{
    "title": "Stanford CS231n Lecture 2. Image Classification",
    "text": "Stanfoard CS231n 2017를 요약한 포스팅입니다. 정보 전달보다 자신을 위한 정리 목적이 강한 글입니다! :) Image Classification Computer Vision의 핵심 Task Input : Image Output : Category Labels Semantic Gap 이미지에서 추출할 수 있는 정보(색, 질감)와 사람들이 원하는 추상적 정보의 차이 Challenges Viewpoint Variation ( 보는 각도 ) Illumination ( 조명 ) Deformation ( 변형 ) Occlusion ( 은폐, 숨김 ) Background Clutter ( 배경과 섞임 ) Intraclass Variation ( 물체의 다양성 ) 상상하는 모든 이미지를 실시간으로 판단하고 싶음 Attempts Find edges Find corners brittle Data-Driven Approach Collect a dataset of images and labels Use Maching Learning to train a classifier Evaluate the classifier on new images Classifier : Nearest Neighbor Train : Memorize all data and labels Predict : Predict the label of the most similiar training image Example Dataset: CIFAR10 10 classes(airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) 50000 training images 10,000 testing images Distance Metric L1(Manhattan) distance L2(Euclidean) distance Hyper Paramter What is the best value of k to use? Whate is the best distinct to use? Choices about the algorithm that we set rather than learn Setting Hyper Paramters Idea 1. Choose hyper paramters that work best on your data Bad : K=1 always work perfectly on training data Idea 2. Split data into train and test, choose hyper parameters that work best on test data Bad : No idea how algorithm will perform on new data Idea 3. Split data into train, val, and test; choose hyper parameters on val and evaluate on test Better! validation set : check accuracy, check how well algorithm is doing idea 4. Cross-Validation: Split data into folds, try each fold as validation and average the results Useful for small datasets, but not used too frequently in deep learning KNN on images never used Very slow at test time Distance metrics on pixels are not informative Curse of dimensionality ( 차원의 저주 ) Parametric Approach Image : Array of 32\\times 32\\times 3 (3072) Function : f(x,W) Output : 10 numbers giving class scores Coming up Loss function : quantifying what it means to have a “good” W Optimization : start with random W and ifnd a W that minimizes the loss ConvNets : tweak the functional form of f Reference Stanfoard CS231n 2017",
    "tags": "cs231 data",
    "url": "/data/2018/05/06/image-classification/"
  },{
    "title": "서포트 벡터 머신(Support Vector Machine, SVM)",
    "text": "카이스트 문일철 교수님의 인공지능 및 기계학습 개론1 5주차 강의를 보고 정리한 포스팅입니다! 서포트 벡터 머신(Support Vector Machine, SVM) Reference 인공지능 및 기계학습 개론1",
    "tags": "ml data",
    "url": "/data/2018/05/04/Support-Vector-Maching-SVM/"
  },{
    "title": "로지스틱 회귀(Logistic Regression)",
    "text": "카이스트 문일철 교수님의 인공지능 및 기계학습 개론1 4주차 강의를 보고 정리한 포스팅입니다! Logistic Regression 로지스틱 회귀는 SVM 같은 기법들이 나오기 전에 널리 사용되고 연구되었던 방법입니다 Optimal Classification에서 S커브 모양인 그래프가 더 잘 분류하고 있는 것을 저번 강의에서 배웠습니다. 로지스틱 함수가 더 효과적인 것을 설명하기 위해 선형 함수와 비교한 그래프입니다! 좌측 그래프에선 잘 보이지 않아 log를 취해 우측 그래프로 비교하고 있습니다. P(Y\\mid X)=0.5인 부분에 로지스틱 함수와 선형 함수의 Decision Boundary를 결정할 수 있습니다. DB를 기준으로 왼쪽은 0, 오른쪽은 1로 구분되는데 빨간 색인 선형 함수의 경우 Error가 많이 발생합니다 로지스틱 함수는 시그모이드의 특징을 갖추고 있습니다 Sigmoid Function Bounded : 범위가 -1 ~ 1로 제한 Differentiable : 다양한 변형이 가능 Real Function : 실제 함수 Defined for all real inputs : 모든 Input에 정의 With positive derivative : 증가하는 모양 샤프한 posterior 변환을 알아볼 수 있음 Logistic Function f(x) = \\frac{1}{1+e^{-x}} 성장 곡선에서 자주 사용 Sigmoid 성질 보유 Derivation을 쉽게 계산 가능 : 최적화할 때 극점을 통해 계산 Logit Function(Logistic의 역함수) f(x)=log(\\frac{x}{1-x}) Logistic Function Fitting 우선 로짓 함수를 Inverse해서 로지스틱 함수로 변환합니다. 로짓의 X는 확률로 표현할 수 있습니다. 그 후, 더 나은 fitting을 위해 Linear shift를 합니다. a, b값을 조절해 압축, 확장, 이동을 합니다. ax+b는 선형 함수의 모양이므로 X\\theta로 변환할 수 있습니다. Logistic Regression Logistic Regression은 주어진 데이터를 로지스틱 함수로 모델링하는 것입니다. Binomial 문제뿐만 아니라, Multinomial 문제도 사용할 수 있습니다. 이후 설명에선 편의를 위해 Binomial 문제로 접근하겠습니다! 베르누이 현상이 주어졌을 때, \\mu(x)를 로지스틱 형태로 변경합니다. 위에서 사용된 X\\theta를 이용해 P(Y\\mid X) 를 구할 수 있습니다. X가 주어진 경우(Train Data), Y가 들어옵니다. 이 경우의 \\theta를 구하는 것이 저희의 목표입니다!! Maximum Conditional Likelihood Estimation(MCLE) N : 데이터의 개수! Class Variable에 대해 모델링하며 log를 취해서 곱을 합으로 변경해줍니다. 그 이후 \\theta를 추정하기 위해 P 자체를 풀어놓고 생각하려고 합니다. P(Y_{i}\\mid X_{i};\\theta) 를 정의한 후, 밑의 식에 넣어줍니다. 그 이후 Y_{i}로 묶은 후 log의 특성상 합쳐서 볼 수 있습니다. X\\theta와 유사해서 치환을 해서 정리합니다 그 결과, \\theta를 로지스틱 function으로 모델링했습니다! Finding the Parameter Not Closed Form이라 Approximation을 해야 합니다! Gradient Method Closed Form이 나오지 않아 계속 approximation 하는 작업을 했습니다. 최적의 해를 찾는 방법 중 GRadient Decent/Ascent Method에 대해 정리해보려고 합니다 Taylor Expansion 하나의 function에 대한 표현을 뜻하며, infinite sum of terms으로 만들 수 있습니다! Taylor 시리즈는 Infinitely differentiable이 가능할 때 사용합니다! Gradient Descent/Ascent 미분 가능한 함수 f(x)와 초기값 x_{1}이 주어졌을 떄, f(x)에 대해 더 높거나 낮은 값이 되도록 반복적으로 이동시키는 방법입니다! 이동하기 위해 방향, 속력 2개를 알아야 합니다. 속력이 느리더라도 방향이 맞으면 올바른 값으로 수렴할 수 있지만, 방향이 틀리면 올바르게 수렴하기 어렵기 때문에 방향이 중요합니다! h는 속력이고 u는 방향을 가지는 단위 벡터입니다. 현재 위치에서 h의 속력으로 u라는 방향으로 이동합니다. 테일러 확장을 적용할 때 방향인 u에 대해 잘 정해야 하며, hu를 최적화한 후 다음 차례의 x값을 구할 수 있습니다. 값을 줄여 나가면 Gradient Descent, 값을 늘리면 Gradient Ascent로 사용합니다 likelihood라서 argmax! Gradiend Ascent를 적용하며 X가 주어진 조건에서 Y가 나올 확률을 최대화하는 \\theta값을 찾기 위해 \\theta값을 반복적으로 업데이트하며 최적화된 값을 얻습니다 선형회귀 선형회귀로 잠시 돌아가면, \\theta=(X^{T}X)^{-1}X^{T}Y도 좋지만, 데이터셋이 커지면 문제가 생기는 단점을 가지고 있습니다. 이 경우 Gradient Descent를 적용하면 최적의 \\theta를 찾을 수 있습니다 나이브 베이즈 vs 로지스틱 회귀 나이브 베이즈와 로지스틱 회귀는 Generative-Discriminative Pair입니다! 나이브 베이즈가 미분을 통해 로지스틱 회귀로 변환할 수 있다는 것을 의미합니다 나이브 베이즈의 Categorical 값을 Numerical 값으로 변경해야 합니다. 변경하기 위해 가우시안 분포, 포아송 분포, 베타 분포 등을 사용할 수 있습니다. 강의에선 가우시안 분포를 따른다고 가정하고 진행했습니다 자세한 전개 과정은 강의를 듣는 것을 추천합니다!! Generative-Discriminative Pair Generaitve Model P(Y\\mid X) = P(X, Y)/P(X) =P(X\\mid Y)P(Y)/P(X) 특징 : 베이지안, 사전 확률, 결합 확률(Joint Probability) 나이브 베이즈 분류기 속도가 빠름 파라미터 수 : 4d+1 Discriminative Model P(Y\\mid X) 특징 : 조건부 확률 로지스틱 회귀 Bias가 적음 파라미터 수 : d+1 결론 로지스틱 회귀가 일반적으로 성능이 좋지만 나이브 베이즈는 prior 정보를 추가가능한 장점이 있습니다. 따라서 주어진 Data Set과 사전 정보에 따라, 문제 상황에 따라 알고리즘을 취사선택하면 될 것 같습니다 Reference 인공지능 및 기계학습 개론1",
    "tags": "ml data",
    "url": "/data/2018/05/03/logistic-regression/"
  },{
    "title": "YOLO(You only look once): Unified, real-time object detection 논문 리뷰",
    "text": "YOLO(You only look once): Unified, real-time object detection 논문을 정리한 글입니다! Intro YOLO 논문은 2015년에 나온 논문으로 (마지막 수정은 2016년 5월) 기존에 나왔던 R-CNN류의 문제점인 속도를 개선했습니다. 성능은 조금 줄이더라도 속도를 빠르게하는 것을 목표로 했으며, R-CNN류에서 1) Bounding Box Regression, 2) Detection Score를 찾는 2가지 Task를 YOLO에서는 1개의 Regression Task로 바꿔서 풀도록 재정의 했습니다. 논문 제목에서 볼 수 있듯, 전체 이미지를 1번만 보고(Yon only look once), 기존에 존재하던 좋은 것들을 합쳤고(Unified), 빠른 속도(Real-time object detection)를 가진다는 특징을 가지고 있습니다 YOLO는 v1~v5가 있는데 이 글은 v5 기반으로 작성되어 있습니다 Abstract YOLO : object detection의 새로운 접근 방법 Single neural network가 bounding box와 class probabilities를 예측 단일 네트워크를 사용하기 때문에 End-to-End로 성능을 최적화 가능 Base YOLO : 45 FPS(frames per second) Fast YOLO : 155 FPS 다른 real-time detector보다 2배 높은 mAP General representations of object를 학습 가능 1. Introduction 기존에 사용하던 DPM, R-CNN의 단점 기존에 발표된 DPM(deformable parts models)나 R-CNN은 sliding window를 사용해 잠재적 bounding box를 구한 후, post-processing을 통해 bounding box를 재조정, 중복 제거, 재측정하는 과정을 가지고 있습니다. 위와 같은 방식은 각각의 component를 따로 학습해야 하기 때문에 느리고 최적화하기 힘듭니다! YOLO 방법론 Simple한 System Input image를 448x448로 resize single convolution network에 넣기 모델의 confidence를 threshold Extremely fast regression 문제로 바꿨기 때문에 복잡한 파이프라인이 필요 없습니다 Demo Globally 전체 이미지를 넣음 Fast R-CNN에 비해 절반 이하의 background errors Generalizable representation object의 일반화된 표현을 학습 새로운 도메인이나 예상치 못한 input에 대해 세분화되지 않고 일반화 가능 ex) 사진으로 학습하고 그림 이미지로 예측할 경우 잘 맞습니다 Context 정보를 많이 사용합니다 training / testing code는 open source로 존재 6. Conclusion YOLO는 simple하고 빠르게 학습 가능! 2. Unified Detection 여러 component들을 단일 neural network로 합침 bounding box를 예측하기 위해 전체 이미지를 사용 이미지에 대한 모든 클래스와 bounding box를 동시에 예측합니다 System input image를 S \\times S grid로 나눕니다 물체의 중심이 grid cell에 속하면 grid cell이 물체를 탐지합니다 각각의 grid cell이 B개의 Bounding box와 BB에 대한 confidence score를 예측합니다 confidence score : box에 물체가 있는지, 상자가 예측한 정확도가 얼마나 정확한지를 나타내는 척도 Pr(Object) * IOU_{pred}^{truth} 물체가 없다면 0 confidence score가 IOU와 동일하길 원함 Bounding box는 5 prediction으로 구성 (x, y) : center of the box w, h : whole image에 대비한 예측값 confidence : 예측된 box와 다른 ground truth box간의 IOU grid cell은 또한 C개의 conditional class probability를 예측 Pr(Class_{i}\\mid Object) : object가 있을 grid cell의 확률 test시에 conditional class probability와 individual box confidence prediction을 곱합니다 Pr(Class_{i}\\mid Object) * Pr(Object) * IOU_{pred}^{truth} = Pr(Class_{i}) * IOU_{pred}^{truth} 각 상자마다 특정 클래스에 대한 확률을 알 수 있습니다 평가시 Pascal VOC를 사용, S=7, B=2, C=20 따라서 final prediction은 7 \\times 7 \\times 30 tensor (v5) v1은 Bounding box를 1개와 7 \\times 7 \\times 24 tensor를 final prediction에서 return합니다! 2.1 Network Design GoogleNet 모델 기반 24 Convolution layer, 2 Fully Connected layer 인셉션 대신 1x1 reduction layer를 넣어 네트워크의 파라미터를 줄였습니다 Fast YOLO는 9개의 Convolution layer 2.2 Training 참고 슬라이드 : YOLO CVPR 2016 ImageNet 1000-class dataset을 사용해 20개의 Convolution layer pretrain pretrain 이후 4 convolutiona layer와 2 fully connected layer 추가(with randomly initialized wieghts) Detection에선 세밀한 정보가 필요하기 때문에 input size 224x224를 448x448로 증가 Bounding Box width, height를 이미지의 width, height로 0~1로 Normalization Bounding Box의 x, y는 특정 grid cell의 offset값 사용 final layer에서 linear activation function 사용 MSE보다 쉽게 최적화할 수 있는 SSE(Sum-Squared Error)를 최적화 합니다 대부분이 object가 존재하지 않기 때문에 0으로 많이 차게 됩니다. 이런 상황이라면 해당 셀의 confidence score가 0으로 수렴하며 object가 포함된 셀의 gradient를 압도할 수 있습니다. 이로 인해 학습이 불안해질 수 있습니다 위 문제를 해결하기 위해 bounding box coordinate predictions의 loss를 증가시키고 confidence predictions for boxes that don’t contain objects의 loss를 감소시킵니다 이 때 사용하는 2개의 파라미터가 \\lambda_{coord}와 \\lambda_{noobj}! \\lambda_{coord}=5 \\lambda_{noobj}=.5 \\lambda_{coord} : coordinates(x,y,w,h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter \\lambda_{noobj} : obj가 있는 box와 없는 box간에 균형을 위한 balancing parameter. (일반적으로 image내에는 obj가 있는 cell보다는 obj가 없는 cell이 훨씬 많으므로) SSE는 큰 BB와 작은 BB의 error를 동일하게 평가합니다. 큰 BB가 중요하다는 것을 반영하기 위해 bounding box의 width와 height sqaure root를 사용합니다 Batch size : 64 Momentum : 0.9, decay of 0.0005 Learning Rate : 0.001 -&gt; 0.01로 상승(epoch마다) 75 epoch동안 0.01, 30 epoch동안 0.001, 30 epoch동안 0.0001 Dropout : 0.5 data augmentation : random scaling and translations of up to 20% of the original image size Loss Function 출처 : 박진우님 블로그 2.3 Inference 참고 슬라이드 : Deep System’s YOLO 2.4 Limitations of YOLO 큰 object와 작은 object의 중심이 비슷할 경우, 둘 중 1개도 인식하지 못하는 경우가 있으며, loss function에서 큰 box에 제곱근을 취하지만 여전히 작은 물체에게 불리한 구조입니다 grid cell이 1개의 클래스만 예측하기 때문에 작은 object가 여러 개 있는 경우 제대로 구분하지 못합니다 학습시 사용한 bounding box의 형태가 아닐 경우(=새로운 bounding box) 예측할 때 어려움이 생깁니다 작은 BB와 큰 BB의 error를 동일하게 처리하므로 잘못된 localization을 발생합니다 fully connected layer를 2번 태워서 x,y,w,h를 맞추려고 하는데 이 값은 애초에 맞추기 힘든 값입니다. 4. Experiment 특이한 것은 Fast R-CNN + YOLO의 앙상블이 가장 점수가 높은 것 YOLO는 배경에 대해 판단하지 않으며, Localization이 되지 않습니다 동시에 나온 것은 Fast R-CNN을 사용하고 Fast R-CNN만 나오면 지워버리는 식으로(false-positive) 앙상블 Reference PR12: 전태균님의 PR-016 박진우님 블로그",
    "tags": "paper data",
    "url": "/data/2018/05/02/YOLO-You-only-look-once-review/"
  },{
    "title": "머신러닝 알고리즘 Cheat Sheet",
    "text": "각종 머신러닝 알고리즘의 Cheat Sheet입니다! 매번 검색하기 번거로워 인터넷에 있는 자료들을 가지고 왔습니다 Dummies 자료 Algorithm Best at Pros Cons Random Forest Apt at almost any machine learning problemBioinformatics Can work in parallelSeldom overfitsAutomatically handles missing valuesNo need to transform any variableNo need to tweak parametersCan be used by almost anyone with excellent results Difficult to interpretWeaker on regression when estimating values at the extremities of the distribution of response valuesBiased in multiclass problems toward more frequent classes Gradient Boosting Apt at almost any machine learning problemSearch engines (solving the problem of learning to rank) It can approximate most nonlinear functionBest in class predictorAutomatically handles missing valuesNo need to transform any variable It can overfit if run for too many iterationsSensitive to noisy data and outliersDoesn’t work well without parameter tuning Linear regression Baseline predictionsEconometric predictionsModelling marketing responses Simple to understand and explainIt seldom overfitsUsing L1 &amp; L2 regularization is effective in feature selectionFast to trainEasy to train on big data thanks to its stochastic version You have to work hard to make it fit nonlinear functionsCan suffer from outliers Support Vector Machines Character recognitionImage recognitionText classification Automatic nonlinear feature creationCan approximate complex nonlinear functions Difficult to interpret when applying nonlinear kernelsSuffers from too many examples, after 10,000 examples it starts taking too long to train K-nearest Neighbors Computer visionMultilabel taggingRecommender systemsSpell checking problems Fast, lazy trainingCan naturally handle extreme multiclass problems (like tagging text) Slow and cumbersome in the predicting phaseCan fail to predict correctly due to the curse of dimensionality Adaboost Face detection Automatically handles missing valuesNo need to transform any variableIt doesn’t overfit easilyFew parameters to tweakIt can leverage many different weak-learners Sensitive to noisy data and outliersNever the best in class predictions Naive Bayes Face recognitionSentiment analysisSpam detectionText classification Easy and fast to implement, doesn’t require too much memory and can be used for online learningEasy to understandTakes into account prior knowledge Strong and unrealistic feature independence assumptionsFails estimating rare occurrencesSuffers from irrelevant features Neural Networks Image recognitionLanguage recognition and translationSpeech recognitionVision recognition Can approximate any nonlinear functionRobust to outliersWorks only with a portion of the examples (the support vectors) Very difficult to set upDifficult to tune because of too many parameters and you have also to decide the architecture of the networkDifficult to interpretEasy to overfit Logistic regression Ordering results by probabilityModelling marketing responses Simple to understand and explainIt seldom overfitsUsing L1 &amp; L2 regularization is effective in feature selectionThe best algorithm for predicting probabilities of an eventFast to trainEasy to train on big data thanks to its stochastic version You have to work hard to make it fit nonlinear functionsCan suffer from outliers SVD Recommender systems Can restructure data in a meaningful way Difficult to understand why data has been restructured in a certain way PCA Removing collinearityReducing dimensions of the dataset Can reduce data dimensionality Implies strong linear assumptions (components are a weighted summations of features) K-means Segmentation Fast in finding clustersCan detect outliers in multiple dimensions Suffers from multicollinearityClusters are spherical, can’t detect groups of other shapeUnstable solutions, depends on initialization Microsoft Azure Machine Learning 자료 Reference Dummies Microsoft Azure Machine Learning",
    "tags": "ml data",
    "url": "/data/2018/05/01/maching-learning-cheat-sheet/"
  },{
    "title": "네이버 AI 해커톤 2018 참가 후기",
    "text": "네이버에서 4월 26일-27일 진행한 네이버 AI 해커톤 2018 결선 참여 후기입니다! 네이버 AI 해커톤 “한계를 넘어 상상에 도전하자!” 라는 슬로건과 함께 네이버 AI 해커톤이 열렸습니다. 참여 신청을 받고, 그 중 Github 랭킹이 높은 100팀과 랜덤으로 100팀을 선정해 예선전을 참여하도록 했습니다 그 후, 예선 1라운드와 2라운드를 진행해 미션별 상위 40팀을 대상으로 해커톤 결선을 진행했습니다! 중복 참여한 팀이 있기에 대략 50팀정도 결선에 참여했습니다 결선 기간동안 sli.do 채널을 운영해 참가자와 운영측의 소통을 진행했습니다! 저는 열리자마자 날씨 기상캐스터를 했습니다 날씨에 민감한 편이라(특히 바람) 이렇게 날씨를 알려드렸는데 운영측에서 좋게 봐주셨습니다!!! 그렇게 저는 프렌즈 스피커를 받았습니다 후후.. 감사합니다 해커톤 1일차(4/26) 인천에서 정자역까지 대략 2시간이 넘게 걸리기 때문에, 부지런히 일어나 네이버 그린팩토리로 향했습니다! 너무 부지런해서 10시 50분까지 집합인데 10시까지 도착했습니다 ㅋㅎ… 그린 팩토리의 1층과 2층 커넥트홀은 너무!! 좋았습니다 역시 네이버는 네이버다! 란 생각을 했습니다 11시쯤 참가자 분들이 많이 도착하셨습니다. 간단한 식사를 받고 커넥트홀에서 대기했습니다! 음료와 샌드위치 너무 맛있었어요! 식사와 함께 클로바 굿즈를 받았습니다. 초록초록한 볼펜과 연습장! 그리고 행사 때 사용할 명찰! 해커톤이 어떻게 진행되는지 알려주시며 후원해주신 인텔분의 인사도 듣고, 춘천 커넥트원으로 향했습니다! 커넥트원은 진짜!! 너무 시설이 좋은데 외부 사진을 찍으면 안되기 때문에 찍지 않았습니다. 정말 좋으니 꼭 가보세요! 해커톤 시작 도착하자마자 자유롭게 강당에서 해커톤이 시작되었습니다. 추가된 데이터에 대한 설명을 듣고 쭈욱-! 달렸습니다. 시작하기 앞서 날씨 기상캐스터인 저는 스피커를 받았습니다 ^_^ 또한 해커톤 기간동안 무리할 수 있는 손목을 위한 아이템을 준비해주셨고, 티셔츠도 받았습니다!! 간식이 정말 많았습니다!!! 심지어 핫식스가 다 떨어진 후, 요청하니 직원분들이 사다주셨어요.. 해커톤 중간에 떡볶이, 순대를 간식으로 먹었습니다! 이건 숙소 내부 사진입니다. 숙소가 정말 좋아요!!! 호텔급이라 감탄했습니다(혹시 사진 내려야하면 연락주세요!!!) 이건 첫 날 저녁! 삼겹살 스테이크가 정말 맛있었어요!!! 이건 그리고 10시에 먹은 치킨… 거의 살만 디룩디룩 찌는 느낌 가득ㅋㅎ 인텔에서도 티셔츠를 주셨습니다!! 인텔에서 네이버 클라우드의 베이스를 지원해주고 있는데, 점점 많은 분들이 알 수 있길!!!! 해커톤 2일차(4/27) 9시에 아침을 먹고, 12시까지 해커톤을 진행했습니다. 참고로 저는 5시에 자서 아침을 스킵했어요…. 그리고 2일차 점심!!!! 점심을 먹은 후 12시 50분부터 김성훈 교수님의 멘트와 시상식이 이루어졌습니다! 1등을 하신 Team Sadang, HCC분들 축하드려요^0^ 그리고 다시 춘천에서 네이버 그린팩토리로 이동했습니다!!! 해커톤 관련 내용 위 글에선 그냥 해커톤을 진행했다! 라고만 작성했는데 조금 더 자세히 작성해보겠습니다. 운영 운영하시는 분들이 정말 많은 준비를 하셨다는 것이 느껴졌어요! 재미있는 멘트도 많이 해주시고, 전체 가위바위보 게임을 통해 선물을 주셨어요 ㅋ_ㅋ 더 발전된 행사를 기대합니다 /ㅁ/ NSML 우선 NSML이란 네이버에서 야심차게 준비하는 시스템을 사용해 해커톤을 진행했습니다. 로컬에서 모델을 만든 후, 명령어를 입력해 NSML로 던져버립니다. 그리고 모델이 학습되며 학습된 모델을 제출(submit)해서 리더보드에 점수를 받을 수 있습니다. (사용시 크레딧을 소모합니다) NSML에 대한 프로그램 자체는 정말 좋습니다. 모델에 집중하고 계속 던져볼 수 있습니다. 그러나 해커톤 당일 MongoDB Error가 계속 나와서 저희 팀은 6시간동안 submit을 1번도 못했습니다. 초반 의욕이 살짝 낮아져서 아쉽지만, 정상화가 된 후부터 새벽 5시까지 달렸습니다!! (NSML 팀분들 정말 고생하셨습니다..!!!ㅠㅠ 다음엔 프로그램이 더 안정적일거라고 믿습니다!!) 그리고 NSML을 사용하고 싶으시면, 현재 제일 빠른 방법은 네이버에 취업하는 것 같습니다^_^(아직 사내에도 모두 공개되진 않았다고 들었지만..) 멘토링 멘토님들이 상주해 계셔서 모델에 대한 피드백을 들을 수 있었습니다. 사실 저희 팀은 멘토님과 커뮤니케이션을 많이 하지 않았습니다. 이게 아쉬운 점으로 남더라구요! (찾아가는 멘토링을 해주신 최경호 멘토님 정말 감사합니다!) 다른 팀분들은 모델에 대한 여러 팁을 듣고, 진로 상담도 하신 분들도 있었습니다! 채용 부스 해커톤이 진행되는 강당 옆에 클로바의 하정우님이 채용 상담을 해주셨습니다. 저희 팀도 함께 찾아갔었는데, 친절하게 맞이해주셔서 정말 좋았습니다! 요새 퇴사하고 어떻게 방향을 잡을까 고민했는데, 정우님이 말씀을 듣고 조금 더 진행 방향을 구체화했습니다! 삽질을 더더더! 해보겠습니다!_! 현재 인턴, 정규직 정말 많이 뽑고있다고 합니다. 관심있으신 분들은 clova-jobs@navercorp.com로 메일 주시면 됩니다! (정우님 이렇게 하면 되는거죠?_?) 물론 저는 네이버와 아무 관계가 없습니다^_ㅠ 느낀 점 모델에 치중하는 것도 좋지만, 데이터 전처리에서 Data Augumentation을 조금 더 시도해볼 걸! 라는 아쉬움. 막판에 데이터를 부풀려서 성능이 향상되었습니다 음소, 음절 단위의 분석이 형태소 분석보다 좋은 경우도 있음! 논문에 나오는 모델이 항상 Best는 아니고, 주체적으로 아이디어 생각해보기 옆 팀분들, 멘토님과 더 많은 커뮤니케이션해보기 옆 팀이였던 DSL팀, 뒷쪽에 계셨던 AI를 사용하여 가장 높은 성과가 예측되는 팀명을 만들었습니다팀 분들이랑 대화를 해보지 못해서 아쉽네요! (Submit이 안되서 약간 여유가 없었네요ㅠ.ㅠ) 대화를 통해 더 나은 모델이 생각나는 경우도 있으니 추후에 참여하시는 분들은 꼭 대화해보세요! 한국에서 이런 대회가 더 많이 열렸으면! 학사, 석사, 박사, 직장인 등 다양한 분들이 대회를 참여했는데, 국내에서 이런 자리가 처음이여서 너무 흥미롭고 재밌었어요. 앞으로도 계속 열리길! 나이가 들수록 밤샘이 힘들다.. 체력을 키우자 최종 등수 예선 1차 4등, 2차 7등이었는데 결선에서 9등이 되었네요. 마감 1시간 전까진 12등이었다가 마감 10분 전에 submit이 되서 막판에 10등 안에 들었습니다! 조금 더 높은 등수를 받으려고 했는데 살짝 아쉬움이 남네요. 하지만 2일간 삽질하면서 체득한 내용이 추후 제게 도움이 될거라 믿습니다 :) 결론 해커톤 준비하시느라 고생하신 분들 모두 감사합니다! 해커톤 참여하신 모든 분들 고생하셨습니다! 푹 쉬세요 :)",
    "tags": "lecture etc",
    "url": "/etc/2018/04/27/naver-ai-hackathon-review/"
  },{
    "title": "Jupyter Notebook에서 Scala 사용하기(In Mac)",
    "text": "Jupyter Notebook에서 Scala를 사용하는 방법에 대해 포스팅 해보겠습니다! Zeppelin에서도 Scala를 사용할 순 있지만, 데이터 분석가는 Jupyter Notebook이 편하기 때문에..! Jupyter Notebook에 Scala 커널을 추가하고 싶었습니다(환경은 Mac OS입니다) 만약 웹에서 Scala를 사용하고 싶으시면, ScalaFiddle을 사용하시면 될 것 같습니다 Intro 찾아보니 Github에 jupyter-scala라는 repository가 있었습니다! 대체할 수 있는 Toree나 Zeppelin에 비해 다재다능하며, 큰 데이터 프레임워크에 즉각적으로 추가할 수 있는 장점이 있다고 합니다(사실 이 부분에 대해 명확한 차이는 보이지 않네요! 추후 찾아봐야 겠습니다) Toree로 설치하고 싶으신 분들은 박준영님의 포스팅을 참고하면 좋을 것 같습니다 :) Install Scala Kernel (Mac) Install sbt 설치되어 있다면 생략 brew install sbt Install jupyter-scala git clone https://github.com/alexarchambault/jupyter-scala.git cd jupyter-scala sbt publishLocal 시간이 좀 많이 걸렸습니다(5분-10분 사이) ./jupyter-scala –id scala-develop –name \"Scala (develop)\" –force 이건 5분 이내로 끝났습니다 그 후 아래 명령어로 커널이 존재하는지 확인해보겠습니다 jupyter kernelspec list &gt;&gt;&gt; Available kernels: scala /Users/byeon/Library/Jupyter/kernels/scala python3 /usr/local/share/jupyter/kernels/python3 위와 같이 scala가 있으면 끝! Install Scala Kernel (Windows 10) Download Git Source git clone https://github.com/jupyter-scala/jupyter-scala 또는 Download ZIP 클릭 Unzip 압축 해제 Run Script 압축 해제한 폴더로 간 후, (bash) . jupyter-scala Jupyter Notebook 킨 후 커널 확인 문제 생길 시 issues를 참고! After Install Scala Kernel 위와 같이 Scala가 생깁니다 :) Scala로 이것저것 Test해봤습니다!! 이제 다음 글에서 Spark를 Jupyter Notebook에서 해보겠습니다!!!! Reference jupyter-scala",
    "tags": "scala development",
    "url": "/development/2018/04/25/scala-in-jupyter-notebook/"
  },{
    "title": "Little Big Data #1 : 다양한 사람들의 데이터 사이언스 이야기 후기",
    "text": "Little Big Data #1 : 다양한 사람들의 데이터 사이언스 이야기 세션 후기입니다! (발표자가 쓰는 행사 후기^_^…) Intro Little Big Data : 다양한 사람들의 다양한 데이터 사이언스 이야기! Apache Zeppelin을 만드는 Zepl에서 진행합니다! LETS(Local Exchange Trading System)을 통해 네트워킹을 진행합니다! BrewDog의 후원을 받았습니다! 한국어 채팅 데이터로 머신러닝 하기 조한석님, 스캐터랩 머신러닝 엔지니어 채팅 데이터를 분석해서 썸 혹은 연애를 과학적으로 분석 발표에서 다루는 내용 한국어 채팅 데이터를 다룬 경험 상황 문제 해결의 방식 한국어 자연어 처리의 어려움 Hell 조사 언어의 변형이 자유로움 커플 채팅 데이터의 어려움 혀 꼬인 소리 맞춤법 혹은 띄어쓰기 채팅에서만 할 수 있는 표현 Preprocess 형태소 분석기 KoNLPY, 꼬꼬마, 트위터, 한나눔 분석기 채팅 데이터를 형태소 분석기에 맞지 않음 ㅠ_ㅠ 형태소 분석기 학습에 사용된 데이터(corpus)의 단어 분포가 채팅 데이터의 단어분포와 다르기 때문에 잘 되지 않음 Sejong Corpus는 정말 바르고 깔끔한 데이터 형태소 분석기 대안 합리적인 기준으로 패턴을 찾고 패턴으로 preprocess 일종의 normalize(표준화) tokenizing과 띄어쓰기 교정이 필수!!! 띄어쓰기 교정 연속된 글자가 주어졌을 때, 그 다음에 띄어쓰기를 할지 말지를 결정하는 Binary Classification 문제 아이디어 : 오류가 적다고 판단되는 데이터를 선택한 뒤, 전체 데이터에서 조금 등장한 패턴을 자주 등장한 패턴으로 수정 데이터에 존재하지 않는 패턴(=어휘)은 잘못 띄어 쓰게 될 확률이 높음 Sequential labeling은 연산량이 많아서 사용하기 힘듬 휴리스틱한 알고리즘을 사용! Tokenizing 여러 단어로 이루어진 문장 혹은 어절에서 단어를 구분하는 것 영어의 경우 띄어쓰기 단위로 token이 나뉘지만 한국어는 그렇지 않음 token : 의미를 가진 최소 단위 의미를 가진 실질 단위를 단어라고 생각한다면 tokenizing 문제는 문장에서 단어를 추출하는 문제로 생각할 수 있음 Cohesion Probability : 연속된 글자의 연관성이 높을수록 단어일 가능성이 높음 김현중님 파이콘 2017 참고하면 좋습니다! Word Embedding - Word2Vec 단어 정보를 이용해 단어를 vector로 변환 word2vec은 쉽게 사용할 수 있지만 OOV(Out-of-vocaburary) 문제가 있음 train시는 문제가 되지 않지만 inference 단계는 문제가 생김 Fasttext word2vec과 유사하지만 두 단어간의 점수 측정하는 부분이 다름 substring 정보를 이용하는 방법(word2vec은 dot product) 글자(character) 단위의 subword를 사용 자모 단위의 subword를 사용 글자 단위보다 자모 단위가 더 작으므로, OOV 문제에 대해 더 유연하게 대처할 수 있음 오타를 잡는 알고리즘을 만들 수 있음 Sentence Similarity 자연어 처리에서 주로 다루는 문제는 두 document간의 유사도를 비교하는 문제 커플 채팅데이터가 일반 document와 다른 점은 short sentence라는 점 BOW + Word Embedding word embedding의 성능에 큰 영향 채팅 데이터에서 학습시킨 word embedding 결과가 이상해 유사한 단어 != 뜻이 비슷한 단어 RNN RNN 본질은 language modeling 주어진 sequence에서 다음에 올 단어 혹은 글자를 예측하는 방향으로 embedding Seq2Seq 같은 output이 나오는 방향으로 embedding이 됨 Term vector 애매하게 embedding될 바에 term vector로 표현! 단어의 변화에 민감. 짧은 문장은 문제가 더 생김 단어의 의미 정보를 이용할 수 없음 ESA Similarity (Explicit Semantic Analysis) 명시적인 정보(=word vector)를 이용 기타 한국어 자연어 처리는 preprocessing이 80% 이상 Zipf’s law corpus에서 나타나는 단어들을 사용빈도 순으로 나열하면, 사용빈도와 해당 단어의 수는 반비례 빈도수가 적은 단어는 과감히 쳐내자 문제 정의를 잘 하는 것이 모델을 구현하는 것보다 중요 현실에서 마주치는 문제는 복합적인 요소들이 작용 기술적 뛰어난 모델 적용하는 것도 좋지만, 간단한 통계 혹은 count based 모델을 사용하는 것이 효과적일 수도 있다 unlabeled data에 직접 label을 달아보자 피자 타임!! 피자와 BrewDog의 맥주를 즐겼습니다! 딥러닝에 필요한 로그 기깔나게 잘 디자인하는 법 백정상님, 구글 클라우드 엔지니어 지난내용 복습 뭘 쌓야아 할지 몰라서 뭐든 쌓아봤어요 로그인 액티비티를 로비 입장시에 찍었어요 클라이언트 리스펀스를 로그 대용으로 썼어요 데이터는 JSON으로 쌓았어요 원시 데이터 바로 분석 쿼리를 해요 딥러닝이 이걸 해결해줄까? 통계적으로 풀지 못하는 문제가 생김 비정상적인 데미지를 만드는 플레이어가 핵 유저인지 알고 싶다 고전적인 머신러닝 기법으로 풀려고 보니 바운더리가 필요한데, 어떤 데미지가 비정상적인지 정상인지 알 수 없음 비정상 데미지를 딥러닝으로 알아보는 방법 정상 데이터가 많으면 충분히 학습시키고 비정상 데미지일 가능성 예측 많지 않으면 딥러닝 모델에 모든 데미지 데이터를 학습시키고 유저를 클러스터링해서 아웃라이어를 잡자 문제를 풀어보자 가설 : 플레이어가 게임을 플레이할 때, 같은 환경에서 다른 유저들이 평균 최대 데미지에 비해 많은 데미지를 만드는 유저 그룹이 뭔가 있을 것이다. 그리고 이들은 적을 것이다 피쳐 엔지니어링 유저인덱스는 너무 세분화니 제외 스테이지별 max data 캐릭터별 밸런스 격차가 있으므로 캐릭터 넣기 허나 위에껀 상상이었음ㅋ 실제 생긴 고민들 무슨 모델을 쓸 것인가 선정하지 않았음 클러스터링 모델도 다양 꼭 딥러닝을 써야하는가? 오토인코더 stageIdx, charIdx, maxDamage -&gt; Auto Encoder -&gt; Loss가 확 튐 아까의 가정이 참이어야 이것을 쓸 수 있음 카드 부정사용 오토인코더로 활용하고 있습니다 피쳐 엔지니어링은 어떻게 하지? Json Raw 데이터 중요한 부분만 뽑아야 함 numeric 데이터만 추출 온라인 트레이닝 / 배치 프로세싱 상황에 따라 다름 저장된 로그에 배치 잡을 돌리고 실제 필요한 건 데이터 중 일부 모든 데이터를 다 때려넣으면 안됨! 구글만큼 데이터가 많으면 괜찮지만.. 아님 전투 로그가 엄청 많음 EDA 탐색적 데이터 분석 게임상에 있는 정보를 기반으로 EDA를 해서 게임상에서 일어나는 특이한 패턴을 찾는데 주력! 장비 레벨에 따른 데미지는 상관관계가 있으니 꼭 넣어야 함 이벤트 유무 어떤 feature가 상관관계가 높은지 알 수 없음 사후 분석!! 이벤트가 일어난 시점의 데이터를 기준으로 저장해야 합니다 로그 디자인 JSON이 적당함 Nested, Repeated 정보를 어떻게 잘 보관하고 검색할 지 고민 게임은 reward 데이터의 경우 flat하면 개수가 달라짐. Nested한 데이터를 잘 저장하는 DB를 사용해야 함 데이터 관리 기존보다 저장 데이터 용량은 더욱 커질 수 밖에 없음 트레이닝 데이터 사이즈를 줄여보는 것도 방법 콜드 데이터는 비용 절감에 주력 feature는 최대한 줄이고 차원 축소 데이터 검증 관리할 로그 종류와 데이터 타입이 많아짐 테스트 기반 검증을 진행 모든 로그 데이터는 검증 로직 테스트를 게임 업데이트가 가능하도록 해야 함! 결론 딥러닝은 모델보다 피쳐가 생명 어떤게 더 중요한지 모르니 모든걸 로그로 쌓고 EDA를 빡세게 해서 특이한 패턴을 찾고 그 패턴을 기반으로 모델 학습 데이터는 테스트 기반 validation 계속해서 시도하고 실패하다보면 원하시는 모델이 나옵니다! 바닥부터 시작하는 데이터 인프라 제 발표를 제가 리뷰하는 것은 민망하니 패스 많은 분들에게 도움이 되었길..! 개인적으론 아쉬움이 남네요(하나의 분야를 자세히 이야기할 걸 그랬나..) 발표 자료 게임회사 주니어 웹 개발자가 바라본 데이터 분석 이야기 파이썬 덕후인 준범님. 넥슨의 어뷰징 탐지 팀! 개인적으로 가지고 있는 프로젝트에서 절대 터질 일 없는 슬픈 서버를 가지고 있었음 게임 회사의 데이터 로그가 엄청 많음! IDC, AWS 대시보드를 만들어 주세요! R Shiny를 사용하고 있음 장고를 사용해 만들었는데 대시보드가 느리다는 클레임이 들어옴 로그 저장소(JSON, Parquet) -&gt; MySQL Aurora에 데이터 적재 Query Optimize를 했는데 엄청 느림. 그래서 날 쿼리로 작성해봄 날 쿼리도 느림ㅠㅠ 인덱스는 타고 있는가? Table Full-SCAN을 하고 있네???? MySQL 등 DB는 인덱스가 있더라도 내장 Optimizer가 인덱스를 무시 강제로 인덱스를 타게 하면 더 빨라질까? 데이터 분석 Task 할당 어뷰징 탐지팀인데 어뷰징은 무엇인가? Fair-Play를 해치는 경우 가계정을 만들어 랜덤 상자 부정 획득 다른 유저의 계정 도용 작업장 핵 장르까지 고려 PC / Mobile 등의 플레이 환경 게임 특성에 따라 조금씩 다른 판별기준 평소와 다른 장소에서 로그인했다면? PC 게임인데 지역이 달라짐 -&gt; 뭔가 이상한데? 추석이면 인정 모바일이면 이동할 때마다 위치가 바뀌므로 정상 게임 장르별 다른 어뷰징 MMORPG FPS 게임 하나하나에 특화된 모델을 만드느냐 vs 비슷한 장르에 확장 가능한 모델을 만드느냐 정량적 접근 vs 정성적 접근 Case 1. 신규 게임이 오픈했는데 어뷰저가 있는 듯? 횡적으로 확장 가능한 모델을 사용 접속기록, 계정명, 플레이타임 등 모든 게임에 다 남고있는 것을 참고 너무 뻔한 어뷰징은 제거 공통 형식으로 로그가 남고 있어서 그대로 적용 가능 이것만으로는 모두 잡을 수는 없음 Case 2. 게임 특성에 대한 이해 어떤 어뷰징이 있는지 모르겠음! 매크로 핵 커뮤니티가 존재하나 등업하기엔 어려움 유저들의 신고를 참고해서 유추 진행 과정 json, parquet을 사용 for문을 사용해서 함수를 적용하니 작업이 안끝나…. 분산 처리를 해보자! Spark, EMR, Cluster PySpark 수많은 로그에서 찾아내는 인사이트는 아니었음 AWS Lambda와 함께한 서버리스 아키텍처 모델의 크기가 제약 패널 토크 윤진석님, 김진중님, 나해빈님, 이문수님과 발표자분들의 패널 토크 시간이었습니다 살다보니 제가 패널 토크도 해보네요.. 생각보다 1시간이 짧았습니다. 데이터 관련 이런 저런 이야기를 주고 받는 시간이었습니다!!! 네트워킹 자리에 계신 분들이랑 많은 대화를 나누었습니다. 도메인이 다른 분들이랑 대화하는 것은 항상 즐거운 것 같아요! 게다가 페이스북에서만 봤던 분들을 직접 볼 수 있어서 정말 좋았습니다 :) 후기 발표자로 참여했지만 다른 분들의 이야기가 정말 흥미로웠습니다! 앞으로도 이런 행사가 자주 생기면 좋겠습니다!!! 제플 짱 &gt;_&lt;/ 후드 진짜 이뻐서 동네 독서실에서 돌아다닐 때 맨날 입고다닐 것 같아요!!!",
    "tags": "lecture etc",
    "url": "/etc/2018/04/21/little-big-data/"
  },{
    "title": "나이브 베이즈 분류기(Naive Bayes Classifier)",
    "text": "카이스트 문일철 교수님의 인공지능 및 기계학습 개론1 3주차 강의를 보고 정리한 포스팅입니다! Optimal Classification 나이브 베이즈 분류기를 배우기 위해서 먼저 Optimal Classification에 대해 알아야 합니다 위 그래프는 Y Class가 초록색일 경우와 빨간색일 경우를 분류하는 분류기의 모습을 보여줍니다. 즉, 주어진 조건 X에 대한 확률 Y로 나타납니다. X가 왼쪽 부분이라면 초록색일 확률이 높으며 X가 우측 부분이면 빨간색일 확률이 높다고 판단합니다. 이것을 식으로 나타내면 주어진 조건 X에서 Y값이 최대가 되는 확률 분포 함수가 Optimal Classification입니다 위 그림은 처음에 봤던 실선 분류기와 직선(점선) 분류기가 같이 있는 그래프입니다. Decision Boundary를 기준으로 좌측에 있을 경우엔 초록색으로 분류되어야 합니다. 이 경우 빨간색일 확률값들은 Error가 됩니다 여기서 Bayes Risk가 나타납니다. Error의 면적에 대한 함수를 뜻하며, R(f^{*})로 표기합니다. 결국 주어진 X값에 대해 Error가 낮은 분류기를 찾는 것이 Optimal Classification입니다. 선형 그래프는 Bayes Risk의 큰 차이를 갖지 않고, 실선 그래프는 상대적으로 큽니다. 따라서 실선으로 모델링하며, error도 실선이 낮습니다 Bayes Classifier를 식으로 표현하면, f^{*}=argmin_{f}P(f(x)\\neq Y) 여기서 f(x)\\to\\hat{y} error을 최소화하기 위해 function approximation을 합니다. Y의 클래스가 2개라고 가정하면 f^{*}(x) = argmax_{Y=y}P(Y=y\\mid X=x) 참고로, 2개의 클래스라면 argmax도 성립되며 given random variable을 switch하기 위해 베이즈 이론을 사용하겠습니다 argmax_{Y=y}P(X=x\\mid Y=y)P(Y=y) Class Prior인 P(Y=y)는 MLE, MAP로 구할 수 있습니다 Likfelihood(Class Conditional Density)인 P(X=x\\mid Y=y)도 쉽게 계산할 수 있습니다 Optimal Classifier는 조건이 1개라면 문제가 되지 않지만, 조건이 많아지면 그 조합(Combination)만큼 문제가 될 수 있습니다. 이것은 나이브 베이즈 분류기가 해결합니다 Naive Bayes Classifier 위에서 나온 Optimal Classification의 argmax_{Y=y}P(X=x\\mid Y=y)P(Y=y)를 계산하려고 하면 몇 개의 파라미터가 필요할까요? P(Y=y) for all y : k-1개가 필요합니다. T를 알면 F를 알 수 있는 것처럼! P(X=x\\mid Y=y)는 (2^{d}-1)k개가 필요합니다. 이 파라미터는 너무 많아서 joint를 모두 해결하기 힘들어집니다 x는 vector value며 vector의 길이는 d로 표현합니다. d를 줄이면 가능하긴 하지만, d를 줄이지 않는 방법은 어떻게 해야할까요? 이것을 나이브 베이즈 분류기가 특정한 가정을 추가해 해결합니다 Conditional Independence Conditional Independence(조건부 독립)을 적용합니다. 각각의 X 변수들이 서로 영향을 미치지 않는 독립적인 관계임을 가정합니다. 독립적일 경우 아래와 같이 표시합니다 P(x,y) = P(x)P(y) y가 주어졌으면 x끼리 independence합니다! 라는 순진한(Naive) 가정을 합니다. 이를 통해 파라미터 개수를 줄일 수 있습니다 추가적으로 P(x_{1}\\mid x_{2},y)=P(x_{1}\\mid y)하면 x_{1}와 x_{2}는 independence! P(x_{1}, x_{2}\\mid y)=P(x_{1}\\mid y)P(x_{2}\\mid y) Conditional vs Marginal Independence 위 이미지에서 Commander가 Go라고 명령했을 때 Officer A와 B의 상태를 보여줍니다 A가 명령에 대해 모를 경우 B가 움직이는 것을 보고 A가 “Commander가 움직이라 했나보다”라고 생각하는데, 이것은 Commander에 대한 정보가 없을 때, B의 정보가 A에게 영향을 주는 것을 뜻합니다! 즉, Independence가 아닙니다 반면 A가 Commander의 말을 들었다면 B가 움직이던 말던 전혀 A에게 도움이 되지 않습니다. 즉, Marginal Indepedence는 인스턴스간 성립되지 않아도 Conditional Independence는 정의될 수 있습니다 다시 돌아와서 조건부 독립을 적용하면 f^{*}(x)= argmax_{Y=y}P(X=x\\mid Y=y)P(Y=y) \\approx argmax_{Y=y}P(Y=y)\\prod_{1\\le i\\le d}P(X_{i}=x_{i}\\mid Y=y) P(X_{i}=x_{i}\\mid Y=y)는 이제 (2-1)dk개의 파라미터가 필요합니다. (1개를 알면 조건부 독립에 의해 다른 1개를 유추할 수 있습니다) 결론적으로 아래와 같이 정리할 수 있습니다 Problem of Naive Bayes Classifier Naive Assumption 현실의 변수들은 대부분 서로 상관관계를 가지고 있습니다. 나이브 베이즈 분류기는 이런 현실을 올바로 반영하지 못합니다 Logistic Regression은 이런 문제를 없애려고 합니다 Incorrect Probability Estimations MLE를 사용해 확률값을 구한다면 관측을 하지 못한 경우는 Estimation을 할 수 없습니다. 따라서 MAP를 사용해 개별 확률값을 구해줘야 합니다 이것은 항상 있는 문제! Text Mining에 적용한 나이브 베이즈 분류기 bag of words를 이용해 텍스트를 수치로 변경합니다 그 이후 수식을 기반으로 구하는 과정으로 강의를 진행하고 있습니다 단, 확률값을 계속 곱하기보다 수식에 log를 씌우면 곱셈이 덧셈이 되기 때문에 구현할 경우 많이 log를 씌우곤 합니다! 구현 TIP이니 알아두도록 합시다! 나이브 베이즈 분류기를 직접 만들어보는 것을 작은 과제로 설정! Reference 인공지능 및 기계학습 개론1",
    "tags": "ml data",
    "url": "/data/2018/04/20/naive-bayes-classifier/"
  },{
    "title": "규칙 기반(Rule-Based) Machine Learning 기초",
    "text": "카이스트 문일철 교수님의 인공지능 및 기계학습 개론1 2주차 강의를 보고 정리한 포스팅입니다! Machine Learning 경험(experience)에 의해 학습(learning) Task에 대해 점점 더 잘 실행되도록! 많은 경험이 있다면 더욱 잘 될 것입니다(=데이터가 많아지면!) Example Task EnjoySPT를 예측하는 Task Sky Temp Humid Wind Water Forest EnjoySPT Sunny Warm Normal Strong Warm Same Yes Rainy Cold High Strong Warm Change No 가정 우리는 Perfect World에 살고 있습니다 관측의 error는 없고, 일관적이지 않은 관측도 없습니다(=일관적이다) random effect가 없습니다 해당 factor로 상황을 완벽히 설명할 수 있습니다 Function Approximation Task를 잘 소화할 수 있는 함수를 만들어야 합니다! Instance, X Feature(Input) : Sunny, Warm, Normal, Strong, Warm, Same Label(Y) : Yes Training Dataset, D : 인스턴스의 관측값 모음들 Hypothesis, H h_{i} : &lt;sunny, warm, ?, ?, ?, Same&gt; -&gt; Yes! &lt; &gt;의 조건이면 Y다! 가정의 개수는 64+@(don’t care) Target Function, C : 알지 못하지만 목표로 하는 함수(데이터를 보고 알아내야 합니다) 다음과 같은 가설 h_{1}, h_{2}, h_{3} 3개와 관측값 x_{1}, x_{2}, x_{3}이 있습니다 h_{1} : &lt;Sunny, ?, ?, ?, Warm, ?&gt; h_{2} : &lt;Sunny, ?, ?, ?, Warm, Same&gt; h_{3} : &lt;Sunny, ?, ?, Strong, Warm, ?&gt; x_{1} : &lt;Sunny, Warm, Normal, Strong, Warm, Same&gt; x_{2} : &lt;Sunny, Warm, Normal, Light, Warm, Same&gt; x_{3} : &lt;Sunny, Warm, Normal, Strong, Warm, Change&gt; x_{1}은 가설 h_{1}, h_{2}, h_{3}에 모두 맞지만, x_{2}는 가설 h_{3}에 맞지 않습니다. h_{1}는 널널한 가설이며 h_{3}는 까다로운 가설입니다 이것은 Generalization vs Specialization 문제와 연결됩니다 Find S 알고리즘 간단한 Find S 알고리즘을 다음과 같이 정의하겠습니다 For instance x in D if x is positive for feature f in O if f_{i} in h == f_{i} in x Do nothing Else f_{i} in h = f_{i} in h U f_{i} in x return h 조금 더 쉽게 설명하면, 첫 가설은 Null Hypotheses로 만듭니다. h_{0}= &lt;\\phi, \\phi, \\phi, \\phi, \\phi, \\phi&gt; x_{1} : &lt;Sunny, Warm, Normal, Strong, Warm, Same&gt; 가 들어오면, Else문으로 가게 되서 아래와 같이 가설이 변경됩니다 h_{1} : &lt;Sunny, Warm, Normal, Strong, Warm, Same&gt; x_{2} : &lt;Sunny, Warm, Normal, Light, Warm, Same&gt;가 들어오면 새로운 경험을 받습니다. Strong해도 나가 노는구나! Strong U Light =&gt; ?로 변환됩니다 h_{2} : &lt;Sunny, Warm, Normal, ?, Warm, Same&gt; 이런 방법으로 가능한 범위(Version Space)를 찾아갑니다! General Boundary와 Specific Boundary 이 사이에 있는 공간을 찾습니다 Candidate Elimination Algorithm Version Space를 구하기 위해 위 알고리즘을 적용합니다 가장 maximally specific, 가장 general한 가설 사이에서 점점 좁혀가는 알고리즘입니다 For instance x in D If y of x is positive Generalize S as much as needed to cover o in x Remove any h in G, for which h(o)!=y If y of x is negative Specialize G as much as needed to exclude o in x Remove any h in S, for which h(o)=y 위와 같은 방법은 Perfect World에선 사용 가능하지만, 현실에선 사용하기 어렵습니다 noise가 존재 decision factor가 존재 가능 룰베이스 기반으론 현실을 반영하기 어렵습니다 위 사항을 해결하기 위해 나온 것 중 하나가 의사결정 나무(Decision Tree)입니다! Entropy Entropy를 이해해야 Decision Tree를 더 잘 이해할 수 있습니다 Entropy 어떤 attribute를 더 잘 check 할 수 있을지 알려주는 지표 불확실성(uncertainty)를 줄여야 합니다!(=높은 entropy) 주어진 상황을 특정 분포로 만들어진 random variable로 판단할 수 있습니다 H[X] = -\\sum_{x}P(X=x) \\log_{b} P(X=x) Conditional Entropy 특정 feature variable이 주어졌을 때의 Entropy 주어진 x에 대해 y의 entropy를 구하는 형태 H(Y\\mid X) = -\\sum_{x}P(X=x)H(Y\\mid X=x) = -\\sum_{x}P(X=x)\\{-\\sum_{y}P(Y=y\\mid X=x)log_{b}P(Y=y\\mid X=x)\\} Information Gain &lt;- A1(307+, 383-) -&gt; &lt;- A9(307+, 393-) -&gt; a(98+, 112-) b(206+, 262-) ?(3+,9-)   t(284+,77-) f(23+, 306-) H(Y) = -\\sum_{y\\in\\{+,-\\}}P(Y=y) \\log_{2} P(Y=y) P(Y=t) =\\frac{307}{307+383} H(Y\\mid A1) = \\sum_{X\\in\\{a,b,?\\}}\\sum_{Y\\in\\{+,-\\}}P(A1 = x, Y=y)\\log_{2}\\frac{P(A9=X)}{P(A9=X, Y=Y)} IG(Y,A_{i}) = H(Y) - H(Y\\mid A_{i}) 특정 condition A_{i} 조건이 주어졌을 때의 정보 이득의 양 앞의 사례에선 A9가 IG이 높습니다 따라서 A9를 root로 만들고 분리된 케이스에서 또 IG를 구하는 방시긍로 트리를 확장합니다 Decision Tree를 만드는 Algorithm ID3, C4.5, CART 등 다양하게 있는데, 여기선 ID3만 설명하겠습니다 initial open node 생성 채울 open node 선택 IG를 활용해 best variable 선택(T 또는 F) 인스턴스를 정렬해 브런치에 넣어줌 class와 label이 동일할 경우 종료 Problem of Decision Tree 디테일한 트리를 만들 경우엔 현재 데이터는 100% 맞을 수 있지만, 새로운 데이터는 예측을 못할 수 있습니다 앞에서도 말했듯이 현실은 noise, inconsistencies를 가지고 있기 때문에 앞으로 올 데이터는 못 맞출 수 있습니다 Linear Regression 이번엔 통계적 기반의 방식을 사용해보겠습니다 housing dataset 13 numerical independent values 1 numerical dependent value linear한 function으로 approximation 하는 것이 머신러닝에서 바라본 linear regression hypothesis를 function의 형태로 정의해보겠습니다 h:\\hat f(x; \\theta) = \\theta_{0} + \\sum_{i=1}^{n}\\theta_{i}x_{i} = \\sum_{i=0}^{n}\\theta_{i}x_{i} n = feature values의 개수 linear는 건들지 말고, \\theta를 잘 정의해보는 것이 목표! \\hat f = X\\theta X = \\begin{bmatrix} 1 &amp; \\cdots &amp; x_{n}^{1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; \\cdots &amp; x_{n}^{D} \\end{bmatrix}, \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\cdots \\\\ \\theta_n \\end{bmatrix} 현실의 noise를 반영하면, f(x; \\theta) = \\sum_{i=0}^{n}\\theta_{i}x_{i}+e = y \\to f=X\\theta+e=Y 이제 \\theta를 추정하기 위해 식을 작성해보겠습니다 \\hat\\theta=argmin_{\\theta}(f-\\hat f)^{2} = argmin_{\\theta}(Y-X\\theta)^2 argmin_{\\theta}(Y-X\\theta)^{T}(Y-X\\theta) = argmin_{\\theta}(Y-X\\theta)^{T}(Y-X\\theta) argmin_{\\theta}(\\theta^{T}X^{T}X\\theta-2\\theta^{T}X^{T}Y+Y^{T}Y) = argmin_{\\theta}(\\theta^{T}X^{T}X\\theta-2\\theta^{T}X^{T}Y) \\triangledown_{\\theta}(\\theta^{T}X^{T}X\\theta-2\\theta^{T}X^{T}Y) = 0 2X^{T}X\\theta-2X^{T}Y=0 \\theta = (X^{T}X)^{-1}X^{T}Y 현재 함수는 linear하기 때문에, 데이터의 끝 부분을 제대로 표현하지 못하고 있습니다. 이를 위해 x를 \\phi라는 함수를 거쳐 새로운 벡터를 만든 후, 이 벡터를 사용해 \\theta를 구해볼 것입니다. x^2, x^3, x^4를 계속 추가해서 만들면 non-linear한 형태가 나타납니다. 하지만 이렇게 끝 부분이 잘 맞는 것이 옳은 것일까요? 관측치가 1개뿐인데 이것을 맞추는 것이 옳을까? 나중에 이에 대한 답을 고민해 볼 예정입니다 Reference 인공지능 및 기계학습 개론1",
    "tags": "ml data",
    "url": "/data/2018/04/19/rule-based-machine-learning-overview/"
  },{
    "title": "머신러닝(Machine Learning)의 수학적 기초",
    "text": "카이스트 문일철 교수님의 인공지능 및 기계학습 개론1 1주차 강의를 보고 정리한 포스팅입니다! MLE(Maximum Likelihood Estimation) 압정을 던져서 앞면과 뒷면이 나오는 게임을 하고 있습니다. 압정을 5번 던져서 앞면(head)이 3번, 뒷면(tail)이 2번 나왔습니다. 이것을 아무것도 모르는 사람에게 설명해야 한다면 어떻게 해야할까요? Binomial Distribution(이항 분포) discrete한(이산적인) 사건에 대한 확률 분포입니다. ex) (앞, 뒤) 이것을 계속 실험해보는 것을 베르누이 실험이라고 합니다 iid(independent and identically distributed)를 가정하고 있습니다 각 이벤트는 독립적이며 동일한 분포를 가진다는 뜻 P(H)=\\theta, P(T)=1-\\theta 가지는 성질 : 항상 양수, 합하면 1 P(HHTTT) = \\theta\\theta(1-\\theta)\\theta(1-\\theta) = \\theta^{3}(1-\\theta)^{2} n=5, k=a_{H}=3, p=\\theta라고 할 경우, \\theta가 주어졌을 때, 데이터 D가 관측될 확률 P(D|\\theta) = \\theta^{a_{H}}(1-\\theta)^{a_{T}} 가정 : 압정 게임의 결과는 \\theta라는 확률 분포를 따른다 가정을 강하게 하려면? binomial 분포보다 맞는 분포를 제시 -&gt; 추후 강의 \\theta를 최적화(best candidate theta를 찾아 D를 설명) -&gt; MLE MLE 관측된 데이터들이 등장할 확률을 최대화하는 \\theta를 찾기! 어떤 모수가 주어졌을 때, 원하는 값들이 나올 Likelihood를 최대로 만드는 모수를 선택하는 방법입니다. 점추정 방식에 속합니다 수식 \\hat\\theta = argmax_{\\theta}P(D|\\theta) = argmax_{\\theta}\\theta^{a_{H}}(1-\\theta)^{a_{T}} = argmax_{\\theta}lnP(D|\\theta) = argmax_{\\theta}ln\\{\\theta^{a_{H}}(1-\\theta)^{a_{T}}\\} = argmax_{\\theta}\\{a_{H}ln\\theta+a_{T}ln(1-\\theta)\\} \\frac{d}{d\\theta}(a_{H}ln\\theta+a_{T}ln(1-\\theta)) = 0 \\frac{a_{H}}{\\theta} - \\frac{a_{T}}{1-\\theta} = 0 \\hat\\theta = \\frac{a_{H}}{a_{H}+a_{T}} 추가 질문 추가적으로 압정을 더 던져서 앞면이 30회 뒷면이 20번 나온 경우, 5번 던진 것과 확률상으론 같은데, 과연 5번 던진 것과 50번 던진 것과는 같을까요? 그에 대한 답은 “여러 번 시도하면서 파라미터 \\hat\\theta를 추론한 것이지, 확정된 값은 아닙니다. 계속 던져보며 error가 줄어든 것입니다!” P(|\\hat\\theta - \\theta^*| \\ge \\epsilon) \\le 2e^{-2N\\epsilon^{2}} 좌측의 \\epsilon(error bound)가 커질수록 우측의 확률은 작아집니다. 또한 우측의 N이 커질수록 우측 값이 작아집니다 이런 것들을 Probably Approximate Correct(PAC) Learning이라고 합니다. ex) 0.01% case의 probably에 \\epsilon=0.01의 Apporiximate? MAP (Maximum a Posteriori Estimation) MLE는 관측값에 따라 값이 너무 민감하게 변한다는 단점이 있기 때문에, 다른 관점으로 바라보자는 사람들이 점점 생겼습니다. 그 중 한명은 베이즈로, 사전 정보를 가미한 \\theta를 찾아보자고 했습니다 P(\\theta\\mid D) = \\dfrac{P(D\\mid\\theta)P(\\theta)}{P(D)} 좌측은 Posterior, 우측은 Likelihood * Prior knowledge / Normalizing constant Normalizing constant는 이미 주어진 사실이라 컨트롤을 할 수 없습니다. \\theta가 바뀌는 것에 영향을 줄 수 없기 때문에 수식에서 많이 생략하곤 합니다 따라서 우리도 이렇게 정리하겠습니다 P(\\theta\\mid D) \\propto P(D\\mid\\theta)P(\\theta) P(D\\mid\\theta) = \\theta^{a_{H}}(1-\\theta)^{a_{T}}라는 것을 이미 MLE에서 알게 되었는데요, 그렇다면 P(\\theta)는 무엇일까요? 이 값은 베타 분포를 따른다고 합니다! P(\\theta) = \\dfrac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}, {B(\\alpha, \\beta)}=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}, \\Gamma(\\alpha)=(\\alpha-1)! 다시 P(\\theta\\mid D) \\propto P(D\\mid\\theta)P(\\theta)를 정리하면, P(\\theta\\mid D) \\propto \\theta^{a_{H}}(1-\\theta)^{a_{T}} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} =\\theta^{a_{H}+\\alpha-1}(1-\\theta)^{a_{T}+\\beta-1} MLE에서 나왔던 모양과 비슷한데, \\alpha, \\beta가 존재할 뿐! \\alpha, \\beta를 조절해(=사전 정보) \\hat\\theta를 추출합니다 베이즈는 이것을 보고 a_{H}와 a_{T}가 점점 커지면, \\alpha, \\beta의 영향이 작아져서 결국 MLE와 MAP는 동일하게 될 것이라고 이야기했습니다 MLE와 MAP의 비교 MLE \\hat\\theta = argmax_{\\theta}P(D\\mid\\theta) \\to \\frac{a_{H}}{a_{H}+a_{T}} MAP \\hat\\theta = argmax_{\\theta}P(\\theta\\mid D) \\to \\frac{a_{H}+\\alpha-1}{a_{H}+\\alpha+a_{T}+\\beta-2} Probability(확률) P(E) \\in R, P(E)\\ge0, P(\\Omega)=1 P(E_{1}\\cup E_{2}..) = \\Sigma_{i-1}^{\\infty}P(E_{i}) if A \\subseteq B, then P(A)\\le P(B) P(\\phi)=0 0\\le P(E) \\le 1 P(A\\cup B) = P(A) + P(B) - P(A\\cap B) 조건부 확률 조건이 붙는 확률 ~일 경우에 -일 확률은? 베이즈 정리 P(D|S) = \\dfrac{P(S|D)P(D)}{P(S)} Probability Distribution assign, mapping이라고 합니다 어떤 이벤트가 발생하는 것을 특정한 값으로 assign 확률밀도 함수, Probability Density Function(PDF) = f(x) 누적분포 함수, Cumulative Distribution Function(CDF) = \\int_{-\\infty}^{\\infty}f(x)dx Normal Distribution 파라미터나 식을 수정해 균일하게 만든 분포 Notation : \\mathcal{N}(\\mu, \\sigma^2) Mean : \\mu Variance : \\sigma^2 \\mathcal{N}(x; \\mu, \\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right) Beta Distribution 범위가 정해진 것을 표현할 경우 주로 사용 확률을 모델링시 사용 Notation : \\text{Beta}(\\alpha, \\beta) Mean : \\frac{\\alpha}{\\alpha+\\beta} Variance : \\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)} \\text{Beta}(\\theta; \\alpha, \\beta) = \\dfrac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}, {B(\\alpha, \\beta)}=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}, \\Gamma(\\alpha)=(\\alpha-1)! Binomial Distribution Notation : \\text{Bin}(N, \\theta) Mean : N \\theta Variance : N \\theta(1-\\theta) \\text{Bin}(x;N,\\theta) = \\binom N x \\theta^x(1-\\theta)^{N-x} \\binom N x =\\dfrac{N!}{x!(N-x)!} Multinomial Distribution binomial을 일반화 군집화, 자연어 처리시 사용 Notation : \\text{Mu}(N,\\theta) Mean : \\text{E}[x_k] = N\\theta_k Variance : \\text{Var}[x_k] = N\\theta_k(1-\\theta_k) \\text{Mu}(x;N,\\theta) = \\binom N x \\prod_{k=1}^K \\theta_k^{x_k} = \\binom N {x_1, \\cdots, x_K} \\prod_{k=1}^K \\theta_k^{x_k} 이 식에서 \\binom N {x_1, \\cdots, x_K} = \\dfrac{N!}{x_1! \\cdots x_K!} Reference 인공지능 및 기계학습 개론1 다크 프로그래머님 블로그 sanghyukchun님 블로그",
    "tags": "ml data",
    "url": "/data/2018/04/18/machine-learning-basic/"
  },{
    "title": "Docker와 쿠버네티스의 이해",
    "text": "네이버 AI 해커톤에 참여하다가 빠르게 도커 사용법을 익혀야해서 찾아본 방법 및 IBM developerWorks 밋업에서 진행한 도커와 쿠버네티스, 두 마리 토끼를 잡자!을 들으며 기록한 Docker, Kubernetes를 정리한 문서입니다 Contents Docker Kubernetes Docker Docker : 컨테이너 기반의 오픈소스 가상화 플랫폼 Images : 컨테이너 실행에 필요한 파일과 설정값 등을 포함하는 것으로, 상태값을 가지지 않고 변하지 않습니다(Immutable) tar 파일을 묶어놓은 file system으로 Template같은 친구들 Container : 이미지를 실행한 상태이며 추가되거나 변하는 값은 컨테이너에 저장됩니다 단, 컨테이너를 삭제하면 내부의 데이터가 삭제됩니다. 이를 해결하기 위해 Volumn을 사용합니다 서비큐라님 Docker 글 : 꼭 읽어보기!! 추천!!! 서비큐라님 Kubernetes 글 IBM Document 도커가 등장하기 전 서버 운영 자체 서버 운영 ⇒ 설정 관리 도구 등장(ansible, chef 등) ⇒ 가상머신 등장 ⇒ 클라우드 등장 ⇒ PaaS 등장 ⇒ 도커 등장 ⇒ 쿠버네티스 등장 ⇒ 서비스메시 등장 서비스메시 요소마다 프록시 서버가 떠서 네트워크 감시 ⇒ 네트워크 가상 머신 등장 immutable 변하지 않는 mutable vs immutable 서버에 설치된 애플리케이션을 새로운 버전으로 업데이트하면 mutable 업그레이드하다 네트워크 이슈로 타임아웃날 수 있음. 논리적으론 맞지만 오류가 날 수 있음 새로운 버전이 설치된 서버의 상태를 이미지로 만들고 교체하면 immutable 개념이 단순해짐 기존 상태를 고려할 필요 없이 통째로 서버를 교체 생각보다 어렵고 느리고 특정 회사의 제품을 써야함 클라우드 이미지 단점 어떻게 만들었는지 모름 생각보다 쓰기 어려움 Platform as a service 서버 운영이 매우 복잡하고 어려움 소스 코드만으로 배포가 가능함 일반화된 프로비저닝으로 사용 가능 스타트업도 초반에 Paas를 씀 단점 애플리케이션을 PaaS 방식에 맞게 작성 서버에 대한 원격 접속 시스템을 제공하지 않음. 새로 띄우면 날라가니 s3를 써야함 배포 패러다임을 바꿔야 함 크론잡, 데이터 분석, 로그 분석, 애플리케이션 성능 모니터링, AB Test, 카나리 배포, 네트워크 스토리 설정을 PaaS 업체에서 제공하지 않으면 사용할 수 없음 Docker VM vs Docker VM : OS 위에 OS가 또 올라감. Docker아 OS 위에 격리만 함 도커의 특징 1) 확장성 도커가 설치되었으면 어디서든 컨테이너 실행 가능 특정 회사/서비스에 종속적이지 않음 개발 서버, 테스트 서버 쉽게 만들 수 있음 2) 표준성 배포 방식이 다 다르지만 run이라는 것 하나로 가능 3) 이미지 4) 설정 환경 변수를 넣어서 관리 5) 자원 첨부 파일을 s3 같은 곳에 업로드 paas처럼 제한이 없지만 클라우드 이미지보다 관리 쉬움 빌드 기록이 남음 복잡한 기술 몰라도 됨 오픈소스 모든 것을 컨테이너로! ⇒ 도커이미지 있으면 사용 Blue - Green 배포 애플리케이션을 업데이트하기 위해 컨테이너를 교체하는 방식 사용 proxy nginx를 사용해서 기존 컨테이너 보다가 새 컨테이너 바라보고 기존꺼 죽임 서비스 디스커버리 컨테이너를 마구 띄우다보니 IP가 뭔지 알아내서 자동으로 연결하고 싶어함 key value storage를 사용해 쓸 수 있음 서버가 2대가 있고, 하나가 추가된 경우 기존 2개의 ip만 알고 있음. key value에 web3에 새 ip와 포트 추가하면 nginx manager가 변경될 경우 이벤트로 감지하고 nginx 설정 파일 새로 만들고 재시작함 docker-gen 컨테이너 오케스트레이션 여러 대의 서버와 여러 개의 서비스를 관리해주는 서비스 스케줄링 적당한 서버에 배포 클러스터링 여러 개의 서버를 하나의 서버처럼 사용 서비스 디스커버리 key value에 저장할 필요 없이 바로 가져올 수 있음 로깅, 모니터링으로 중앙에서 관리 가능 docker swarm docker에서 만든 컨테이너 오케스트레이션 도구 업데이트가 잘 안되고 있음 Kubernetes 대규모에 적합, 다양한 생태계 Docker Install json parser인 jq 설치 sudo apt install -y jq Docker 설치 curl -fsSL https://get.docker.com/ | sudo sh sudo usermod -aG docker $USER sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # check (re-login) docker version docker-compose version # reboot sudo reboot Docker 기본 명령어 docker run hello-world, docker container run hello-world : 기존에 명령어를 전자같이 사용했으나, 17년 초중반부터 명령어가 후자같이 세분화되고 있습니다 docker images 현재 사용 가능한 image 목록을 출력. -a 옵션을 주면 모든 것을 보여줌 docker ps 현재 사용 가능한 컨테이너 목록을 출력. -a 옵션을 주면 모든 것을 보여줌 dokcer pull docker pull &lt;아이디&gt;/&lt;이미지 이름&gt;:&lt;태그&gt; docker hub에 있는 이미지를 가지고 옴 docker 이미지로 container 실행하고 싶은 경우 docker run -it &lt;아이디&gt;/&lt;이미지 이름&gt;:&lt;태그&gt; /bin/bash : -it 옵션과 함께 실행하면 실행한 명령이 Console에 붙어서 진행됩니다. i는 interactive, t는 tty를 의미합니다 docker container run &lt;container 이름&gt; ls -l : ls -l 명령어를 실행하며 컨테이너를 실행해라! docker container run -it --name &lt;container 별명&gt; &lt;image 이름&gt; /bin/ash : --name을 통해 container 이름을 부여합니다. container 이름을 부여하지 않으면 랜덤하게 생성됩니다 exit된 docker container 접속 docker container start &lt;container ID&gt; docker container에서 명령어 실행 docker container exec &lt;container ID&gt; ls : exec은 container에서 명령어를 실행! exec로 실행하면 ps auxf에 살아있습니다 docker diff 컨테이너가 부모 이미지와 파일 변경 사항을 확인할 수 있는 명령어 docker container commit docker commit &lt;container ID&gt; &lt;아이디&gt;/&lt;이미지 이름&gt;:&lt;태그&gt; 새로운 도커 이미지 생성 docker push image docker push &lt;아이디&gt;/\\&lt;이미지 이름\\&gt;:&lt;태그&gt; docker hub에 이미지 업로드 docker build docker build --tag &lt;아이디&gt;/\\&lt;이미지 이름\\&gt;:&lt;태그&gt; . Dockerfile에 있는 곳에서 명령어를 치면 파일에 나온대로 이미지 생성 Dockerfile 형식 FROM ubuntu:14.04 MAINTAINER Foo Bar &lt;foo@bar.com&gt; RUN apt-get update RUN apt-get install -y nginx RUN echo \"\\ndaemon off;\" &gt;&gt; /etc/nginx/nginx.conf RUN chown -R www-data:www-data /var/lib/nginx EXPOSE 8080 WORKDIR /etc/nginx CMD [\"nginx\"] COPY app.js FROM : 어떤 이미지를 기반으로 할지 설정 MAINTAINER : 메인테이너 정보 RUN : 쉘 스크립트 혹은 명령 실행 이미지 생성 중에는 사용자 입력을 받을 수 있어서, apt-get install에서 -y 옵션을 꼭 넣어줘야 함. 안 넣으면 Fail RUN은 한줄이 이미지 하나로 빌드됨 EXPOSE : 포트 노출 CMD : 컨테이너가 시작되었을 때 실행할 실행 파일 또는 쉘 스크립트 WORKDIR : CMD에서 설정한 실행 파일이 실행될 디렉터리 COPY : 말 그대로 복사 Docker Container Stop docker container rm &lt;container ID/Name&gt; : container 삭제! 돌아가고 있는 container는 삭제가 되지 않습니다. 먼저 중지한 후, 삭제해야 합니다! 그러나 -f 조건을 주면 바로 삭제할 수 있습니다 Delete all containers docker rm $(docker container ls -a -q) docker container ls -a -q는 container의 id만 출력합니다 Delete all images docker rmi $(docker images -q) docker image ls -q | xargs docker image rm : docker image들을 삭제! xargs는 앞의 결과를 인자로 받습니다 Docker port and volume 순서대로 따라해보면 됩니다! docker container run -d --name mynginx nginx : -d 조건을 줘서 데몬으로 실행 docker containers ls : 현재 PORT가 나옵니다! docker container exec mynginx hostname -I : hostname-IP 출력하면 여러 IP가 나옵니다 ifconfig docker0 : docker가 네트워크도 격리된 환경을 만들었습니다!! IP가 유사하면 서로 접근할 수 있습니다 curl http://&lt;container IP&gt; 하면 접근을 한 것을 알 수 있습니다 docker container run -d --name mynginx2 -p 8080:80 nginx:alpine : alpine 리눅스 기반으로 호스트의 8080 포트를 컨테이너의 80으로 설정합니다 docker container ls를 하면 PORTS에 0.0.0.0:8080-&gt;80/tcp가 나타납니다! 컨테이너 안에있던 친구들이 외부로 노출된 것을 알 수 있습니다! cd ~/workspace한 후, docker container cp mynginx2:/etc/nginx/conf.d/default.conf . 를 하면 컨테이너 안에 있던 친구들을 호스트로 꺼내왔습니다! 파일에 location을 보면 root : /usr/share/nginx/html이라고 나와있습니다 echo \"hello world!\" &gt; index.html을 하신 후, docker container cp index.html mynginx2:/usr/share/nginx/html/index.html 하면 파일이 바뀜을 알 수 있습니다 docker container run -d --name mynginx3 -p 8083:80 -v /home/ibmcloud/workspace:/usr/share/nginx/html nginx:alpine : workspace 디렉토리를 연결해서 실행해보니 8083도 위에서 만든 8080가 동일합니다! 여기서 -v 옵션을 줘서 다양하게 디렉터리를 공유할 수 있습니다! docker system prune : 멈춰있는 container를 모두 지우고, 태그안된 image 등을 깔끔하게 삭제합니다 Build Docker image git clone https://github.com/IBM/container-service-getting-started-wt cd container-service-getting-started-wt/Lab\\ 1 cat package.json cat app.js cat Dockerfile Dockerfile은 이미지를 만드는 방법을 나타냅니다 EXPOSE 8080 # 8080 포트 노출 CMD node app.js # app.js 실행! COPY app.js app.js를 마지막으로 하면 빌드 과정에서 달라집니다!(COPY package.json 같은 것들이 app.js에 종속됨) 이미지의 순서가 정말 중요합니다 docker image build -t hello-world:1 . : docker 빌드 docker image inspect hello-world:1 : 도커 이미지를 검사합니다 docker container run -d -p 8080:8080 --name hello-world-a hello-world:1 : 방금 만든 이미지를 사용해 container를 실행합니다 docker container logs &lt;container name&gt; : 로그를 확인할 수 있습니다! docker container exec hello-world-a hostname ip addr |grep inet , docker container exec hello-world-b hostname ip addr |grep inet 해보면 각자의 Network가 생성되어 있습니다! docker container stop hello-world-a hello-world-b : c를 제외하고 삭제합니다 ps auxf |grep app.js |grep node해서 process 번호를 확인하고 sudo kill &lt;PID&gt;를 하면 컨테이너가 종료됩니다. c는 kill signal을 주고 로그가 남습니다! 이 죽은 애들을 살려주는 곳이 쿠버네티스! Kubernetes Docker는 Host에 배포하는 방식 보통 socket 파일로 되는데, 포트로 오픈 내부망에 포트를 오픈하고 ip docker run centurion이란 툴을 만듬 2~3대라면 이정도도 괜찮음 그러나 서버가 많아지면? 컨테이너 관리도구 춘추전국시대 그러나 쿠버네티스가 평정 Kubernetes Native Platform! Cloud Support 컨테이너 플랫폼 Knative : Serverless Istio : Service Mesh Kubeflow : Machine Learning 기본 기능 상태 관리 : 상태를 선언하고 선언한 상태를 유지 / 노드가 죽거나 컨테이너 응답이 없을 경우 자동으로 복구 스케줄링 : 클러스터의 여러 노드 중 조건에 맞는 노드를 찾아 컨테이너를 배치 클러스터 : 가상 네트워크를 통해 하나의 서버에 있는 것처럼 통신 서비스 디스커버리 : 서로 다른 서비스를 쉽게 찾고 통신할 수 있음. mysql라고 띄우면 mysql로 부를 수 있음 리소스 모니터링 : cAdvisor를 통한 리소스 모니터링 스케일링 : 리소스에 따라 자동으로 서비스를 조정함 RollOut/RollBack : 배포/롤백 및 버전 관리 빠른 업데이트 다양한 배포 방식 Daemon Set Replica Set ⇒ deployment Stateful sets Job Replication Controller ⇒ 거의 deprecated, replica set으로 변함 Ingress 설정 domain path를 작성하면 쉽게 설정할 수 있음 Namespace &amp; Label RBAC 내부적 롤을 서브젝트, 리소스, 오퍼레이션으로 관리함 Kubernetes Object The Illustrated Children’s Guide to Kubernetes https://www.cncf.io/the-childrens-illustrated-guide-to-kubernetes/ 컨테이너를 바로 쓰지 않고 Pod으로 감싸서 사용 ReplicaSet : Pod을 복제해서 쓰는 개념 Servuce : 일종의 로드 밸런서 Volume : 저장소 Namespace : 전체를 묶음 Object Spec YAML 파일을 사용 원하는 상태를 다양한 오브젝트에 라벨을 붙여 정의(yaml)하고 사용 Server - Agent Master : API server 확장성을 고려해 다양한 모듈이 기능별로 쪼개져 있음 마스터가 죽으면 API server를 관리할 수 없어서 보통 3개 정도 사용함 마스터가 죽어도 노드는 살아있음, update를 못할뿐 Node : 컨테이너가 돔, kubelet 마스터 서버와 통신하며 필요한 pod을 생성하고 네트워크와 볼륨 설정 kubectl 라는 명령도구 사용함 단점 복잡한 개념 공부할게 너무 많고, 구성 관리하려면 알아야할 내용이 많아서 배보다 배꼽이 더 크다고 생각할 수 있음 복잡한 설치 클라우드를 이용합시다 무거운 환경 아무것도 안띄어져 있어도 기본 리소스 사용이 많은편 복잡한 설정파일 모든 설정이 마이크로서비스 스럽게 나뉘어져 있어서 길고 복잡함 앱을 만들면 Container로 실행합니다. 이걸 쿠버네티스에서 pod (포드, 팟)이라고 부릅니다. 각각의 pod는 1개의 IP를 가집니다! 모이면 리플리카셋이 되고, 이것들을 deployment라고 배포합니다. 이런 친구들은 워커 노드에서 돌아갑니다. Master Node : 스케쥴링 Worker Node : 실제 일을 하는 친구들 쿠버네티스는 마스터를 다 관리해줍니다! 요새 IBM, GCP 등에 마스터를 알아서 관리해주고 워커만 저희가 보면 됩니다 중요한 개념 내부적 엔진의 역할은 딱 1개 현재 상태(current state)와 관리자가 원하는 상태를 계속 비교해서(diff) 원하는 상태로 계속 바꿔줌(act) storage 체크하는 애는 storaeg만, 네트워크 체크하는 애는 네트워크만 계속 체크함 쿠버네티스는 컨테이너를 pod이라는 개념으로 감싸서 씀 개발자 입장에서 짜는 프로그램을 만든다 하면 if문을 써서 어떤 노드가 있고, 빈 공간이 있는지 체크해서 cpu 메모리가 적은 곳에 할당한다 이럴텐데 스토리 쿠버네티스는 pod을 생성 ⇒ 생성 요청된 것을 감시하다가 있으면 할당 ⇒ 실제 노드 서버에 컨테이너를 띄우는 할당이 아니라 이 pod은 3번 노드에 띄우겠다 정보만 올림 ⇒ 할당은 되었지만 실행은 안됨 ⇒ 3번 노드 입장에서 자신에게 할당되었지만 실행 안된거가 있는지 확인 ⇒ 있다면 도커로 띄우고 ⇒ Pod의 상태를 알려주고 ⇒ 아 이제 다 할당이 되었구나! 이런 흐름 5개의 모듈이 분리가 되서 사용됨 결국 모듈이 많지만 같은 메커니즘으로 운영됨 cloud native 사이트에 가면 소속된 레플리케이션이 많이 있음 cncf cloud native interactive landscape https://landscape.cncf.io/ 이 레플리케이션이 쿠버네티스와 밀접한 관계를 가짐. 쿠버네티스 쓰면 레플리케이션이 잘 돌아감 개발자가 기능 개발을 할 경우 브랜치 따고 테스트하고 자동화를 함 젠킨스가 빌드를 해서 이미지 올리고, 빌드한 후 또다른 git branch에 배포하라는 명령어를 push함 소스 ⇒ 빌드 ⇒ 쿠버 클러스터에 이 브런치를 띄워줘라는 push 쿠버네티스는 상태를 만들도록 노력함 application에 feature가 3개 있으면 3개가 모두 뜸 미묘한 차이 : 띄워라고 명령을 보내는게 아니라 이 3개의 컨테이너가 있으면 좋겠어! 라고 싱크함 쿠버네티스의 장점 스케쥴링을 잘해줌 죽어도 스스로 잘 살려줌 확장성 로드 밸런싱 롤아웃/롤백이 자동으로 진행 설정을 관리 Service 서비스의 종류 https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0 총 4개 1) ClusterIP 쿠버네티스 클러스터에 팟이 3개 있고, 팟을 통하기 위해 서비스 하나가 있음 이 서비스가 클러스터 IP임 redis를 만들고 두개의 팟을 나누고, 레디스 접근하기 위해 서비스 오픈이 되어야 함 여태한 것은 컨테이너 띄우기만 했지 접근을 할 수 없음 컨테이너 접근하기 위해 ClusterIP를 만들어야 함 내부에서 사용하는 작은 로드 밸런서라고 생각하면 됨 Pod을 하나만 만들어도 Service가 하나 생성됨 ⇒ IP를 가지고 있음 2) NodePort 이 클러스터 IP는 클러스터에서 사용하는 대표 IP인데 외부에서 사용할 수 없고, 내부 통신만 가능 노드포트로 오픈하면 외부에서 오픈할 수 있는 것이 모든 노드에 열림. 하나의 노드에 연결하면 클러스터 IP에 연결됨 모든 노드에 열리다보니 더 좋은 것이 로드밸런서에 더 좋음 3) LoadBalancer 클라우드에서만 존재하는 개념 회사에 서버 3대를 띄우고 쿠버 설정을 하면 로드밸런서 설정을 해야함 이게 엄청 추상적 개념인데, 일반적 조립 서버에선 로드 밸런서가 없음 모든 노드의 포트가 아니라 로드밸런서를 통해 클러스터 IP로 접근 이 친구의 단점은 바라보는 서비스는 1대임. 서비스 10개 띄우면 10개를 봐야함 ⇒ 이걸 위해 Ingress 4) Ingress nginx가 있어서 다 80 포트로 받지만 path에 따라 서비스를 분기할 수 있음 Install kubectl cli sudo apt update &amp;&amp; sudo apt install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" |sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt update sudo apt install -y kubectl kubectl을 설치해야 합니다!(단 처음 설치하면 tab이 안됨) kubectl version : 쿠버네티스 버전 명시(처음엔 Client만 나올겁니다) source &lt;(kubectl completion bash) : 자동완성 가능하도록 설정 kubectl completion bash |sudo tee /etc/bash_completion.d/kubectl : 종료해도 자동완성 가능하도록 설정! Deploying apps into clusters 문서 bx login : au-syd 선택! bx cr region-set : 개인 저장소를 사용하기 위한 명령어. container repository의 약자( cs) ap-south 선택! bx cr namespace-list : 이름을 출력! export MY_REGISTRY_NAMESPACE=&lt;namespace&gt; : 환경설정 추가 bx cr namespace-add $MY_REGISTRY_NAMESPACE : 네임스페이스 추가!! bx cr login : cat ~/.docker/config.json을 사용해 로그인합니다 docker image ls : 현재 로컬에 있는 docker 이미지 출력 docker image tag hello-world:1 registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPACE/hello-world:1 : 같은 Image ID를 가지는 image가 생성됩니다 docker image push registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPAC/hello-world:1 : 이미지가 IBM 개인 저장소로 push! bx cr image-list : 개인 저장소에 있는 image를 출력합니다 docker image tag hello-world:2 registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPACE/hello-world:2 docker image push registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPAC/hello-world:2 hello-world:2도 올려줍니다! bx cs clusters : 명령어가 cs로 바뀌었습니다! container service export MY_CLUSTER_NAME=&lt;cluster 이름&gt; bx cs workers $MY_CLUSTER_NAME : worker들의 설정을 보여줍니다 bx cs cluster-config $MY_CLUSTER_NAME : export KUBECONFIG ~를 그대로 복사해서 터미널에서 실행! 해당 config에 쿠버네티스 정보가 나타납니다 kubectl version : 이제 Server도 출력됩니다! kubectl version --short : 짧게 Version 출력 Worker 올리는 부분 kubectl run hello-world-deployment --image=registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPACE/hello-world:1 : 해당 image를 가지고 와서 쿠버네티스 실행! kubectl get pod : pod 생성되었는지 확인 kubectl get deployment : 방금 deploy한 친구의 정보가 나옵니다. deployment 뒤에 이름을 붙여서 볼수도 있음 kubectl get deployment hello-world-deployment -o yaml : -o yaml을 사용해 더 상세한 데이터를 보여줍니다 replicas : 1개로 되어있음 rollingUpdate : deployment 정책! 1개씩만 올리고 내림 kubectl describe deployment hello-world-deployment : 현재 상태를 보여줍니다 kubectl get replicaset : replicaset이 나타납니다 kubectl get replicaset -o yaml : 특정 replicaset 정보를 가지고 옵니다(여기서 pod 관리 정책이 있음) kubectl get pod : pod이 생겼습니다!! kubectl get pod -o yaml : pod은 특정 ip를 가지고 있습니다! hostIP도 나타나며 다른 정보들도 포함되어 있습니다 pod 외부에서 확인하고 싶을 경우 외부에서 확인하기 위해선 cluster ip, node port 열어주기 등의 방법이 필요합니다. 여기선 node port를 열어보겠습니다 kubectl expose deployment/hello-world-deployment --type=NodePort --port=8080 --name=hello-world-service --target-port=8080 kubectl describe service hello-world-service : 해당 서비스 설명 출력됩니다. 8080:30384/TCP가 되어있습니다! bx cs workers $MY_CLUSTER_NAME : 해당 IP를 확인한 후, nodePORT를 연결하면 접속이 됩니다! kubectl edit deployment hello-world-deployment : deployment를 수정합니다! replicas의 수를 수정할 수 있습니다 kubectl scale --replicas=5 deployment hello-world-deployment : replicas 개수를 조절할 수 있습니다 kubectl get pod : 하면 개수가 증가됨을 볼 수 있습니다 kubectl rollout status deployment hello-world-deployment : rollout 상태를 보여줍니다 kubectl describe replicaset를 입력하면 4개가 추가됨을 볼 수 있습니다 watch -n 1 curl http://워커아이피:노드포트/ : 1초마다 해당 내용이 가져오는데, 어떤 팟을 쓰는지 볼 수 있습니다 Version 오류 발생시 대처가 어떻게 되는지? export MY_REGISTRY_NAMESPACE=&lt;ID&gt; kubectl set image deployment hello-world-deployment hello-world-deployment=registry.au-syd.bluemix.net/$MY_REGISTRY_NAMESPACE/hello-world:3 : 아직 3을 안만들었지만 생성됨 kubectl rollout status deployment hello-world-deployment : 2개가 업데이트되고 진행이 안됩니다 kubectl describe pod : 상태가 다릅니다 kubectl get pod : 이미지를 PULL하다 에러가 뜸! kubectl rollout undo deployment hello-world-deployment : 롤백!!!!! kubectl get pod : 멀쩡한 것을 볼 수 있습니다 Reference 서비큐라님 블로그 도커 튜토리얼 : 깐 김에 배포까지 Docker에서 apt-get update가 실패할 때 Docker 컨테이너 이미지 생성 Image doe not contain ‘sudo’ IBM developerWorks 밋업, 도커와 쿠버네티스, 두 마리 토끼를 잡자! IBM Document",
    "tags": "linux development",
    "url": "/development/2018/04/17/docker-kubernetes/"
  },{
    "title": "Domain Name System(DNS)의 이해",
    "text": "생활코딩 WEB2 - Domain Name System을 수강하며 내용을 정리한 글입니다. 이 수업을 들으니 제가 얼핏 알았던 개념을 확실하게 알 수 있었고, 대표적인 면접 문제인 브라우저의 URL 입력창에 www.naver.com 을 입력하면 무슨 일이 벌어질까요?란 질문을 왜 하는지 알 것 같습니다(다른 네트워크 개념과 프론트 개념도 알아야 하겠지만!) 수업소개 도메인 이름을 자신의 서버 컴퓨터에 부여하는 방법을 알려주는 수업 개론 Host : 인터넷에 연결된 컴퓨터 한대 한대 IP Address : host끼리 통신을 하기 위해 필요한 주소 IP 주소를 기억하는 것이 어렵기 때문에 Jon Postel과 Paul Mockapetris에 의해서 DNS(Domain Name System)이 나왔습니다 DNS Server : 수많은 IP 주소와 도메인이 저장되어 있습니다 웹에서 www.naver.com 을 입력하면 DNS Server에 www.naver.com의 IP 주소를 알려준 후, 그 IP 주소를 토대로 접속하는 것입니다 IP 주소와 Hosts의 개념 2대의 컴퓨터가 인터넷 통신을 위해 반드시 필요한 것은 IP 주소입니다. 비단 컴퓨터뿐만이 아닌, 인터넷이 연결된 것은 Host라고 부릅니다. 단, 모든 IP 주소를 기억하는 일은 너무 어렵습니다. 이것을 해결하기 위해 운영체제마자 hosts라는 파일이 존재합니다. 이 파일에 IP와 도메인 이름을 저장해두면, 도메인 이름을 통해 다른 host에 접근할 수 있습니다(DNS을 사용하지 않고도!) Hosts 파일을 설정하는 방법 Hosts 위키피디아의 Location in file system에 OS별 경로가 나와있습니다 맥 OS의 경우 /etc/hosts에 있습니다 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost IP 주소 도메인 이름 도메인 이름과 보안 hosts 파일을 변조해서 평소에 사용하던 도메인 이름을 입력할 경우, 기존과 다른 사이트로 접근하게 할 수 있습니다. 이런 hosts 파일은 보안에 취약하기 때문에 변조가 되지 않도록 해줘야 합니다! 이 파일의 보완을 위해 백신을 사용하는 것을 추천합니다 자주 일어나는 사례는 은행 사이트를 동일하게 만들어서 개인 정보를 입력하도록 하는 일입니다. 이런 일을 fishing이라고 합니다. 이것을 확인하기 위해 사이트 주소 앞을 보면 https로 시작하는 사이트는 보안이 안전하며, 변조된 것인지 알 수 있습니다(http는 안전하지 않습니다) DNS의 태동 before DNS Stanford Research Institute에서 전 세계의 hosts 파일을 관리했습니다. 위 기관의 hosts 파일을 덮어쓰기한 후, 내 컴퓨터에서 도메인 이름으로 접속했습니다. 처음엔 이 방식도 정말 유익했지만, 인터넷이 커지며 점점 문제점이 발견되었습니다. hosts 파일을 다운로드하지 않으면 추가된 호스트 이름을 사용할 수 없으며 SRI에서 수작업으로 IP와 도메인 이름을 갱신해서 시간과 비용이 들었습니다. After DNS DNS에게 이 IP는 ~라는 Domain 이름을 갖고 싶습니다!라고 요청하면(자동화되어 있습니다) DNS에 갱신된 내용이 저장됩니다 그 후, 여러분들의 컴퓨터에서 와이파이 혹은 인터넷이 연결되면 DNS Server에 있는 IP 주소가 DHCP를 통해 셋팅됩니다. 웹 브라우저에서 도메인 이름을 입력하면 먼저 로컬의 hosts 파일에서 찾은 후, 없다면 DNS Server에 Domain 이름의 IP를 요청하고 받습니다. Public DNS의 사용 DNS Server의 IP나 Domain을 알고 있어야 Server에 접근할 수 있습니다. 이런 정보는 ISP(Internet Service Provider : SK 브로드밴드, SKT 등)가 자동으로 셋팅해주고 있습니다 그러나 경우에 따라 통신사가 지정한 DNS Server를 사용하고 싶지 않을 경우가 있습니다(속도가 느리거나, 보안, Privacy 이슈로) public dns server를 검색하면 다양한 곳에서 만든 DNS Server를 볼 수 있습니다. Mac 시스템 환경설정 - 네트워크 - 고급 - DNS를 누르면 현재 사용하는 DNS Server를 볼 수 있습니다. 여기에서 DNS Server 설정 Windows Network and Sharing Center - Connections 옆 글자(WIFI 혹은 Ethernet) - Properties(속성) - Internal Protocal Version - Properties - Use the following DNS server address에 설정 DNS의 내부 원리 도메인 이름의 구조 여러분들이 서버를 운영하는 생산자가 된다면, 이 내용을 알면 덜 혼란스럽고 반드시 도움이 될 것입니다! :) DNS Server DNS Server는 IP 주소와 Domain 이름을 기억하는 기능과 Client가 이름을 물어보면 IP를 알려주는 기능을 갖고 있습니다. 수천대의 서버가 같이 협력하고 있습니다. 맨 뒤에는 사실 .이 생략되어 있습니다. 각각의 부분들은 부분들을 담당하는 독자적인 Server Computer가 존재합니다. Root는 Top-level을 담당하는 Server의 목록과 IP를 알고 있으며, Top-level은 Second-level, Second-level은 sub의 목록과 IP를 알고 있습니다(상위 목록이 직속 하위 목록을 알고 있음) 최초 root 네임서버의 IP 주소에게 blog.example.com을 물어보면 .com을 담당하는 Top-level을 알려주고, Top-level은 example.com을 담당하는 Second-level을 알려주고, Second-level은 blog.example.com 담당하는 sub DNS Server에게 물어보고, sub가 해당 IP 주소를 알려줍니다! 계층적인 구조를 가지고 있습니다 도메인 이름 등록 과정과 원리 DNS register ICANN - Registry 등록소 - Registrar 등록대행자 - Registrant 등록자 관계 A, NS는 Record Type을 뜻합니다. A는 최종적인 IP 주소이며 NS는 Name Server를 뜻합니다 nslookup 사용법 nslookup은 도메인 이름에 대한 정보를 조회할 때 사용하는 도구입니다. dig라는 도구도 좋지만, 윈도우에선 제공하고 있지 않습니다 Terminal에서 아래와 같이 입력합니다 nslookup example.com &gt;&gt;&gt; Server: 168.126.63.1 Address: 168.126.63.1#53 Non-authoritative answer: Name: example.com Address: 93.184.216.34 위 명령어는 nslookup -type=a example.com와 동일한 명령어입니다 처음 나오는 Address는 컴퓨터에 연결된 DNS Server IP가 나타납니다. Non-authoritative answer는 cache가 응답한 경우 발생한 것입니다. 직접 물어보고 싶다면 example.com의 name server가 누군지 알아내야 합니다 nslookup -type=ns example.com &gt;&gt;&gt; Server: 168.126.63.1 Address: 168.126.63.1#53 Non-authoritative answer: example.com nameserver = b.iana-servers.net. example.com nameserver = a.iana-servers.net. Authoritative answers can be found from: a.iana-servers.net internet address = 199.43.135.53 b.iana-servers.net internet address = 199.43.133.53 a.iana-servers.net has AAAA address 2001:500:8f::53 b.iana-servers.net has AAAA address 2001:500:8d::53 위에서 보면 nameserver가 나타납니다. 2개의 서버가 운영되고 있습니다. nameserver 이름을 알고난 후, 아래와 입력하면 직접 요청할 수 있습니다 nslookup example.com a.iana-servers.net &gt;&gt;&gt; Server: a.iana-servers.net Address: 199.43.135.53#53 Name: example.com Address: 93.184.216.34 나의 도메인 이름 장만하기 등록대행자를 통해 등록소에게 등록해야 합니다! 등록대행자는 다양하게 존재하며, 이번 강의에선 freenom.com을 사용할 것입니다. 1년간 무료로 사용할 수 있도록 제공하고 있습니다 도메인을 구입할 땐 만료일이 언젠지 반드시 적어두세요! 까먹으면 큰일이 날 수도 있어요! freenom에서 name server도 제공해줍니다! 그 이후로는 그냥 클릭만 하면 쉽게 설정할 수 있습니다 :) DNS record와 CNAME 레코드의 이해 도메인 이름에 대한 정보 한건 한건을 DNS Record라고 합니다. DNS Record 타입을 살펴보고, IP 주소가 아닌 도메인 이름에 대한 별명을 지정하는 방법으로서 CNAME Record 타입에 대해 알아보겠습니다! 검색 포탈에서 dns recode로 검색하면 Type별 설명을 볼 수 있습니다! CNAME은 도메인의 별명을 지정! freenom에선 add record에 CNAME을 설정할 수 있습니다 Github pages에 도메인 연결하기 내가 서버를 운영하지 않고 남이 사용하는 서버를 사용하는 경우 도메인을 연결하는 방법에 대해 알아보겠습니다! Github의 경우 settings - custom domain에 설정하면 됩니다!! 추가적으로 http를 사용하는 경우, https를 사용하는 경우 등의 내용은 Github 공식 문서에 잘 나와 있습니다.",
    "tags": "web development",
    "url": "/development/2018/04/16/domain-name-system/"
  },{
    "title": "Learning Natural Language Inference using Bidirectional LSTM model and Inner Attention 리뷰",
    "text": "Learning Natural Language Inference using Bidirectional LSTM model and Inner Attention을 정리한 글입니다! Abstract encoding-based model for recognizing text en-tailment 2가지 과정을 겪음 word-level의 bidi-rectional LSTM에 Sentence representation을 만들기 위해 Average Pooling 더 나은 representation을 얻기 위해 average pooling을 attention으로 대체 Inner Attention : 첫 단계에서 얻는 representation 더 적은 파라미터로 나은 퍼포먼스 1. Introduction 한 쌍의 문장이 주어질 때, 전제로부터 합리적 추론을 할 수 있는지 측정하는 것이 목표 RTE의 3가지 types Entailment(=true) Contardiction(=false) Neutral(=unknown) 지금까지 제안된 딥러닝 접근법은 크게 두 그룹이 존재 sentence encoding-based models : core of former methods matching encoding based models : 문장 사이의 관계를 모델링하고 sentence representation을 생성하지 않음 일반적으론 sentence encoding-based model에 초점을 맞춤 : LSTMs-based model, GRUs-based model, TBCNN-based model, SPINN based model Single directional LSTM과 GRU는 문맥 정보를 활용하지 못하는 약점이 있으며, CNN은 단어 순서(word order)에 대한 정보를 활용하지 못함 Bidirectional LSTM은 위에서 언급된 단점을 해결하기 위해, 두 방향으로 sequence를 처리해 이전과 이후의 context를 사용 Feature Engineering이나 외부 리소스를 필요하지 않는 RTE를 위해 통합된 프레임 워크를 제안함 기본 모델은 Premise와 Hypothesis에 BiLSTM을 구축 basic mean pooling이 대략 이 문장이 무엇을 말하는지에 대한 직감을 만들어 줍니다. 이 representation을 만든 후, 양쪽에 Inner-Attention을 적용 이 메커니즘은 분류에 정확하고 집중 sentence representation을 생성에 도움이 됨 또한 hypothesis와 premise에서 동일한 단어를 제거해 성능을 향상시킴 4. Conclusion and Future work RTE를 풀기 위해 Inner-Attention을 사용한 BiLSTM based model을 제안 Our future work QA, Para-phrase and Sentence Text Similarity 등에도 사용 시도 sentence vector를 완전히 사용하는 방법 시도 2. Our approach 2.1 Sentece Encoding Module 이 모델의 핵습 2가지 방식 average pooling은 sentence vector를 생성하기 위해 BiLSTM 위에 구축 같은 문장에 attention 메카니즘 적용 + 이전 단계에서 생성된 representation을 사용 Inner-attention 아이디어는 사람이 한 문장을 읽을 때, 과거의 경험에 따라 문장의 어느 부분이 더 중요한지에 대한 직관을 형성할 수 있다는 생각으로 만들었습니다 M = tanh(W^{y}Y+W^{h}R_{ave}\\otimes e_L) \\alpha = softmax(w^{T}M) R_{att}=Y\\alpha^T Y는 biLSTM의 output vector로 구성된 행렬 R_{ave}는 mean pooling layer의 output \\alpha는 attention vector R_{att}는 attention-weighted sentence representation 2.2 Sentence Matching Module 3가지 matching methods(premise와 hypotheses의 관계를 추출) Concat ( two representation ) Element-wise product Element-wise difference Finally, SoftMax layer 3. Experiments 3.2 Parameter Setting trainging objective : cross-entropy loss, minibatch SGD with the Rmsprop for optimization batch size : 128 dropout rate : 0.25 pretrained 300D Glove 840B vector use to initialize the word embedding Out-of-vocabulrary words are randomly initialize during training, all of these embedding are not updated",
    "tags": "paper data",
    "url": "/data/2018/04/08/Learning-Natural-Language-Inference-using-Bidirectional-LSTM-model-and-inner-attention/"
  },{
    "title": "Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation 리뷰",
    "text": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation을 정리한 글입니다! SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation 의미론적 텍스트 유사성 평가(다국어/교차 언어) Abstract Semantic Textual Similarity (STS)는 문장의 유사도를 측정 사용되는 곳 Machine Translation(MT) Summarization Generations Questions Answering(QA) Short answer grading Semantic search Dialog and conversations systems MTQD 데이터를 기반으로 2017 Task가 이루어졌으며 총 31 팀이 참가. 17 팀이 all language tracks 1. Introduction STS는 textual entailment, semantic relatedness and paraphrase detection을 포함 STS와 Textual entailment, Paraphrase detection의 차이점 gradations of meaning overlap을 포착 유무 (의미의 중첩) STS와 Semantic relatedness와 차이점 의미의 중첩은 둘 다 잘 표현하나 Semantic relatedness는 관계에 대해 명확하지 않음(예를 들어 밤과 낮은 관련성이 높으나 비슷하진 않음) 처음엔 형태가 매칭되거나 문법 유사성으로 나타나는 lexical sementics에 집중했으며, 그 이후엔 strong new similarity signal을 찾아냄 가장 좋은 퍼포먼스는 앙상블! feature engineered 모델과 deep learning 모델을 합친 경우 English sentence pairs에 집중. 영어는 잘 연구된 문제라 인간의 판단과 70-80의 correlation을 보임 9. Conclusion 기존과 차이점은 아라비아어, 스페인어, 영어, 터키어에 초점을 맞췄다는 것. 그러나 Arbic-English, Turkish-English는 약한 상관관계를 얻음. 이것은 로버스트 모델을 더 개선할 필요가 있음 2. Task Overview 성능은 인간의 판단과 머신 점수의 Pearson상관 관계에 의해 0~5 점수로 측정. 중간 값은 의미상 해석할 수 없는 중첩 수준을 뜻합니다. 3. Evaluation Data Stanford Natural Language Inference(SNLI) corpus (단, 교차 언어 중 하나는 WMT 데이터) Sentence Embedding 사용하며 meaning overlap은 cosine similarity 사용해 측정 6. System Evaluation 6.4 Baseline The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence ECNU 팀 딥러닝 + feature engineers model(RF, GB, XGB) 앙상블 Feature : n-gram overlap, edit distance, longest comment prefix/suffix/substring, tree-kernels, word alignments, summarization, MT evaluation metrics(BLEU, GTM-3, NIST, WER, ME_TEOR, ROUGE), kernel similarity of bags of words, bags of dependencies, pooled word-embedding 딥러닝 모델은 sentence embedding시 average word embedding, projected word embedding, deep averaging network 또는 lstm을 사용해 차별화함 4개의 딥러닝 + feature model 3개 평균점수 BIT WordNet과 BNC word frequencies를 기반으로 한 sentence information content를 사용 cosine similarity of summed word embeddings with an IDF weighting scheme 사용 Setence IC는 ECNU를 제외한 모든 시스템보다 성능이 좋았음 Senetence IC를 word embedding similarity와 결합하는 것이 가장 좋음 HCTI Convolution Deep Structured Semantic Model(CDSSM) 사용 Sentence embedding은 2개의 cnn에서 생성 아키텍쳐는 ECNU의 딥러닝과 유사 MITRE ECNU와 유사. 딥러닝 + Feature Engineering Feature : alignment similarity, TakeLab STS, string similarity measures(matching n-grams, summarization, MT Metrics), RNN/RCNN, BiLSTM FCICU Sense-base alignment를 사용하는 BabelNet 사용 CompiLIG Best Spanish-English 퍼포먼스 Feature : cross-lingual conceptual similarity using DBNary, cross-language Multi-Veg word embeddings, Byrchcin and Svoboda’s improvements LIM-LIG 오직 weighted word embedding만 사용했으며 Arabic 2등 Word embedding시 uniform sentence embedding과 POS, IDF weighting schemes를 합침 DT_Team English에서 2등 DSSM, CDSSM을 합친 딥러닝 모델과 Feature Engineering Feature : unigram over-lap, summed word alignments scores, fraction of unaligned words, difference in word counts by type, Min to max ratios of words by type SEF@UHH Spanish-English 1등 cosine, negation of Bray-Curtis dissimilarity and vector correlation을 사용한 Paragraph로 Unsupervised 유사도 구함 L_1-L_2 PAIR를 단일 언어 L_1-L_1, L_2-L_2로 변경",
    "tags": "paper data",
    "url": "/data/2018/04/02/Semantic-Textual-Similarity-Multilingual-and-Cross-lingual-Focused-Evaluation/"
  },{
    "title": "Pytorch를 활용한 Advanced Sequence models",
    "text": "김성동님의 Pytorch를 활용한 딥러닝 입문 중 Advanced Sequence Model 파트 정리입니다. Machine Translation Statistical Machine Translation Source Language(X)를 토대로 Target Language(Y)로 번역하는 Task 예시 : 스페인어를 Input으로 넣고, Output을 영어! 목적 함수 argmax_yP(y|x) source 문장이 주어졌을 때, 가장 좋은 Target 문장을 찾는 것 argmax_yP(x|y)P(y) 으로 식을 분해 P(x|y)는 Translation Model : 어떻게 번역하는지 학습 Parallel Corpus가 필요 (병렬 코퍼스) P(y)는 Language Model : 어순 및 문법 보정 P(x|y)를 P(x,a|y)로 변경 여기서 a는 Alignment로 Source language와 Target language의 단어간 상호 관련성을 의미 one to one, one to many, many to many alignment가 필요 실제로 이 확률을 계산하기 위해서는 모든 가능한 경우의 수를 다 대조해봐야 하기 때문에 너무 비싼 연산입니다. 실제론 너무 확률적으로 낮은 것들은 무시해버리고 Heuristic Search Algorithm 사용합니다 Neural Machine Translation Sequence to Sequence 번역에 Neural Network를 사용했습니다. 2014년부터 기존의 Machine Translation 성능을 엄청 끌어올렸습니다 특별히 Sequence-to-Sequence라는 새로운 형태의 뉴럴넷 구조를 사용하기 시작! Encoder와 Decoder 역할을 하는 2개의 RNN을 사용 Decoder는 Encoder의 Last Hidden state를 초기 Hidden state로 사용하여 Target 문장에 대한 Language model을 학습 &lt;START&gt;, &lt;END&gt; 토큰 : 문장의 시작과 끝을 알려주며, 디코더 입장에선 condition에 따라 디코딩을 시작하고, end 토큰이 나오면 디코딩을 멈춰라 NMT는 확률 P(y|x)을 바로 계산합니다(Statisctical에선 2개로 나눠서 계산했음) Source 문장(x)과 이전(T-1)까지의 Target 문장이 주어졌을 때 step T에 y_T일 조건부 확률 학습 End to End Training Decoder의 각 time step에서 발생한 Loss(Cross-Entropy)를 사용하여 두 모델의 파라미터를 한번에 업데이트 torchtext에러 Field의 파라미터 eos : end of sentence, init token : start of sentence Greedy Decoding 각 step에서 가장 확률이 높은 단어(토큰)를 디코딩(단어 level) 가장 쉽고 계산 효율적 문장 level의 확률을 고려하지 못함 한번 디코딩한 단어는 무조건 사용되어야 합니다(취소나 변경이 안됨) 단어 레벨에서 가장 좋은 것을 찾는 것이 반드시 올바른 문장을 만드는 것은 아닙니다 Beam Search Decoding 만약 Vocabulary의 크기가 V라고 할 때, 길이가 T인 문장 레벨의 모든 가능한 경우의 수를 고려하려면 O(V_T)의 계산복잡도 필요한데, 이것은 약간 비효율적인 방법 따라서 Beam Search Decoding을 사용 각 step에서 가장 확률적으로 높은 K개만 고려하자! 최종적으로 K^T 조합 중 가장 확률이 높은 시퀀스를 선택! Greedy Decoding 구현이 간단함 계산 효율적 문장 level의 확률을 고려하지 못함 Beam Search Decoding : 문장 level의 확률을 고려 좀 더 나은 디코딩 결과(Greedy와 비교해서) 구현 어려움 좀 더 비싼 계산 복잡도 학습 시 적용 어려움? (최근 나온 논문 -&gt; Sequence-to-Sequence Learning as Beam-Search Optimization) 성능 평가 BLEU score BiLingual Evaluation Understudy 번역 시스템이 좋은지 안 좋은지 판단할 때 accuracy, f1 metric으로 평가하면 문제가 생김 =&gt; 여러 문장으로 표현할 수 있음 정답을 여러가지로 제시 Modified Precision 문장 각각 해당 단어를 카운트해서 그 맥스값을 분자로 사용 과대 평가나 중복 Count를 막을 수 있음 유니그램 바이그램 N gram BLEU Score는 최적의 평가 방법은 아니지만 자동 평가지표로서 매우 유용함 인간이 전부 정성적으로 평가하면 매우 비싸고, 그냥 일반적인 평가지표(Precision/Recall/F1/Accuracy)를 사용하면 제대로 평가할 수 없기 때문에 대안으로 사용 현대의 MT의 평가는 대부분 BLEU를 통해 하고 있음 https://github.com/Maluuba/nlg-eval (직접 구현 못해도 오픈소스로 다수 존재) Other tasks using seq2seq Summarization (요약) : 본문 -&gt; 요약문 Dialogue (대화) : 상대방의 발언 -&gt; 봇의 발언 Parsing (파싱) : 단순 문장 -&gt; 파싱된 결과 Code generation (코드 생성) : 자연어 -&gt; 파이썬 코드 Attention Source Language의 정보를 하나의 고정된 벡터로 인코딩하기 때문에 디코더는 한정된 정보만 사용할 수 있습니다(Information Bottleneck!) 디코더의 각 step에서 Source sequence의 특정 부분에 집중하여 디코딩! 고정된 벡터에 의존하는게 아니라 동적으로 Source sequence를 가중합하여 디코딩에 사용합니다 1) 디코더의 hidden state와 인코더의 hidden states를 내적하여 attention score를 각각 구합니다 2) attention score에 Softmax를 사용하여 확률로 바꾼다(합이 1) 3) 인코더의 각 step에서 attention distribution과 hidden state를 사용하여 가중합을 구한다. ⇒ Context Vector 4) 디코더의 hidden state와 Attention을 사용해 구해낸 Context vector를 concat하여 디코딩! Result NMT system의 성능을 엄청 상승시켰음 (Seq2Seq를 사용하는 다른 테스크들의 성능도 상승) Information Bottleneck을 해결함 Vanishing gradient 문제 역시 완화(Gradient path 증가) 약간의 Interpretability 얻을 수 있음(Alignment) Advanced-Attention Abstractive Summarization Text Summarization 테스크 본문 -&gt; 요약문 기존 요약 모델들은 조금만 길게 요약하려고 하면 같은 말이나 의미의 문장을 반복하는 경향이 있었음 NMT와 비슷하게 sequence to sequence를 사용 Intra-Temporal Attention 이전 디코딩 step에서 집중했던 부분은 이번 step에서는 덜 집중하게 만든다 (중복된 요약을 막는 효과) Self Attention 일반적인 Attention은 Encoder와 Decoder의 hidden state 사이에서 Attention score 구했습니다 Self Attention은 Decoder의 hidden state들 사이에서 Attention score을 구합니다 Seq2Seq뿐만 아니라 일반적인 RNN의 구조에서 각 hidden state 간의 attention을 적용하는 방법들을 Self-Attention(Intra-Attention)이라 부름. (Hidden states 간의 Multiple Weighted Sums) Transformer 기존 RNN의 단점 기존의 RNN 기반의 Sequence model은 순차적인 연산탓에 병렬화하기 어려움(=느리다) Long term Dependency의 문제를 해결하기 위해 Attention mechanism 사용 Transformer 사용 기계 번역 테스크 Encoder - Decoder 컨셉 유지 RNN이나 CNN 구조 사용 X 모든 것을 Self-Attention으로 처리 (Multi-Head Attention) Key의 개수가 많아질수록 Softmax 안에 있는 QK^T의 값의 분산이 커질 수 있어서 Key vector의 크기로 스케일링 Q = V = K Word vector들을 쌓아서 서로가 서로에게 Attention(self-Attention) 한 단어와 다른 단어 사이의 어텐션(dot-product)은 한번에 하나씩 밖에 할 수 없기 때문에 워드 벡터에 Linear 연산 후, 병렬로 다른 단어들과도 h번의 attention을 수행(Multi-Head Attention)",
    "tags": "pytorch data",
    "url": "/data/2018/03/24/pytorch-sequence-model/"
  },{
    "title": "비전공자를 위한 SQL",
    "text": "데이터 사이언스를 공부하기 전, 마케팅/광고를 공부했습니다. 마케터가 배우면 제일 좋을 도구가 GA일까? 엑셀일까? 이것저것 고민하다가 그 당시 내린 결정은 SQL이었습니다. 일단 데이터를 스스로 가지고 올 수 있어야 분석을 할 수 있습니다(개발자들에게 매번 부탁하면 업무의 디펜던시가 개발자에게 걸리니..) 개발 경험이 없는 분들이 쉽게 SQL을 다룰 수 있도록 글을 작성할 예정입니다 최근 BigQuery 관련 슬라이드를 만들었습니다. 비전공자분들도 이해할 수 있도록 꽤 많은 내용을 다루었습니다. BigQuery의 모든 것(기획자, 마케터, 신입 데이터 분석가를 위한) 입문편 참고하셔도 좋을 것 같습니다 :) 이 글의 특징 온라인으로 쿼리를 날려볼 수 있는 sqlfiddle에서 간단한 실습 실제 데이터 분석시 날려볼 쿼리 위주로 준비하고 있습니다 계속 업데이트 할 예정입니다(최종 수정일 : 19년 6월 8일) ToDo Join LIKE Pivot Window Function SQL SQL은 Structured Query Language의 약자로 관계형 데이터베이스 관리 시스템(RDBMS)의 데이터를 관리하기 위해 설계된 특수 목적의 프로그래밍 언어입니다. 그냥 데이터를 뽑아내기 위한 도구라고 생각하면 조금 이해하기 쉬울 것 같습니다. 대부분의 회사에서 데이터를 RDB에 저장하고 있습니다. 많이 사용되는 것은 MySQL, PostgreSQL, MariaDB, Oracle 등이 있습니다 기본적인 구조는 다음과 같습니다. 지금은 이해가 안되실텐데 그냥 그렇구나~ 하고 쭉 글을 읽어주세요! SELECT [컬럼 이름] FROM [테이블 이름] WHERE [조건] GROUP BY [그룹화할 컬럼] HAVING [그룹화한 뒤 조건] LIMIT [제한할 개수] SELECT / FROM 우선 가장 중심이 되는것은 SELECT와 FROM입니다! 예를 들면 “서랍에 있는 연필을 찾고 싶은” 경우 SQL문으로 변경하면 다음과 같습니다 SELECT 연필 FROM 서랍 SELECT 뒤에는 찾고싶은 목적 대상들(Column)을 나열하면 되고, FROM 뒤에는 찾을 대상이 있는 공간(Table)을 작성하면 됩니다 WHERE 만약 위의 예에서 조건을 추가해서 “서랍에 있는 연필 중 길이가 10cm 이상인 연필을 찾고 싶은” 경우를 SQL문으로 변경하면 다음과 같습니다 SELECT 연필 FROM 서랍 WHERE 길이 &gt;= 10cm 조건을 주고싶은 경우엔 WHERE 뒤에 작성해주시면 됩니다. WHERE 절은 엑셀 함수에서 IF와 비슷한 느낌이라고 생각하시면 좋을 것 같습니다 조건을 늘려 “서랍에 있는 연필 중 길이가 10cm 이상이고 색상이 빨간색인 연필을 찾고 싶은” 경우 SELECT 연필 FROM 서랍 WHERE 길이 &gt;= 10 AND 색상 = \"빨간색\" 심화 지식 WHERE 뒤에 조건의 순서를 어떻게 하냐에 따라서 결과가 나오는 속도가 다른 경우가 있습니다. 이것은 인덱스와 관련이 있습니다. 회사의 개발자(Database를 담당하시는)에게 저희 DB 인덱스가 어떻게 되어있는지 알려주세요!라고 물어보시면 알려주실 겁니다! 인덱스를 어떻게 타야하는지는(=WHERE에 어떻게 작성해야 빠른지) 다른 글을 통해 알려드리겠습니다 쿼리를 조금 짜보신분들에겐 정말 쉬운 SELECT문이지만, 처음 해보시는 분들을 위해 실습을 해보겠습니다 실습 환경 실습 환경은 별도 구성 없이 온라인에서 진행합니다 http://sqlfiddle.com/으로 들어가신 후, 왼쪽 창에 아래 내용을 복사 붙여넣기하고 Build Schema 버튼을 눌러주세요!( MySQL 5.6 사용 ) CREATE TABLE IF NOT EXISTS `user_log` ( `index` INTEGER NOT NULL AUTO_INCREMENT, `user_id` VARCHAR(6) NOT NULL, `event` VARCHAR(200) NOT NULL, `event_date` date NOT NULL, PRIMARY KEY (`index`, `user_id`) ) DEFAULT CHARSET=utf8; INSERT INTO `user_log` (`user_id`, `event`,`event_date`) VALUES ('1', 'login_facebook', '2018-03-12'), ('1', 'write_posting', '2018-03-12'), ('1', 'write_comment', '2018-03-12'), ('1', 'view_posting', '2018-03-12'), ('1', 'view_posting', '2018-03-12'), ('2', 'login_facebook', '2018-03-12'), ('2', 'view_posting', '2018-03-12'), ('2', 'view_posting', '2018-03-12'), ('2', 'write_comment', '2018-03-12'), ('2', 'logout', '2018-03-12'), ('2', 'login_facebook', '2018-03-13'), ('3', 'login_google', '2018-03-13'), ('3', 'write_posting', '2018-03-13'), ('3', 'view_posting', '2018-03-13'), ('3', 'view_posting', '2018-03-13'), ('3', 'purchase_item', '2018-03-15'), ('3', 'write_comment', '2018-03-14'), ('1', 'view_posting', '2018-03-14'), ('4', 'view_posting', '2018-03-14'), ('5', 'purchase_item', '2018-03-13'); 제가 임의로 만든 테이블로 Table 이름은 user_log입니다. 블로그 서비스를 하는 업체의 유저 로그 데이터라고 생각해주세요! 이 테이블엔 user_id, event, event_date가 기록되어 있습니다. 이제 우리는 화면의 우측에 쿼리를 작성한 후, RUN SQL 버튼을 눌러봅시다!!!! 1. “1”번 유저의 모든 이벤트 로그를 확인해보세요 날짜는 고려하지 않으셔도 됩니다 Column : ? Table : ? WHERE : ? 정답 SELECT user_id, event, event_date FROM user_log WHERE user_id = '1'; 결과 user_id event event_date 1 login_facebook 2018-03-12 1 write_posting 2018-03-12 1 write_comment 2018-03-12 1 view_posting 2018-03-12 1 view_posting 2018-03-12 1 view_posting 2018-03-14 3줄로 원하는 데이터를 뽑아냈습니다! 데이터를 보면 1번 유저는 3월 12일에 페이스북으로 로그인했습니다. 그리고 포스팅을 1번 작성했고 포스팅을 2번 봤습니다. 3월 14일에 포스팅을 1번 봤습니다. 쿼리를 통해 이런 정보를 알 수 있었는데, 여기서 아쉬운 점은 같은 이벤트가 중복해서 2줄로 나와있는 점입니다. 위 결과에선 1번 유저가 view_posting을 3월 12일에 2번 했다는 것을 저희가 직접 Count를 해야합니다. 이것을 쿼리로 간단하게 해보겠습니다! GROUP BY 여기서 나오는 개념은 GROUP BY 입니다! GROUP BY [컬럼 이름] 이런 방식으로 사용하는데, 직관적으로 설명하자면 컬럼들을 그룹화한다(aggregate)라고 생각해주세요 GROUP BY에 대한 이해를 돕기 위해 그림을 그려봤는데, 아래와 같은 과정을 통해 결과가 나타납니다 우선 GROUP BY하기 위해 같은 값들을 모아두고, 그 후에 연산(COUNT 혹은 SUM 같은 집계 함수)을 수행합니다 코드를 통해 보여드리겠습니다 SELECT user_id, event, event_date, COUNT(DISTINCT user_id) AS 'unique', COUNT(user_id) AS 'total' FROM user_log WHERE user_id = '1' GROUP BY user_id, event, event_date; 처음 보는 것들을 설명드리겠습니다 COUNT 개수를 Count하는 친구입니다 COUNT(개수를 셀 컬럼) 이런 방식으로 사용합니다 COUNT(user_id)를 하면 user_id의 개수를 세주는 것입니다 DISTINCT COUNT(DISTINCT user_id)를 하면 중복을 제외한 고유한 user_id의 개수를 세주는 것입니다 AS ‘unique’ 값을 센 후 이름을 unique로 칭하겠다라는 뜻입니다 위 쿼리의 결과는 아래와 같습니다 user_id event event_date unique total 1 login_facebook 2018-03-12 1 1 1 view_posting 2018-03-12 1 2 1 view_posting 2018-03-14 1 1 1 write_comment 2018-03-12 1 1 1 write_posting 2018-03-12 1 1 여기서 또 조금 아쉬운 점은 event_date 기준으로 정렬되어 있지 않은 점!!! 이건 ORDER BY 로 해결할 수 있습니다 SELECT user_id, event, event_date, COUNT(DISTINCT user_id) AS 'unique', COUNT(user_id) AS 'total' FROM user_log WHERE user_id = '1' GROUP BY user_id, event, event_date ORDER BY event_date; ORDER BY ORDER BY [컬럼 이름] 으로 사용합니다. 기본 옵션은 오름차순이며 DESC을 붙여주면 내림차순으로 정렬됩니다. ORDER BY event_date DESC 이런 식으로 하면 최신 날짜부터 정렬됩니다 결과 user_id event event_date unique total 1 write_posting 2018-03-12 1 1 1 view_posting 2018-03-12 1 2 1 write_comment 2018-03-12 1 1 1 login_facebook 2018-03-12 1 1 1 view_posting 2018-03-14 1 1 위 결과를 해석하면 1번 유저는 write_posting을 3월 12일에 1회 했고, view_posting을 2회 했습니다! WHERE 조건에 user_id를 1로 고정했기 때문에 unique값은 모두 1입니다. 만약 id를 제외하고 event, event_date만 뽑으면 어떻게 될까요? 코드 SELECT event, event_date, COUNT(DISTINCT user_id) AS 'unique', COUNT(user_id) AS 'total' FROM user_log GROUP BY event, event_date ORDER BY event_date; 결과 event event_date unique total login_facebook 2018-03-12 2 2 write_posting 2018-03-12 1 1 write_comment 2018-03-12 2 2 view_posting 2018-03-12 2 4 logout 2018-03-12 1 1 purchase_item 2018-03-13 1 1 view_posting 2018-03-13 1 2 login_google 2018-03-13 1 1 login_facebook 2018-03-13 1 1 write_posting 2018-03-13 1 1 view_posting 2018-03-14 2 2 write_comment 2018-03-14 1 1 purchase_item 2018-03-15 1 1 이제 특정 이벤트가 날짜별로 몇명이 했고, 몇번 했는지를 알 수 있습니다! 조금 더 쉽게 보고싶다면 데이터를 엑셀로 가져가 그래프로 그리면 될 것 같습니다! 다시 돌아와서 또 다른 쿼리를 짜볼게요. 아마 데이터를 자주 보는 회사라면 DAU, WAU, MAU라는 말을 들을 수 있을거에요! DAU를 뽑아내는 쿼리를 만들어 봅시다 2. DAU 뽑기 DAU의 정의 : Daily Active User로 저희 서비스에서 어떤 이벤트라도 했던 사람을 Active로 정의하겠습니다 이 경우 어떻게 쿼리를 날려야 할까요? 직접 쿼리를 짜보세요! 정답 SELECT event_date, COUNT(DISTINCT user_id) AS 'DAU' FROM user_log GROUP BY event_date ORDER BY event_date; 결과 event_date DAU 2018-03-12 2 2018-03-13 3 2018-03-14 3 2018-03-15 1 2-1. DAU가 2 이상인 날짜 뽑기 GROUP BY를 통해 나온 값을 조건으로 걸고싶은 경우는 어떻게 해야할까요? 이럴 경우엔 HAVING이란 친구가 나옵니다 HAVING GROUP BY의 바로 아래에 작성해주시면 됩니다 HAVING 조건 그룹화를 하기 전 컬럼에 대한 조건이라면 WHERE, 그룹화를 한 후의 컬럼에 대한 조건은 HAVING을 사용하는 것입니다 처음 SQL 접하시는 분들이 자주 하시는 질문 WHERE과 HAVING의 차이가 무엇인가요? WHERE은 현재 Table에서 조건을 뽑아내는 것이고 HAVING은 그룹화한 후 결과에서 조건을 뽑는 것입니다. 동시에 사용하는 경우도 있으며, HAVING은 주로 GROUP BY와 함께 쓰입니다 정답 SELECT event_date, COUNT(DISTINCT user_id) AS 'DAU' FROM user_log GROUP BY event_date HAVING DAU &gt;= 2 ORDER BY event_date; 결과 event_date DAU 2018-03-12 2 2018-03-13 3 2018-03-14 3 짧은 정리 맨 처음에 보여드렸던 기본적인 SQL 구조를 다시 보여드릴게요. 생각보다 많은 것이 보일거에요! SELECT [컬럼 이름] FROM [테이블 이름] WHERE [조건] GROUP BY [그룹화할 컬럼] HAVING [그룹화한 뒤 조건] LIMIT [제한할 개수] 여기서 설명하지 않은 LIMIT은 보여줄 결과를 제한해주는 것입니다. 개수 제한이 필요할 경우 사용하면 됩니다 :) SQL은 집합적 관점에서 접근하면 조금 더 쉬운데, 여태까지 배운 내용을 집합으로 표현해보겠습니다 Table에서 WHERE 조건에 해당하는 값들을 찾은 후, SELECT! Join Join은 2개 이상의 Table을 조합해 새로운 가상 Table처럼 만들어 결과로 보여줍니다. 여러 Table을 연결한다고 생각하면 좋을 것 같습니다 Join은 왜 필요할까? 데이터베이스에 대한 지식이 없을 땐, 모든 Data를 하나의 Table에 넣으면 Join이 필요없을텐데 왜 굳이 Table을 나눠서 저장할까?라는 의문을 가졌습니다. 조금 더 공부한 결과 관계형 데이터베이스는 정규화 과정을 거쳐 데이터 중복을 최소화해 데이터를 관리합니다. 이 정규화 과정을 거치면 Table끼리 관계(Relation)를 갖게 됩니다. 또한 저장 공간의 효율성과 확장성이 증가됩니다! Table에 저장된 데이터를 효과적으로 검색하기 위해 Join을 사용합니다. 제 사례를 들자면, User의 상태 데이터는 user_state table에 저장하고 User의 로그 데이터는 user_log table에 저장했습니다. 그리고 다양한 행동 패턴을 분석할 때 (예를 들어, 블로그에 누적 글이 3개 이상인 유저들과 3개 미만인 유저들의 행동 패턴을 비교하고 싶을 경우) Join을 사용했습니다 Join의 종류 Inner Join Cross Join Self Join Outer Join Left Outer Join Right Outer Join Full Outer Join Join 문법 SELECT FROM TABLE_A as a LEFT JOIN (SELECT FROM TABLE_B) as b ON a.column1 = b.column1 ToDo : Join 문제 만들기 설명글 추가 어떻게 쓰는지 그 후 설명",
    "tags": "sql development",
    "url": "/development/2018/03/18/sql-for-everyone/"
  },{
    "title": "Pytorch를 활용한 RNN",
    "text": "김성동님의 Pytorch를 활용한 딥러닝 입문 중 RNN 파트 정리입니다. Language Modeling 철수와 영희는 식탁에 앉아 사과를 __(A)__ (A)에 들어올 단어는? 먹었다! 이런 아이디어 기반해서 만들어진 것이 Language Model P(x^{(t+1)}=w_j|x^{(t)}, ...,x^{(1)}) 키보드의 자동 완성 기능, 서치 엔진에서 쿼리 자동 완성 기능 모두 Language Model을 적용한 Application으로 볼 수 있음 음성 인식을 할 때도 Language Model이 쓰임! 해당 단어만 잘못 들었을 경우(noise가 껴있다거나) 이전까지 인지한 단어를 기반으로 단어를 추론! N-gram으로 모델링을 합니다. gram은 gramma의 줄임말 철수와 영희는 식탁에 앉아 사과를 ______ Unigram : 토큰 하나가 변수가 됨 : “철수”, “와”, “영희”, “는”, “식탁”, “에” Bigram : 두개의 토큰이 하나의 변수가 됨 : “철수 와”, “와 영희”, “영희 는”, “는 식탁”, “식탁 에”, “에 앉아” Trigram : 3개의 토큰이 하나의 변수 : “철수 와 영희”, “와 영희 는”, “영희 는 식탁”, “는 식탁 에”, “식탁 에 앉아” 4-gram : 4개의 토큰 : “철수 와 영희 는”, “와 영희 는 식탁”, “영희 는 식탁 에”, “는 식탁 에 앉아” N-gram : N개의 토큰이 하나의 변수가 됨 가정 : t+1의 확률은 이전 n-1개의 단어(토큰)에만 의존한다 문제점 앞의 정보를 무시하고 있음. 가정 자체에 한계 존재 n-1 이전의 맥락을 모델링할 수 없음 해당 n-gram이 Corpus에 없거나 희소한 경우 확률이 0이나 매우 낮게 나올 수도 있음 n이 커질수록 더욱 확률은 희박해짐 Corpus에 있는 n-gram을 모두 카운트해서 저장해야 하기 때문에 모델의 공간 복잡도가 O(exp(n)) 위 문제점 때문에 Neural Language Model로 접근하기 시작함 Window-based Language Model 고정된 Window size(~n-1)를 인풋으로 받는 FFN(Feed Forward Neural Network) 카운트할 필요가 없기 때문에 Sparsitiy 문제 없음 모델의 사이즈도 작음 여전히 고정된 window size에 의존하기 때문에 Long-term Context를 포착하지 못함 토큰의 윈도우 내의 위치에 따라 다른 파라미터를 적용 받음(Parameter sharing이 없음) Recurrent Neural Network 모든 Timestamp에서 같은 Parameter를 공유! Input의 길이가 가변적입니다 time t의 hidden state는 이전 모든 time step x를 인풋으로 받는 함수 g의 아웃풋으로 볼 수 있습니다(모두 연결되어 있으니까-!) Notation 인풋의 차원에 대한 감이 있어야 합니다! x는 word vector 모든 Time Step에서 Parameter를 Sharing! RNN 참고 자료 예시 뭐 먹 을까? D=3로 총 4개의 Timestamp가 있어서 4x3 매트릭스를 indexing했습니다 0.1 0.1 0.2 0.5 0.2 0.3 1.0 0.0 0.2 0.1 0.1 0. 첫 행이 h^{(0)}! 마지막 step의 Hidden state는 “뭐 먹을까?”라는 문장을 인코딩한 벡터로 볼 수 있습니다 input_size = 10 # input dimension (word embedding) D hidden_size = 30 # hidden dimension H batch_size = 3 length = 4 rnn = nn.RNN(input_size, hidden_size,num_layers=1,bias=True,nonlinearity='tanh', batch_first=True, dropout=0, bidirectional=False) input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D &lt;= batch_first hidden = Variable(torch.zeros(1,batch_size,hidden_size)) # 1,B,H (num_layers * num_directions, batch, hidden_size) output, hiddne = rnn(input, hidden) output.size() # B, T, H hidden.size() # 1, B, H # (배치 사이즈, 시퀀스 길이, input 차원)을 가지는 Input # (1,배치 사이즈, hidden 차원)을 가지는 초기 hidden state 나는 너 좋아 오늘 뭐 먹지 Batch Size, 2 Time : 문장의 길이, 3 Dimension : 인풋의 차원, 10 Style마다 먼저 쓰는 것이 다른데, B, T, D로 많이 사용하곤 함 (batch_first=Ture) hidden은 마지막 hidden state를 뜻합니다 하이퍼 파라미터 세팅 ?에 들어갈 것은 무엇일까요? E : VxD W_e : DxH W_h : HxH U : HxV 모든 timestep에서 그 다음에 올 단어를 예측하고 그 오차를 Cross Entropy로 구하면 됩니다! TorchText 링크 Tokenize, Vocab 구축, Tensor로 감싸주는 프로세스등을 진행할 수 있습니다 Field 데이터 전처리 파이프라인을 정의하는 클래스 Tokensize, Unkwown 태그, Vocab 구축, 문장에서 숫자는 Num이란 태그로 대체 등등의 과정을 파이프라인이라고 할 수 있는데, 이것을 정의하는 클래스 Code # 1. Field 선언 tagger = Kkma() tokenize = tagger.morphs TEXT = Field(tokenize=tokenize,use_vocab=True,lower=True, include_lengths=True, batch_first=True) # tokenize는 함수!, lower는 대문자를 소문자로 바꿔줌, include_lengths는 input을 (input, length)로 쪼개줌 LABEL = Field(sequential=False,unk_token=None, use_vocab=True) # sequential이 true면 토크나이즈를 함 # 2. 데이터셋 로드 train_data, test_data = TabularDataset.splits( path=\"data/\", # 데이터가 있는 root 경로 train='train.txt', validation=\"test.txt\", format='tsv', # \\t로 구분 #skip_header=True, # 헤더가 있다면 스킵 fields=[('TEXT',TEXT),('LABEL',LABEL)]) # TabularDataset은 csv, tsv 포맷을 갖는 데이터셋 # 꺼내오고 싶다면 one_example = train_data.examples[0] one_example.TEXT one_example.LABEL # 3. Vocabulary 구축 TEXT.build_vocab(train_data) LABEL.build_vocab(train_data) TEXT.vocab.itos # 4. iterator 선언 train_iter, test_iter = Iterator.splits( (train_data, test_data), batch_size=3, device=-1, # device -1 : cpu, device 0 : 남는 gpu sort_key=lambda x: len(x.TEXT),sort_within_batch=True,repeat=False) # x.TEXT 길이 기준으로 정렬 TEXT.vocab.itos[1] for batch in train_iter: print(batch.TEXT) print(batch.LABEL) break # 5. 모델링 &lt;pad&gt; token 길이를 맞춰주기 위한 padding 토큰 Backpropagation for RNN 최대한 간단한 RNN. h^{(t)} = W_hh^{(t-1)} 타입 스텝마다 J^{(t)}/W_h 미분한 것을 더하면 됩니다 backpropagation through time(BPTT) 이찬우님 Back Propagation 영상 LSTM(Long Short Term Memory) 기본 RNN은 Timestamp이 엄청 길면 vanish gradient가 생기고 hidden size를 고정하기 때문에 많은 step을 거쳐오면 정보가 점점 희소해집니다 이것을 극복하기 위해 만들어진 LSTM 긴 Short term Memory hidden state말고 cell state라는 정보도 time step 마다 recurrent! Forget Gate 이번 시점의 인풋 x_t와 이전까지의 hidden state h_{t-1}을 인풋으로 받아서 Cell state 중 잊어버릴 부분을 결정합니다 Input Gate time step t의 새로운 정보 중 얼마나 Cell state에 반영할지 결정 하는 Input gate 기존의 Cell state에 까먹을만큼 까먹고,새로운 정보를 받아들일만큼 받아들인다 Output Gate Cell state에 tanh한 결과 정보 중 얼마만큼의 비율로 이번 hidden state로 만들지 정하는 Output gate Shortcut connection NN에서 하나 이상의 layer를 skip하는 구조 ResNet에서 실험한 여러 Shortcut connection 중 exclusive gating과 같은 아이디어 Cell state에 여러 gate function을 사용하여 해당 layer의 연산을 거치지 않은 Information이 계속 그 다음 step으로 전달 될 수 있습니다 전부 곱셈으로 이루어져있는 RNN의 해당 부분(이전 Hidden state에서 다음 Hidden state로 Recurrent하는)을 gate function을 이용해 덧셈으로 대체합니다! Code input_size = 10 hidden_size = 30 output_size = 10 batch_size = 3 length = 4 num_layers = 3 rnn = nn.LSTM(input_size,hidden_size,num_layers=num_layers,bias=True,batch_first=True,bidirectional=True) input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D hidden = Variable(torch.zeros(num_layers*2,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size) cell = Variable(torch.zeros(num_layers*2,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size) output, (hidden,cell) = rnn(input,(hidden,cell)) print(output.size()) print(hidden.size()) print(cell.size()) linear = nn.Linear(hidden_size*2,output_size) output = F.softmax(linear(output),1) output.size() GRU(Gated Recurrent Unit) 조경현 박사님이 제안한 구조 LSTM과 유사하게 생겼는데, LSTM을 더 간략화한 구조 hidden state만 흘러가고 cell state는 없음 Update gate는 이번 step에서 계산한 hidden을 얼마나 update할지 결정한다. (update 되는만큼 기존의 정보를 잊는다.) LSTM의 forget, input gate를 하나의 Update gate로! 만약 z가 0이라면 이번 step의 히든 스테이트는 이전 레이어의 히든 스테이트를 그대로 Copy합니다(identity mapping) Code input_size = 10 # input dimension (word embedding) D hidden_size = 30 # hidden dimension H batch_size = 3 length = 4 rnn = nn.GRU(input_size,hidden_size,num_layers=1,bias=True,batch_first=True,bidirectional=True) input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D hidden = Variable(torch.zeros(2,batch_size,hidden_size)) # 2,B,H output, hidden = rnn(input,hidden) print(output.size()) print(hidden.size()) Bidirectional RNN 인풋 시퀀스를 양방향(forward, backward)으로 연결하며 hidden state를 계산 2개의 hidden state가 필요 RNN에선 레이어를 많이 쌓는다고 반드시 좋아지는 것은 아닙니다! Standard From(RNN) class RNN(nn.Module): def __init__(self,input_size,embed_size,hidden_size,output_size,num_layers=1,bidirec=False): super(RNN,self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers if bidirec: self.num_directions = 2 else: self.num_directions = 1 self.embed = nn.Embedding(input_size,embed_size) self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec) self.linear = nn.Linear(hidden_size*self.num_directions,output_size) def init_hidden(self,batch_size): # (num_layers * num_directions, batch_size, hidden_size) hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)) cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)) return hidden, cell def forward(self,inputs): \"\"\" inputs : B,T \"\"\" embed = self.embed(inputs) # word vector indexing hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell output, (hidden,cell) = self.lstm(embed,(hidden,cell)) # Many-to-Many output = self.linear(output) # B,T,H -&gt; B,T,V # Many-to-One #hidden = hidden[-self.num_directions:] # (num_directions,B,H) #hidden = torch.cat([h for h in hidden],1) #output = self.linear(hidden) # last hidden return output Dropout / Layer Normalization Recurrent connection(가로 방향)에는 Dropout을 적용하지 않고, 나머지 connection(세로 방향)에만 Dropout을 적용합니다!! Recurrent Connection에 Dropout을 적용하면 과거의 정보까지 잃어버리게 되기 때문입니다- Layer Normalization RNN에선 레이어 노말라이제이션이 표준이 되가고 있습니다 Batch와는 독립적으로 Layer의 Output 자체를 Normalization합니다. (Batch size의 의존성 X) LSTM, GRU에 적용하는 것은 복잡할 수 있지만, RNN에 적용한다면! Pytorch에서는 0.4 버전부터 정식으로 사용 가능할 것으로 예상됩니다(현재 0.3.1) Sequence Tagging 연속된 시퀀스에 태그를 다는 테스크 POS tagging, NER, SRL Text 분류는 many to one Language Model, Sequence Tagging은 many to many Named Entity Recognition(NER) 엔티티의 이름을 인지 보통 2개 이상의 토큰이 하나의 Entity를 구성 B : Entity의 시작 I : B로 시작한 Entity에 속함 O : Entity가 아님",
    "tags": "pytorch dl data",
    "url": "/data/2018/03/17/pytorch-rnn/"
  },{
    "title": "Pytorch를 활용한 자연어 처리(NLP)",
    "text": "김성동님의 Pytorch를 활용한 딥러닝 입문 중 자연어처리 파트 정리 파일입니다. Distributed Word Representation NLP Task는 지금까지 봤던 접근법이랑(CNN류) 많이 다릅니다. RNN을 배우기 전에 Word Vector를 알아야 하기 때문에 이 내용을 추가했습니다 파이토치가 짱이야. 아닌데? 텐서플로우가 짱이지 구글이 만들었잖아 그래도 파이토치는 쉬워 페북도 장난 아니야 아마존이 만든 맥스넷이란 것도 좋더라 다음 대화에서 어떤 정보를 얻을 수 있을까요? 파이토치(페북) ≈ 텐서플로우(구글) ≈ 맥스넷(아마존) 사람은 문맥을 통해 각 단어의 유사함과 관계를 쉽게 추론할 수 있음! 컴퓨터에게 넣기 위해선 컴퓨터가 이해할 수 있도록 변형해줘야 합니다. 이 과정을 Word Representation이라고 합니다 Word Representation Idea/Thing을 여러 Symbol로 표현할 수 있습니다.(같은 것을 다르게 표현할 수 있습니다) 여기서 각 단어간의 관계도 알 수 있습니다(유사한지) WordNet 단어 의미를 그래프 형태로 출력 사람이 직접 구축해야 함(비싼 비용) 신조어의 의미를 이해 못함 뉘앙스를 놓치기 쉬움 주관적임 단어 간의 정확한 유사도 계산이 어려움(얼마나 유사한지) 그래프상 몇 노드가 연결되는지 정도만 알 수 있음 One-hot Vector 파이토치 [1,0,0,0,0,0,0,0] 짱 [0,1,0,0,0,0,0,0] 텐서플로우 [0,0,1,0,0,0,0,0] 구글 [0,0,0,1,0,0,0,0] 쉽다 [0,0,0,0,1,0,0,0] 페북 [0,0,0,0,0,1,0,0] 맥스넷 [0,0,0,0,0,0,1,0] 아마존 [0,0,0,0,0,0,0,1] 단어 하나를 하나의 Discrete Variable로 취급 각 단어의 구분은 가능하자 유사도를 측정할 수는 없음 두 벡터를 내적해서 유사도를 측정! 만약 1만개의 단어를 One-hot vector로 표현하면? 1만 차원의 벡터가 필요해 차원의 저주가 발생함 Distributed word representation 그렇다면, 단어의 의미를 분산시켜 벡터로 표현하자! 그 단어의 속성은 함꼐 쓰이는 단어(맥락)에 의해 결정될 것이다 Dense한 Vector를 만들어봄! 파이토치 [0.6, -0.2, 0.7, 0.3, 0.7, -0.2, 0.1, 0.1] 텐서플로우 [0.4, -0.1, 0.6, -0.2, 0.6, -0.2, 0.3, 0.4] 고양이 [-0.3, 0.2, 0.1, 0.2, -0.2, 0.1, -0.3, 0.1] 유사도 파이토치^T * 텐서플로우 = 1.15 파이토치^T * 고양이 = -0.26 비교적 낮은 차원의(50 ~ 1000차원) 벡터에 단어의 의미를 분산해 Dense한 벡터로 표현 Word vector라고 부름 NLP Task NLP Task 연습 Bag of Words : Count 방식으로 표현 1. Tokenize 문장을 단어 또는 형태소 단위로 토큰화 형태소 : 언어를 이루는 최소 단위 문장이란 것은 단어의 연속이기 때문에 리스트로 표현할 수 있음 토큰은 문장, 단어, character, 형태소가 될 수 있음 한국어는 형태소로 쪼개는 방법에 유효함 token = nltk.word_tokenize(\"Hi, my name is sungdong. What's your name?\") print(token) &gt;&gt;&gt; ['Hi', ',', 'my', 'name', 'is', 'sungdong', '.', 'What', \"'s\", 'your', 'name', '?'] # 꼬꼬마 형태소 분석기 token = kor_tagger.morphs(\"안녕하세요! 저는 파이토치를 공부하는 중입니다.\") print(token) &gt;&gt;&gt; ['안녕', '하', '세요', '!', '저', '는', '파이', '토치', '를', '공부', '하', '는', '중', '이', 'ㅂ니다', '.'] 2. Build Vocab 단어의 인덱스를 가지고 있어야 함(각 자리에 맞게 넣어주기 위해) 따라서 Vocab을 구축 각 단어의 ID를 붙여주는 작업 word2index={} # dictionary for indexing for vo in token: if word2index.get(vo)==None: word2index[vo]=len(word2index) print(word2index) &gt;&gt;&gt; {'하': 1, '.': 13, '를': 8, '중': 10, '안녕': 0, '파이': 6, '세요': 2, '토치': 7, '저': 4, '!': 3, '공부': 9, 'ㅂ니다': 12, '이': 11, '는': 5} 3. One-hot Encoding 자신의 인덱스에는 1을 채우고, 나머지엔 0을 채움 def one_hot_encoding(word,word2index): tensor = torch.zeros(len(word2index)) index = word2index[word] tensor[index]=1. return tensor torch_vector = one_hot_encoding(\"토치\",word2index) print(torch_vector) &gt;&gt;&gt; 0 0 0 0 0 0 0 1 0 0 0 0 0 0 유사도를 계산해보면, (One Hot Encoding의 한계) py_vector = one_hot_encoding(\"파이\",word2index) py_vector.dot(torch_vector) &gt;&gt;&gt; 0.0 Bag of Words를 통한 분류 train_data = [[\"배고프다 밥줘\",\"FOOD\"], [\"뭐 먹을만한거 없냐\",\"FOOD\"], [\"맛집 추천\",\"FOOD\"], [\"이 근처 맛있는 음식점 좀\",\"FOOD\"], [\"밥줘\",\"FOOD\"], [\"뭐 먹지?\",\"FOOD\"], [\"삼겹살 먹고싶어\",\"FOOD\"], [\"영화 보고싶다\",\"MEDIA\"], [\"요즘 볼만한거 있어?\",\"MEDIA\"], [\"영화나 예능 추천\",\"MEDIA\"], [\"재밌는 드라마 보여줘\",\"MEDIA\"], [\"신과 함께 줄거리 좀 알려줘\",\"MEDIA\"], [\"고등랩퍼 다시보기 좀\",\"MEDIA\"], [\"재밌는 영상 하이라이트만 보여줘\",\"MEDIA\"]] test_data = [[\"쭈꾸미 맛집 좀 찾아줘\",\"FOOD\"], [\"매콤한 떡볶이 먹고싶다\",\"FOOD\"], [\"강남 씨지비 조조 영화 스케줄표 좀\",\"MEDIA\"], [\"효리네 민박 보고싶엉\",\"MEDIA\"]] # 0. Preprocessing train_X,train_y = list(zip(*train_data)) # 1. Tokenize train_X = [kor_tagger.morphs(x) for x in train_X] # 2. Build Vocab word2index={'&lt;unk&gt;' : 0} for x in train_X: for token in x: if word2index.get(token)==None: word2index[token]=len(word2index) class2index = {'FOOD' : 0, 'MEDIA' : 1} print(word2index) print(class2index) # 3. Prepare Tensor def make_BoW(seq,word2index): tensor = torch.zeros(len(word2index)) for w in seq: index = word2index.get(w) if index!=None: tensor[index]+=1. else: index = word2index['&lt;unk&gt;'] tensor[index]+=1. return tensor train_X = torch.cat([Variable(make_BoW(x,word2index)).view(1,-1) for x in train_X]) train_y = torch.cat([Variable(torch.LongTensor([class2index[y]])) for y in train_y]) # 4. Modeling class BoWClassifier(nn.Module): def __init__(self,vocab_size,output_size): super(BoWClassifier,self).__init__() self.linear = nn.Linear(vocab_size,output_size) def forward(self,inputs): return self.linear(inputs) # 5. Train STEP = 100 LR = 0.1 model = BoWClassifier(len(word2index),2) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(),lr=LR) for step in range(STEP): model.zero_grad() preds = model(train_X) loss = loss_function(preds,train_y) if step % 10 == 0: print(loss.data[0]) loss.backward() optimizer.step() # 6. Test index2class = {v:k for k,v in class2index.items()} for test in test_data: X = kor_tagger.morphs(test[0]) X = Variable(make_BoW(X,word2index)).view(1,-1) pred = model(X) pred = pred.max(1)[1].data[0] print(\"Input : %s\" % test[0]) print(\"Prediction : %s\" % index2class[pred]) print(\"Truth : %s\" % test[1]) print(\"\\n\") &lt;unk&gt;란 단어는 모르는 단어를 처리하기 위해 넣은 단어(인덱스가 존재하지 않으면 unk!) Long Tensor로 변환 Word2Vec 우린 엄청 많은 Text를 가지고 있음(인터넷상에 텍스트는 많음) 모든 단어는 fixed vocabulary (Train에 정의한 단어가 fixed voca가 됨) 단어의 속성은 주변 단어로부터 결정된다라는 전제가 있었는데, 주변 단어가 Input이고 중심 단어가 Output이 나오는 CBOW 모델과 중심 단어가 Input이고 주변 단어가 Output인 Skip-gram이 있습니다 Skip-gram 중심 단어가 있으면 주변 단어가 나올 조건부 확률을 구할 수 있음 윈도우 사이즈는 하이퍼 파라미터 Learnable Embedding Matrix 7개의 단어를 5차원의 Vector로 임베딩하고 싶은 경우엔 7*5의 Embedding Matrix가 필요함 1*7와 7*5를 행렬곱하면 자신의 Index로 인덱싱 Embedding Matrix를 학습하는 것이 Word2Vec # Pytorch embed = nn.Embedding(총 단어의 갯수, 임베딩 시킬 벡터의 차원) embed.weight &gt;&gt;&gt; Parameter Containing : 학습 가능 Embedding 모듈은 index를 표현하는 LongTensor를 인풋으로 기대하고 해당 벡터로 인덱싱합니다 따라서 원핫벡터로 명시적으로 바꿔주지 않아도 됩니다 Object Function Corpus : 텍스트의 뭉치 각 토큰마다 그 단어가 중심단어가 될 수 있음 중심 단어가 등장했을 때, 맥락 단어가 함께 등장할 확률을최대화하는 방향으로 Parameter를 업데이트 최적화를 쉽게하기 위해 Log로 바꿔서 곱을 합으로 변경 -를 붙여서 Log-Likelihood를 최소화 각 단어는 Center Word와 Context word가 될 수 있음 u_o^T*v_c : 내적을 해서 유사도를 구함 데이터셋 예시 list(nltk.ngrams(tokenized, 5)) 예시 I have a puppy . His name is Bori . I love him . Corpus : 텍스트의 뭉치 T : 14 (코퍼스 내의 단어의 갯수) m : 2 (Window size, 모델러가 정할 하이퍼 파라미터) V : 11 (코퍼스 내의 단어의 집합. 중복을 제거한 집합) Corpus에서 단어 집합(Vocabulary)을 구해서 Index를 매긴다. Window size를 정하고 T개의 데이터셋를 준비한다. Center word와 Context word를 표현할 2개의 Embedding Matrix를 선언한다. P(o|c)를 구해서 Negative log-likelihood(loss)를 구한다. Gradient Descent를 사용하여 loss를 최소화한다. 학습이 끝난 뒤에는 Center vector와 Context vector를 평균해서 사용한다. 코드 예시 1 Corpus에서 단어 집합을 구해 Index를 매김 corpus = \"I have a puppy. His name is Bori. I love him.\" tokenized = nltk.word_tokenize(corpus) vocabulary = list(set(tokenized)) # 단어의 집합(중복 x) print(tokenized) print(vocabulary) word2index={} for voca in vocabulary: if word2index.get(voca)==None: word2index[voca]=len(word2index) print(word2index) 2 Window size를 정하고 데이터를 준비 WINDOW_SIZE = 2 windows = list(nltk.ngrams(['&lt;DUMMY&gt;'] * WINDOW_SIZE + tokenized + ['&lt;DUMMY&gt;'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) train_data = [] for window in windows: for i in range(WINDOW_SIZE * 2 + 1): if i == WINDOW_SIZE or window[i] == '&lt;DUMMY&gt;': continue train_data.append((window[WINDOW_SIZE], window[i])) print(train_data[:WINDOW_SIZE * 2]) # &gt;&gt;&gt; [('I', 'have'), ('I', 'a'), ('have', 'I'), ('have', 'a')] # 각 단어를 index로 바꾸고 LongTensor로 바꿔주는 함수 def prepare_word(word, word2index): return Variable(torch.LongTensor([word2index[word]])) X_p,y_p=[],[] for (center,context) in train_data: X_p.append(prepare_word(center, word2index).view(1, -1)) y_p.append(prepare_word(context, word2index).view(1, -1)) train_data = list(zip(X_p,y_p)) train_data[0] 3 Center word와 Context word를 표현할 2개의 Embedding Matrix를 선언 center_embed = nn.Embedding(len(word2index),3) context_embed = nn.Embedding(len(word2index),3) print(center_embed.weight) print(context_embed.weight) center,context = train_data[0] center_vector = center_embed(center) context_vector = context_embed(context) print(center_vector) print(context_vector) # 배치 사이즈 : 1 4 P(o|c)를 구해서 Negative log-likelihood(loss)를 구한다 # 분자값 score = torch.exp(context_vector.bmm(center_vector.transpose(1,2))).squeeze(2) score # 분모값 # 시퀀스(단어들의 연속된 리스트)가 들어오면 LongTensor로 매핑 def prepare_sequence(seq, word2index): idxs = list(map(lambda w: word2index[w], seq)) return Variable(torch.LongTensor(idxs)) vocabulary_tensor = prepare_sequence(vocabulary,word2index).view(1,-1) print(vocabulary_tensor) vocabulary_vector = context_embed(vocabulary_tensor) norm_scores = vocabulary_vector.bmm(center_vector.transpose(1, 2)) norm_scores = torch.sum(torch.exp(norm_scores,1)) print(norm_scores) # 결과 score/norm_scores 이 정도의 문장이라면 T*2m 만큼의 배치 사이즈로 한번에 J(θ) 구할 수 있지만, 보통은 코퍼스의 크기가 매우 크기 때문에 미니 배치로 Negative log likelihood를 구해서 업데이트한다.(SGD) 학습 후에는 두 벡터를 평균내서 최종 Word Vector로 사용함 빈도 수가 적은 단어는 stopwords로 지정 실습 Negative Sampling word2vec의 비효율성을 개선하려고 한 논문 기존 방식의 문제점 : J(θ)를 계산하는 것이 매우 비싸고 비효율적 보통의 코퍼스는 Vocabulary의 크기가 1만개 이상이다. 즉, 이 Softmax 연산이 매우 비싼 연산이다. (== 학습이 느림) 문제점을 해소하기 위해 2가지 방법을 제시. 그 중 1개가 Negative Sampling P(w)=U(x)^(3/4)/Z Unigram 분포! 3/4라는 지수의 역할 : 빈도가 낮은 단어가 샘플링될 확률을 높여줌(Negative Sample) SoftMax는 비싼 연산이기 때문에 다른 방법이 많이 고안됨(면접 질문~) Negative Sampling을 하면 vector간 연산이 가능해짐 Glove : Global Vector Timestep t 기준으로만 다른 주변단어와 co-occurrence를 포착했는데, 코퍼스(문서) 전체 기준으로 모든 Co-occurrence를 반영할 수는 없을까? I have a puppy . His name is Bori . I love him . And also I have a lovely cat. word frequency have 2 . 1 love 1 also 1 Co-occurrence 위 두 방식을 융합함! 통계적 정보를 반영해 fixed voca에 적용 Object Function 코퍼스 내의 모든 단어 W에 대해, 두 단어 벡터의 내적(유사도)과 두 단어의 Co-occurrence 확률(P_{ij})의 차이를 최소로 만들자 특정 단어 간의 Co-occurrence는 다른 것들에 비해 과하게 높을 수가 있습니다. 그래서 이렇게 Co-occurrence가 너무 큰 경우의 영향을 줄이기 위한 Weighting Function 을 사용합니다(f(P_{ij})) f(x)= if x &lt; x_{max} : (x/x_{max})^a otherwise : 1 Process Corpus에서 단어 집합(Vocabulary)을 구해서 Index를 매긴다. Window size를 정하고 T개의 데이터셋를 준비한다. Center word와 Context word를 표현할 2개의 Embedding Matrix를 선언한다. 전체 코퍼스에 대해 Co-occurrence matrix를 구축한다. =&gt; 단어셋이 클수록 메모리 에러가 남(VxV = 1억..) Objective function을 이용해 J(θ)를 구하고 여기에 -를 붙여서 loss로 만든다 Gradient Descent를 사용하여 loss를 최소화한다. 학습이 끝난 뒤에는 Center vector와 Context vector를 덧셈해서 사용한다. Pretrained word vector https://github.com/stanfordnlp/GloVe https://github.com/mmihaltz/word2vec-GoogleNews-vectors 대용량의 코퍼스에 미리 학습된 Word Vector들을 다운받아 사용할 수 있음. 많은 딥러닝 기반의 NLP 모델에선 이러한 Pre-trained word vector를 사용해 초기화함 Word Vector Evaluation Intrinsic a:b == c:? 데이터셋을 4개의 pair를 준비하고 구멍을 만듬 cosine similarity를 사용해서 Distance가 가장 짧은 i를 찾음 d = argmax_i((x_b-x_a+x_c)^T*x_i)/(||x_b-x_a+x_c||) GloVe의 경우 더 오랜 훈련시킬수록 모델의 정확도가 더 높아지는 경향이 있음(수렴 역시 더 빠름!) 무조건 GloVe가 좋다고할 순 없지만 대체로 좋은 편 여러 사람이 두 단어의 관계에 대한 상관관계를 1~10 사이의 점수를 매기면 이를 평균내서 Test Set으로 구축하긴 함(But.. 하기 힘듬 Extrinsic 실제 task를 평가 TensorboardX를 사용한 시각화 코드 필요한 데이터셋 학습이 끝난 weight matrix를 가지고 옴(345, 30) 각 인덱스에 맞는 단어(라벨) Search에 토치를 검색해서 볼 수 있음 from tensorboardX import SummaryWriter import pickle import os # 텐서보드 데이터 파일 초기화 try: shutil.rmtree('runs/') except: pass writer = SummaryWriter(comment='-embedding') matrix = (model.embedding_u.weight.data + model.embedding_v.weight.data)/2 label = [index2word[i] for i in range(len(index2word))] writer.add_embedding(matrix, metadata=label) writer.close() !tensorboard --logdir runs --port 6006 이미지를 크게 보시려면 우측마우스 클릭 후 새 창에서 이미지를 열어주세요! Using Pretrained Word Vector 자신이 가지고 있는 코퍼스보다 더 큰 코퍼스로 학습 된 매트릭스!! using Word2Vec, GloVe, FastText…. 이전 Transfer Learning에서 Fine tuning했던 것과 비슷한 효과를 기대함. 단어 단위에서의 Representation Power 빌리기! 거의 모든 딥러닝 NLP 모델에서의 기본재료 Unsupervised Learning! 레이블과 상관없이 큰 코퍼스로 학습을 시킨 후, 우리가 사용하려는 embedding matrix에 반영 코드 Gensim : 오직 word vector만을 위한 라이브러리! 굉장히 빠름! import torch import torch.nn as nn from torch.autograd import Variable import torch.optim as optim import torch.nn.functional as F import numpy as np import nltk import torchtext from konlpy.tag import Kkma tagger = Kkma() import gensim torchtext.vocab.pretrained_aliases corpus = open('data/corpus.txt','r',encoding=\"utf-8\").readlines() corpus = [c[:-1] for c in corpus] tokenized = [tagger.morphs(c) for c in corpus] model = gensim.models.Word2Vec(tokenized, size=15, window=5, min_count=2, workers=4) model.most_similar(\"토치\") model.wv.save_word2vec_format(\"data/word_vector_sample.bin\",binary=True) # 저장 pretrained_vectors_model = gensim.models.KeyedVectors.load_word2vec_format(\"data/word_vector_sample.bin\",binary=True) pretrained_vectors_model['토치'] vocab = list(pretrained_vectors_model.vocab.keys()) # Word2Vec에서 사용한 vocab pretrained_vectors=[] for vo in vocab: pretrained_vectors.append(pretrained_vectors_model[vo]) pretrained_vectors = np.vstack(pretrained_vectors) pretrained_vectors.shape # Init embedding matrix class MyModel(nn.Module): def __init__(self,vocab_size,embed_size): super(MyModel,self).__init__() self.embed = nn.Embedding(vocab_size,embed_size) def init_embed(self,pretrained_vectors): self.embed.weight.data = torch.from_numpy(pretrained_vectors).float() def forward(self,inputs): return self.embed(inputs) model = MyModel(len(vocab),15) model.embed.weight model.init_embed(pretrained_vectors) model.embed.weight CNN을 사용한 Sentence Classification 예시 FastText Word2Vec이나 GloVe와 같은 Word level representation model의 문제점은 선정의한 단어셋에 대한 매트릭스만을 학습시킬 수 있다는 것입니다 즉, 단어셋에 없는 단어를 만나면 아예 Indexing 자체를 할 수 없게 됩니다. 이러한 문제를 Out of Vocabulary(OOV)라고 부릅니다 FastText는 Subword Information을 이용하여 Word representation을 시도합니다. OOV 문제를 어느 정도 중화시켰습니다",
    "tags": "pytorch data",
    "url": "/data/2018/03/10/nlp-python/"
  },{
    "title": "OKKY 특강 - IT 회사 탐방기",
    "text": "OKKY에서 진행한 황후순님의 IT회사 탐방기 강연 후기입니다. 발표 자료 마이다스 아이티에서 기계분야 구조해석 시뮬레이션 소프트웨어, 건축 분야 BIM 소프트웨어, 인공지능 면접 솔루션 등을 만드심 하고싶은 것 : AI 분야 + 영어 =&gt; 싱가폴을 다녀왔습니다 싱가포르의 기업 페이스북 마이다스아이티보다도 더 밥이 잘 나옴! 나라별 음식 모든 층이 계단으로 연결되어 있음. face to face로 빠르게 일을 할 수 있음 포토존 전 세계 페이스북 사용자가 얼마나 접속했는지, 오늘 가입자가 몇명인지에 대해 나와있음. 큰 대시보드가 있음 가장 열려있는 느낌! 면접 : 1달, 2달 면접을 하면서 연봉을 측정 -&gt; 측정 이상을 부르면 협상 ㄱㄱ! AI 대기업은 석, 박사급만 뽑고 중국의 칭와대급 아니면 합격 불가 Localization(아시아 지역) StashAway 페이스북 맞은편에 있는 WeWork 핀테크 투자회사 면접 : 면접자가 먼저 연봉 제시 -&gt; 그에 맞는 면접을 준비 Scala 기반 회사지만 면접자가 선호하는 언어로 진행. 언어는 배우면 그만 20시간 이상 일을하면 짜름 개발자의 가치 == 연봉 Grab 동남아시아의 우버 모두 후드를 입고 있음. 살짝 추워야 머리 회전이 잘된다-? Google Android Developer라 코딩 테스트는 패스 화상으로 면접 진행 (라이브 코딩) 6개월간 영어 공부만 빡세게 하시고 가심 해외도 첫 연봉이 제일 중요 혼자 백날 공부해봤자 많이 늘지 않고, 회사 들어가서 하는것이 제일 효율이 좋음 영어 공부? 싱가폴가서 면접을 봐라 -&gt; 어느 정도 언어만 되면 충분. 실력이 중요 어려우면 직접 공부하고 꾸준히 갈고닦기 싱가포르 : 개발자가 일하기 좋은 나라 국내 기업 Naver Clova 정자역에서 10분 거리 네이버 전체의 AI를 담당하며 프로토 타입을 만든 후 제품에 반영 AI 분야에서 학사도 상관 없음. 면접자가 기여할 바가 있으면 그 Job으로 뽑음 성과가 내면 출근(아침에 찍고)하고 집에 가도됨 =&gt; 본인 업무만 명확히 하면 됨 우아한 형제들 배달의 민족 평생직장 따윈 없다. 최고가 되어 떠나라! 많은 기술력을 보유한 회사! 유망하게 성장중 카카오 관리자가 어떻게 해야 안될까? 개발을 계속 하고싶어함 카카오뱅크 인터넷 은행 다 잡고 성장중 구글코리아 영어를 중요하게 생각하며, 그것보다 더 중요한 것은 커뮤니케이션! 소통과 공감 자신이 했던 일을 PR해서 평가함 자신이 하고 싶은 일을 미친듯이 하는 천재들이 넘치는 회사 후순님은 구글 본사 딥러닝 조직을 가려고..! 삼성SDS SI 회사지만 점점 아웃소싱을 인소싱으로 바꾸는 중 카드를 1층부터 찍지 않아서 SDS 앞 마당(?)까지 종종 갈 수 있음 -&gt; 점점 열리고 있는 중 고객의 Needs를 맞춰줄 수 있는 것도 대단한 능력 셈웨어 MathFreeOn이라는 툴을 만들고 있음 실패는 없다. 모두 성공을 위한 과정이다 기술로 대표님이 하고싶은 꿈이 있으심. 꾸준히 진행중 추후 물리, 지구과학까지 확장할 예정 데일리 인텔리전스 다빈치라는 AI 플랫폼을 만들고 있는 회사 더루프 데일리 인텔리전스 아래에 있음 블록체인 글로벌 시장에서 20위건 인프런 온라인 강의 플랫폼 800% 이상 성장을 하고 있음 오마이랩 VP, CTO분들로 유명 코더블 SW교육 커리큘럼, 책을 만드는 에듀테크 + 출판사 체력이 정말 중요한 시대. 하고싶은 것을 하려면 체력이 있어야 합니다! 프로 vs 고수 프로는 무엇이고, 고수는 무엇일까? 프로 - 프로는 돈을 움직이는 사람 - 자신의 가치에 맞게 움직이고 회사에서 원하는 방향을 따르는 사람 - 고객이 원하는 것을 개발할 수 있는 사람 고수 - 프로처럼 일을 함 - 고객의 Needs + 자신의 생각을 담음 전설 - 일을 Art로 하고 있음 - 사회에 나오지 않음.. 나와주세요 일을 프로로 하고, 개인적인 일은 고수가 되서 Art적 마인드를 섞어보자! 관리자 vs 테크니션 굉장히 많은 기술들이 IT에 쏟아져 나오고, 매니징 관련도 많이 나오고 있음 정말 많은 분야가 있기 때문에 하고싶은 것을 깊게하면 되고, 가고자 하는 회사에 대한 기술을 공부 필요에 의해서 해야지, 신기술이 나왔다고 무작정 쫓는건 좀- 비추 관리자를 했지만 테크니션을 놓지 않으려고 하심. 똑같은 사람은 하나도 없음. 하고싶은 것을 선택하면 된다! 공부는 공부의 결과를 낳는다 공부할 것이 너무 많다. 공부를 하면 할수록 많은 것들이 나옴 그냥 인정하자ㅋ 다 하려고 욕심부리지 말고 내가 잘하는거나 더 잘하자- 하고싶은 일을 하다보니 다 되더라- 개발을 좋아하고 열정이 있는 사람을 뽑았음. 언어는 그저 도구고 필요할 때 보면 됨 책보다 API Document를 보는 습관을 갖자 책을 보고 맹신하지 말고 직접 다 해보기! 책이 올바르게 작성되었는지 검증할 수 있을 정도의 실력을 갖기 코드도 당연히 짜보고, 번역서라면 논문도 보고, 검색도 많이 해보고 꾸준히 공부를 해보자 기술을 공부하는 것보다 시야(그릇)를 넓혀야 많이 배울 수 있을 것 같음. 그리고 많은 사람을 만나봐라-",
    "tags": "lecture etc",
    "url": "/etc/2018/03/01/okky-spectial-lecture/"
  },{
    "title": "딥러닝에서 사용되는 여러 유형의 Convolution 소개",
    "text": "An Introduction to different Types of Convolutions in Deep Learning을 번역한 글입니다. 개인 공부를 위해 번역해봤으며 이상한 부분은 언제든 알려주세요 :) Convolution의 여러 유형에 대해 빠르게 소개하며 각각의 장점을 알려드리겠습니다. 단순화를 위해서, 이 글에선 2D Convolution에만 초점을 맞추겠습니다 Convolutions 우선 convolutional layer을 정의하기 위한 몇개의 파라미터를 알아야 합니다 2D convolution using a kernel size of 3, stride of 1 and padding 역자 : 파란색이 input이며 초록색이 output입니다 Kernel Size : kernel size는 convolution의 시야(view)를 결정합니다. 보통 2D에서 3x3 pixel로 사용합니다 Stride : stride는 이미지를 횡단할 때 커널의 스텝 사이즈를 결정합니다. 기본값은 1이지만 보통 Max Pooling과 비슷하게 이미지를 다운샘플링하기 위해 Stride를 2로 사용할 수 있습니다 Padding : Padding은 샘플 테두리를 어떻게 조절할지를 결정합니다. 패딩된 Convolution은 input과 동일한 output 차원을 유지하는 반면, 패딩되지 않은 Convolution은 커널이 1보다 큰 경우 테두리의 일부를 잘라버릴 수 있습니다 Input &amp; Output Channels : Convolution layer는 Input 채널의 특정 수(I)를 받아 output 채널의 특정 수(O)로 계산합니다. 이런 계층에서 필요한 파라미터의 수는 I*O*K로 계산할 수 있습니다. K는 커널의 수입니다 Dilated Convolutions ( 확장된 Convolution ) (a.k.a. atrous convolutions) 2D convolution using a 3 kernel with a dilation rate of 2 and no padding Dilated Convolution은 Convolutional layer에 또 다른 파라미터인 dilation rate를 도입했습니다. dilation rate은 커널 사이의 간격을 정의합니다. dilation rate가 2인 3x3 커널은 9개의 파라미터를 사용하면서 5x5 커널과 동일한 시야(view)를 가집니다. 5x5 커널을 사용하고 두번째 열과 행을 모두 삭제하면 (3x3 커널을 사용한 경우 대비)동일한 계산 비용으로 더 넓은 시야를 제공합니다. Dilated convolution은 특히 real-time segmentation 분야에서 주로 사용됩니다. 넓은 시야가 필요하고 여러 convolution이나 큰 커널을 사용할 여유가 없는 경우 사용합니다 역자 : Dilated Convolution은 필터 내부에 zero padding을 추가해 강제로 receptive field를 늘리는 방법입니다. 위 그림에서 진한 파란 부분만 weight가 있고 나머지 부분은 0으로 채워집니다. (receptive field : 필터가 한번 보는 영역으로 사진의 feature를 추출하기 위해선 receptive field가 높을수록 좋습니다) pooling을 수행하지 않고도 receptive field를 크게 가져갈 수 있기 때문에 spatial dimension 손실이 적고 대부분의 weight가 0이기 때문에 연산의 효율이 좋습니다. 공간적 특징을 유지하기 때문에 Segmentation에서 많이 사용합니다 Transposed Convolutions (a.k.a. deconvolutions or fractionally strided convolutions) 어떤 곳에선 deconvolution이라는 이름을 사용하지만 실제론 deconvolution이 아니기 때문에 부적절합니다. 상황을 악화시키기 위해 deconvolution이 존재하지만, 딥러닝 분야에선 흔하지 않습니다. 실제 deconvolution은 convolution의 과정을 되돌립니다. 하나의 convolutional layer에 이미지를 입력한다고 상상해보겠습니다. 이제 출력물을 가져와 블랙 박스에 넣으면 원본 이미지가 다시 나타납니다. 이럴 경우 블랙박스가 deconvolution을 수행한다고 할 수 있습니다. 이 deconvolution이 convolutional layer가 수행하는 것의 수학적 역 연산입니다. 역자 : 왜 deconvolution이 아닌지는 링크에 나와있습니다 Transposed Convolution은 deconvolutional layer와 동일한 공간 해상도를 생성하기 점은 유사하지만 실제 수행되는 수학 연산은 다릅니다. Transposed Convolutional layer는 정기적인 convolution을 수행하며 공간의 변화를 되돌립니다. 2D convolution with no padding, stride of 2 and kernel of 3 혼란스러울 수 있으므로 구체적인 예를 보겠습니다. convolution layer에 넣을 5x5 이미지가 있습니다. stride는 2, padding은 없고 kernel은 3x3입니다. 이 결과는 2x2 이미지가 생성됩니다. 이 과정을 되돌리고 싶다면, 역 수학 연산을 위해 input의 각 픽셀으로부터 9개의 값을 뽑아야 합니다. 그 후에 우리는 stride가 2인 출력 이미지를 지나갑니다. 이 방법이 deconvolution입니다 Transposed 2D convolution with no padding, stride of 2 and kernel of 3 transposed convolution은 위와 같은 방법을 사용하지 않습니다. deconvolution과 공통점은 convolution 작업을 하면서 5x5 이미지의 output을 생성하는 것입니다. 이 작업을 하기 위해 input에 임의의 padding을 넣어야 합니다. 상상할 수 있듯, 이 단계에선 위의 과정을 반대로 수행하지 않습니다. 단순히 이전 공간 해상도를 재구성하고 convolution을 수행합니다. 수학적 역 관계는 아니지만 인코더-디코더 아키텍쳐의 경우 유용합니다. 이 방법은 2개의 별도 프로세스를 진행하는 것 대신 convolution된 이미지의 upscaling을 결합할 수 있습니다 역자 : Transposed Convolution는 일반적인 convolution을 반대로 수행하고 싶은 경우에 사용하며, 커널 사이에 0을 추가합니다 위 그림은 일반적인 convolution 연산을 행렬로 표현한 것입니다 transposed convolution 연산을 행렬로 표현한 것입니다. 첫 이미지의 sparse 매트릭스 C를 inverse해서 우변(Y output)에 곱해줍니다. 그러면 Input의 값을 구할 수 있습니다. 정리하면 전치(transpose)하여 우변에 곱해주기 때문에 transposed convolution이라 부릅니다. Up-sampling with Transposed Convolution 번역 글을 참고하시면 명쾌하게 이해가 될거에요! Convolution Operation은 input values와 output values 사이에 공간 연결성을 가지고 있습니다. 3x3 kernel을 사용한다면, 9개의 values가(kernel) 1개의 value(output, 문지른 후의 결과물)와 연결됩니다. 따라서 many to one 관계라고 볼 수 있습니다 Transposed Convolution은 1개의 value를 9개의 values로 변경합니다. 이것은 one to many 관계라고 볼 수 있습니다. (2019년 9월) 역자 : 위 convolution 매트릭스 연산의 그림에 오류가 있습니다. 한번 어떤 부분이 오류인지 생각해보신 후, 답이 궁금하시면 댓글을 확인해주세요 :) 제보해주신 정종원님, 김남욱님 감사합니다 Separable Convolutions separable convolution에선 커널 작업을 여러 단계로 나눌 수 있습니다. convolution을 y = conv(x, k)로 표현해봅시다. x는 입력 이미지, y는 출력 이미지, k는 커널입니다. 그리고 k=k1.dot(k2)로 계산된다고 가정해보겠습니다. 이것은 K와 2D convolution을 수행하는 대신 k1와 k2로 1D convolution하는 것과 동일한 결과를 가져오기 때문에 separable convolution이 됩니다. Sobel X and Y filters 이미지 처리에서 자주 사용되는 Sobel 커널을 예로 들겠습니다. 벡터 [1, 0, -1]과 [1, 2, 1].T를 곱하면 동일한 커널을 얻을 수 있습니다. 동일 작업을 하기 위해 9개의 파라미터 대신 6개가 필요합니다. 이 사례는 Spatial Separable Convolution의 예시이지만, 딥러닝에선 사용되지 않습니다 수정 : 사실 1xN, Nx1 커널 레이어를 쌓아 Separable convolution과 유사한 것을 만들 수 있습니다. 이것은 최근 유망한 결과를 보여준 EffNet라는 아키텍쳐에서 사용되었습니다. 뉴럴넷에선 depthwise separable convolution라는 것을 주로 사용합니다. 이 방법은 채널을 분리하지 않고 spatial convolution을 수행한 다음 depthwise convolution을 수행합니다. 예를 들어보겠습니다. 16개의 input 채널과 32개의 output 채널에 3x3 convolutional 레이어가 있다고 가정하겠습니다. 16개의 채널마다 32개의 3x3 커널이 지나가며 512(16*32)개의 feature map이 생성됩니다. 그 다음, 모든 입력 채널에서 1개의 feature map을 병합하여 추가합니다. 32번 반복하면 32개의 output 채널을 얻을 수 있습니다. 같은 예제에서 depthwise separable convolution을 위해 1개의 3x3 커널로 16 채널을 탐색해 16개의 feature map을 생성합니다. 합치기 전에 32개의 1x1 convolution으로 16개의 featuremap을 지나갑니다. 결과적으로 위에선 4068(16*32*3*3) 매개 변수를 얻는 반면 656(16*3*3 + 16*32*1*1) 매개변수를 얻습니다 이 예는 depthwise separable convolution(depth multiplier가 1이라고 불리는)것을 구체적으로 구현한 것입니다. 이런 layer에서 가장 일반적인 형태입니다. 우리는 spatial하고 depthwise한 정보를 나눌 수 있다는 가정하에 이 작업을 합니다. Xception 모델의 성능을 보면 이 이론이 효과가 있는 것으로 보입니다. depthwise seprable convolution은 매개변수를 효율적으로 사용하기 때문에 모바일 장치에도 사용됩니다 역자 : 채널, 공간상 상관성 분리를 하는 인셉션 모델을 극단적으로 밀어붙여 depthwise separable convolution 구조를 만듭니다. input의 1x1 convolution을 씌운 후 나오는 모든 채널에 3x3 convolution을 수행합니다. 원글에서 separable convolution의 설명이 부족한 것 같아 PR12의 영상을 보고 이해했습니다 유재준님의 PR-034: Inception and Xception과 이진원님의 PR-044: MobileNet을 보면 이해가 잘됩니다 :) Questions? 이것으로 여러 종류의 convolution 여행을 끝내겠습니다. Convolution에 대한 짧은 요약을 가지고가길 바라며 남아있는 질문은 댓글을 남겨주시고, 더 많은 convolution 애니메이션이 있는 Github를 확인해주세요 번역시 참고한 자료 라온피플 Deconvolutional Network 라온 피플 Dilated Convolution 라온 피플 Semantic Segmentation Separable Kernel Convolution MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications Convolution Arithmetic in Deep Learning Part 2 Tensorflow KR - PR12 유튜브 강의",
    "tags": "dl data",
    "url": "/data/2018/02/23/introduction-convolution/"
  },{
    "title": "데이터 사이언스 인터뷰 질문 모음집",
    "text": "데이터 사이언스 분야의 인터뷰 질문을 모아봤습니다. (데이터 분석가 / 데이터 사이언티스트 / 데이터 엔지니어) 구직자에겐 예상 질문을 통해 면접 합격을 할 수 있도록, 면접관에겐 좋은 면접 질문을 할 수 있도록, 딥러닝 공부하는 분들에겐 용어를 알 수 있도록 도와드리기 위해 본 문서를 만들게 되었습니다. 다만 여기에 나온 모든 것들을 알 필요는 없습니다. 특정 분야에 이런 것들이 있나보다~ 정도의 키워드만 가져가도 충분히 좋을 것 같습니다! (단, 테마가 어색하거나 질문이 이상한 것들이 존재할 수 있으니 꼭 선별해서 보시길 부탁드립니다!!!) 정답은 따로 작성하지 않으며 질문 위주로 작성했습니다. ( 하용호님, 남세동님의 허락을 받아 내용을 포함했습니다 ) 질문의 테마를 넣기 애매할 경우엔 제가 임의로 넣었습니다. 또한 특정 분야는 제 얕은 지식으로 문제를 작성했으니, 문제가 부족하다고 느끼시면 언제든 댓글 날려주세요 :) Github에서 보실 분들은 링크를 클릭해주세요 Data Science 공부에 대한 전반적인 내용이 궁금하신 분은 I-want-to-study-Data-Science 문서를 보시면 좋을 것 같아요 :) 개발 전반적인 면접 질문은 Interview_Question_for_Beginner를 추천합니다! 신입 데이터 엔지니어의 경우 해당 문서에 있는 CS 내용을 숙지하면 좋을 것 같습니다 Contents 공통 질문 프로젝트 통계 및 수학 분석 일반 머신러닝 딥러닝 딥러닝 일반 컴퓨터 비전 자연어 처리 강화학습 GAN 추천 시스템 데이터베이스 데이터 시각화 시스템 엔지니어링 분산처리 웹 아키텍쳐 서비스 구현 대 고객 사이드 개인정보 공통 질문 왜 해당 직군으로 지원했나요? 왜 저희 회사에 지원하셨나요? 해당 직군의 매력이 무엇이라고 생각하나요? 해당 직군에서 본인의 장점은? 해당 직군을 하면서 이루고자 하는 목표는? 해당 직군을 하기 위해 어떤 노력을 했나요? 왜 저희가 지원자를 뽑아야 하나요? 지원자의 단점은 무엇인가요? 목차로 이동 프로젝트 데이터를 어떻게 구했나요? 해당 프로젝트에서 왜 이 알고리즘을 사용했나요? 그 알고리즘과 유사한 알고리즘이 존재하지 않나요? 해당 알고리즘의 단점은? 해당 프로젝트에서 지원자는 어떤 일을 했나요? 해당 프로젝트에서 지원자가 느낀 점은? 해당 프로젝트를 다시 진행한다고 하면 어떻게 할 것인가요? Kaggle에서 수상을 하면 데이터 분석을 잘 할까요? 목차로 이동 통계 및 수학 고유값(eigen value)와 고유벡터(eigen vector)에 대해 설명해주세요. 그리고 왜 중요할까요? 샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요? 확률 모형과 확률 변수는 무엇일까요? 누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요 베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포 / 디리클레 분포에 대해 설명해주세요. 혹시 연관된 분포가 있다면 연관 관계를 설명해주세요 조건부 확률은 무엇일까요? 공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요 신뢰 구간의 정의는 무엇인가요? p-value를 고객에게는 뭐라고 설명하는게 이해하기 편할까요? p-value는 요즘 시대에도 여전히 유효할까요? 언제 p-value가 실제를 호도하는 경향이 있을까요? A/B Test 등 현상 분석 및 실험 설계 상 통계적으로 유의미함의 여부를 결정하기 위한 방법에는 어떤 것이 있을까요? R square의 의미는 무엇인가요? 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요? 중심극한정리는 왜 유용한걸까요? 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요. 요즘같은 빅데이터(?)시대에는 정규성 테스트가 의미 없다는 주장이 있습니다. 맞을까요? 어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요? “likelihood”와 “probability”의 차이는 무엇일까요? 통계에서 사용되는 bootstrap의 의미는 무엇인가요. 모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요? 베이지안과 프리퀀티스트간의 입장차이를 설명해주실 수 있나요? 검정력(statistical power)은 무엇일까요? missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요? 아웃라이어의 판단하는 기준은 무엇인가요? 콜센터 통화 지속 시간에 대한 데이터가 존재합니다. 이 데이터를 코드화하고 분석하는 방법에 대한 계획을 세워주세요. 이 기간의 분포가 어떻게 보일지에 대한 시나리오를 설명해주세요 출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까? 필요한 표본의 크기를 어떻게 계산합니까? Bias를 통제하는 방법은 무엇입니까? 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요 목차로 이동 분석 일반 좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요? “상관관계는 인과관계를 의미하지 않는다”라는 말이 있습니다. 설명해주실 수 있나요? A/B 테스트의 장점과 단점, 그리고 단점의 경우 이를 해결하기 위한 방안에는 어떤 것이 있나요? 각 고객의 웹 행동에 대하여 실시간으로 상호작용이 가능하다고 할 때에, 이에 적용 가능한 고객 행동 및 모델에 관한 이론을 알아봅시다. 고객이 원하는 예측모형을 두가지 종류로 만들었다. 하나는 예측력이 뛰어나지만 왜 그렇게 예측했는지를 설명하기 어려운 random forest 모형이고, 또다른 하나는 예측력은 다소 떨어지나 명확하게 왜 그런지를 설명할 수 있는 sequential bayesian 모형입니다.고객에게 어떤 모형을 추천하겠습니까? 고객이 내일 어떤 상품을 구매할지 예측하는 모형을 만들어야 한다면 어떤 기법(예: SVM, Random Forest, logistic regression 등)을 사용할 것인지 정하고 이를 통계와 기계학습 지식이 전무한 실무자에게 설명해봅시다. 나만의 feature selection 방식을 설명해봅시다. 데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering을 어떻게 풀어야할까요? 목차로 이동 머신러닝 Cross Validation은 무엇이고 어떻게 해야하나요? 회귀 / 분류시 알맞은 metric은 무엇일까요? 알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …) 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요? Local Minima와 Global Minima에 대해 설명해주세요. 차원의 저주에 대해 설명해주세요 dimension reduction기법으로 보통 어떤 것들이 있나요? PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요? LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요? Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요? 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요? SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? 거기서 어떤 장점이 발생했나요? 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요. Association Rule의 Support, Confidence, Lift에 대해 설명해주세요. 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요? 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요? 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요? 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요? ROC 커브에 대해 설명해주실 수 있으신가요? 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요? K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고) L1, L2 정규화에 대해 설명해주세요 XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요? 앙상블 방법엔 어떤 것들이 있나요? SVM은 왜 좋을까요? feature vector란 무엇일까요? 좋은 모델의 정의는 무엇일까요? 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요? 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요? OLS(ordinary least squre) regression의 공식은 무엇인가요? 목차로 이동 딥러닝 딥러닝 일반 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는? 왜 갑자기 딥러닝이 부흥했을까요? 마지막으로 읽은 논문은 무엇인가요? 설명해주세요 Cost Function과 Activation Function은 무엇인가요? Tensorflow, Keras, PyTorch, Caffe, Mxnet 중 선호하는 프레임워크와 그 이유는 무엇인가요? Data Normalization은 무엇이고 왜 필요한가요? 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등) 오버피팅일 경우 어떻게 대처해야 할까요? 하이퍼 파라미터는 무엇인가요? Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요? 볼츠만 머신은 무엇인가요? 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는? Non-Linearity라는 말의 의미와 그 필요성은? ReLU로 어떻게 곡선 함수를 근사하나? ReLU의 문제점은? Bias는 왜 있는걸까? Gradient Descent에 대해서 쉽게 설명한다면? 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까? GD 중에 때때로 Loss가 증가하는 이유는? 중학생이 이해할 수 있게 더 쉽게 설명 한다면? Back Propagation에 대해서 쉽게 설명 한다면? Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는? GD가 Local Minima 문제를 피하는 방법은? 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은? Training 세트와 Test 세트를 분리하는 이유는? Validation 세트가 따로 있는 이유는? Test 세트가 오염되었다는 말의 뜻은? Regularization이란 무엇인가? Batch Normalization의 효과는? Dropout의 효과는? BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는? GAN에서 Generator 쪽에도 BN을 적용해도 될까? SGD, RMSprop, Adam에 대해서 아는대로 설명한다면? SGD에서 Stochastic의 의미는? 미니배치를 작게 할때의 장단점은? 모멘텀의 수식을 적어 본다면? 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까? 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까? Back Propagation은 몇줄인가? CNN으로 바꾼다면 얼마나 추가될까? 간단한 MNIST 분류기를 TF, Keras, PyTorch 등으로 작성하는데 몇시간이 필요한가? CNN이 아닌 MLP로 해도 잘 될까? 마지막 레이어 부분에 대해서 설명 한다면? 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면? 만약 한글 (인쇄물) OCR을 만든다면 데이터 수집은 어떻게 할 수 있을까? 딥러닝할 때 GPU를 쓰면 좋은 이유는? 학습 중인데 GPU를 100% 사용하지 않고 있다. 이유는? GPU를 두개 다 쓰고 싶다. 방법은? 학습시 필요한 GPU 메모리는 어떻게 계산하는가? TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는? 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가? 목차로 이동 컴퓨터 비전 OpenCV 라이브러리만을 사용해서 이미지 뷰어(Crop, 흑백화, Zoom 등의 기능 포함)를 만들어주세요 딥러닝 발달 이전에 사물을 Detect할 때 자주 사용하던 방법은 무엇인가요? Fatser R-CNN의 장점과 단점은 무엇인가요? dlib은 무엇인가요? YOLO의 장점과 단점은 무엇인가요? 제일 좋아하는 Object Detection 알고리즘에 대해 설명하고 그 알고리즘의 장단점에 대해 알려주세요 그 이후에 나온 더 좋은 알고리즘은 무엇인가요? Average Pooling과 Max Pooling의 차이점은? Deep한 네트워크가 좋은 것일까요? 언제까지 좋을까요? Residual Network는 왜 잘될까요? Ensemble과 관련되어 있을까요? CAM(Class Activation Map)은 무엇인가요? Localization은 무엇일까요? 자율주행 자동차의 원리는 무엇일까요? Semantic Segmentation은 무엇인가요? Visual Q&amp;A는 무엇인가요? Image Captioning은 무엇인가요? Fully Connected Layer의 기능은 무엇인가요? Neural Style은 어떻게 진행될까요? CNN에 대해서 아는대로 얘기하라 CNN이 MLP보다 좋은 이유는? 어떤 CNN의 파라미터 개수를 계산해 본다면? 주어진 CNN과 똑같은 MLP를 만들 수 있나? 풀링시에 만약 Max를 사용한다면 그 이유는? 시퀀스 데이터에 CNN을 적용하는 것이 가능할까? 목차로 이동 자연어 처리 One Hot 인코딩에 대해 설명해주세요 POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요? 문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요? 뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을까요? 음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요? 잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요? 영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요? 뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요? Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요? 영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요? TF-IDF 점수는 무엇이며 어떤 경우 유용한가요? 한국어에서 많이 사용되는 사전은 무엇인가요? Regular grammar는 무엇인가요? regular expression과 무슨 차이가 있나요? RNN에 대해 설명해주세요 LSTM은 왜 유용한가요? Translate 과정 Flow에 대해 설명해주세요 n-gram은 무엇일까요? PageRank 알고리즘은 어떻게 작동하나요? depedency parsing란 무엇인가요? Word2Vec의 원리는? 그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는? 그 그림에서 오른쪽 파라메터들의 의미는 무엇일까? 남자와 여자가 가까울까? 남자와 자동차가 가까울까? 번역을 Unsupervised로 할 수 있을까? 목차로 이동 강화학습 MDP는 무엇일까요? 가치함수는 무엇일까요? 수식으로도 표현해주세요 벨만 방정식은 무엇일까요? 수식으로도 표현해주세요 강화학습에서 다이나믹 프로그래밍은 어떤 의미를 가질까요? 한계점은 무엇이 있을까요? 몬테카를로 근사는 무엇일까요? 가치함수를 추정할 때 어떻게 사용할까요? Value-based Reinforcement Learning과 Policy based Reinforcement Learning는 무엇이고 어떤 관계를 가질까요? 강화학습이 어려운 이유는 무엇일까요? 그것을 어떤 방식으로 해결할 수 있을까요? 강화학습을 사용해 테트리스에서 고득점을 얻는 프로그램을 만드려고 합니다. 어떻게 만들어야 할까요? 목차로 이동 GAN GAN에 대해 아는대로 설명해주세요 GAN의 단점은 무엇인가요? LSGAN에 대해 설명해주세요 GAN이 왜 뜨고 있나요? Auto Encoder에 대해서 아는대로 얘기하라 MNIST AE를 TF나 Keras등으로 만든다면 몇줄일까? MNIST에 대해서 임베딩 차원을 1로 해도 학습이 될까? 임베딩 차원을 늘렸을 때의 장단점은? AE 학습시 항상 Loss를 0으로 만들수 있을까? VAE는 무엇인가? 간단한 MNIST DCGAN을 작성한다면 TF 등으로 몇줄 정도 될까? GAN의 Loss를 적어보면? D를 학습할때 G의 Weight을 고정해야 한다. 방법은? 학습이 잘 안될때 시도해 볼 수 있는 방법들은? 목차로 이동 추천 시스템 추천 시스템에서 사용할 수 있는 거리는 무엇이 있을까요? User 베이스 추천 시스템과 Item 베이스 추천 시스템 중 단기간에 빠른 효율을 낼 수 있는 것은 무엇일까요? 성능 평가를 위해 어떤 지표를 사용할까요? Explicit Feedback과 Implicit Feedback은 무엇일까요? Impicit Feedback을 어떻게 Explicit하게 바꿀 수 있을까요? Matrix Factorization은 무엇인가요? 해당 알고리즘의 장점과 단점은? SQL으로 조회 기반 Best, 구매 기반 Best, 카테고리별 Best를 구하는 쿼리를 작성해주세요 추천 시스템에서 KNN 알고리즘을 활용할 수 있을까요? 유저가 10만명, 아이템이 100만개 있습니다. 이 경우 추천 시스템을 어떻게 구성하시겠습니까? 딥러닝을 활용한 추천 시스템의 사례를 알려주세요 두 추천엔진간의 성능 비교는 어떤 지표와 방법으로 할 수 있을까요? 검색엔진에서 쓰던 방법을 그대로 쓰면 될까요? 안될까요? Collaborative Filtering에 대해 설명한다면? Cold Start의 경우엔 어떻게 추천해줘야 할까요? 고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요? 위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다. 목차로 이동 데이터베이스 PostgreSQL의 장점은 무엇일까요? 인덱스는 크게 Hash 인덱스와 B+Tree 인덱스가 있습니다. 이것은 무엇일까요? 인덱스 Scan 방식은 무엇이 있나요? 인덱스 설계시 NULL값은 고려되야 할까요? Nested Loop 조인은 무엇일까요? Windows 함수는 무엇이고 어떻게 작성할까요? KNN 알고리즘을 쿼리로 구현할 수 있을까요? MySQL에서 대량의 데이터(500만개 이상)를 Insert해야하는 경우엔 어떻게 해야할까요? RDB의 char와 varchar의 차이는 무엇일까요? 구글의 BigQuery, AWS의 Redshift는 기존 RDB와 무슨 차이가 있을까요? 왜 빠를까요? 쿼리의 성능을 확인하기 위해 어떤 쿼리문을 작성해야 할까요? MySQL이 요새 느리다는 신고가 들어왔습니다. 첫번째로 무엇을 확인하시고 조정하시겠나요? 동작하는 MySQL에 Alter table을 하면 안되는 이유를 설명해주세요. 그리고 대안을 설명해주세요 빡세게 동작하고 있는 MySQL을 백업뜨기 위해서는 어떤 방법이 필요할까요? 목차로 이동 데이터 시각화 네트워크 관계를 시각화해야 할 경우 어떻게 해야할까요? Tableau같은 BI Tool은 어느 경우 도입하면 좋을까요? “신규/재방문자별 지역별(혹은 일별) 방문자수와 구매전환율”이나 “고객등급별 최근방문일별 고객수와 평균구매금액”와 같이 4가지 이상의 정보를 시각화하는 가장 좋은 방법을 추천해주세요 구매에 영향을 주는 요소의 발견을 위한 관점에서, 개인에 대한 쇼핑몰 웹 활동의 시계열 데이터를 효과적으로 시각화하기 위한 방법은 무엇일까요? 표현되어야 하는 정보(feature)는 어떤 것일까요? 실제시 어떤 것이 가장 고민될까요? 파이차트는 왜 구릴까요? 언제 구린가요? 안구릴때는 언제인가요? 히스토그램의 가장 큰 문제는 무엇인가요? 워드클라우드는 보기엔 예쁘지만 약점이 있습니다. 어떤 약점일까요? 어떤 1차원값이, 데이터가 몰려있어서 직선상에 표현했을 때 보기가 쉽지 않습니다. 어떻게 해야할까요? 목차로 이동 시스템 엔지니어링 지속적인 Cron 작업이 필요합니다. (dependency가 있는 작업들도 존재합니다) 어떻게 작업들을 관리할까요? 처음 서버를 샀습니다. 어떤 보안적 조치를 먼저 하시겠습니까? SSH로의 brute-force attack을 막기 위해서 어떤 조치를 취하고 싶으신가요? 프로세스의 CPU 상태를 보기 위해 top을 했습니다. user,system,iowait중에 뭐를 제일 신경쓰시나요? 이상적인 프로그램이라면 어떻게 저 값들이 나오고 있어야 할까요? iowait이 높게 나왔다면, 내가 해야하는 조치는 무엇인가요? (돈으로 해결하는 방법과 소프트웨어로 해결하는 방법을 대답해주세요) 동시에 10개의 컴퓨터에 라이브러리를 설치하는 일이 빈번히 발생합니다. 어떤 해결책이 있을까요? screen과 tmux중에 뭘 더 좋아하시나요? vim입니까. emacs입니까. 소속을 밝히세요. 가장 좋아하는 리눅스 배포판은 뭡니까. 왜죠? 관리하는 컴퓨터가 10대가 넘었습니다. 중요한 모니터링 지표는 뭐가 있을까요? 뭐로 하실건가요? GIT의 소스가 있고, 서비스 사용중인 웹서버가 10대 이상 넘게 있습니다. 어떻게 배포할건가요? 목차로 이동 분산처리 Apache Beam에 대해 아시나요? 기존 하둡과 어떤 차이가 있을까요? 좋게 만들어진 MapReduce는 어떤 프로그램일까요? 데이터의 Size 변화의 관점에서 설명할 수 있을까요? 여러 MR작업의 연쇄로 최종결과물이 나올때, 중간에 작업이 Fail날수 있습니다. 작업의 Fail은 어떻게 모니터링 하시겠습니까? 작업들간의 dependency는 어떻게 해결하시겠습니까? 분산환경의 JOIN은, 보통 디스크, CPU, 네트워크 중 어디에서 병목이 발생할까요? 이를 해결하기 위해 무엇을 해야 할까요? 암달의 법칙에 대해 말해봅시다. 그러므로 왜 shared-nothing 구조로 만들어야 하는지 설명해봅시다. shared-nothing 구조의 단점도 있습니다. 어떤 것이 해당할까요? Spark이 Hadoop보다 빠른 이유를 I/O 최적화 관점에서 생각해봅시다. 카산드라는 망한것 같습니다. 왜 망한것 같나요? 그래도 활용처가 있다면 어디인것 같나요. TB 단위 이상의 기존 데이터와 시간당 GB단위의 신생 로그가 들어오는 서비스에서 모든 가입자에게 개별적으로 계산된 실시간 서비스(웹)를 제공하기 위한 시스템 구조를 구상해봅시다. 대용량 자료를 빠르게 lookup해야 하는 일이 있습니다. (100GB 이상, 100ms언더로 특정자료 찾기). 어떤 백엔드를 사용하시겠나요? 느린 백엔드를 사용한다면 이를 보완할 방법은 뭐가 있을까요? 데이터를 여러 머신으로 부터 모으기 위해 여러 선택지가 있을 수 있습니다. (flume, fluentd등) 아예 소스로부터 kafka등의 메시징 시스템을 바로 쓸 수도 있습니다. 어떤 것을 선호하시나요? 왜죠? 목차로 이동 웹 아키텍쳐 트래픽이 몰리는 상황입니다. AWS의 ELB 세팅을 위해서 웹서버는 어떤 요건을 가져야 쉽게 autoscale가능할까요? 왜 Apache보다 Nginx가 성능이 좋을까요? node.js가 성능이 좋은 이유와 곁들여 설명할 수 있을까요? node.js는 일반적으로 빠르지만 어떤 경우에는 쓰면 안될까요? 하나의 IP에서 여러 도메인의 HTTPS 서버를 운영할 수 있을까요? 안된다면 왜인가요? 또 이걸 해결하는 방법이 있는데 그건 뭘까요? 개발이 한창 진행되는 와중에도 서비스는 계속 운영되어야 합니다. 이를 가능하게 하는 상용 deploy 환경은 어떻게 구현가능한가요? WEB/WAS/DB/Cluster 각각의 영역에서 중요한 변화가 수반되는 경우에도 동작 가능한, 가장 Cost가 적은 방식을 구상하고 시나리오를 만들어봅시다. 목차로 이동 서비스 구현 크롤러를 파이썬으로 구현할 때 BeautifulSoup과 Selenium의 장단점은 무엇일까요? 빈번한 접속으로 우리 IP가 차단되었을 때의 해결책은? (대화로 푼다. 이런거 말구요) 당장 10분안에 사이트의 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? 타 서비스를 써도됩니다. 신규 방문자와 재 방문자를 구별하여 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? R의 결과물을 python으로 만든 대시보드에 넣고 싶다면 어떤 방법들이 가능할까요? 쇼핑몰의 상품별 노출 횟수와 클릭수를 손쉽게 수집하려면 어떻게 해야 할까요? 여러 웹사이트를 돌아다니는 사용자를 하나로 엮어서 보고자 합니다. 우리가 각 사이트의 웹에 우리 코드를 삽입할 수 있다고 가정할 때, 이것이 가능한가요? 가능하다면, 그 방법에는 어떤 것이 있을까요? 고객사 혹은 외부 서버와의 데이터 전달이 필요한 경우가 있습니다. 데이터 전달 과정에서 보안을 위해 당연히(plain text)로 전송하는 것은 안됩니다. 어떤 방법이 있을까요? 목차로 이동 대 고객 사이드 고객이 궁금하다고 말하는 요소가 내가 생각하기에는 중요하지 않고 다른 부분이 더 중요해 보입니다. 어떤 식으로 대화를 풀어나가야 할까요? 현업 카운터 파트와 자주 만나며 실패한 분석까지 같이 공유하는 경우와, 시간을 두고 멋진 결과만 공유하는 케이스에서 무엇을 선택하시겠습니까? 고객이 질문지 리스트를 10개를 주었습니다. 어떤 기준으로 우선순위를 정해야 할까요? 오프라인 데이터가 결합이 되어야 해서, 데이터의 피드백 주기가 매우 느리고 정합성도 의심되는 상황입니다. 우리가 할 수 있는 액션이나 방향 수정은 무엇일까요? 동시에 여러개의 A/B테스트를 돌리기엔 모수가 부족한 상황입니다. 어떻게 해야할까요? 고객사가 과도하게 정보성 대시보드만을 요청할 경우, 어떻게 대처해야 할까요? 고객사에게 위클리 리포트를 제공하고 있었는데, 금주에는 별다른 내용이 없었습니다. 어떻게 할까요? 카페24, 메이크샵 같은 서비스에서 데이터를 어떻게 가져오면 좋을까요? 기존에 같은 목적의 업무를 수행하던 조직이 있습니다. 어떻게 관계 형성을 해 나가야 할까요. 혹은 일이 되게 하기 위해서는 어떤 부분이 해소되어야 할까요. 인터뷰나 강의에 활용하기 위한 백데이터는 어느 수준까지 일반화 해서 사용해야 할까요? 고객사가 우리와 일하고 싶은데 현재는 capa가 되지 않습니다. 어떻게 대처해야 할까요? 목차로 이동 개인정보 어떤 정보들이 개인정보에 해당할까요? ID는 개인정보에 해당할까요? 이를 어기지 않는 합법적 방법으로 식별하고 싶으면 어떻게 해야할까요? 국내 개인 정보 보호 현황에 대한 견해는 어떠한지요? 만약 사업을 진행하는데 장애요소로 작용한다면, 이에 대한 해결 방안은 어떤 것이 있을까요? 제3자 쿠키는 왜 문제가 되나요? 목차로 이동 Reference 하용호님 자료 남세동님 자료 Data Science Interview Questions &amp; Detailed Answers Deep Learning Interview Questions and Answers Must know questions deeplearning : 객관식 딥러닝 문제 My deep learning job interview experience sharing Natural Language Processing Engineer Interview Questions",
    "tags": "dl data",
    "url": "/data/2018/02/17/datascience-interivew-questions/"
  },{
    "title": "글또 1회차 모임 후기 및 다짐",
    "text": "글또 1회차 모임을 2월 10일 토요일에 진행했습니다. 모집글을 올리기 전엔 “잘 될까-?”라고 생각했지만 사실 잘 안되도 상관없었습니다. 지인들과 소소하게 글쓰기를 하면 되거든요! 글또 규모 총 17분이 지원해주셨고 몇분은 사정이 생겨 참여를 포기하고 13분과 글또를 진행하게 되었습니다. 아직 Deposit을 납부하지 않은 분도 계시지만, 일단 소규모로 시작하게 되었습니다 오프라인 모임은 즐겁고 화기애애했던 것 같고 열심히 할 수 있을 것 같은 예감! 모임의 장으로서 다짐 모임의 장인만큼 많은 분들이 글쓰기 습관이 생기도록 서포트할 예정입니다 :) 개인적인 다짐 개인적으론 계속 진행하고 있는 프로젝트 내용에 대한 글을 시리즈로 작성하는 것!! SQL 시리즈와 컴퓨터비전쪽을 먼저 작성하지 않을까 생각되네요. 한 분야를 빠르게 작성하고 다른 분야로 넘어갈지 동시에 진행할지 고민중입니다 2월 설날쯤 어떤 글을 작성할지 더 깊게 고민해볼 예정입니다 혁우님, 나영님, 태우님, 현아님, 지환님, 지연님, 단비님, 주원님, 준범님, 근택님, 민호님, 진한님, 저 모두 파이팅입니다 :)",
    "tags": "diary",
    "url": "/diary/2018/02/11/geultto/"
  },{
    "title": "Pytorch Basic(1)",
    "text": "pytorch 튜토리얼을 보고 개인적으로 정리하는 포스팅입니다 What is PyTorch? 두 청중에게 타겟팅한 도구 GPU에서 Numpy의 대체물 굉장히 유연하고 빠르게 제공되는 딥러닝 연구 플랫폼 Tensors Tensor들은 Numpy의 ndarrays와 유사하며 Tensor는 컴퓨팅 파워를 증가하기 위해 GPU를 사용할 수 있습니다 x = torch.Tensor(5, 3) y = torch.rand(5, 3) print(y.size()) Operations 연산에 대해 많은 문법이 있습니다 더하기에 대한 다양한 케이스를 보겠습니다 # 1 print(x + y) # 2 print(torch.add(x, y)) # 3 result = torch.Tensor(5, 3) torch.add(x, y, out=result) print(result) # 4 y.add_(x) print(y) _가 post-fixed된 연산들은 텐서를 대체합니다. x.copy_(y)는 x를 변경 인덱싱도 사용 가능합니다 print(x[:, 1]) Resize 텐서의 모양을 변경하고 싶을 경우 torch.view를 사용합니다 x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) print(x.size(), y.size(), z.size()) http://pytorch.org/docs/master/torch.html 위 문서에 다양한 연산들이 더 있으니 문서를 참고하면 좋을 것 같습니다 Numpy Bridge Torch의 Tensor를 Numpy array로 변환 가능합니다 a = torch.ones(5) b = a.numpy() print(a) print(b) a.add_(1) print(a) print(b) 위 연산을 하면 a에 수정했는데 b도 값이 변해있습니다 import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) 위 연산도 a와 b의 값이 동일 CUDA Tensors .cuda method를 통해 Tensor들을 GPU로 보낼 수 있습니다 if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Autograd mechanics 이번 부분은 autograd가 어떻게 작동하고 operations이 어떻게 기록되는지에 대한 개론 부분입니다. 모든 것을 반드시 이해할 필요는 없지만 이 개념에 대해 친숙해지면 좋습니다. 이것을 효율적으로 사용할수록, 깨끗하고 효율적 프로그램을 작성할 수 있고 디버깅을 쉽게 도와줍니다 Excluding subgraphs from backward 모든 Variable은 2가지 flags를 가지고 있습니다. requires_grad와 volatile. 둘 다 하위 그래프를 제외한 그래디언트 계산을 통해 효율성을 증가시킵니다 requires_grad 그라디언트가 필요한 Single Input이 있을 경우 이것의 output는 또한 그라디언트가 필요합니다. 반대로, 모든 input에 그래디언트가 필요하지 않은 경우엔 output 또한 필요하지 않습니다. 모든 변수들이 그래디언트가 필요하지 않다면 Backward 연산은 하위 그래프에서 이루어지지 않습니다 &gt;&gt;&gt; x = Variable(torch.randn(5, 5)) &gt;&gt;&gt; y = Variable(torch.randn(5, 5)) &gt;&gt;&gt; z = Variable(torch.randn(5, 5), requires_grad=True) &gt;&gt;&gt; a = x + y &gt;&gt;&gt; a.requires_grad False &gt;&gt;&gt; b = a + z &gt;&gt;&gt; b.requires_grad True 당신의 모델의 부분을 고정하고 싶은 경우나 이미 그래디언트 값을 사용하지 않을 것을 알고 있을 경우 유용합니다. 예를 들어 pretrain된 CNN을 사용하고 싶을 경우, 고정하는 부분의 requires_grad를 바꿔주기만 하면 충분합니다. model = torchvision.models.resnet18(pretrained=True) for param in model.parameters(): param.requires_grad = False # Replace the last fully-connected layer # Parameters of newly constructed modules have requires_grad=True by default model.fc = nn.Linear(512, 100) # Optimize only the classifier optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) How Autograd encodes the history Autograd는 reverse automatic differentiation system입니다. (역 자동 차별화 시스템?) 개념적으로 Autograd는 작업을 진행하면서 데이터를 생성한 모든 작업을 기록한 그래프를 기록합니다. 나뭇잎이 입력 변수고 루트가 출력 변수인 DAG 제공, 이 그래프를 뿌리부터 잎까지 추적하면 Chain Rulte을 통해 자동으로 그래디언트를 계산할 수 있습니다 내부적으로 autograd는 그래프를 Fucntion 객체의 그래프로 나타내며 apply()를 적용해 그래프를 평가한 결과를 계산할 수 있습니다. 전달 경로를 계산할 때 autograd는 요청된 계산을 동시에 수행하고 그라디언트를 계산하는 함수를 나타내는 그래프를 작성합니다. forwards pass가 완료되면 우리는 이 그래프를 backward pass를 통해 평가할 수 있습니다 주목할 것은 매번 이터레이션으로부터 매번 그래프가 재생성된다는 것이며 이것은 모든 반복에서 그래프의 전체 모양과 크기를 변경할 수 있는 Python 제어 흐름문을 사용할 수 있는 것입니다. Train전에 모든 경로를 인코딩할 필요는 없습니다! In-place operations on Variables autograd에 있는 대체 연산은 제공되지만 대부분 권장하지 않습니다. Autograd의 공격적인 버퍼 해제 및 재사용은 매우 효율적이기 때문에 내부 작업으로 메모리 사용량이 감소할 경우가 없습니다. 과도한 메모리 사용량이라면 inplace를 사용하고 아니라면 사용하지 않으면 됨 제한하는 이유는 2가지가 있습니다 그라디언트를 계산하는데 필요한 값을 덮어쓰기 때문에 수치적으로 불안정할 수 있습니다. 추가 작업이 필요할 것입니다 inplace 연산은 그래프를 다시 작성해야 합니다. 동일한 저장 영역을 참조하는 변수가 여러개면 내부 오류 기능이 생길 수 있습니다",
    "tags": "pytorch data",
    "url": "/data/2018/02/03/pytorch-1/"
  },{
    "title": "Hidden Technical Debt in Machine Learning Systems 리뷰",
    "text": "Hidden Technical Debt in Machine Learning Systems 논문을 읽고 정리한 포스팅입니다. 머신러닝 시스템에서 발생할 수 있는 기술 부채에 대한 이야기와 실제 제품 도입시 아키텍쳐에 대해 나와있는 논문입니다. 의역이 있을 수 있으며 제가 잘못 이해한 것은 언제나 지적해주시면 감사하겠습니다!!! 1. 기술 부채(technical debt) 기술 부채라는 용어가 있습니다. 실행속도와 엔지니어링 품질 사이의 딜레마를 지칭하며 모든 부채가 나쁘진 않지만 부채는 복리가 되어 점점 쌓여 유지보수 비용의 상승, 깨지기 쉬운 시스템이 되어 혁신의 속도를 늦추는 역할을 합니다. 전통적인 방식으로 기술 부채를 관리하는 방법은 아래와 같습니다 코드 리팩토링 유닛 테스트 향상 사용되지 않는 레거시(legacy) 코드 삭제 의존성 감소 엄격한 API 관리 체계적으로 문서 작성 위와 같은 기술부채는 머신러닝 시스템에도 적용됩니다. 라이브 시스템을 적용하면서 머신러닝 커뮤니티들은 점점 다양한 경험을 축적했습니다. 그 결과 “머신러닝 시스템은 develop과 deploy는 상대적으로 빠르고 저렴하지만 시간이 지남에 따라 모델을 관리하는 것은 어렵고 비용이 많이 소모된다”라는 의견을 내놓았습니다. 또한 머신러닝 시스템의 부채는 대부분 코드 레벨보다 시스템 레벨에 존재합니다. 이 논문에선 머신러닝 알고리즘에 대해서가 아닌 이런 부채가 왜 생기고, 어떻게 해야 부채를 줄일 수 있는지에 대해 알려주고 있습니다. 사실 ML 코드는 정말 일부분에 지나지 않는다고 말해주고 있습니다-!! 2. 머신러닝 시스템 기술부채의 원인 전통적인 소프트웨어 공학에선 캡슐화(encapsulation)와 모듈 설계(Module Design)를 통해 강한 추상화 경계(Strong abstraction boundaries)를 만듭니다. 이 경계를 통해 독립된 환경을 만들어 유지보수가 가능한 코드를 만들어 복잡성이 큰 소프트웨어를 성공적으로 만들고 운영하고 있습니다. 강한 추상화 경계를 통해 해당 컴포넌트의 입력값과 출력값에 대해 논리적 일관성(logical consistency)과 불변성(invariants)을 구현했습니다. 하지만 기계학습 시스템에서 적용하긴 어렵습니다. 그 이유는 외부 데이터에 의존하지 않고 소프트웨어 로직만으로 구현이 불가능하기 때문입니다. 번역이 난해해서 원문을 작성하자면 There desired behavior cannot be effectively expressed in software logic wihtout dependency on external data 따라서 강한 추상화 경계가 외부 데이터의 유입으로 무너지기 때문에 기술 부채가 점점 쌓이게 됩니다 2.1 얽힘(Entanglement) 머신러닝 시스템은 신호들을 섞습니다. 이 뜻을 풀어보면, 다양한 기능(데이터 전처리, 학습, 예측, 시각화)이 서로 유기적으로 얽혀있어 머신러닝의 아웃풋을 생성합니다. 이 얽힘 때문에 시스템으로 격리를 통한 점진적 개선이 불가능합니다 CACE 원칙이 있습니다 : 어떤 것을 바꾸면 모든 것이 바뀐다(Changing Anything Changes Everything). 머신러닝 문제에 설명드리자면 특정 변수를 넣고 학습한 모델과 특정 변수를 제외하고 학습한 모델은 아예 다르기 때문에 시스템이 흔들릭 됩니다. 변수만 해당되는 것이 아닌 가중치나 하이퍼 파라미터를 변경시켜도 시스템에 영향력이 미치게 됩니다. 이러한 문제를 해결하기 위해 3가지 방식을 사용합니다 앙상블 활용 : 문제를 하부 문제로 쪼갤수 있으며, 모듈 운영방식이 개별적으로 운영할 때보다 효과가 클 경우 유용 고차원 시각화 도구를 사용 : 다차원 공간에서 효과를 슬라이싱으로 찾아내는 것 더 정교한 정규화 방법론을 사용 2.2 지속적 보정(Correction cascade) 특정 문제 A에 대한 모델 m_a이 존재하고 다른 A` 문제가 있는 경우가 있습니다. 이 경우 약간 보정해 m'_a를 만드는 시도를 하곤 합니다. 하지만 이렇게 하면 m_a에 대한 의존성으로 향후 모형을 개선하거나 유지보수가 어렵게 됩니다. 이럴 경우 기존 모형에서 weight를 추출한 후, 추가로 풀려고 하는 문제에 변수를 추가해 모형을 개발하면 기술적 부채를 줄일 수 있습니다 2.3 미신고 고객(Undeclared Consumers) 머신러닝 시스템에서 나온 예측값이 실시간 혹은 로그파일에 기록되곤 합니다. 그 로그 파일을 인풋으로 받아 다른 시스템에서 사용될 수 있습니다(파이프라인에서 앞단의 output이 뒷단의 input이 되는 케이스) 이런 경우를 전통적 소프트웨어 공학에서는 가시성 부채(visibility debt)로 부릅니다. 시스템을 사용하는 고객 중 일부는 미신고 고객이 되서 조용하게 다른 시스템의 입력값으로 사용됩니다. 미신고된 고객은 시스템의 다른 모델에 숨겨진 타이트한 결합(hidden tight coupling of model)이 되어 최고로 비싸며 최악의 경우 위험합니다. 모델을 수정할 경우 결과에 대한 이해가 부족해지고 다른 부분에 영향을 미칩니다. 예를 들어, 클릭율(CTR) 시스템이 광고 시스템 중 글꼴 크기와 연결되어 있는 경우 숨은 피드백 루프를 따라 클릭율을 높이는 기계학습 시스템은 글꼴 크기를 커지게 합니다. 하지만 우리가 원하는 것은 글꼴 크기를 커지게 하는 것이 아닌, 광고의 전환이 목표입니다. 3. 코드 의존성보다 데이터 의존성이 비용이 더 크다 전통적 소프트웨어 공학에선 의존성 부채(dependency debt)가 코드 복잡성의 상단 부분을 차지하고 있습니다. 기계학습 시스템에선 데이터 의존성(data dependency)가 이 역할을 합니다. 코드 의존성은 정적 분석, 연결 그래프를 통해 상대적으로 쉽게 탐색되곤 합니다(=IDE를 통해 디버깅이 쉬운 편입니다) 그러나 데이터 의존성을 탐지하는 방법론은 최근에 이루어지고 있습니다 3.1 불안정한 데이터 의존성(Unstable Data Dependencies) 머신러닝 시스템에서 사용하는 다양한 Feature들은 여러 시스템을 통해 입력을 받습니다. 예를 들면 크롤링을 통해 Database에 넣고 그 Database를 모델이 알 수 있도록 전처리하는 한 후 Input으로 넣고, 어떤 데이터는 클라이언트 어플에서 나와서 특정 작업(맵리듀스같은)을 통해 Input으로 넣게 됩니다. 따라서 이 앞단의 시스템이 조금이라도 변화하면 전체 머신러닝 시스템에 영향을 미치게 됩니다. 코드를 작성할 때 버전 관리를 하듯 데이터 버전 관리하는 방식이 도입되고 있지만 데이터 버전 관리 자체가 비용이 되고, 관리하기 힘들기 때문에 기술부채로 되돌아 옵니다 3.2 활용도가 낮은 데이터 의존성(Underutilized Data Dependencies) 활용도가 낮은 데이터들은 필요없는 패키지들입니다. 활용도가 낮은 데이터들이 머신러닝 시스템에 몇가지 방식으로 잠입하게 됩니다 Legacy Feature : 기계학습 초기 시스템엔 예측 변수로 포함되었으나 시간이 지나면서 중요도가 떨어지고 필요없게 된 변수 Bundled Feature : 변수가 기술적/사업적 이유로 한꺼번에 포함된 경우(다른 변수과 함께 투입된 변수) ϵ Feature : 모델의 정확도(accuracy)를 올리기 위해 매우 적은 기여를 하는 변수를 다수 포함 Correlated Features : 강한 상관관계를 가지는 변수들이 포함되는 경우입니다. 상관관계가 높은 변수들이 있다면 1개만 선택해서 넣는 것이 좋습니다 위와 같은 데이터들은 정기적으로 중요도가 낮은 변수를 찾아내 제거하고 관리해줘야 합니다 3.3 데이터 의존성 정적 분석(Static Analysis of Data Dependencies) 전통적인 코드에선 컴파일러와 빌드 시스템이 의존성에 대한 정적 분석을 형성해주곤 했습니다. 그러나 데이터 의존성에 대한 정적 분석에 대해선 도구나 경험이 상대적으로 부족한 상황입니다. 데이터 사전을 구축해 주기적으로 관리하는 방법 혹은 자동화된 Feature 관리 시스템을 사용하는 방법이 제시되고 있습니다. 4. 피드백 루프 (Feedback Loops) 라이브 머신러닝 시스템의 핵심기능 중 하나는 시간이 지남에 따라 그들의 행동이 모델의 결과에 영향을 미치는 것입니다. 이것을 분석 부채(Analysis debt) 라고 부릅니다. 행동에 따라 다른 현상을 보이기 때문에 릴리즈하기 전 모델의 상황을 예측하기 어렵습니다. 이런 피드백 루프는 다른 형태로 진행될 수 있으며 어떤 경우든 탐지하기 어렵습니다 4.1 직접적인 피드백 루프(Direct Feedback Loops) 모델이 미래에 사용될 교육 데이터 선택에 직접적으로 영향을 줍니다. 이것을 해결하기 위해 bandit algorithms(이건 어떤 정의인지 모르겠습니다)를 사용해야 하지만 일반적으로 Supervised 알고리즘을 사용합니다. 여기서 bandit 알고리즘은 일반적으로 필요한 동작 공간의 크기에 맞게 확장되지 않는 것이 문제입니다. 이것을 해결하기 위해 무작위화(randomization)을 사용해 주어진 모델에 의해 영향을 받지 않도록 데이터의 특정 부분을 분리합니다 4.2 숨겨진 피드백 루프(Hidden Feedback Loops) 직접적인 피드백 루프는 분석하는데 비용이 많이 듭니다. 그러나 직접적 피드백 루프는 머신러닝 연구자가 찾은 통계학적 접근에 대해 집중합니다. 반면 숨겨진 피드백 루프는 두개의 시스템이 서로 간접적으로 영향을 미치는 경우를 뜻합니다. 예를 들면 웹페이지에 표시할 제품을 선택하는 부분과 관련 리뷰를 선택하는 부분이 있습니다. 한 시스템을 개선하면 사용자가 변경 사항에 반응해 다른 요소를 더 많이 혹은 덜 클릭해 시스템의 동작이 변경될 수 있습니다. 5. 머신러닝 안티패턴 5.1 접착제 코드(Glue Code) 오픈소스 라이브러리를 사용할 경우 일반적으로 접착제 코드 시스템 디자인 패턴(Glue code system design pattern) 으로 귀결됩니다. 당장은 문제가 없지만 장기적으로 기술부채 문제를 야기할 수 있습니다. 우리가 라이브러리에서 사용하는 부분은 작은 부분인데, 그 부분이 변경된다면 우리의 코드도 수정해야 합니다. 또한 범용 머신러닝 시스템은 많은 문제를 해결하는데 초점을 맞춘 반면 실무에 적용된 시스템은 한 문제에 대해 확장성이 크게 개발되는 경향이 있기 때문에 딜레마가 생기곤 합니다. 이에대한 해법으로 R, Python으로 짜여진 코드를 C++, 자바, C로 재구현되는 것이 권장됩니다. 이렇게 하면 접착제 코드를 줄이며 테스트 및 유지보수도 쉽게 될 수 있습니다 5.2 파이프라인 정글(Pipeline Jungle) 접착제 코드의 특수한 경우에 파이프라인 정글이 데이터 전처리 과정에서 보이곤 합니다. 전처리 과정도 다양하게 존재합니다. 데이터를 긁어오는 과정(scraping), 병합(join), 샘플링(sampling), 그리고 중간 처리 과정에서 나타나는 파일들로 난잡하게 됩니다. 이렇게 파이프라인을 구축하면 오류를 탐지하기도 어렵고 장애 발생시 복구도 어렵습니다. 특히 End to End 테스트는 진행하기 어려울 수 있습니다 파이프라인 정글은 전체 아키텍쳐를 보면서 데이터 수집, Featrue Extraction, 전처리 과정을 천천히 나눠서 설계하면 피할 수 있습니다. 연구개발과 엔지니어링이 하나의 팀에서 접근하면 접착제 코드와 파이프라인 정글을 쉽게 해결할 수 있습니다 5.3 죽은 실험 경로(Dead Experimental Codepaths) 실제 운영준인 코드에 추가로 Branch를 생성해 다양한 실험을 통해 기존 모델의 성능을 높이는 노력을 많이 합니다. 하나의 브랜치는 괜찮지만 많은 사람들이 다양한 아이디어를 갖고 모형에 실험을 하게 된다면, 오랜 시간이 지나면 관리하기 힘든 기술부채를 만들게 됩니다. 정기적으로 사용하지 않거나 용도가 다 되었다고 판단되는 브랜치는 Dead Flag로 정의해 정리하는 작업이 반드시 필요합니다 5.4 추상화 부채(Abstraction Debt) 위의 이슈들은 모두 머신러닝 시스템을 지원할 강한 추상성이 부족하단 사실을 강조합니다. 데이터 스트리밍, 모델 또는 예측을 설명하는 인터페이스가 존재하는지 반문하면서 추상화가 더욱 필요하다고 말합니다. 특히 분산학습의 경우 맵리듀스가 널리 사용되는 이유는 강력한 분산학습 추상화가 없기 때문이라고 합니다. 맵리듀스는 반복적인 머신러닝 알고리즘의 빈약한 추상이라고 나와있습니다. 관련 논문을 몇개 제시했는데, 추가적으로 읽으면 이해에 더 도움이 될 것 같습니다 5.5 일반적인 상황(Common Smells) 전통적 소프트웨어 엔지니어링에선 컴포넌트와 시스템에서 겪을 수 있는 상황을 디자인합니다. 머신러닝 시스템에도 이런 상황을 정의했습니다 Plain-Old-Data Type Smell : 일반적으로 데이터는 float이나 integer형을 인코딩됩니다. 모델 매개변수는 로그를 취했는지, 임계값 기준인지를 알아야 하며, 예측은 이를 생성한 모델 및 사용 방법에 대한 정보를 알아야 합니다 Multiple-Language Smell : 주어진 언어로 시스템의 특정 부분을 사용하고 싶은 경우(다국어 지원) 해당 언어가 작업에 편한 라이브러리를 가질수도 있고, 아닐수도 있습니다. 하지만 진짜 중요한 것은 여러 언어를 사용하면 효과적인 테스트 비용이 증가하며 번역에 대한 오너십을 타인에게 주기 어려울 수 있습니다(개발자가 라이브러리를 통해 번역했다고 치면, 이에 대한 검증은 개발자가 해야되는 것인가? 다른 사람이 해야하는 것인가? 등..) Prototype Smell : 프로로 타입을 통해 새로운 아이디어를 테스트하는 것은 편리합니다. 그러나 프로토 타입도 증가할수록 자체 비용이 소요되며 추후엔 프로토 타입을 생산 솔루션으로 사용할 수 있는 위험이 있습니다. 또한 프로토타입에서 발견된 결과는 실제 현실을 많이 반영하진 않습니다 6. 설정 부채(Configurations Debt) 머신러닝 시스템에선 점점 환경설정이 축적됩니다. 어떤 feature를 사용할지, 데이터를 얼마나 선택할지, 알고리즘의 learning rate를 얼마로 설정할지 등등 다양하게 고려할 것들이 많아집니다. 이런 설정이 복잡해지는 경우 컴포넌트들의 연산이 실수가 일어날 수 있습니다. 따라서 다음과 같은 규칙을 명확하게 하는 것이 좋습니다 이전 환경설정에서 작은 변경은 쉬워야 합니다 매뉴얼 error, omissions, oversights를 만드는 것은 어렵습니다 두개의 환경 설정의 차이에 대해 쉽게 볼 수 있어야 합니다 설정에 따른 기본적인 팩트를 검증하는 것이 쉬워야 합니다 : 사용된 Feature의 수, 데이터 종속성 등 사용하지 않거나 중복된 설정을 감지할 수 있어야 합니다 환경 설정은 레포지토리를 통해 전체 코드 검토를 통해 진행되야 합니다 7. 외부 세계의 변화를 다루기(Dealing with Changes in the External World) 7.1 고정된 임계치 설정(Fixed Thresholds in Dynamic Systems) 머신러닝 시스템에서 동작을 일으키도록 의사결정 임계값 설정이 필요합니다. 확률로 어떤 클래스 분류를 한다고 하면 0.7 이상이면 해당 클래스로 간주한다 라는 로직이 있스비다. 이 임계치는 수작업으로 설정되어 있으니 자동화하는 방식을 검토하는 것도 좋습니다 7.2 모니터링 및 테스트(Monitoring and Testing) 소프트웨어 시스템에선 단위테스트와 통합 테스트를 하는 것도 필요하지만 머신러닝 시스템은 예측력과 머신러닝 시스템이 취하는 동작에 대한 모니터링이 필요합니다. 그 전에 “무엇을 모니터링 할 것인가”에 대한 주요한 질문을 생각해주세요. 머신러닝 시스템에선 실험가능한 변수가 명백하게 주어지지 않을수도 있기 때문에 이런 질문에 대한 답변을 생각해야 합니다. 그 후 아래와 같은 것을 확인해주세요 Prediction Bias : 머신러닝 시스템이 의도한대로 움직이는지? Action Limit : 원치않는 상황이 발생할 경우 경고를 알리거나 수동으로 전환이 쉬워야 합니다 Up-Stream Producers : 업스트림 프로세스를 모니터링할 수 있어야 합니다. 어떤 것이 실수했는지, 그래서 뒤의 작업이 진행 안되었는지 등을 이해해야 합니다. 파이썬을 사용한다면 Airflow를 적용하면 좋을 것 같습니다 8. ML과 관련 부채의 다른 영역 8.1 데이터 테스트 부채(Data Testing Debt) input 데이터가 올바르게 들어가고 있는지 확인해야 합니다. 기본적으로 이치에 맞는 데이터인지 검증하는 것은 유용하나 input의 분포에 따라 모니터링의 난이도가 변화됩니다 8.2 재현성 부채(Reproducibility Debt) 다시 실험을 진행해서 유사한 결과가 나타나는 것은 과학자로서 매우 중요합니다. 이런 엄격하게 현실을 재현하기 위해 무작위 알고리즘(Randomized algorithms), 비결정론(non-determinism inherent), 초기조건 의존(reliance on intial conditions) 등을 사용하곤 합니다 8.3 프로세스 관리 부채(Process Management Debt) 유사한 모델에 대해 안전하고 자동으로 업데이트하는 문제나 비즈니스 우선 순위에 따라 모델간 리소스를 관리하고 할당(거기에 시각화까지), 파이프라인에서 데이터 흐름 감지 등 다양한 프로세스 관리에 대한 것도 알 수 있어야 합니다 8.4 문화적 부채(Cultural Debt) 가끔 머신러닝 연구자와 엔지니어간의 의견이 다를 수 있습니다. 팀의 문화에 따라 feature를 제거할지, 차원을 줄일지, 속도를 우선시할지에 대해 논의할 수 있습니다 9. 결론 기술 부채는 유용한 은유지만 엄격한 통계적 기준은 제공하지 않습니다. 시스템에서 기술적 부채를 측정하거나 부채의 비용을 계산하는 방법은 무엇일까요? 팀이 여전히 빠르게 움직일 수 있다는 답변은 시간이 지남에 따라 부채의 비용이 증가하는 것이 너무 분명하기 때문에 적절한 답은 아닙니다. 유용한 질문은 아래와 같습니다 완전히 새로운 알고리즘 방식을 실제 규모에 얼마나 쉽게 테스트할 수 있나요? 모든 데이터 종속성이 전이될 수 있나요? 시스템에 대한 새로운 변화의 영향을 정확하게 측정할 수 있나요? 한 모델이나 신호를 개선하면 다른 작업이 저하되나요? 팀에 새로운 직원이 들어왔을 경우 얼마나 빨리 일을 시작할 수 있을까요? 후기 논문은 매번 읽고 있었는데 처음으로 논문을 읽고 블로그에 남기네요. 사실상 번역 느낌이 강하네요. 다음부터는 조금 더 요약하는 습관을 가져서 올려야겠습니다 :) 시간도 오래 소요되었기 때문에, 조금 더 효율적으로 논문 리뷰하는 방법을 아는 분들은 댓글주시면 정말 감사하겠습니다!!!!",
    "tags": "paper data",
    "url": "/data/2018/01/28/hidden-technical-debt-in-maching-learning-systems/"
  },{
    "title": "OpenCV - 이미지/비디오 읽기",
    "text": "OpenCV Python으로 이미지/비디오 읽기! OpenCV를 이용해 이미지 파일을 읽고 보고 저장하는 방법에 대해 알아보겠습니다 키워드 : cv2.imread(), cv2.imshow(), cv2.imwrite(), cv2.VideoCapture(), cv2.VideoWriter() 이미지 읽기 (Image Read) import cv2 import numpy as np img = cv2.imread('test.jpg', cv2.IMREAD_COLOR) cv2.imread(fileName, flag) : fileName은 이미지 파일의 경로를 의미하고 flag는 이미지 파일을 읽을 때 옵션입니다 flag는 총 3가지가 있습니다. 명시적으로 써줘도 되고 숫자를 사용해도 됩니다 cv2.IMREAD_COLOR(1) : 이미지 파일을 Color로 읽음. 투명한 부분은 무시하며 Default 설정입니다 cv2.IMREAD_GRAYSCALE(0) : 이미지 파일을 Grayscale로 읽음. 실제 이미지 처리시 중간 단계로 많이 사용합니다 cv2.IMREAD_UNCHAGED(-1) : 이미지 파일을 alpha channel 까지 포함해 읽음 &gt;&gt;&gt; image.shape (206, 207, 3) image.shape 함수는 이미지 파일의 모양을 return합니다 순서대로 Y축, X축, 채널의 수를 의미합니다 ** OpenCV에선 색을 표현할 경우 BGR 순으로 표현합니다! ** 이미지 보기 (Image Show) cv2.imshow('image', img) cv2.waitKey(0) cv2.destroyAllWindows() cv2.imshow(tital, image) : title은 윈도우 창의 제목을 의미하며 image는 cv2.imread() 의 return값입니다 cv2.waitKey()는 키보드 입력을 대기하는 함수로 0이면 key 입력이 있을때까지 무한대기합니다. 특정 시간동안 대기를 하려면 ms값을 넣어주면 됩니다. 또한 ESC를 누를 경우 27을 리턴합니다. 이런 리턴 값을 알면 버튼에 따라 다른 동작을 실시하도록 로직을 설계할 수 있습니다 cv2.destroyAllWindows() : 화면에 나타난 윈도우를 종료합니다. 일반적으로 위 3개는 같이 사용됩니다 예제코드 import cv2 fname = './data/test.jpg' original = cv2.imread(fname, cv2.IMREAD_COLOR) gray = cv2.imread(fname, cv2.IMREAD_GRAYSCALE) unchange = cv2.imread(fname, cv2.IMREAD_UNCHANGED) cv2.imshow('Original', original) cv2.imshow('Gray', gray) cv2.imshow('Unchange', unchange) cv2.waitKey(0) cv2.destroyAllWindows() 이미지 저장하기 cv2.imwrite('result.png', original) 예시를 통해 어떻게 사용하는지 알려드리겠습니다! import cv2 img = cv2.imread('test.jpg', cv2.IMREAD_GRAYSCALE) cv2.imshow('image',img) k = cv2.waitKey(0) if k == 27: # esc key cv2.destroyAllWindow() elif k == ord('s'): # 's' key cv2.imwrite('testgray.png',img) cv2.destroyAllWindow() ESC키를 누를 경우 창을 닫고 s키를 누를 경우 저장하는 코드입니다 Window 창 조절하기 import cv2 img = cv2.imread('test.jpg', cv2.IMREAD_GRAYSCALE) cv2.namedWindow('image', cv2.WINDOW_NORMAL) cv2.imshow('image',img) k = cv2.waitKey(0) if k == 27: # esc key cv2.destroyAllWindow() elif k = ord('s'): # 's' key cv2.imwrite('testgray.png',img) cv2.destroyAllWindow() cv2.namedWindow()가 추가되었습니다. 이 함수는 윈도우를 고정으로 할 것인지, 사용자가 크기를 조절할 수 있는지를 나타냅니다 cv2.WIDNOW_AUTOSIZE : 원본 이미지 크기로 고정해 윈도우를 생성 cv2.WINDOW_NORMAL : 원본 이미지 크기로 윈도우를 생성하되 사용자가 크기를 조절할 수 있음 Matplotlib을 사용해 시각화하기 일반적으로 *.py 파일로 실행하면 윈도우가 잘 나옵니다. 그러나 쥬피터 노트북에서 작업하는 경우(특히 Mac 환경) 무한 로딩이 걸리는 경우가 있습니다. 이 경우 matplotlib 라이브러리로 시각화를 하곤 합니다 import cv2 from matplotlib import pyplot as plt img = cv2.imread('test.jpg', cv2.IMREAD_COLOR) b, g, r = cv2.split(image) image2 =cv2.merge([r,g,b]) plt.imshow(im2) plt.xticks([]) # x축 눈금 plt.yticks([]) # y축 눈금 plt.show() OpenCV는 BGR로 사용하지만 Matplotlib는 RGB로 이미지를 보여주기 때문에 cv2.split(image)를 사용해 이미지 파일을 분리한 후, cv2.merge()를 이용해 이미지 b,r을 변경했습니다 카메라로부터 비디오 읽기 import cv2 cap = cv2.VideoCapture(0) if cap.isOpen(): print('width: {}, height : {}'.format(cap.get(3), cap.get(4)) while True: ret, fram = cap.read() if ret: gray = cv2.cvtColor(fram, cv2.COLOR_BGR2GRAY) cv2.imshow('video', gray) k == cv2.waitKey(1) &amp; 0xFF if k == 27: break else: print('error') cap.release() cv2.destroyAllWindows() cv2.VideoCapture()를 사용해 비디오 캡쳐 객체를 생성할 수 있습니다. 안의 숫자는 장치 인덱스(어떤 카메라를 사용할 것인가)입니다. 1개만 부착되어 있으면 0, 2개 이상이면 첫 웹캠은 0, 두번째 웹캠은 1으로 지정합니다 cap.isOpen() : 비디오 캡쳐 객체가 정상적으로 Open되었는지 확인합니다 while True: 특정 키를 누를때까지 무한 반복을 위해 사용했습니다. 이 스타일로 많이 코딩합니다! ret, fram = cap.read() : 비디오의 한 프레임씩 읽습니다. 제대로 프레임을 읽으면 ret값이 True, 실패하면 False가 나타납니다. fram에 읽은 프레임이 나옵니다 cv2.cvtColor() : frame을 흑백으로 변환합니다 cap.release() : 오픈한 캡쳐 객체를 해제합니다 파일로 비디오 읽기 (Video Read) 파일을 직접 읽기도 정말 간단합니다 import cv2 cap = cv2.VideoCapture('vtest.avi') while(cap.isOpened()): ret, frame = cap.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) cv2.imshow('frame',gray) if cv2.waitKey(1) &amp; 0xFF == ord('q'): break cap.release() cv2.destroyAllWindows() 비디오 저장 import cv2 cap = cv2.VideoCapture(0) fourcc = cv2.VideoWriter_fourcc(*'DIVX') out = cv2.ViewoWriter('output.avi', fourcc, 25.0, (640,480)) while (cap.isOpend()): ret, frame = cap.read() if ret: # 이미지 반전, 0:상하, 1 : 좌우 frame = cv2.flip(frame, 0) out.write(frame) cv2.imshow('frame', frame) if cv2.waitKey(0) &amp; 0xFF == ord('q'): break else: break cap.release() out.release() cv2.destroyAllWindows() cv2.VideoWriter(outputFile, fourcc, frame, size) : fourcc는 코덱 정보, frame은 초당 저장될 프레임, size는 저장될 사이즈를 뜻합니다 cv2.VideoWriter_fourcc('D','I','V','X') 이런식으로소 사용하곤 합니다 적용 가능한 코덱은 DIVX, XVID, MJPG, X264, WMV1, WMV2 등이 있습니다 Reference OpenCV-Python Tutorial",
    "tags": "vision data",
    "url": "/data/2018/01/23/opencv-1/"
  },{
    "title": "맥에서 MySQL 설치하기",
    "text": "맥에서 MySQL 설치하기, mysql Install MAC에 대한 포스팅입니다. 리눅스를 많이 사용하지만, 로컬 테스트용을 위해 맥에도 MySQL을 설치해야 합니다. 계속 참조할 것 같아 문서로 남깁니다 Install MySQL Mac dmg 파일을 받는 방법, brew 설치, 직접 빌드하는 방법 등 다양하게 있지만 가장 간편하고 자동으로 환경설정을 해주는 brew를 사용해 설치하겠습니다 mysql 최신 버전 설치 brew install mysql mysql 시작 mysql.server start root 비밀번호 설정 mysql_secure_installation Would you like to setup VALIDATE PASSWORD plugin? : 비밀번호 가이드로 비밀번호 설정을 한다면 yes Remove anonymous users? : 익명사용자를 삭제할지? yes하면 접속시 -u 옵션을 반드시 명시해야 합니다 Disallow root login remotely? : localhost외 ip에서 root 아이디로 접속가능을 허락할지? yes하면 원격접속 불가능하니 no Remove test database and access to it? : mysql에 기본적으로 설치되는 test 디비를 삭제할지? 저는 yes Reload privilege tables now? : 권한을 변경했을 경우 yes charset 설정 mysql -uroot -p로 로그인한 후, status; 입력해서 설정 확인 latin이 있다면 vi /etc/my.cnf 또는 vi /etc/mysql/my.cnf 으로 수정 [client] default-character-set=utf8 [mysql] default-character-set=utf8 [mysqld] collation-server = utf8_unicode_ci init-connect='SET NAMES utf8' character-set-server = utf8 데몬 실행 brew services start mysql MySQL 접속하기 터미널에서 mysql -u root -p 을 통해 접속 특정 호스트에 접속되어 있다면 mysql --host=&lt;HOST IP&gt; -u root --password=\"PASSWORD\"을 사용해 접속! (LOAD DATA를 사용할 예정이라 저는 –local-infile=1을 추가해서 사용하는 편입니다",
    "tags": "sql development",
    "url": "/development/2018/01/18/Install-MySQL-mac/"
  },{
    "title": "Google Cloud SDK 시작 및 계정 전환",
    "text": "Google Cloud SDK(gcloud) 시작 및 계정 전환하는 것에 대해 알려드리려고 합니다! 구글 클라우드 플랫폼을 회사 계정, 개인 계정으로 사용하는 경우, 로컬에서 구글 클라우드 SDK 계정을 전환할 필요가 있습니다 1. gcloud 설치 (Mac OS) Google Cloud SDK인 gcloud 설치하기! 공식 문서 터미널에서 아래 명령어 실행해주세요 wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-164.0.0-darwin-x86_64.tar.gz tar -zxvf google-cloud-sdk-164.0.0-darwin-x86_64.tar.gz cd google-cloud-sdk ./install.sh 2. gcloud 설정 gcloud init 해당 명령어를 입력하면 config 내용들이 나옵니다! 만약 config가 없다면 바로 첫 작업이 진행되고, 설정되어 있는 설정이 있다면 위 사진같은 내용들이 출력될 것입니다 Create a new configurations을 위해 2를 입력해줍니다 그 후, config 이름을 설정해주세요! 저는 byeon으로 했습니다 그 이후 Choose the account you would like to use to perform operations for this configurations: 이란 내용이 나옵니다. 기존에 있던 계정은 1, 새 계정으로 로그인은 2를 눌러주세요(계정의 수마다 숫자가 다를 수 있습니다) 저는 1을 입력했습니다! 그 이후 region 설정을 하게 됩니다. 저는 Compute Engine의 지역인 asia-east1을 그대로 사용하기로 결정했습니다 3. 계정 전환 이제 gcloud 설정은 모두 끝났고, gcloud 계정 전환하는 방법을 알려드리겠습니다! 2개의 명령어만 기억하면 됩니다 :) gcloud config configurations document gcloud config configurations list 현재 로컬에 저장되어 있는 설정 출력 gcloud config configurations activate [configuration name] 해당 이름의 계정으로 전환",
    "tags": "basic gcp",
    "url": "/gcp/2018/01/11/config-gcloud-account/"
  },{
    "title": "Apache Airflow - Workflow 관리 도구(1)",
    "text": "오늘은 Workflow Management Tool인 Apache Airflow 관련 포스팅을 하려고 합니다. 이 글은 1.10.3 버전에서 작성되었습니다 최초 작성은 2018년 1월 4일이지만, 2020년 2월 9일에 글을 리뉴얼했습니다 슬라이드 형태의 자료를 원하시면 카일스쿨 6주차를 참고하시면 좋을 것 같습니다 :) Apache Airflow를 사용하는 이유 데이터 엔지니어링에선 데이터 ETL(Extract, Transform, Load) 과정을 통해 데이터를 가공하며 적재함 머신러닝 분야에서도 모델 학습용 데이터 전처리, Train, Prediction시 사용 가능 위와 같은 경우 여러개의 Sequential한 로직(앞의 output이 뒤의 input이 되는)이 존재하는데 이런 로직들을 한번에 관리해야 함 관리할 로직이 적다면 CRON + 서버에 직접 접속해 디버깅 하는 방식으로 사용할 수 있지만, 점점 관리할 태스크들이 많아지면 헷갈리는 경우가 생김 이런 Workflow Management 도구는 airflow 외에도 하둡 에코시스템에 우지(oozie), luigi 같은 솔루션이 있음 Apache Airflow의 장점 Apache Airflow는 Python 기반으로 만들어졌기 때문에, 데이터 분석을 하는 분들도 쉽게 코드를 작성할 수 있음 Airflow 콘솔이 따로 존재해 Task 관리를 서버에서 들어가 관리하지 않아도 되고, 각 작업별 시간이 나오기 때문에 bottleneck을 찾을 때에도 유용함 또한 구글 클라우드 플랫폼(BigQuery, Dataflow)을 쉽게 사용할 수 있도록 제공되기 때문에 GCP를 사용하시면 반드시 사용할 것을 추천함 Google Cloud Platform에는 Managed Airflow인 Google Cloud Composer가 있음 직접 환경을 구축할 여건이 되지 않는다면 이런 서비스를 사용하는 것을 추천 :) Airflow Architecture Airflow Webserver - 웹 UI를 표현하고, workflow 상태 표시하고 실행, 재시작, 수동 조작, 로그 확인 등 가능 Airflow Scheduler 작업 기준이 충족되는지 여부를 확인 종속 작업이 성공적으로 완료되었고, 예약 간격이 주어지면 실행할 수 있는 작업인지, 실행 조건이 충족되는지 등 위 충족 여부가 DB에 기록되면, task들이 worker에게 선택되서 작업을 실행함 1) Airflow 설치 Airflow는 pip로 설치 가능 pip install apache-airflow==1.10.3 Extra Packages가 필요하다면 아래 명령어로 설치 가능 pip install apache-airflow[gcp]==1.10.3 # zsh이라면 # pip install 'apache-airflow[gcp]'==1.10.3 Airflow initdb airflow initdb는 처음 db를 생성하는 작업을 함 airflow initdb werkzeug 관련 오류가 발생한다면 아래 명령어로 라이브러리 설치 pip install werkzeug==0.15.4 Airflow 실행 airflow는 webserver와 scheduler를 실행시킴 webserver는 웹 서버를 담당하고, scheduler가 DAG들을 스케줄링(실행)함 airflow webserver -p 8080 # 터미널 새 창을 열어서 아래 커맨드 입력 airflow scheduler 기본적으로 설치할 경우 ~/airflow에 폴더가 생김 폴더 내부에 있는 파일 간단 설명 airflow.cfg : Airflow 관련 설정 airflow.db : sqlite 데이터베이스 dags 폴더(없다면 mkdir dags로 생성!) : DAG 파일이 저장되는 장소 만약 1.10.0 이상 버전을 설치할 때, RuntimeError가 발생할 경우(RuntimeError: By default one of Airflow’s dependencies installs a GPL dependency) 환경 변수를 정의한 후 설치 export AIRFLOW_GPL_UNIDECODE=yes pip install apache-airflow webserver를 실행했으니 localhost:8080에서 UI를 확인할 수 있음 DAG는 Directed Acyclic Graph의 약자로 Airflow에선 workflow라고 설명함 Task의 집합체 메인 화면엔 정의되어 있는 DAG들을 확인할 수 있음 현재는 많은 example이 존재 example을 보고싶지 않다면 airflow.cfg에서 load_examples = False로 설정하면 됨 Schedule은 예정된 스케쥴로 cron 스케쥴의 형태와 동일하게 사용 Owner는 소유자를 뜻하는 것으로 생성한 유저를 뜻함 Recent Tasks/DAG Runs에 최근 실행된 Task들이 나타나며, 실행 완료된 것은 초록색, 재시도는 노란색, 실패는 빨간색으로 표시됨 2) DAG 생성 DAG 생성하는 흐름 (1) default_args 정의 누가 만들었는지, start_date는 언제부턴지 등) (2) DAG 객체 생성 dag id, schedule interval 정의 (3) DAG 안에 Operator를 활용해 Task 생성 (4) Task들을 연결함( &gt;&gt;, &lt;&lt; 활용) Airflow는 $AIRFLOW_HOME(default는 ~/airflow)의 dags 폴더에 있는 dag file을 지속적으로 체크함 Operator를 사용해 Task를 정의함 Operator가 인스턴스화가 될 경우 Task라고 함 Python Operator, Bash Operator, BigQuery Operator, Dataflow Operator 등 Operator 관련 자료는 공식 문서 참고 Operator는 unique한 task_id를 가져야 하고, 오퍼레이터별 다른 파라미터를 가지고 있음 아래 코드를 dags 폴더 아래에 test.py로 저장하고 웹서버에서 test DAG 옆에 있는 toggle 버튼을 ON으로 변경 templated_command에서 % 앞뒤의 # 제거해주세요! from airflow import models from airflow.operators.bash_operator import BashOperator from datetime import datetime, timedelta # start_date를 현재날자보다 과거로 설정하면, # backfill(과거 데이터를 채워넣는 액션)이 진행됨 default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2020, 2, 9), 'email': ['airflow@airflow.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=5)} # dag 객체 생성 with models.DAG( dag_id='test', description='First DAG', schedule_interval = '55 14 * * *', default_args=default_args) as dag: t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) # BashOperator를 사용 # task_id는 unique한 이름이어야 함 # bash_command는 bash에서 date를 입력한다는 뜻 t2 = BashOperator( task_id='sleep', bash_command='sleep 5', retries=3, dag=dag) templated_command=\"\"\" # #을 삭제해주세요 {#% for i in range(5) %#} echo \"{#{ ds }#}\" echo \"{#{ macros.ds_add(ds, 7)}#}\" echo \"{#{ params.my_param }#}\" {#% endfor %#} \"\"\" t3 = BashOperator( task_id='templated', bash_command=templated_command, params={'my_param': 'Parameter I passed in'}, dag=dag) # set_upstream은 t1 작업이 끝나야 t2가 진행된다는 뜻 t2.set_upstream(t1) # t1.set_downstream(t2)와 동일한 표현입니다 # t1 &gt;&gt; t2 와 동일 표현 t3.set_upstream(t1) 다시 정리하면 DAG 객체 생성 -&gt; Operator를 활용해 Task 작성 -&gt; Task를 연결하는 방식 {#{ ds }#}, {#{ macros }#}는 jinja template을 의미함 실제 사용시엔 #를 제외해주세요. 블로그 테마 때문에 추가함 Macros reference, Jinja Template 참고하면 자세한 내용이 있음 scheduler를 실행시켜 둔 상태라면 DAG들이 실행됨 3) Airflow 명령어 DAG 파일을 추가하고, 웹서버에 보이기까지 시간이 좀 걸릴 수 있음 DAG 파일들을 확인하고 싶은 경우 airflow의 dags 폴더 아래에 있는 dag들을 출력함 airflow list_dags 특정 DAG의 task를 출력하고 싶은 경우 test라는 DAG의 task 출력 airflow list_tasks test Tree 형태로 출력 airflow list_tasks test --tree 특정 Task를 test하고 싶은 경우 date 날짜로 실행함 airflow test [DAG id] [Task id] [date] 예시 : airflow test test print_date 2020-02-09 Airflow scheduler 실행 DAG들이 실행됨 airflow scheduler Airflow 관련 help 명령어 airflow -h Airflow Webserver UI 특정 DAG을 클릭하고 Graph View를 클릭하면 아래와 같은 화면이 보임 print_date Task를 수행한 후, sleep Task와 templated Task를 실행함 Tree View를 누르면 아래 화면이 나타남 빨간색 네모 안에 있는 초록색 칸을 클릭하면 아래와 같은 설정이 나옴 Task의 로그 확인(View Log), 실행(Run), 실행 상태 초기화(Clear) 등을 할 수 있음 Task 재실행시 Run을 누르는 방법과 Clear를 누르는 방법이 있음 4) Connections 설정 Airflow에서 MySQL, Google Cloud, Slack 등에 연결하고 싶은 경우 Admin - Connections에 설정을 저장해야 함 Google Cloud BigQuery 설정하고 싶은 경우 Project Id : 구글 클라우드 콘솔에 나오는 Project Id Keyfile Path : json keyfile의 경로를 입력 Scopes : Scope 문서를 확인! https://www.googleapis.com/auth/cloud-platform 입력 Airflow 다양한 Tip pip로 설치하면 생기는 기본 설정 sequential executor : 기본적으로 1개만 실행할 수 있음. 병렬 실행 불가능 celery executor를 사용해 병렬로 실행할 수 있음 이건 RabbitMQ나 Redis가 필요함 sqlite : 기본 meta store는 sqlite인데, 동시 접근이 불가능해서 병렬 불가능 병렬 실행하기 위해 mysql이나 postgresql을 사용해야 함 위 설정을 서버에 매번 하는 일은 번거로운 일 Docker로 만들면 쉽게 가능 docker-airflow Github에 보면 이미 만들어진 Dockerfile이 있음 docker-compose로 실행하는 방법도 있고, Airflow 버전이 올라가면 빠르게 업데이트하는 편 Airflow의 DAG 폴더(default는 ~/airflow/dags)를 더 쉽게 사용하려면 해당 폴더를 workspace로 하는 jupyter notebook을 띄워도 좋음 데이터 분석가도 쉽게 DAG 파일 생성 가능 Error: Already running on PID XXXX 에러 발생시 Airflow webserver 실행시 ~/airflow/airflow-webserver.pid에 process id를 저장함 쉘에서 빠져나왔지만 종료되지 않는 경우가 있음 아래 명령어로 pid를 죽이면 됨(다른 포트를 사용할 경우 8080쪽 수정) kill -9 $(lsof -t -i:8080) Task간 데이터를 주고 받아야 할 경우 xcom 사용 Admin - xcom에 들어가면 값이 보임 xcom에 데이터 저장(xcom_push) task_instance = kwargs['task_instance'] task_instance.xcom_push(key='the_key', value=my_str) 다른 task에서 데이터 불러오기(xcom_pull) task_instance.xcom_pull(task_ids='my_task', key='the_key') 참고로 PythonOperator에서 사용하는 python_callable 함수에서 return하는 값은 xcom에 자동으로 push됨 DAG에서 다른 DAG에 종속성이 필요한 경우 ExternalTaskSensor 사용 1개의 DAG에서 하면 좋지만, 여러 사람이 만든 DAG이 있고 그 중 하나를 사용해야 할 경우도 있음 특정 DAG을 Trigger하고 싶은 경우 TriggerDagRunOperator 사용 예제 특정 Task의 성공/실패에 따라 다른 Task를 실행시키고 싶은 경우 Airflow Trigger Rule 사용 예를 들어 앞의 두 작업중 하나만 실패한 경우 Document 참고 Jinja Template이 작동하지 않는 경우 우선 provide_context=True 조건을 주었는지 확인 Jinja Template이 있는 함수의 파라미터가 어떤 것인지 확인 Operator마다 Jinja Template을 해주는 template_fields가 있는데, 기존 정의가 아닌 파라미터에서 사용하고 싶은 경우 새롭게 정의 class MyPythonOperator(PythonOperator): template_fields = ('templates_dict','op_args') Airflow 변수를 저장하고 싶은 경우 Variable 사용 Admin - Variables에서 볼 수 있음 json 파일로 변수 저장해서 사용하는 방식을 자주 사용함 from airflow.models import Variable config=Variable.get(f\"{HOME}/config.json\", deserialize_json=True) environment=config[\"environment\"] project_id=config[\"project_id\"] Task를 그룹화하고 싶은 경우 dummy_operator 사용 workflow 목적으로 사용하는 경우도 있음. 일부 작업을 건너뛰고 싶은 경우, 빈 작업 경로를 가질 수 없어서 dummy_operator를 사용하기도 함 1개의 Task이 완료된 후에 2개의 Task를 병렬로 실행하기 task1 &gt;&gt; [task2_1, task_2_2] 앞선 Task의 결과에 따라 True인 경우엔 A Task, False인 경우엔 B Task를 실행해야 하는 경우 BranchPythonOperator 사용 python_callable에 if문을 넣어 그 다음 task_id를 정의할 수 있음 단순하게 앞선 task가 성공, 1개만 실패 등이라면 trigger_rule만 정의해도 됨 Airflow에서 Jupyter Notebook의 특정 값만 바꾸며 실행하고 싶은 경우 Papermill 사용 참고 문서 UTC 시간대를 항상 생각하는 습관 갖기 execution_date이 너무 헷갈림 2017년에 Airflow 처음 사용할 때 매우 헷갈렸던 개념 박호균님의 블로그 참고 추후 다른 글로 정리할 예정 Task가 실패했을 경우 슬랙 메세지 전송하기 Integrating Slack Alerts in Airflow 참고하면 잘 나와있음 Hook이란? Hook은 외부 플랫폼, 데이터베이스(예: Hive, S3, MySQL, Postgres, Google Cloud Platfom 등)에 접근할 수 있도록 만든 인터페이스 대부분 Operator가 실행되기 전에 Hook을 통해 통신함 공식 문서 참고 머신러닝에서 사용한 예시는 Github 참고 airflow Github에 많은 예제 파일이 있음 Context Variable이나 Jinja Template의 ds를 사용해 Airflow에서 날짜를 컨트롤 하는 경우, Backfill을 사용할 수 있음 과거값 기준으로 재실행 단, 쿼리에 CURRENT_DATE() 등을 쓰면 Airflow에서 날짜를 컨트롤하지 않고 쿼리에서 날짜 컨트롤하는 경우라 backfill해도 CURRENT_DATE()이 있어서 현재 날짜 기준으로 될 것임 airflow backfill -s 2020-01-05 -e 2020-01-10 dag_id 추천 자료 Airflow Tutorial : Video 자료 존재 Lyft의 Airflow 활용 사례 Awesome Apache Airflow : Airflow 관련 링크 모음 ETL best practices with Airflow : 1.8 버전이지만 내용들이 좋음 제 Github : Airflow example 저장하는 중 제 다른 Github : ML에서 활용한 Airflow Airflow: Lesser Known Tips, Tricks, and Best Practises Airflow and XCOM: Inter Task Communication Use Cases Docker Airflow Github Airflow about subDAGs, branching and xcom What’s coming in Apache Airflow 2.0",
    "tags": "engineering data",
    "url": "/data/2018/01/04/airflow-1/"
  },{
    "title": "Google Cloud Platform 가입 및 소개",
    "text": "이 글은 Google Cloud Platform(GCP) 가입 및 소개를 위한 글입니다! AWS도 좋은 클라우드 환경이지만, 다른 클라우드도 각각의 장점을 가지고 있습니다. 개인적으로 생각하는 GCP의 장점은 1) 처음 가입시 $300를 주기 때문에 이것저것 해보며 클라우드 환경에 적응할 수 있습니다 2) 빅데이터 관련 서비스가 매우 좋습니다(BigQuery, Dataflow, BigTable, CloudML 등) Google Cloud Platform 가입하기 1) Gmail 가입하기(이 과정은 생략) 2) Google Cloud Platform 접속 “무료로 사용해 보기” 클릭 3) GCP 가입하기 무료 평가판 서비스 약관에 “예” 계정 유형 : 개인 이름 및 주소 입력 결제 수단 입력 모두 입력하면 1달러가 결제됩니다(추후 취소처리됩니다) 가입 완료! 프로젝트 생성 “빈 프로젝트 생성” 클릭해주세요! 프로젝트 이름과 프로젝트 ID 설정 필요한 경우 직접 수정해주세요! 프로젝트 ID는 우측에 수정 버튼을 클릭하면 변경 가능합니다 제 계정은 변경했습니다 GCP Console 몇가지만 설명드리면 (1) 기본적인 메뉴들이 위치합니다 결제 : 사용한 금액, 잔여 Credit, 클릭시 (5)와 동일한 곳으로 이동 남은 크레딧을 보고싶을 경우 https://console.cloud.google.com/billing로 이동 지원 : 구글에게 지원이 필요할 경우 사용 IAM 및 관리자 : IAM 설정, 서비스 계정, 유저 역할 결정 (2) 컴퓨팅 App Engine : 구글 앱엔진 Compute Engine : 컴퓨팅 엔진 (AWS의 EC2) Kubernetes Engine : 쿠버네티스 클러스터 (3) 저장소 : Bigtable, Datastore, Storage(AWS의 S3), SQL, Spanner 등 (4) Stackdriver : 구글 클라우드의 로그는 모두 스택드라이버에 기록됩니다. 추적 혹은 모니터링을 클릭하면 됩니다 좌측 메뉴 중 하단에 있는 것을 잘라왔습니다 빅 데이터의 BigQuery, Dataproc(하둡/스파크 클러스터), Dataflow, ML 엔진 등이 정말 좋은 서비스입니다",
    "tags": "basic gcp",
    "url": "/gcp/2018/01/01/gcp-intro/"
  },{
    "title": "빅데이터 생태계(Bigdata Ecosystem) - 벤더 및 아파치 재단",
    "text": "빅데이터 생태계에 대한 포스팅입니다. 빅데이터 벤더들, 오픈소스, 아파치 재단 및 아파치 프로젝트에 대한 내용입니다! 빅데이터 벤더 기존 데이터베이스, 데이터웨어하우스, 컨설팅 등으로 돈을 버는 회사가 존재 Oracle, Teradata, Splunk 등 고성능 DB, 머신을 비싸게 판매 컨설팅이 주요 상품 빅데이터 시대가 되며 변함 Hadoop을 필두로 오픈소스 제품들이 시장을 점유 Cloudera (2008) Hadoop 배포판 CDH를 배포 Enterprise Hadoop Hortonworks (2011) Cloudera와 유사한 비즈니스 MapR (2009) Cloudera와 유사한 비즈니스 기업용 배포판에 초점 (비싸지만 안정성, 성능 위주) 빅데이터 벤더들의 기술력이 뛰어나기 때문에 배포판을 선택하는 것도 좋은 선택! 오픈소스 소스코드가 공개된 소프트웨어 반드시 무료는 아님. 가끔 유료도 있음 사용하는 이유 개발자가 원하는대로 기능을 변경 가능 소스코드를 통해 학습 코드를 확인 가능해서 보안 측면에서 유리 회사에 종속되지 않기에 안정적 대표적 오픈소스 프로젝트 리눅스, 아파치 웹서버, 하둡 아파치 재단 Apache 프로젝트들을 지원하기 위해 설립된 비영리 재단 돈을 지원하진 않고 Jira 등 인프라/프로젝트 방향성 멘토링 2600명 이상의 자원봉사자 350개 이상의 프로젝트를 지원 프로젝트로 신청하는 방식 프로젝트 퀄리티, 커뮤니티 활성화 등을 고려 Incubating 프로젝트(1~2년) -&gt; Top Level Project 프로젝트 커뮤니티 개발자 커뮤니티 사용자 커뮤니티 커미터 프로젝트 커미티 (의회) 프로젝트 커미티 의장 (PMC) 프로젝트 진행상태, 커뮤니티 상태에 대한 레포트 발간 문제가 생겼을 시 중재 아파치 정신 커뮤니티 코드보다 커뮤니티 협업, 토론, 의견수렴 다양성 있는 커뮤니티가 바람직 커미터 프로젝트에 기여할 능력이 있고 실제로 기여한 사람들 개방성 실용주의 아파치 라이센스, MIT 라이센스는 그냥 사용하고 이 프로젝트를 사용했다고 적어두면 됨 기부",
    "tags": "engineering data",
    "url": "/data/2018/01/01/bigdata-ecosystem/"
  },{
    "title": "2017년 회고, 2018년 계획",
    "text": "남에게 보여주기 위해서가 아닌, 나를 돌아보기 위한 글(따라서 반말) 2016년 2017년을 말하기 전, 2016년부터 살짝 언급하고 가야할 것 같다. 2016년엔 패스트캠퍼스에서 데이터 사이언스 스쿨 과정을 수료했다. 사실 과거에 빅데이터 동아리 Boaz에서 활동해서 이론은 어느정도 알고 있었다. 단지 내가 R을 능숙하게 사용하지 못했던 점이 아쉬웠다. 그 아쉬운 점을 해소하기 위해 패캠을 찾아갔다. 파이썬을 처음 배우면서 데이터 분석의 재미를 찾아갔고 8월에 수료했다. 다른 동기들은 바로 취업을 하기 위해 원서를 넣었지만(물론 나도 조금 넣었지만) 10월에 인생 첫 유럽 여행이 계획되어 있어 진지하게 임하진 않았다. 그리고 어머니와 함께 간 유럽 여행은 정말 즐거웠고, 시야가 넓어졌다. 그 이후엔 데이터 분야에서 다양한 것들을 경험하려고 노력했다. 데이터 시각화를 위한 D3.js, 데이터 분산처리를 위한 하둡, 스파크, 웹을 만들기 위한 Flask, node.js, 그리고 데이터 경진대회인 Kaggle 까지 공부하고 이것 저것 만들다 2016년은 끝났다. 12월에 고민했던 것은 “대학원에 갈까?” 결국 가지 않기로 결정했다. 2017년 1월 - 학교 도서관, 스밥 에디터 우선 취업에 대한 생각이 깊지 않아 졸업유예를 신청하고 학교 도서관에 매일 갔었다. 그 당시 학교에서 진행하는 해킹 수업을 들었는데, C를 처음 짜보며 아.. 파이썬 짱… 여태까지 코딩을 편하게 했음을 알 수 있었다. 페이스북 그룹인 스타트업, 식사는 하셨습니까? 라는 곳에서 1년 운영진을 뽑고 있는 글을 봤다. 스타트업의 다양한 분들을 만나는 것을 즐기고 글을 쓰는 것도 좋아하기 때문에 에디터에 지원했다. 매달 새로운 스타트업을 만나며 그분들의 열정에 자극받았다. 시선을 바꿔 그것을 사업으로 만드는 것. 정말 멋진 능력이라고 생각했다. 2월 - Datanada, Retrica Tensorflow kr에서 John Park님의 구인 공고를 보고 문의를 드렸더니, 회사에 와서 1주일간 Kaggle을 해볼 생각이 있냐는 제안을 받았다. (심지어 공부하는건데 유급이었다!) 1주일간 신나게 캐글을 했다. 이 때 후기 그리고 동시에 카메라를 만드는 스타트업 Retrica 데이터 분석가에 지원했다. 위에서 말했듯이 취업에 생각이 크게 없어 가볍게 면접을 보러 갔다. 면접을 보러 갔는데 면접관분이 말을 너무 잘하셨다. 어디서 말과 논리로 지는 경우가 많지 않았는데 레트리카 COO님은 논리적이면서 말을 잘하셔서 회사에 대한 호기심이 +++! 되었다. 대표님 면접을 통해 기술적으로 배울 것이 많을 회사라고 생각했고 결국 합격해서 레트리카를 다니게 되었다. 그리고 월말에 9xd 행사에 가서 많은 분들을 뵙게 되었다. 그 이후에도 참여하고 싶었는데 인기가 너무 많아 참석이 어려웠다. 3월 - 적응기 회사에 들어와서 처음인 것이 너무 많았다. 우선 맥북, 구글클라우드 플랫폼. 조금씩 적응하며 회사에 필요한 것들을 진행했다 같은 직군인 분이 1분 계셨는데, 내가 들어갔을 당시엔 서버 개발자분이 웹에 뿌려주는 지표를 엑셀에 저장해 레포트를 만들었다. 웹에 있는 데이터를 복붙해서 엑셀에 저장하다보니(게다가 복붙할 양이 꽤 많았다) 완전 고생하고 계셨다. 크롤러 만들어서 csv로 떨구는 코드를 작성했다. 또 슬랙에 매일 주요 지표를 보내주는 bot을 만들었다. 이것도 참 간단한 일 그리고 사내연애를 시작했다(..!) 4월 - Dashboard, BigQuery 회사에서 전체적 지표를 볼 수 있는 곳은 존재했지만, 무언가 편하게 볼 곳이 없어서 대시보드를 구축하는 일을 했다. 그리고 서버 개발자분이 하시던 일을 나한테 다 인수인계를 해주셨다(..) 그 때 서버 개발자분의 코드를 처음 보며 와.. 역시 초고수의 코드는 이런 것인가..(참고로 서버 개발자분은 카카오 출신이신데 현재는 go매니아) 코드 가지고 공부를 꽤 했다. 그리고 Superset이라는 Airbnb에서 만든 오픈소스 가지고 대시보드를 만들었다. 생각보다 정말 간단했다(DB만 있으면 연결해주고 인스턴스 띄워주면 끝) 물론 이 친구도 단점이 존재하긴 했다. BigQuery에 직접 연결은 불가능 그리고 애정하는 BigQuery에 대해 많은 공부를 하고 자료를 찾아봤다. 이 당시에 국내에 자료가 많지 않아.. NBT에 재직하시는 분에게 페이스북으로 질문을 꽤 했는데 정말 답변을 잘 해주셔서 큰 도움이 되었다!!!!(감사합니다 재광님) 5월 - 삽질, 삽질, SQL 이 기간엔 정말 많은 삽질을 했다. 데이터 분석을 위해서 이리저리… 국가를 뛰어넘어 어떤 지역에서 앱을 많이 사용하는지 지도로 뿌려보고, 거기서 아이디어를 찾아 하나씩 파보고 그런 일을 했었다. 어떤 사람들이 앱을 사용하는가에 대해 많은 생각을 했던 시간. 동시에 BigQuery로 리텐션까지 뽑아내는 일까지 해야했다. 사실 GA를 쓰면 좋겠지만 하루에 사용하는 유저가 워낙 많아 비싸서.. 쿼리로 리텐션을 추출했다. 이 과정은 빅쿼리를 공부하기보다 SQL 기초부터 공부를 했다. 집합적인 관점에서 쿼리 짜는 방법을 계속 익혔다. 이 과정에서 R&amp;D를 담당하시는 분에게 많이 물어보고 배웠다. 쿼리로 할 수 있는 것들이 꽤 많아지게 된 시기 6월 - 데이터 이벤트 갈아엎기 데이터 이벤트에 대한 표준 규정이 없었다. 그리고 중복 이벤트도 있었고, 클라이언트 개발자분들도 부담스러워하는 로그 구조여서 갈아 엎었다. 이벤트 수를 대폭 줄이고, 파라미터도 정말 필요한 것만 남겨두었다 현재 구글에 계신 백정상님의 슬라이드쉐어를 먼저 봤다면 더 잘했을텐데.. 라는 아쉬움도 있지만! 데이터 내부에 많은 변화가 있었다. 그리고 이 때 BigQuery를 더 많이 공부해서, 가공-가공-가공 파이프라인을 3단계까지 만들었다. 조금 더 데이터를 압축해서 필요한 것만 남겼다. 7월 - Tableau 4월에 만든 Superset의 대체재를 찾다가 유료지만 좋은 Tableau를 사용해보자고 회사에 건의했다. 회사에서 결제해주셔서 사용하는데 정말 좋은 도구..!(단점은 유료) 위에서 쿼리로 작성했던 것을 시각화로 다 뿌려주고, BigQuery와 직접 연결해서 정말 좋았다. 여기서도 존재하는 단점은 1. BigQuery는 쿼리 비용이 나간다는 점(태블로에서 그래프를 그릴 때) 2. BigQuery와 Tableau의 연결이 엄청 빠릿빠릿하진 않다는 점. 그래서 지금도 다른 툴을 사용해야하나 고민중이다. 단점이 있더라도 워낙 좋은 도구라 사용중(만약 웹에 직접 코딩하라고 했으면 못했을듯ㅋㅎ) 그리고 Google Cloud에서 진행하는 행사에 참여해 회사의 대표로 참석하게 되었다. 우리의 킹갓제네럴엠퍼럴 서버 개발자분이 구글 클라우드를 다 활용해 아키텍쳐를 아름답게 만드셔서 조대협님이 박수를 쳤는데, 그 효과로 나도 어깨가 으쓱-했다. 이 때 GCP를 사용하는 많은 분들을 만나 좋았다 8월 - 데이터 분석 위에서 보면 계속 데이터 엔지니어링에 초점이 맞춰있었다. 데이터 분석 직군인데 데이터를 쌓는 부분이 미약했기 때문에(그리고 이쪽이 재미있었다) 엔지니어링에 심취했었다. 이러면 안된다! 라는 생각으로 데이터 분석을 계속 했다. 데이터가 많아 샘플링을 해서 랜덤 포레스트에 던져보기도 하고, 노가다식으로 특정 이벤트를 토대로 리텐션을 다 계산해보고, 레포트 작성하고.. 그 와중에 데이터 로그 설계도 하고, 데이터 QA, 요청 데이터 취합 등을 했다 이 시기에 내가 데이터 분석, 데이터 엔지니어링, 머신러닝(분석과 머신러닝이 겹치지만 이렇게 분류하고) 중 분석보단 엔지니어링, 머신러닝을 즐긴다는 것을 확실히 알게 되었다. 그리고 BigQuery 페이스북 그룹의 부운영자를 맡게 되었다. 아, 졸업했다!(대졸자로 승격!) 9월 - Kaggle 진짜 8월까진 일에 심취했다. 다른 내용이 없ㅋ엉ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 돌이켜보면 짧은 기간에 많은 것을 해봐서 개인의 성장이 폭풍!!!으로 되었던 시기였다 캐글뽀개기에서 진행하는 스터디에 참여했다. 돌이켜보면 이 때 정말 좋은 팀원분들을 만났고! 좋은 등수를 받고 있다(현재 Zillow 대회 62등/3778팀) 캐글을 할 때 CPU 32 core, 200g ram, gpu 머신을 펑펑 사용해봤다..ㅎ(남은 크레딧 소진) Datalab에 대해서도 많이 알게 되었다 10월 - 살짝 번아웃 살짝 번아웃 증세가 나타났던 시기였다. 넓은 바다에서 길잃던 시기라고 해야하나, 조금 지쳤던 시기. 사실 내가 욕심이 많아 번아웃이 온 것이라 생각했다. 적절하게 일하며 적절하게 잘해야된다라고 다짐. 그래서 이 시기부터 칼퇴근을 시작했다. 회사에선 AB Test도 해보고 이것저것 조급해하지 않으며 여유롭게 보냈다 추석 연휴에 쉬는 날이 많아 BigQuery Tutorial을 작성했다! Star는 43개인데 더 받고싶으면 따봉충인가 헤헷 개발자 모임을 진행하며 다양한 시야를 갖게 되었다. 재밌었고 내가 행사 기획같은 것을 잘.. 하나..ㅎ 라는 생각도 들었다 11월 - CNN을 활용한 야한 사진검출, 비전 스터디 레트리카 앱의 소셜 부분에서 야한 사진을 올리는 외국인들이 참 많이 있었다. 기존에 구글 vision api를 사용했지만 범용적이라 잘 맞추지 못했다. 오퍼레이션팀에서 손으로 직접 차단을 하는 것을 보고 안타까워서 혼자 진행해봤다. 대략 5일만에 만들어야 했다(..) 먼저 데이터 수집. 앱 내의 데이터는 터무니없이 적었고 크롤링하면 되지 뭐!!! 하고 시작했다. 그러나 ㅋㅋㅋㅋㅋㅋㅋㅋ 구글엔 야한 사진이 거의 없다. 있어도 너무 깨끗한 사진이다(실제 앱에서 나오는 사진은 배경도 있고, 어둡거나 그런데 구글의 사진들은 배경이 거의 다 단일색) 꾸역꾸역 찾아서 vgg16 로 만들어서 돌렸다. 그 결과 70%정도…… 분류…… 하지만 데이터가 불균형하고 조금 잘못된 데이터도 있어서 사용하기엔 무리라고 생각했다. 검색해보니 NSFW Score라는 야후에서 공개된 모델이 있었다! 낼름 사용해서 적용했다. 데이터를 얼마나 넣었는지 궁금할정도로 남자 성기, 여자 가슴을 잘 잡는다. 심지어 자세까지 잡는다(..) ResNet을 사용해서 확실히 성능이 좋았다. 우리 데이터를 살짝 넣고 튜닝했다 이 시기에 야한 사진이 아닌 적나라한 사진을 너무 많이 봤다. 나는 괜찮은데 내 뒷자리에 계신 직원분들이 내 모니터를 보고 무슨 생각을 했을까..^^.. 또한 비전 스터디를 시작하게 되었다. 갓엠퍼러짱짱인 우태강님과 함께 하는데, 같이 하는 상현님, 영택님도 열심히 해서! 정말 배울 것이 많은 스터디다. opencv와 CNN류 알고리즘을 많이 알게 되는중. 이젠 GAN을 학습할 예정 12월 - GCP 행사 주최, 보직 변경 12월엔 Google Cloud User Group에서 진행한 컨퍼런스에 집중했다. 발표를 준비할 시간이 없어 발표자로 지원하진 못했지만 나중엔 발표를 해야겠다! 라고 다짐. 온라인에서만 보던 GCP분들을 많이 보게 되었다. 물론 지식도 증가했다. 점점 GCP의 늪에 빠지게 되었다(애저도 써보고 싶은데 후후….) 그리고 회사에서 보직 변경을 제안 받았다. 데이터 분석 -&gt; 백엔드 엔지니어 사실 백엔드라고 되어 있지만 레트리카의 서버는 Go로 이루어져 있고 주니어에게 맡길 생각이 전혀! 없을거란 것을 알고 있다. 내게 주어진 롤은 분산처리, 그리고 추천 엔진 만드는 일, 기타 딥러닝을 사용해 부가가치 생성하는 일- 정도라고 인지하고 있다 현재 이 일을 위해 Apache Beam, Dataflow를 공부하고 있다. Contributor에 도전하고 싶다 개발적 역량을 늘릴 수 있고, 기존엔 사수가 없이 진행하다보니 답답한 것들이 많았지만 지금은 배울 것이 있는분과 함께할 수 있어서 제안을 승낙했다. 12월 22일부터 18년 1월 1일까지 휴가-! 계속 공부만 하고 있다. 공부하고 써먹기 위한 방법을 생각하고 있다 그리고 리드미에서 멘토링을 계획하고 있다. 이제 많은 분들의 고민을 들어주고 싶다 2018년 계획 정말 가볍게 쓰려고 했는데 양이 이렇게나 많다. 이 글을 누가 읽을까..^^ 읽다가 중간에 이탈할 것 같다!!! 그래도 1년간 어떤 일이 있었는지 기록하는 것은 정말 중요하다 생각하기에 나에게 칭찬해주고 싶다 18년 계획은 아래와 같다 블로그 글쓰기 모임 진행 : 글또라는 이름으로 지었다. 아마 1월부터 진행할 예정인데, 강제로 돈을 걷고 포스팅하면 돌려드리는 형식으로 진행할 예정이다. 그렇기 때문에 많은 인원을 내가 커버할 수가 없다. 아마 지인이나 소수로 운영하다 잘되면 크게 넓힐듯..! 건강 챙기기 : 체력이 하락하고 살은 찌고 있다..^^.. 이제 운동을 하며 건강을 챙겨야겠다 현재 하고있는 일에 대한 전문성 + 딥러닝쪽 특화 캐글은 꾸준히 참여 멘토링도 간간히 진행 뭐 가볍게 이렇게 적어두고 나중에 생각해야지!!! 아 그리고 어쩌다 보니 Github를 꾸준히 사용하는데, 그냥 공부한 것을 꾸준히 적다보니 이렇게 되었다. 코딩-&gt;알게된 내용-&gt;기록 이런 루프였는데 계속 진행해야겠다-!! 0년차 데이터 분석가지만 많은 성장을 이룬 2017년 고맙고 더 열심히 살아야지!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
    "tags": "diary",
    "url": "/diary/2017/12/30/2017-retrospect/"
  },{
    "title": "Bash Tab 자동 완성시 대소문자 설정 방법",
    "text": "안녕하세요! 오늘은 Bash에서 Tab 자동 완성 기능에 대해 설명드리려고 합니다. 리눅스 터미널에서 Tab 자동완성 기능(tab-auto-complete)은 매우 유용합니다!! 설정을 하지 않았을 경우 대소문자를 구분합니다. 예를 들면 Workspace에 들어가기 위해선 cd wor에서 탭을 누르는 경우엔 반응하지 않고, cd Wor 에서 탭을 눌러야만 자동 완성이 됩니다 이런 상황이 매우 불편하기 때문에 설정을 on으로 시켜줍니다! 특정 계정에서만 이 설정을 유지하고 싶다면 ~/.inputrc에 설정하고, 모든 계정에 설정하고 싶으면 /etc/inputrc 에 설정해주면 됩니다 sudo vi /etc/inputrc 텍스트 파일에 아래와 같은 문구 추가 set completion-ignore-case on Esc + :wq! 를 누르고 저장하며 빠져나오기 설정한 후, 터미널을 재시작 해주면 정상 작동합니다!",
    "tags": "linux development",
    "url": "/development/2017/12/28/terminal-auto-complete/"
  },{
    "title": "GAN(Generative Adversarial Network) 기초",
    "text": "최근 핫한 Generative Adversarial Network, GAN을 학습한 내용을 정리한 문서입니다 Introduction Supervised Learning Discriminative Model : Input이 들어갔을 때 Output이 나옴 Unsupervised Learning Generative Model : train 데이터 분포를 학습 파란색이 실제 데이터의 분포고 빨간색은 실제 데이터의 분포를 근사! 두개의 차이값을 줄이는 것이 목표 Generative Adversarial Network Discriminator를 먼저 학습 : 진짜 이미지가 들어가면 진짜로 구분, 가짜 이미지는 가짜로 구분 input : 이미지의 고정된 벡터 output : 진짜 / 가짜 : 1 (sigmoid를 통해 0.5 기준으로 classification) generator는 랜덤한 코드를 받아서 이미지를 생성 -&gt; 그리고 discriminator를 속여야 함(1이 나오도록 학습) discriminator object 함수(loss) discriminator는 목적함수를 최대화하는 것이 목표 x~p_data(x) : 확률 밀도함수 z : 랜덤한 벡터(표준 정규분포나 100차원 벡터로 샘플링)을 g에게 줬을 경우 이미지를 생성 처음에 형편없는 이미지를 만들기 때문에 discriminator는 가짜라고 확신을 함. 그러면 D(G(z))가 0에 가깝게 나옴(= 기울기의 절대값이 엄청 작음) 기울기를 크게 하기 위해 log(x)그래프를 max!!! =&gt; 기울기가 무한대 초반에 generator가 굉장히 안좋은 상황에서discriminator가 가짜라고 확신하는 상황을 빨리 벗어나기 위해 이런 트릭을 사용 generator 목적 함수 generator는 우측의 식을 최소화 코드 import torch import torch.nn. as nn D = nn.Sequential( nn.Linear(784 ,128), nn.ReLU(), nn.Linear(128, 1), nn.Sigmoid()) G = nn.Sequential( nn.Linear(100, 128), nn.ReLU(), nn.Linear(128, 784), nn.Tanh()) # 생성된 값이 -1 ~ 1 criterion = nn.BCELoss() # Binary Cross Entropy Loss(h(x), y), Sigmoid Cross Entropy Loss 함수라고도 불림. -ylogh(x)-(1-y)log(1-h(x)) d_optimizer = torch.optim.Adam(D.parameters(), lr=0.01) g_optimizer = torch.optim.Adam(G.parameters(), lr=0.01) # 충돌하기에 2개의 optimizer를 설정 while True: # train D loss = criterion(D(x), 1) + criterion(D(G(z)), 0) loss.backward() # 모든 weight에 대해 gradient값을 계산 d_optimizer.step() # train G loss = criterion(D(G(z)), 1) loss.backward() g_optimizer.step() # generator의 파라미터를 학습 왜 이론적으로 잘 되는가? 최적화하는 것이 서로 다른 확률분포간의 차이를 줄여주기 때문에 실제 Generator가 실제와 가까운 이미지를 만들 수 있다! Variants of GAN Deep Convolutional GAN (DCGAN, 2015) CNN을 사용해서 discriminator를 생성하고 deconvolutional network를 통해서 generator를 만든 모델 아직까지도 가장 선호되면서 간단히 만들 수 있는 모델 핵심 : Pooling Layer를 사용하지 않음! (사용하면 unpooling할 때 blocking한 이미지를 생성) 대신 Stride가 2 이상인 convolution과 deconvolution을 사용함 Adam Optimizer의 모멘텀 텀이 0.5, 0.999 2개가 있음. 64x64를 생성할 때 저 파라미터를 사용하면 성능이 좋아서 이렇게 사용중 Generator에 들어가는 latent vector를 통해 연산을 할 수 있음(Word2vec같이!) : Z vector가 선형적 관계를 가짐 Least Squares GAN (LSGAN) 기존엔 discriminator를 속이기만 하면 됨. discirminator를 완벽히 속인 친구들. 좋진 않음! 우리의 목적은 진짜 데이터와 가까워야 함 D = nn.Sequential( nn.Linear(784 ,128), nn.ReLU(), nn.Linear(128, 1)) # sigmoid를 없앰 G = nn.Sequential( nn.Linear(100, 128), nn.ReLU(), nn.Linear(128, 784), nn.Tanh()) D_loss = torch.mean((D(x) -1)**2) + torch.mean(D(G(z))**2) G_loss = torch.mean((D(G(z))-1)**2) # cross entropy loss와 차이 : decision boundary가 1에 가깝도록 만듬 Semi-Supervised GAN discriminator가 진짜, 가짜를 구분하지 않고 클래스를 구분하게 됨. 기존 10개의 클래스 + fake 위쪽은 discriminator쪽은 Supervised Learning, generator는 Unsupervised Learning Auxiliary Classifier GAN(ACGAN, 2016) discriminator가 하는 일이 2가지 진짜를 구분 (sigmoid) 진짜든 가짜든 클래스를 구분 (softmax) Multi-task learning generator도 하는 일이 2가지 진짜, 가짜를 구분 클래스 구분 여태는 generator에 집중했던 GAN들이 많다면 discriminator에 집중! Extensions of GAN CycleGAN : Unpaired Image to Image Translation 얼룩말을 말로 바꾸고 여름을 겨울로 바꾸는 이미지의 스타일을 바꾸는 것 pair example이 없이 unsupervised learning을 통해 모델을 학습 generator는 latent vector를 받지 않고 이미지를 input으로 받음. 인코더-디코더같은 느낌 discriminator는 얼룩말 이미지를 말 이미지로 바꾸고 싶음. 말 이미지를 주고 이 이미지가 진짜다라고 학습을 하고 generator는 얼룩말을 받고 말로 바꿈 얼룩말 사진을 주고 말을 뛰는 모습을 생성하면 속을 수 있음! style transfer는 모양을 유지 얼룩말 이미지를 다시 말로 바꿈!! StackGAN 텍스트를 주고 텍스트에 해당하는 이미지를 만듬 Task에 집중하지 않고 한번에 고해상도를 학습할 경우 힘들다는 한계가 존재함. 64x64를 먼저 만들고 업샘플링 과정을 겪음 Visual Attribute Transfer CycleGAN처럼 Input이 왼쪽, 오른쪽 2개로 들어감 User-Interactive Image Colorization 흑백 사진을 칼라로 변환해줌(사용자가 원하는 색으로 변경 가능) Future of GAN Boundary Equilibrium GAN (BEGAN) 해당 로스함수가 줄어들 때 학습이 잘되더라(하지만 이것은 휴리스틱하게 나온 결과) discriminator auto encoder 구조라 복잡한 편 Reconstruction Loss weight normalization을 사용 단점 : 로스를 얻기 위해 z값을 학습해야함 Deconvolution Checkboard Artifacts 좋은 upsampling을 찾아야 하지 않을까- Deconvolution을 많이 사용했었는데 이건 output을 불균형하게 생김(체크보드 패턴처럼 이미지가 생김) Resize-Convolution을 사용하면 up sampling은 룰베이스 방식으로 한 후, convolution stride=1을 하고 필터링을 여러 레이어로 쌓게 됨! - 골고루 고려할 수 있음 BEGAN에선 resize-convlution을 사용했음 Machine Translation (Seq2Seq) Supervised learning은 영어로 주면 한글의 한 문장을 나옴. 이게 Supervised learning의 한계라 생각하고 GAN을 활용 Reference 1시간만에 GAN 완전 정복하기 초짜 대학원생 입장에서 이해하는 GAN",
    "tags": "dl data",
    "url": "/data/2017/12/27/gan/"
  },{
    "title": "리눅스 awk 사용법",
    "text": "awk awk : 데이터를 조작하고 리포트를 생성하기 위해 사용하는 언어입니다. 리눅스에서 사용하는 awk는 GNU 버전의 gawk로 심볼릭 링크되어 있습니다 간단한 연산자를 명령라인에서 사용할 수 있으며, 큰 프로그램을 위해 사용될 수 있습니다. awk는 데이터를 조작할 수 있기 때문에 쉘 스크립트에서 사용되는 필수 툴이며, 작은 데이터베이스를 관리하기 위해서도 필수입니다 Alfred Aho, Peter Weinberger, Brian Kernighan 3명이 만들었는데 이들의 이름 이니셜을 가져와서 awk라고 부릅니다 awk 프로그래밍 형식 awk 명령어를 입력한 다음, 작은따옴표로 둘러싸인 패턴이나 액션을 입력하고 마지막엔 입력 파일 이름. 파일 이름을 지정하지 않으면 키보드 입력에 의한 표준 입력을 받음. 그리고 awk는 입력된 라인들의 데이터들을 공백 또는 탭을 기준으로 분리해 $1부터 시작하는 각각의 필드 변수로 분리해 인식 awk 형식 awk 'pattern' filename awk '{action}' filename awk 'pattern {action}' filename 예시 $ vi awkfile 홍 길동 3324 5/11/96 50354 임 꺽정 5246 15/9/66 287650 이 성계 7654 6/20/58 60000 정 약용 8683 9/40/48 365000 $ awk '{print $0}' awkfile &gt; 홍 길동 3324 5/11/96 50354 임 꺽정 5246 15/9/66 287650 이 성계 7654 6/20/58 60000 정 약용 8683 9/40/48 365000 $ awk '{print $1}' awkfile &gt; 홍 임 이 정 $ awk '/길동/' awkfile &gt; 홍 길동 3324 5/11/96 50354 $ awk '/정/{print \"\\t안녕하세요? \" $1, $2 \"님!\"}' awkfile &gt; 안녕하세요? 임 꺽정님! 안녕하세요? 정 약용님! $ awk '/홍/{print $1, $3}' awkfile &gt; 홍 3324 $ df | awk '$4 &lt; 100000' : |을 이용해 파이프라인 생성 &gt; devfs 368 368 0 100% 638 0 100% /dev map -hosts 0 0 0 100% 0 0 100% /net map auto_home 0 0 0 100% 0 0 100% /home awk 동작원리 awk는 파일 또는 파이프를 통해 입력 라인을 얻어와 $0라는 내부 변수에 라인을 입력. 각 라인은 레코드라고 부르고, newline에 의해 구분 라인은 공백을 기준으로 각각의 필드나 단어로 나뉜다. 필드는 $1부터 시작. 많게는 100개 이상의 필드를 저장할 수 있음 내장 변수인 FS라고 부르는 필드 분리자가 공백을 할당받는다. 필드가 콜론이나 대시와 같은 문자에 의해 분리되면 새로운 필드 분리자로 FS의 값을 변경할 수 있다 awk는 화면에 필드를 출력할 때 print 함수를 사용 콤마는 출력필즈 분리자(OFS)와 매핑되어 있으며 공백을 할당받음 OFMT 변수 숫자를 출력할 때 숫자 포맷 제어할 경우 사용. 간단히 printf를 사용할 수도 있지만, OFMT를 지정할 수 있음. default는 %.6g로 소수점 6자리 $ awk 'BEGIN{OFMT=\"%.2f\"; print 1.23412, 15E-3}' &gt; 1.23 0.01 printf 함수 포매팅된 깔끔한 출력을 할 경우 사용 newline을 제공하지 않기 때문에 newline이 요구되면 \\n을 사용해야함 c : 문자, d : 10진수, f : 실수, x : 16진수 -이 붙으면 좌츠겡서 시작되고 기본형이면 우측에서 시작 $ awk '{printf \"The name is %-20s Number is %4d\\n\", $1\" \"$2, $3}' awkfile &gt; The name is 홍 길동 Number is 3324 The name is 임 꺽정 Number is 5246 The name is 이 성계 Number is 7654 The name is 정 약용 Number is 8683 awk -f 옵션 awk 액션과 명령이 파일에 작성되어 있다면 -f 옵션을 사용 awk -f [awk 명령파일] [awk 명령을 적용할 텍스트 파일] $ vi awkcommand {print \"안녕하세요 \" $1, $2\"님\"} {print $1, $2, $3, $4, $5} $ awk -f awkcommand awkfile &gt; 안녕하세요 홍 길동님 홍 길동 3324 5/11/96 50354 안녕하세요 임 꺽정님 임 꺽정 5246 15/9/66 287650 안녕하세요 이 성계님 이 성계 7654 6/20/58 60000 안녕하세요 정 약용님 정 약용 8683 9/40/48 365000 레코드와 필드 레코드 awk는 입력 데이터를 볼 수 없지만 포맷 또는 구조는 볼 수 있다. 레코드라고 불리는 각 라인은 newline으로 분리 NR 변수 : 각 레코드들의 번호는 awk의 빌트인 변수 NR에 저장된다. 레코드가 저장된 다음 NR의 값은 하나씩 증가한다 $ awk '{print NR, $0}' awkfile &gt; 1 홍 길동 3324 5/11/96 50354 2 임 꺽정 5246 15/9/66 287650 3 이 성계 7654 6/20/58 60000 4 정 약용 8683 9/40/48 365000 필드 각 레코드는 디폴트로 공백이나 탭으로 분리된 필드라는 워드로 구성된다. NF에 필드의 수를 유지하며 라인당 100개의 필드를 가질 수 있다 $ awk '{print $1, $2, $5, NF}' awkfile &gt; 홍 길동 50354 5 임 꺽정 287650 5 이 성계 60000 5 정 약용 365000 5 필드 분리자 빌트인 변수 FS는 입력 필드 분리자의 값을 가지고 있음. default는 공백과 탭. FS 값을 변경하기 위해선 -F을 사용하며 -F 다음에 오는 문자가 새로운 필드 분리자가 됨 $ vi awkfile_FS 홍 길동 :3324 :5/11/96 :50354 임 꺽정 :5246 :15/9/66 :283502 $ awk -F: '/홍/{print $1, $2}' awkfile_FS &gt; 홍 길동 3324 $ awk -F'[ :\\t]' '/홍/{print $1, $2}' awkfile_FS &gt; 홍 길동 awk와 정규표현식 정규표현식은 슬래시로 둘러싸인 문자들로 구성된 패턴 $ awk '/^정/{print $1, $2, $3}' awkfile &gt; 정 약용 8683 $ awk '/^[A-Z][a-z]+ /' awkfile2 &gt; Hong KilgDong 3324 5/11/96 50354 match 연산자(~) : 표현식과 매칭되는 것이 있는지 검사하는 연산자 $ awk '$2 !~ /g$/' awkfile2 : 2번 필드가 g로 끝나지 않는 라인 출력 &gt; Lee Seongkye 7654 6/20/58 60000 POSIX 문자 클래스 $ awk '/[[:lower:]]+g[[:space:]]+[[:digit:]]' awkfile2 &gt; Hong KilgDong 3324 5/11/96 50354 비교 표현식 어떤 상태가 참일때만 액션이 수행 $ awk '$3 &gt; 7000{print $1, $2}' awkfile &gt; 이 성계 정 약용 조건 표현식 표현식을 검사하기 위해 ?와 :를 사용. if/else가 하는 역할과 같은 결과를 의미 $ awk '{max={$1 &gt; $2) ? $1 : $2; print max}' filename if $1 &gt; $2: max = $1 else: max = $2 산술 연산자 계산을 통해 필터링 가능 논리 연산자 &amp;&amp; : AND 연산 || : OR 연산 ! : NOT 연산 $ awk '$3 &gt; $5 &amp;&amp; $3 &lt;= 100' filename awk 변수 BEGIN 패턴 awk가 입력 파일의 라인들을 처리하기 이전에 실행되며 액션 블록 앞에 놓임 입력 파일 없이 테스트할 수 있고, 빌트인 내장 변수(OFS, RS, FS)들의 값을 변경할 경우 사용 END 패턴 어떤 입력 라인과도 매칭되지 않고, 입력 모든 라인이 처리된 후 실행됨 BEGIN만 사용할 경우엔 아규먼트 파일명을 적지 않아도 되지만 END 블록을 사용할 경우엔 반드시 아규먼트 파일을 적어야 함 $ awk '/Tom/{count++}END{print \"Tom was found \" count \" times.\"}' awkfile5 awk 리다이렉션 awk결과를 리눅스 파일로 리다이렉션할 경우 쉘 리다이렉션 연산자를 사용 파일명은 큰따옴표로 둘러쌰아 함 &gt; 심볼이 사용될 때 파일이 오픈되고 잘려짐 $ awk -F: '$4 &gt;= 60000 {print $1, $2 &gt; \"new_file\"}' awkfile5 getline 함수 표준 입력, 파이프, 현재 처리되고 있는 파일로부터 입력을 읽기 위해 사용. 입력의 다음 라인을 가져와 NF, NR, FNR 빌트인 변수를 설정 레코드가 검색되면 1을 리턴하고, 파일의 끝이면 0을 리턴. 에러가 발생하면 -1을 리턴 $ awk 'BEGIN{ \"date \" | getline d; split(d, year) ; print year[6]}' &gt; 2009 $ awk 'BEGIN{while (getline &lt; \"/etc/passwd\" &gt; 0)lc++; print lc}' filename awk 파이프 awk 프로그램에서 파이프를 오픈하고 또다른 파이프를 오픈하기 전에 기존 파이프는 닫아주어야 한다. 파이프 심볼의 오른쪽 명령은 큰따옴표(““)로 둘러싸야 한다 $ awk '{print $1, $2 | \"sort-r\"}' cars END블록에 close를 사용해 파이프를 꼭 닫아줘야 함 $ awk '{print $1, $2 | \"sort -r\"} END{close(\"sort -r\")}' cars",
    "tags": "linux development",
    "url": "/development/2017/12/20/linux-6/"
  },{
    "title": "리눅스 sed 사용법",
    "text": "텍스트 처리를 위한 유틸리티는 대표적으로 sed(stream editor)와 awk가 있습니다. 이번엔 sed에 대해 학습해보려고 합니다 정규 표현식을 사용하고, 기본 입출력은 표준 입력과 출력을 사용합니다. 파이프를 통해 한쪽의 출력을 다른 쪽으로 넘길 수 있습니다 주어진 주소 범위에 대해 입력의 어떤 줄을 처리할 것인지 결정합니다. 주소 범위에는 라인 번호 또는 패턴을 사용할 수 잇습니다. 예를 들어 5d라고 하면 다섯 번째 라인을 삭제하라는 것이며 windows/d는 “windows”를 포함하는 모든 라일을 삭제하라는 의미입니다 sed 동작 원리 sed 스트림 에디터는 한번에 하나의 파일 또는 하나의 입력으로부터 한 라인만 처리하고 모니터로 출력합니다. 이 명령은 vi 에디터에서 사용 가능하며, 저장된 라인은 패턴 공간이라고 부르는 임시 버퍼에서 처리합니다 -임시 버퍼에 있는 라인의 처리가 한번 끝나면 임시 버퍼에 있는 라인은 모니터로 보내집니다. 라인이 처리된 다음 임시 버퍼에서 라인은 제거되고 다음 라인이 임시 버퍼로 읽혀지고 처리되고 출력됩니다 예제 8d 입력의 8번째 줄을 삭제 /^$/d 빈 줄을 모두 삭제 1,/%$/d 첫 줄부터 처음 나타내는 빈 줄까지 삭제 /Jones/p “Jones”를 포함하는 줄만 출력 (-n 옵션 사용) s/GUI//g “GUI”가 나오는 줄에서 GUI를 ‘‘로 변경(삭제)",
    "tags": "linux development",
    "url": "/development/2017/12/17/linux-5/"
  },{
    "title": "리눅스 grep 사용법",
    "text": "grep 입력되는 파일에서 주어진 패턴 목록과 매칭되는 라인을 검색한 다음 표준 출력으로 검색된 라인을 복사해서 출력 정렬 관련 옵션을 사용하면 정렬해 출력할 수 있음 grep의 검색 범위는 메모리 제한을 넘어가지 않는 범위에서 입력 라인의 제한이 없으며 하나의 라인 안의 전체적인 문자들도 매칭할 수 있음 입력 파일의 마지막 바트가 newline이 아니라면 grep은 작업을 수행한다. grep 메타문자 ^ : 라인의 시작 $ : 라인의 끝 . : 하나의 문자 매칭 * : 문자가 없거나 그 이상의 문자들이 매칭 [] : []안의 문자 중 하나라도 매칭 [^] : []안의 문자 중 하나도 매칭되지 않는 문자 grep [옵션] [패턴] [파일명] 일반 옵션 -b : 검색된 라인에 블록 번호를 붙여서 출력 -c : 매칭된 라인을 디스플레이하지 않고 매칭된 라인의 수를 출력 -h : 파일명은 출력하지 않음 -i : 패턴에서 사용되는 문자열에서 대소문자를 모두 검색 -l : 패턴에 의해 매칭된 라인이 하나라도 있는 파일의 이름만 출력. 출력시 각 파일명은 newline으로 분리 -n : 매칭된 라인을 출력할 때 파일상의 라인 번호를 함께 출력 -s : 조용히 진행. 에러 메세지를 출력하지 않음 -v : 패턴과 매칭되지 않는 라인만 출력 -w : \\&lt;과 \\&gt;로 둘러싸인 하나의 단어를 표현식으로 검색 예제 grep -i ls ~/.bash* | grep -v history /root 디렉터리 아래에 .bas로 시작하는 모든 파일들에서 ls라는 문자열을 검색하는데, 대소문자를 구분하지 않고 모두 검색하며 결과값 중 history 문자열을 포함하지 않는 줄을 출력 grep korea /etc/passwd echo $? 검색한 내용이 존재하지 않기 때문에 검색에 실패했으므로 종료상태값은 1 ls -l | grep '^d' 디렉터리만 출력 ps aux | grep '^ *multi' ps aux의 출력을 grep과 파이프하고 라인 앞에 공백을 포함해 multi가 있는 라인 출력 egrep grep의 확장으로서 추가적인 정규표현식 메타문자들을 사용할 수 있음 egrep에 추가된 메타문자들 + : + 앞의 문자 중 하나 이상이 매칭되는 문자 ? : 바로 앞의 문자 하나가 없거나 하나가 매칭되는 문자 a|b : a 또는 b와 매칭되는 문자(or) () : 문자 그룹 fgrep Fixed grep of Fast grep 정규 표현식 메타문자들은 사용할 수 없기 때문에 특수 문자 및 $ 문자들은 문자 그대로 인식",
    "tags": "linux development",
    "url": "/development/2017/12/16/linux-4/"
  },{
    "title": "구글 BigQuery 개요",
    "text": "본 문서는 Github Tutorial의 내용을 재구성한 것입니다! BigQuery란? Google에서 드레멜 엔진을 사용해 만든 페타 바이트 규모의 저비용 데이터 웨어하우스입니다. 구글에서 관리해주기 때문에 사용자가 별도의 서버나 물리적 하드웨어에 대해 스트레스를 받을 일이 없습니다. 일반적인 rdb나 noSQL보다 속도가 월등히 빠르며, 몇초 안에 TB를 스캔할 수 있습니다. 또한 Google Cloud Storage에서 데이터를 읽어 분석할 수 있습니다 데이터 저장 및 컴퓨팅의 개념을 분리해 독립적으로 비용을 지불할 수 있습니다 웹 UI 혹은 Command Line tool을 사용해 BigQuery를 이용할 수 있으며 Rest API와 클라이언트 라이브러리가 준비되어 있습니다 ( python, java, c#, go, node.js, php, ruby ) Firebase 와 연동해서 사용할 경우 100000% 정말 좋습니다! ( 모든 데이터가 누락없이 쌓이는데, 신경쓸 필요가 없습니다! ) Legacy SQL과 Standard SQL 2가지 방식으로 쿼리를 날릴 수 있습니다 (세부 문법이 살짝 다릅니다!) key, index가 따로 존재하지 않고 Full Scan을 합니다 Select할 때, Column base로 비용을 부과합니다 Third Party 도구들과 호환됩니다 ( Tableau, Google Analytics 360 suite ) BigQuery 시작 Google Cloud 회원 가입 Google 로그인 Project 생성 및 선택 단, 처음에 체험판 등록하면 300$ 제공합니다 BigQuery API 등록 BigQuery UI console 위 화면은 BigQuery에 처음 진입하면 나오는 화면입니다. BigQuery Console이 디자인 및 기능을 개선한 버전이 개발중입니다! ‘COMPOSE QUERY’를 누르면 쿼리를 날릴 수 있는 화면이 보입니다! 1) Navigation bar COMPOSE QUERY : 이 버튼을 누르면 쿼리를 날릴 수 있는 창이 나타납니다! (우측과 같은 Query Editor가 생겨요) Query History : 조회했던 쿼리가 기록이 나타납니다. 무한정으로 쿼리 History를 볼 수 있는것은 아니에요! Job History : Job 작업의 기록이 나타납니다. Job은 load data, export data(table), copy data(table) My Project라고 써있는 곳은 ‘Project’ 부분입니다! 현재 저는 4개의 Dataset이 존재합니다 Public Datasets : BigQuery에서 사용할 수 있는 무료 Datasets입니다! 연습하기에 정말 좋은 자료들입니다 2) Query Editor New Query라고 써있는 곳은 쿼리의 이름을 나타냅니다. 그 아래의 흰 공간에서 Query를 입력할 수 있습니다 우측의 UDF Editor는 java script로 사용자 정의 함수( User Define Function )를 만들 수 있습니다 Run Query : 쿼리를 실행하는 버튼으로 단축키는 command(ctrl) + Enter입니다 Save View : 쿼리 결과(View)를 저장하는 버튼으로 legacy SQL을 사용할 경우 가능합니다(개인적으론 Save View보단 다른 table로 저장하는 것을 선호해요) Format Query : 형식에 맞게 자동으로 수정해줍니다 Show Options : 다양한 옵션 설정이 가능합니다. 예를 들어 쿼리 결과를 Table로 저장하는 것, 거대한 Result도 허락하는 것, Query Cashing 유무, Legacy / Standard SQL 선택 등이 가능합니다 3) Querys list 이 부분은 Query history를 누를 경우 생깁니다. History가 나오고 Filter로 검색해볼 수 있습니다 Datasets의 Table을 클릭할 경우, 이 부분에 Table에 대한 정보가 나타납니다 Run Query를 실행한 경우 이 부분에 쿼리 결과가 나타납니다 Table Detail Schema : 해당 테이블의 Schema(Column 이름, 타입, 설명)가 있습니다 Details : 해당 테이블과 관련된 정보가 포함되어 있습니다(Table ID, Size, Number of Rows, Creation Time, Labels) Preview : 해당 테이블에 데이터가 어떻게 적재되어있는지 볼 수 있습니다!! 유용한 기능입니다 Refresh : 새로고침 Query Table : 클릭하면 테이블에 쿼리를 날릴 수 있는 기본구조가 구성됩니다 Copy Table : 테이블을 Copy Export Table : 테이블을 Export합니다. CSV, Json, Avro로 가능하며 GZIP으로 압축할 수 있습니다 (저장하는 공간은 Google Cloud Storage) Delete Table : 테이블 삭제 Query Result Results : 쿼리의 결과가 나타나는 공간입니다 Explanation : 쿼리의 단계별 연산에 대한 설명이 적혀있습니다 Job Information : 쿼리 Job에 대한 정보가 기록되어 있습니다 Save as Table : 지정하는 Table로 쿼리의 결과를 저장합니다 Save to Google Sheets : 구글 스프레드시트로 결과를 저장합니다. 저는 데이터 요청이 있을 경우, 대부분 스프레드시트로 공유합니다! Keyboard shortcuts Windows/Linux Mac Action Ctrl + Enter Cmd + Enter 현재 Query를 실행 Tab Tab 자동 완성기능 Ctrl Cmd Table 이름을 Highlight Ctrl + / Cmd + / 현재 줄 주석처리 Ctrl + Shift + F Cmd + Shift + F Query Format 맞추기",
    "tags": "BigQuery gcp",
    "url": "/gcp/2017/12/14/bigquery-1/"
  },{
    "title": "리눅스 정규 표현식과 패턴 검색",
    "text": "정규표현식 검색에서 사용할 매칭되는 같은 문자들의 패턴 메타 문자 정규 표현식에서는 문자 그대로의 의미 이상으로 해석되는 메타문자 라고 부르는 문자들의 집합을 사용 연산자 효과 . 모든 문자 1개와 일치 ? 앞에 존재하는 문자가 있을수도, 없을수도 있을 경우 사용 * 앞에 존재하는 문자가 0번 혹은 그 이상 반복되는 문자를 찾을 때 사용 + 앞에 존재하는 문자가 1번 혹은 그 이상 반복되는 문자를 찾을 때 사용 [] 대괄호 사이에 존재하는 문자들 중 하나에 일치 [a-z] a부터 z까지 모든 영문자 소문자와 일치 ^ 대괄호 사이에 존재할 때는 부정 [^a], 대괄호 밖에서는 문자 열의 시작과 일치 $ ^와 반대로 문자열의 끝과 일치할 경우 {N} 정확히 N번 일치 {N,} N번 또는 그 이상 일치 {N, M} 적어도 N번 일치하지만 M번 일치를 넘지 않음 \\b 단어 끝의 공백 문자열 \\B 단어 끝이 아닌 곳에서의 공백 문자열 \\&lt; 단어 시작에서의 공백 문자열을 의미. \\&lt;linux : linux 문자열로 시작하는 단어를 포함한 라인(vi, grep) \\&gt; 단어 끝에서의 공백 문자열을 의미. linux\\&gt; : linux 문자열로 끝나는 단어를 포함한 라인 확장 브래킷 브래킷 의미 [:alnum:] [A-Za-z0-9] 알파벳 문자와 숫자로 이루어진 문자열 [:alpha:] [A-Za-z] 알파벳 문자 [:blank:] [\\x09] 스페이스와 탭 [:cntrl:] 컨트롤 제어 문자 [:digit:] [0-9] 숫자 [:graph:] [!-~] 공백이 아닌 문자(스페이스, 제어 문자들을 제외한 문자) [:lower:] [a-z] 소문자 [:print:] graph와 유사하지만 스페이스 문자를 포함 [:punct:] [!-/:-@[-'{-~] 문장 부호 문자 [:space:] [\\t\\v\\f] 모든 공백 문자(newline 줄바꿈, 스페이스, 탭) [:upper:] [A-Z] 대문자 [:xdigit:] 16진수에서 사용할 수 있는 숫자 Vim에서 정규 표현식을 사용한 검색 vim에서 ESC키를 누르고 /검색할 문자열 형태를 입력하고 엔터를 누르면 검색 가능 /없이$ : 없이로 끝나는 문자열 검색 /...세 : 4개 문자로 구성된 문자열 중 마지막 문자가 “세”로 끝나는 문자열 검색 /o*ve : o로 시작되는 문자부터 ve로 끝나는 모든 문자열 검색 /[Ll]ove : Love, love",
    "tags": "linux development",
    "url": "/development/2017/12/13/linux-3/"
  },{
    "title": "개발 블로그 시작",
    "text": "개발 블로그 시작 기존엔 네이버 블로그를 하고 있었는데, 네이버 블로그는 상품 혹은 경험 관련 내용을 위주로 올려서(예전에 마케팅, 광고를 했던 경험 기반) 개발 내용을 추가하기 쉽지 않았다. 게다가 구글 검색에 결과가 잘 나오지 않아 Github 블로그를 만들기 시작! 목표는 Github TIL 폴더에 있는 내용들을 다 텍스트로 정리하는 것! 사실 깃허브에 있는 내용들을 그냥 옮겨와도 될 정도로, 깃허브를 블로그처럼 사용했는데 이젠 조금 차곡차곡 정리해서 보고 싶은 욕구가 가득-! 주변 개발자분들 중 블로그를 하는 것이 습관이 안된 분들이 있는데, 지인들과 함께 블로그 글쓰기 모임을 만들어 강제로 글쓰게 만들 생각도 있는데 이건 2018년도부터 할 것 같다! 웹페이지가 모바일로 보기 편해서 복습하기도 좋다 :) 잘 부탁해 블로그야-",
    "tags": "diary",
    "url": "/diary/2017/12/08/diary/"
  },{
    "title": "리눅스 쉘 스크립트 맛보기",
    "text": "2.1 리눅스의 부팅 과정과 로그인 쉘 init init과 연결된 수많은 프로세스들이 자신만의 프로세스 ID(PID)를 가지고 생성되기 시작 init의 PID는 1 init 프로세스가 시스템을 초기화하고 터미널 라인을 오픈하기 위한 작업을 시작하고, 표준 입력과 표준 출력, 표준 에러를 설정 ps -ef 를 입력하면 PID와 같이 출력 표준 입력 (0): 키보드로부터 입력받는 것 표준 출력 (1) / 표준 에러(2) : 모니터로 출력하는 것 파일 디스크립터 0 1 2 ( 식별자 ) 사용 사례 : bash 스크립트를 작성할 경우 표준 에러를 표준 출력으로 돌리고, 에러인 경우 dev/null로 찍는 경우 등등 standard error redirection https://unix.stackexchange.com/questions/111611/what-does-the-rc-stand-for-in-etc-rc-d 그 이후 로그인 프롬포트를 보여줌 init 프로세스는 /etc/rc.d/init.d 에 런레벨별로 설정되어 있는 쉘 스크립트를 실행. 이 스크립트들은 chkconfig 명령을 사용해 부팅 시 자동으로 실행할 것인지, 실행하지 않을 것인지 설정할 수 있음 부팅 시 수행할 런레벨은 /etc/inittab에 설정되어 있음( 우분투는 /etc/init/rc-sysinit.conf!) 런레벨은 0부터 6까지 존재하는데, 주로 텍스트 모드 런레벨인 3번(주로 서버)과 그래픽 모드 런레벨인 5번을 주로 사용(주로 데스크탑) 현재는 level 2를 사용 중 rc1.d, rc2.d, rc3.d, rc4.d, rc5.d, rc6.d에 설정 파일이 들어가있음 (runlevel configuration) K로 시작하는 것은 kill, S로 시작하는 것은 Start를 의미하며 파일의 prefix를 수정하면 바로 반영 로그인 프롬포트 /bin/login 프로그램이 /etc/passwd 파일에 있는 첫 번째 필드를 체크하기 위해 유저 아이디를 검증하고, 존재하면 패스워드 검증 패스워드가 맞다면 /etc/passwd 파일의 마지막에 설정되어있는 SHELL 변수에는 로그인 쉘을, USER와 USERNAME 변수에는 로그인 이름을 할당 로그인이 끝나면 /etc/passwd 파일에서 유저 라인의 마지막 단계에 입력되어 있는 프로그램을 실행. 일반적으로 배시 쉘로 설정되어 있음 쉘 초기화 파일들 /etc/profile 시스템 전역 쉘 변수 초기화 유저가 쉘에 로그인하면 가장 먼저 이 곳의 파일을 읽음. 이 파일에는 PATH, USER, LOGNAME, MAIL, HOSTNAME, HISTSIZE, INPUTRC 등의 쉘 변수들이 선언되어 있음 전역 리드라인 초기화 파일인 /etc/inputrc 파일을 읽어들이도록 되어 있으며, 프로그램들의 전역 환경을 설정하는 파일을 포함하고 있는 /etc/profile.d 디렉터리를 읽어들이도록 구성되어 있음 /etc/bashrc 쉘 함수와 Alias를 위한 시스템 전역 변수 정의 ~/.bash_profile 유저 개인의 환경 설정 파일 시스템 전역이 아닌 유저 자신만의 PATH와 시작 프로그램을 추가적으로 설정 ~/.bashrc 유저 개인의 Alias 및 변수 설정 파일 유저 자신의 개인적 명령어 앨리아스를 정의할 수 있으며, /etc/bashrc 파일에서 시스템 전역 변수를 읽은 다음, 특별한 프로그램을 위한 변수를 설정할 수 있음 ~/.bash_logout 로그아웃 설정 파일 로그아웃 절차를 포함하고 있음. ex) 로그아웃하면 터미널 윈도우가 사라짐 source 명령 쉘 환경 설정 파일 즉시 적용하기. . 명령도 같은 기능을 수행 zsh의 경우 ~/.zshrc 에 설정 파일이 존재! 명령 라인 파싱 히스토리 치환 명령라인은 토큰 또는 단어 단위 히스토리 업데이트 인용 진행 앨리아스 치환 / 함수 정의 리다이렉션, 백그라운드, 파이프 설정 변수 치환이 수행 명령 치환이 수행 globbing 이라는 파일명 치환이 수행 명령이 실행 명령어 타입 앨리아스 키워드 함수 빌트인 내장명령 실행 파일 프로세스와 쉘 프로세스 : 유일한 PID 번호에 의해 식별될 수 있는 실행 프로그램 실행 프로그램의 데이터와 스택, 프로그램 포인터와 스택 포인터 그리고 프로그램을 실행하기 위해 필요한 모든 정보들로 구성 쉘 : 로그인 프로세스를 완료했을 때, 시작하는 특별한 프로그램으로 프로세스임 커널 : 프로세스를 제어하고 관리 시스템 콜 쉘 스크립트로부터 명령을 실행했을 때, 빌트인 내부 코드 / 디스크 저장장치에서 명령을 찾고 실행된 명령을 정렬 커널 영역의 기능을 사용자 모드가 사용 가능하게, 즉 프로세스가 하드웨어에 직접 접근해서 필요한 기능을 사용할 수 있게 해줌 fork 시스템 콜 : 콜프로세스의 복사본을 생성 자식 프로세스는 fork가 호출된 다음 실행하고 두 프로세스는 CPU 공유. 자식 프로세스는 부모 프로세스의 환경, 오픈된 파일, 실제적 유저 ID, umask, 현재 작업 디렉터리, 시그널의 복사본을 가짐 명령을 실행했을 때 아래와 같은 Flow를 가짐 쉘은 명령 라인을 파싱하고 첫 단어가 빌트인 명령인지, 디스크에 존재하는 실행 명령인지 판단 빌트인 명령 -&gt; 곧바로 처리 디스크에 존재 -&gt; 부모 쉘의 복사본을 만들기 위해 fork 시스템 콜 호출 자식 프로세스는 명령을 찾기 위해 PATH에 정의되어 있는 경로 검색하고 리다이렉션, 파이프, 명령 치환, 백그라운드 프로세싱을 위한 파일 디스크립터를 설정 자식 프로세스가 동작하는 동안 부모 프로세스는 잠시 멈춤 wait 시스템 콜 : 자식 프로 세스 하나가 종료될 때까지 부모 프로세스를 대기상태로 유지 wait가 성공하면 자식 프로세스가 종료되고, 종료상태를 가지고 있는 자식 프로세스의 PID를 리턴 자식 프로세스가 종료되기 전 부모 프로세스가 종료된다면, init 프로세스는 자식 프로세스를 고아가 된 좀비 프로세스로 만듬 부모를 대기상태로 만드는 것 + 프로세스가 정상적으로 종료하도록 보증하는 역할 exec 시스템 콜 : 터미널에 명령을 수행한 다음, 쉘은 새로운 쉘 프로세스를 fork. 자식 쉘은 타이핑된 명령을 수행 쉘은 프로그램을 위해 PATH를 검색 -&gt; 프로그램이 발견되면 명령의 이름과 함께 exec 시스템 콜을 호출 커널은 이 프로그램을 메모리에 로드하며 자식 쉘은 새 프로그램으로 오버랩 새 프로그램은 자식 프로세스가 되고 실행을 시작(모든 환경 변수, 오픈 파일, 유저 정보를 물려받음) exit 시스템 콜 : 중지 자식 프로세스를 종료할 때 자식 프로세스는 sigchild 시그널을 보내고 자식의 종료상태를 부모 프로세스가 받아들이도록 대기. 종료상태는 0~255의 숫자이며 0은 성공적으로 실행되었단 뜻이며 0이 아닌 종료상태는 프로그램 실행리 실패했다는 뜻 프로세스 종료하기 CTRL-C 또는 CTRL-\\ 키를 이용해 종료할 수 있으며, kill 명령을 사용해 종료할 수 있음. 변수 변수 타입 쉘 변수들은 대문자로 정의하며 2가지의 변수 타입을 가지고 있음 전역 변수 전역 변수 또는 환경 변수들은 모든 쉘에서 사용할 수 있으며, env 명령을 사용하면 환경 변수들을 출력해볼 수 있음 echo $PATH : /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin 이런식으로 나오는데 쉘은 명령어를 찾을 때마다 이 디렉토리를 순서대로 검사함 추가하고 싶으면 vi ~/.bash_profile 에서 export PATH=$PATH:/path/to/add 이런식으로 추가 지역 변수 지역 변수는 현재의 쉘에서만 사용할 수 있음. 옵션 없이 set 빌트인 명령을 사용하면 환경 변수를 포함해 모든 변수들과 함수들의 목록을 보여줌 다음 변수는 배시 쉘 프로그래밍 부분에서 나올 예정 (문자열 변수, 정수형 변수, 상수형 변수, 배열 변수) 변수 생성 변수들은 기본적으로 대문자로 생성. 숫자를 포함할 순 있지만 숫자로 시작할 수 없다 = 앞뒤로 공백이 없어야 한다. VAR=\"value\" 이런식! 변수를 출력할 경우엔 echo 를 사용. echo $VAR, echo \"$VAR\", echo \"${VAR}\" 모두 동일한 기능 수행 ★★★ 큰따옴표(““)를 쓰면 $str의 실제 값을 출력할 수 있고(변수의 값) 작은따옴표(‘‘)를 쓰면 그냥 $str이란 문자열 자체를 출력함! 밖에 큰따옴표를 쓰고 작은따옴표는 안에 쓰는 경우가 많음 지역 변수를 환경 변수로 만들기 현재 쉘에 만든 변수들은 현재 쉘에서만 사용할 수 있음. 이 경우 export 빌트인 명령을 사용하면 지역 변수를 환경 변수로 만들 수 있음 해제는 unset 자식 쉘에서 export 명령을 사용한다고 해서 부모 쉘에 영향을 주지는 못함 특수 파라미터 변수들 (Vㅔ리 중요) $* : 1부터 시작하는 위치 파라미터의 확장, 큰 따옴표로 구분하고 IFS 특수 변수의 첫 문자로 구분되는 각 파라미터의 값으로 하나의 단어를 확장하며, 전체 파라미터값을 가지고 있다. 만약 IFS가 NULL이거나 해제되어 잇으면 파라미터는 스페이스로 구분 $@ : 1부터 시작하는 위치 파라미터의 확장이며, 큰 따옴표로 확장되면 각 파라미터는 하나의 구분 단어로 확장되고, 전체 파라미터값을 가짐 $# : 십진수의 위치 파라미터 전체 개수를 의미 $? : 최근 실행된 포그라운드 파이프라인의 종료상태를 가지고 있음(0은 정상적으로 종료) $- : 실행하자마자 set 내장명령을 통해 또는 쉘 자체에 의해 설정된 현재 옵션 플래그로 확장 $$ : 현재 쉘의 프로세스 ID $! : 가장 최근에 백그라운드로 실행된 프로세스의 ID를 가지고 있음 $0 :쉘 또는 쉘 스크립트의 이름을 가지고 있음 $_ : 아규먼트 목록을 사용해 실행된 쉘스크립트의 절대 경로를 가지고 있음 본 쉘 본 쉘 shbang 라인 : 쉘 스크립트의 첫 라인에 #!/bin/sh를 입력해서 커널에게 이 파일은 본 쉘 스크립트라고 인지해야 함 주석은 #을 사용 와일드 카드 : *,?,[] 는 파일명 확장을 위해 사용되고, &lt;,&gt;,2&gt;,&gt;&gt;,| 문자들은 표준 IO를 위해 사용된다. 문자들이 인터프리터에 의해 해석되지 않도록 하기 위해선 인용부호(', '')로 감싸주면 됨 read : 사용자의 입력을 읽고 read 명령의 오른쪽에 적는 변수에 사용자 입력값을 할당 read word echo \"입력한 단어는: $word\" 조건문 if~then 문장을 사용하며 if문의 끝에는 반드시 fi를 적어 if문이 끝났음을 표시 if [표현식] then 명령 문장 블록 elif [표현식] then 명령 문장 블록 else 명령 문장 블록 fi case문 case \"$color\" in blue) echo $color is blue ;; green) echo $color is green ;; red|orange) echo $color is red or orange ;; *) echo \"Not a color\" # default esac 루프문 : while, until, for 3가지 종류가 존재 while : 뒤에 오는 명령이나 표현식이 treu면 계속해서 do와 done 사이 문장 실행 until : while와 비슷하지만 until 뒤에 오는 명령이 false가 될 때까지 do와 done 사이 문장 실행. 거의 사용하지 않음 for : 단어목록을 통해서 반복, for 다음으로 변수 이름이 오고 in 다음 단어 목록이 온다. 그리고 do와 done 키워드 사이의 문장을 수행 파일 테스팅 : test은 상태 표현식을 판단하기 위해 사용. 파일, 디렉터리, plain 텍스트, 읽을 수 있는 파일 등의 속성을 테스트하기 위해 옵션을 가지고 있음 -d : 디렉터리인지 테스트 -f : 파일인지 테스트 -r : 현재 사용자가 읽을 수 있는지 테스트 -s : 파일 크기가 0보다 큰지 테스트 -w : 현재 사용자가 파일에 쓰기 가능한지 테스트 -x : 현재 사용자가 파일을 실행할 수 있는지 테스트 #!/bin/sh if [ -f file ] then echo \"file exists\" else echo \"file not found\" fi if [ -d file ] then echo \"file is a directory\" else echo \"file is not a directory\" fi if [ -s file ] then echo \"file is not of zero length\" else echo \"file size is zero length\" fi if [ -r file -a -w file ] then echo \"file is readable and writable\" else echo \"file is not read/write\" fi 함수 #!/bin/sh lister(){ echo \"현재 디렉터리는 'pwd'입니다\" echo \"현재 디렉터리 파일은 아래와 같습니다\" ls } lister 배쉬 쉘 문법과 구조 대부분의 리눅스에서 기본 쉘로 bash(Bourne Again SHell) 쉘을 사용하고 있음 로그인 과정 init 프로세스 생성 getty 프로세스 생성 로그인 프롬포트를 보여줌 /bin/login 프로그램이 실행되고 로그인 아이디를 입력하면 로그인 패스워드 입력을 기다림 입력받은 패스워드가 정확하면 환경 설정이 초기화되고 로그인 쉘을 시작 /etc/profile 시스템 파일을 찾아 명령 라인에서 실행 유저의 홈디렉터리에 있는 유저의 초기 파일인 .bash_profile 파일 실행, .bashrc 라는 환경 파일을 실행 달러($) 기호를 모니터에 보여주며 유저의 명령을 기다림 배시 쉘 옵션 설정을 위한 set, shopt 명령 set 명령어는 특수한 빌트인 내장 옵션들을 켜고, 끄는 역할을 함 set -o : 쉘의 설정값 출력 set -o option : 옵션을 사용, set +o option : 옵션을 사용하지 않음 set -[a-z] : 옵션을 사용하는 단축형 set +[a-z] : 옵션을 사용하지 않는 단축형 allexport braceexpand emacs errexit errtrace functrace hashall histexpand history ignoreeof interactive-comments keyword monitor noclobber noexec noglob nolog notify : 백그라운드 job이 종료되었을 때 유저에게 알려줌 nounset onecmd physical pipefail posix privileged verbose vi xtrace shopt 명령은 배시 쉘에서 사용되는 set 명령으로 새로운 버전의 쉘 옵션 내장명령어. -p 옵션을 사용해 모든 옵션을 출력해볼 수 있으며, -u 옵션을 사용해 옵션 셜정을 off할 수 있고, -s 옵션을 사용해 on할 수 있음 배열 배시 쉘은 1차원 배열 생성을 지원. 배열은 하나의 변수 이름에 숫자 목록, 이름 목록, 파일 목록 등의 단어 목록 집합을 할당할 수 있음. 빌트인 함수 declear -로 x[0] 형태로 생성 배열의 요소를 가져올 때에는 ${배열명[index]} 형식을 사용하며 -ar 옵션을 사용하면 읽기전용 배열이 생성 배열 요소의 개수를 파악할 경우엔 ${#names[*]}을 사용 배열의 인덱스를 사용해 특정 인덱스에만 값을 넣을 수 있음 함수 쉘의 컨텍스트 안에 명령들의 그룹 이름을 사용한다(자식 프로세스가 생성되지 않음) 함수가 한번 정의되면 쉘 메모리에 적재되기 때문에 함수가 호출될 때 디스크로부터 읽어들일 필요가 없다. 함수는 스크립트의 모듈을 향상시키기 위해 사용 실행될 때 프롬프트에서 정의될 수 있지만 대부분 유저의 초기화 파일(.bash_profile)에 정의한다. 그리고 함수는 호출되기 이전에 반드시 정의되야 함 함수 정의 본 쉘 방식 : 함수명을 적고 빈 괄호를 적은 다음 함수 정의 function 키워드를 사용. 컬리 브레이스({) 앞뒤에는 반드시 공백이 있어야 함 정의 부분은 세미콜론(;)으로 분리된 명령어들로 구성되며, 마지막 명령어는 세미 콜론(;)으로 끝나야 함 declare -F 명령을 실행하면 정의되어 있는 함수명을 출력할 수 있으며, unset -f 함수명을 사용해 현재 쉘에서 정의한 함수를 제거할 수 있다 exec 명령 새로운 프로세스를 시작하지 않고 현재 프로그램을 대신하여 사용될 수 있다. 표준 출력 또는 표준 입력은 서브쉘을 생성하지 않고 exec 명령으로 변경될 수 있다. 만약 파일이 exec 명령으로 오픈되면 파일의 끝까지 한 라인씩 읽게 된다 파이프 파이프 심볼(|)의 왼쪽 명령의 결과를 가져와서 오른쪽 명령의 입력으로 사용하도록 함 who | wc -l 파이프는 3단계의 처리를 수행 who 명령의 결과값이 커널 버퍼로 보내지고, wc -l 명령은 버퍼로부터 데이터를 읽은 다음, 결과를 모니터 스크린으로 보냄",
    "tags": "linux development",
    "url": "/development/2017/12/06/linux-2/"
  },{
    "title": "리눅스 쉘과 명령어 기초",
    "text": "리눅스와 쉘 쉘(Shell) : 운영체제에서 제공하는 명령을 실행하는 프로그램이며 운영체제의 관리하에 있는 파일, 프린팅, 하드웨어 장치, 애플리케이션과의 인터페이스를 제공한다 즉, 운영 체제에서 제공하는 각종 명령들을 쉘 인터페이스에서 실행하면 운영체제가 그 명령에 해당하는 일을 수행 쉘 스크립트 : 인터프리터로서 리눅스 시스템에서 지원하는 명령어들의 집합을 묶어서 프로그램화한 것 du -h * | sort -nr &gt; $HOME/script.txt du : 용량을 알아보기 위한 명령어 | : 결과값을 다음 명령으로 연결해 다음 명령의 아규먼트로 사용 sort : 알파벳 또는 숫자 기준으로 정렬. -n : 숫자값을 기준으로 정렬, -r : 반대의 순서(내림차순) &gt; : 리다이렉션은 앞의 명령 결과를 다음에 나오는 파일명으로 저장 리눅스 bash 쉘에서 $ 표시는 일반유저의 쉘을 의미하고 # 표시는 root 유저의 쉘을 의미함 쉘 스크립트를 작성할 때 스크립트의 최상단에 본 쉘인 경우 #!/bin/sh, 배시 쉘인 경우 #!/bin/bash, 파이썬일 경우 #!/bin/python 와 같이 실행할 언어를 지정해야 한다 which 명령어는 환경 변수에 지정된 PATH 변수의 경로 내에서 실행 파일의 이름을 검색하고자 할 때 사용 whereis는 명령의 실행 파일, 소스, 맨페이지의 위치를 검색하는 명령 리눅스 환경 리눅스는 핀란드 헬싱키 대학의 학생이었던 리누스 토발즈의 취미 생활로 탄생 리눅스의 특징 Free OS 구입에 따른 비용을 지출하지 않아도 됨 Unix Like 유닉스처럼 리눅스는 C 언어로 작성됨 유닉스처럼 리눅스는 멀티유저/멀티태스킹을 지원하는 네트워크 OS 유닉스처럼 리눅스는 프로그래밍 개발환경에 매우 적합 유닉스처럼 다양한 하드웨어 플랫폼을 지원 Intel x86 processor Macintosh PC Cyrix processor AMD processor Sun Microsystems Sparc processor Alpha Processor(Compaq) Open Source 그 외에 안정성, 강건성, 보안성, 높은 성능 등의 특징도 존재 리눅스 파일 시스템 리눅스에서 최상위(root) 디렉터리는 /로 표시하고 그 하위에 /root, /usr, /etc, /boot, /tmp 등으로 구분하며, 각 디렉터리들은 파티션으로 구성될 수 있음. 하나의 파티션이 하나의 디렉터리가 될 수 있음 파티션을 디렉터리에 매칭시키기 위해서 mount라는 개념을 사용 /dev 디렉터리는 여러 가지 디바이스 관련 파일이 있는 디렉터리 /dev/null 파일은 어떤 문자를 이 장치로 보내면 문자를 모두 지워버림 /dev/zero 파일은 새로운 파일을 생성할 때 0으로 채우기 위해 사용 /dev/tty : 프로그램이 실행되고 있는 터미널 윈도우 또는 콘솔 /dev/dsp : 사운드 카드에 AU 사운드 파일을 실행하는 인터페이스 /dev/fd0 : 첫 플로피 드라이버 /dev/hda1 : IDE 하드 디스크의 첫 파티션. hda2 처럼 하나의 하드 디스크에 여러개릐 파티션을 구성할 수 있음 /dev/sda1 : SCSI, S_ATA 하드 디스크의 첫 파티션 /dev/hda1 파티션을 /backup 디렉터리에 마운트 하려면 아래와 같이 입력 sudo mount -t ext3 /dev/hda1 /backup 쉘 스크립트 준비운동 쉘 : 키보드로부터 명령을 입력받아 OS가 그 명령을 수행하도록 하는 프로그램 CLI, GUI를 가지고 있음 리눅스에서 사용할 수 있는 쉘의 종류를 확인하려면 /etc/shells 파일을 출력해보면 된다 : cat /etc/shells Bash : 리눅스에 기본 탑재된 일반적인 쉘로 sh 본 쉘과 호환되기 때문에 대부분 sh와 bash에서 모두 작동 echo : 뒤이어 입력되는 문자열을 모니터로 출력 (print) 리눅스의 모든 환경 변수는 대문자로 구성되어 있음 env : 환경 변수 출력 쉘 스크립트 구성요소 if ~ else와 fore, while 등의 쉘 루프 명령어 grep, awk, cut 등의 텍스트 처리 유틸 w, who, free 등의 바이너리 명령어 쉘 스크립트를 사용하는 이유 쉘 스크립트는 유저 또는 파일로부터 입력을 받아 모니터에 데이터를 출력 동일한 작업을 반복하여 수행하고자 할 때 자동화할 수 있다 시간을 절약할 수 있음 자신만의 파워툴, 파워 유틸리티를 만들 수 있음 관리자 작업을 커스터마이징 할 수 있음 서비스 환경 설정과 유저 추가와 같은 작업에서 에러를 줄일 수 있음 실제적인 쉘 스크립트 사용 예는 다음과 같음 리눅스 시스템 모니터링 데이터 백업과 스냅샷 생성 Oracle, MySQL 데이터베이스 백업을 위한 덤프 작업 시스템 경고메세지를 이메일로 받기 시스템 리소스를 잡아먹는 프로세스 찾아내기 여유 메모리와 사용량 찾아내기 로그인한 모든 유저와 현재 무엇을 하고 있는지 찾아내기 네트워크 서비스가 정상적인지 아닌지 알아보기 모든 실패한 로그인 정보 찾아내기 보안 정책에 따른 유저 관리 BIND의 서버 환경에서 zone 엔트리 추가 쉘 스크립트를 사용하면 안될때 (프로그래밍 언어를 사용해야 할 경우) 리소스에 민감한 작업들, 특히 속도가 중요한 요소일 때(정렬, 해쉬 등) 강력한 산술 연산 작업들, 특히 임의의 정밀도 연산이나 복소수를 써야할 때(C++) 플랫폼 간 이식성이 필요할 때(C) 구조적 프로그래밍이 필요한 복잡한 복잡한 애플리케이션 업무에 아주 중요하거나 회사의 미래가 걸렸단 확신이 드는 애플리케이션 보안상 중요해서 시스템 무결성을 보장하기 위해 외부의 침입이나 크래킹, 파괴 등을 막아야 할 필요가 있을 때 서로 의존적인 관계에 있는 여러 컴포넌트로 이루어진 프로젝트 과도한 파일 연산이 필요할 때(Bash 쉘은 제한적인 직렬적 파일 접근을 하고, 특히 불편하고 불충분한 줄 단위 접근만 가능) 다차원 배열이 필요할 때 링크드 리스트나 트리 같은 데이터 구조가 필요할 때 그래픽이나 GUI를 만들고 변경하는 등의 작업이 필요할 때 시스템 하드웨어에 직접 접근해야할 때 포트나 소켓 IO가 필요할 때 예전에 쓰던 코드를 사용하는 라이브러리나 인터페이스를 써야할 필요가 있을 때 독점적이고 소스 공개를 안하는 애플리케이션을 만들어야 할 때 man : 도움말 매뉴얼을 출력하는 명령어 한글로 보고싶다면 LANG=ko_KR.UTF8을 실행 리눅스 기본 명령어들의 형식 리눅스에서 환경 변수를 제외한 거의 모든 명령어들은 소문자로 구성되어 있으며, 윈도우와 달리 대소문자를 구분 date 입력하면 날짜가 나오는데, date '+%H:%M'을 입력하면 해당 포맷으로 변경. -u 옵션을 주면 UTC 시간이 출력 다중 명령어 사용 하나의 라인에서 여러 개의 명령을 실행하려면 하나의 명령 다음에 ;를 추가해 명령이 끝났음을 지정해주면 됨. 단, 첫 명령이 실패해도 두번째 명령은 반드시 실행됨 &amp;&amp;를 명령어 사이에 붙이면 첫 명령이 정상적으로 종료했을 경우에만 두 번째 명령을 수행함 ||를 붙이면 각각의 모든 명령을 수행 명령 히스토리 history에 최근 실행한 명령들이 저장되어 있음 최근 실행한 명령을 다시 실행하기 위해 !을 사용. 다음에 매칭되는 문자를 최근 명령에서 찾아서 실행 -1같이 마이너스 숫자를 지정할 경우 최근 명령부터 숫자만큼의 명령을 실행 !!을 사용하면 가장 최근 명령어를 다시 실행 리눅스 디렉터리 여행 pwd : 현재 디렉터리 위치 보기 ls : 파일과 디렉터리 목록 보기 ls -l로 출력된 컬럼의 의미는 아래와 같음 [파일/디렉터리 퍼미션][하드 링크파일 수][소유자][그룹][파일 크기][수정 시간][파일명] cd : 현재 쉘의 디렉터리 변경, 이동 less, more, cat : 텍스트 파일 보기 less는 q를 누르면 원래 셀 위치의 다음 라인으로 돌아오고, more는 q를 누르면 원래 쉘 위치로 돌아오지 않음, cat은 쉘 프롬포트를 리턴 file : 파일 타입 보기 tree : 디렉터리의 트리 구조 print -d : 디렉터리만 / -a : 모든 것 / -f : 파일만 / -L : 디렉터리 깊이 지정 tree -L1 / : 최상위 root(/) 부터 깊이가 1인 디렉터리와 파일 출력 파일/디렉터리 퍼미션 파일에 대한 접근 권한을 설정하는 것으로 - 문자는 일반 파일, d 문자는 디렉터리, c 문자는 캐릭터 디바이스, b는 블록 디바이스, l 은 심볼릭 링크 파일, s는 소켓 파일을 의미. 그 다음 3개의 문자 단위는 파일 소유자의 read, write, execution, 그룹의 read, write, execution, 모든 사용자의 read,write,execution을 표시 하드 링크 파일 수 하드 링크 : 리눅스상에서 동일한 파일시스템 내의 파티션에서 동일한 inode 정보를 가지는 파일. 하드 링크가 복사는 아님!!! ln 원본파일 하드링크파일명 명령을 사용해 하나의 링크 파일을 생성하면 원본 파일과 하드 링크 파일은 동일한 inode 정보를 가지게 되고, 어느 하나의 파일이 변경되면 두 파일 모두 동일한 내용과 크기로 변경 심볼릭 링크 : ln -s 원본파일 심볼릭링크파일명을 사용하며 원본 파일의 이름만 링크. 원본 파일을 삭제하면 심볼릭 링크 파일은 가리킬 파일명이 삭제되었기 때문에 의미없는 파일이 됨 리눅스 기본 디렉터리 구성 / : 루트 디렉터리 /boot : 리눅스 커널과 부트로더가 위치하는 디렉터리 /etc : 시스템 환경 설정 파일이 위치한 디렉터리 /etc/passwd : 유저의 각종 정보 저장 /etc/shadow : 유저 패스워드를 암호화하여 저장 /etc/fstab : 시스템이 부팅될 때 참고하는 마운트할 디바이스 테이블을 저장 /etc/hosts : 네트워크 호스트 이름과 IP 주소 /etc/rc.d/init.d 또는 /etc/init.d : 부팅시 시작할 여러가지 시스템 서비스 스크립트 /etc/resolv.conf : 시스템에서 외부로 접속할 때 참고할 네임서버 /etc/sysconfig/i18n : 부팅시 언어셋 변수와 폰트 변수를 설정 /etc/sysconfig/iptables : 리눅스 방화벽 iptables 환경 설정 /etc/sysconfig/network : 부팅시 네트워크를 지우너할 것인지와 호스트명을 설정 /etc/sysconfig/network-scripts/ifcfg-eth0 : 부팅시 사용할 첫 이더넷 카드의 정보 저장 /bin, /usr/bin : 실행 프로그램들. /bin엔 기본적인 실행 프로그램, /usr/bin엔 사용자들이 사용할 실행 프로그램 /sbin, /usr/sbin : 시스템 관리자를 위한 프로그램. 대부분 root를 위한 프로그램 /usr : 사용자 애플리케이션을 지원하기 위한 다양한 파일이 위치 /usr/local : 소프트웨어 설치시 또는 로컬머신에서 사용할 파일들이 위치. 소스파일을 기본 옵션으로 컴파일하면 /usr/local 디렉터리에 기본 설치가 되며 이 때 실행 파일은 /usr/local/bin에 위치 /var : 운영 중인 시스템의 변화를 체크할 수 있는 로그 파일들이 위치 /var/log : 로그 파일이 위치하는 디렉터리 /var/spool : 메일 메세지와 프린트 작업과 같이 프로세스를 위한 큐를 잡아놓기 위해 사용되는 디렉터리 /lib : 공유 라이브러리 파일 /home : 유저별 홈디렉터리가 존재하는 개인 홈디렉터리. useradd(adduser) 명령을 사용해 유저를 생성하면 \"/home/유저아이디 형식으로 유저 홈디렉터리가 생성. 이때 기본적으로 생성되는 파일들은 /etc/skel 디렉터리 아래의 파일들임 /tmp : 임시 파일들이 저장되는 디렉터리 /dev : 리눅스 시스템에서 사용하는 디바이스 장치 파일들 /proc : 이 디렉터리는 파일을 포함하지 않음. 현재 시스템 CPU 정보를 출력하기 위해 /proc/cpuinfo 파일을 출력 현재 프로세스의 정보가 저장되어 있음 ps -ef를 입력하고 나오는 pid 이름으로 되어있는 폴더를 들어가보면 다양한 정보들이 나와있음 해당 폴더에서 cat status를 입력하면 정보들이 나오는데 주로 봐야하는 것은 VmRSS, VmSwap 등이 있음. 스왑이 되면 어떻게 되는지 문서를 볼 것 /media, /mnt : 마운트 포인트를 위해 사용되는 디렉터리 마운트 : 디바이스 장치를 사용하기 위해 프로세스와 연결하는 것 입출력 리다이렉션과 파이프 표준 출력 표준 출력 : 결과를 모니터에 출력. 파일 디스크립터 숫자값으로 1로 표기 &gt; 문자를 사용해 표준 출력을 파일로 리다이렉션해 저장할 수 있음 ls &gt; ls.txt &gt;&gt; : 파일에 추가(append)할 경우 사용 cat test1.txt &gt;&gt; ls.txt 만약 텍스트 파일의 내용을 모두 삭제하고 싶다면 /dev/null로 읽어서 파일로 리다이렉션 cat /dev/null &gt; lsls.txt 표준 입력 - 키보드 : 파일 디스크립터 숫자로 0 표준 출력 - 모니터 : 파일 디스크립터 숫자로 1 표준 에러 - 모니터 : 파일 디스크립터 숫자로 2 2&gt;&amp;1 : 표준 출력이 전달되는 곳으로 표준 에러를 전달하는데, 마지막에 &amp;를 사용해 현재 명령을 백그라운드로 실행 표준 입력 표준 입력 : 키보드로부터 데이터를 입력받는 것. 파일 디스크립터 숫자값으로 0 키보드를 대신해 파일로부터 입력을 받는 경우 &lt; 문자를 사용 sort &lt; ls.txt sort &lt; ls.txt &gt; sorted_lx.txt : ls.txt 파일의 정렬 결과를 sorted_ls.txt 파일로 저장 파이프 | 문자를 사용해 두 명령어를 이어줌. 앞에서 실행한 명령의 결과값을 뒤에 적은 명령어의 입력으로 사용 즉, 파이프로 연결된 하나의 표준 출력을 다른 명령의 표준 입력으로 사용 ls -lt | head find . -type f -print | wc -l : 현재 디렉터리에서 있느 파일을 출력해 출력 라운의 수를 카운트한 다음 화면에 출력되도록 한 파이프 예제 필터 파이프에는 여러 필터를 사용할 수 있는데, 필터는 표준 입력을 받아서 이 필터로 연산을 한 다음, 그 결과를 표준 출력으로 보냄 sort, uniq, grep, fmt, pr, head, tail, tr, sed, awk 등 퍼미션 리눅스와 같은 멀티태스킹, 멀티유저를 지원하는 UNIX 시스템에서는 파일 및 디렉터리에 대해 퍼미션이라는 접근 권한을 사용 chmod : 파일, 디렉터리에 대한 접근 권한 변경 read, write, execute 권한 지칭 rwx 3자리 문자는 2진수로 표시해 2^n으로 인식 rwx rwx rwx = 111 111 111 = 777 rw- rw- rw- = 110 110 110 = 666 rwx --- --- = 111 000 000 = 700 리눅스에서 touch 또는 vi을 사용해 파일을 생성하면 기본 퍼미션이 644(rw-r–r–)로 설정됨. 기본 퍼미션은 umsk로 설정되어 있는데, 이 설정은 /etc/bashrc에 설정되어 있음 su : 일시적으로 수퍼유저 또는 다른 유저로 전환 chown : 파일, 디렉터리의 소유자를 변경(change file owner and group) chown 명령은 슈퍼 유저만 사용 가능 chown multi.multi perm.txt chown root:root perm.txt chgrp : 파일, 디렉터리의 그룹 소유자 변경 chgrp multi perm.txt lsattr, chattr : 파일들에 대한 특정한 속성을 부여할 수 있음 lsattr : 파일들의 속성을 출력 chattr : 파일들의 속성을 변경. chattr +i [파일명] 제거의 경우엔 -를 사용 i : i속성은 슈퍼유저라도 변경, 삭제 등의 어떤 조작도 불가능 (immutable) a : a속성은 내용 추가는 가능하지만 슈퍼 유저라도 파일 삭제는 불가능 (append only) 잡 컨트롤 리눅스 멀티 태스킹 : 여러 개의 프로세스를 동시에 사용하는 것 리눅스 커널은 동시에 실행되는 각 프로세서들을 관리할 수 있도록 구성되어 있으며, 프로세스 통제 및 관리할 수 있음 ps : 시스템에서 실행되고 있는 프로세스 목록 보기 kill : 프로세스에게 kill 시그널 보내기 시그널 : 비동기식 이벤트 처리 메커니즘을 제공하는 소프트웨어 인터럽트 SIGHUP : hangup, 로그아웃 또는 접속을 종료할 때 발생하는 시그널로 특정 프로세스가 이용하는 설정 파일을 변경시키고 변화된 내용을 곧바로 적용하고자 할 때 사용 SIGINT : interrupt, 현재 작동 중인 프로세스의 동작을 멈출 때 사용. &lt;Ctrl+C&gt; SIGQUIT :quit, SIGINT와 같이 사용자가 터미널에서 종료키를 누를 때 커널에 의해 보내짐. 비정상적으로 종료하게 되므로 코어 파일을 생성하고 종료 SIGKILL : kill, 해당 프로세스의 실행을 강제로 중지. 가장 많이 사용되는 시그널 SIGSEGV : segmentation violation, 메모리 접근이 잘못되었을 때, 즉 프로세스가 포인터를 잘못 사용하여 정해진 영역 이외의 메모리 영역을 침범했을 때 발생 SIGTERM : terminate, 정상적인 종료 프로세스에 정의되어 있는 정상적인 종료 방법에 의해 프로세스를 종료 파이썬에서도 signal 신호를 받을 수 있음. 특정 신호가 발생하면 ~~을 수행해! 라고 설정 가능. 링크 참고 kill -l : kill 명령으로 OS에게 보낼 수 있는 시그널 kill %[번호] : jobs에 나타난 프로세스 제거 kill [PID 번호] : ps명령으로 출력된 프로세스 제거 jobs : 현재 쉘에서 자신의 프로세스 목록 보기 bg : 프로세스를 백그라운드로 보내기 fg : 프로세스를 포그라운드로 가져오기 시스템 관리자 명령어 정리 유저와 그룹 user : 현재 로그인하고 있는 유저들을 출력. who -q와 동일 groups : 현재 쉘에 접속해 있는 유저가 속해 있는 그룹을 출력. GROUPS 환경 변수를 출력, 그룹 이름이 아닌 그룹 번호만 가지고 있음 chown, chgrp : 소유자, 소유자 그룹 변경 useradd, userdel : 유저를 추가, 삭제할 때 사용하는 명령 adduser : useraddd에 심볼릭 링크되어 있음 유저를 추가한 다음, passwd [유저 아이디]를 통해 패스워드 지정 유저를 삭제할 때, 유저 디렉터리까지 삭제하고 싶으면 -r 명령을 사용 usermod, groupmod : 그룹 이름이나 아이디 변경시 사용 id : 유저의 아이디, 그룹아이디, 소속된 그룹명 등을 출력 lid : 유저가 소속되어 있는 그룹 출력 who : 현재 로그인되어 있는 유저 목록 출력 w` : 모든 유저에 대한 정보 출력. who 명령의 확장 logname : 현재 유저의 로그인명 su : 한 유저가 다른 유저로 전환. 유저가 주어지지 않으면 기본적으로 슈퍼 유저(root)로 설정 sudo : 슈퍼 유저(root) 또는 다른 유저로 명령을 실행 /etc/sudoers 파일에 정의되어 있음 passwd : 유저의 패스워드를 생성/변경 ad : /var/log/wtmp 파일로부터 유저의 로그인 시간을 시간 단위로 출력 last : /var/log/wtmp로부터 모든 유저의 마지막 로그인 시간을 출력. reboot 아규먼트를 사용하면 reboot한 날짜, 시간을 출력 newgrp : 자신이 소속된 그룹을 새 그룹으로 변경/추가하지만 현재 쉘을 빠져나오면 초기화됨 터미널 tty : 현재 유저의 터미널 출력 -&gt; /dev/ttys000 라고 나옴 stty : 터미널 설정을 출력하거나 변경할 수 있음 setterm : 터미널의 설정갑을 변경 tset : 터미널 설정을 초기화 하고 터미널 타입을 출력 mesg : 다른 유저가 자신의 터미널에 접근하는 것을 제어, mesg y : 자신의 터미널에 쓰기 허용, 기본 설정. mesg n : 자신의 터미널에 쓰기 불허 wall : 접속해 있는 모든 유저에게 메시지를 전송할 때 사용 write : 유저를 지정해 메시지를 발송할 경우 사용 정보와 통계 uname : 커널 정보와 같은 시스템 정보를 출력해 볼 수 있음 arch : 시스템 아키텍처를 출력. uname -m 명령과 동일 lastlog : /var/log/lastlog 파일을 참고해 모든 유저의 마지막 로그인 시간을 출력 lsof : 오픈된 파일의 목록을 출력. -i 옵션을 사용하면 오픈되어 있는 네트워크 소켓 파일들을 출력할 수 있음 strace : 주어진 명령을 실행할 때 호출하는 시스템 콜과 시그널을 추적하는 명령 ltrace : 주어진 명령을 실행할 때 호출하는 라이브러리 콜을 추적하는 명령 nc : TCP와 UDP 포트 커넥션과 리슨을 출력. 포트 접속을 위해서는 호스트명과 포트번호를 아규먼트로 사용하면 되고, -z 옵션과 포트번호 범위를 지정하면 포트로 접속이 가능한 상태인지 체크할 수 있으며, 검색할 포트 범위는 1-80 형식을 사용함 free : 메모리와 캐시 사용량을 Byte 단위로 출력 해당 명령어를 사용하면 used free shared buffers cached이 나타남. 이와 관련된 내용은 링크를 참고 procinfo : /proc 파일 시스템에 대한 정보를 출력 lsdev : 설치된 디바이스 장치들의 목록을 출력 du : 디스크 파일 사용량을 출력. 재귀적으로 출력하며 특정 디렉터리를 지정 가능. -h : 킬로바이트 단위로 출력 / -sh : 현재 디렉터리 아래의 전체 용량을 출력 df : 파일시스템의 파티션 사용량을 출력. -h : M, G 단위로 출력 dmesg : 부팅 시 콘솔에 출력된 메세지들을 출력 stat : 주어진 파일의 각종 정보를 출력 vmstat : 버추얼 메모리 통계 출력 netstat : 현재 네트워크 통계와 정보를 출력. 현재 오픈되어 있는 포트 목록을 출력하고자 한다면 -lptu 옵션을 사용 uptime : 현재 시간과 시스템이 종료/재부팅되지 않고 계속 운영되고 있는 기간, 현재 접속자 수, 평균 부하를 출력. load average가 3 이상이면 시스템 성능이 현저히 떨어짐 hostname : 시스템의 호스트명을 출력 hostid : 호스트 머신을 32비트 16진수 숫자의 식별자로 출력 readelf : elf 바이너리 파일의 정보 출력 size : 바이너리 실행 파일 또는 아카이브 파일의 세그먼트 크기를 출력 시스템 로그 logger : 시스템 로그를 기록하는 명령이며, 로그는 /var/log/messages 시스템 로그 기록 파일에 저장 logrotate : 시스템 로그 파일을 관리하기 위해 사용하며, 로테이트, 압축, 삭제, 이메일 발송 등의 기능을 사용. 일반적으로 cron을 사용해 주기적으로 logratete를 실행해 로그 파일을 관리. 환경 설정 파일로는 /etc/logrotate.conf 파일을 사용 잡 컨트롤 ps : 현재 실행중인 프로세스 통계(PID, 프로세스 실행 시간, 실행 유저 등)를 출력. 트리 형식으로 출력하기 위해선 --forest 옵션을 사용 ps -ef 를 자주 사용 pgrep, pkil : 이름 또는 다른 속성을 사용해 시그널 프로세스를 검색 pstree : 프로세스 목록을 트리 형식으로 출력, -p 옵션을 사용하면 프로세스 아이디도 출력 top : 전반적인 시스템 상황을 출력하며 기본값으로 3초마다 refresh nice : 스케줄링 우선권을 조정해 프로그램을 실행하는 명령 아무런 옵션도 주어지지 않을 경우 nice는 상속받은 현재의 스케줄링 우선권을 출력 조정수치가 생략되면 명령의 nice 값은 10이 됨. 음의 수치까지 부여할 수 있으며 -20 ~ 19까지 조정할 수 있음 renice 명령으로 실행 중인 프로세스의 우선권을 변경할 수 있으며, skill, snice 명령으로 시그널을 보내거나 프로세스 상황을 리포팅 할 수 있음 Ss, S&lt;, Ss+, SN 등으로 표기 nohup : 적은 명령에 대해 Hangup(프로세스 중단) 신호를 무시한 채 수행하도록 하는 명령 nohup.out 파일에 출력 내용을 추가 자동으로 백그라운드로 보내지 않기 때문에 명령행 뒤에 &amp; 문자를 추가해 실행하거나 명시적으로 백그라운드로 실행 실행 파일을 백그라운드로 실행(&amp;)하고 현재 쉘을 종료해도 실행 파일의 수행이 중단되지 않음 bash 쉘 옵션 중 huponexit은 현재 쉘을 빠져나갈 때 SIGHUP 시그널(정지 신호)을 모든 job에게 보내는 옵션인데, 이 옵션의 기본값은 off라 현재 쉘을 종료하더라도 백그라운드로 실행된 프로세스는 종료되지 않고 계속 실행 nohup python execute.py &gt; output &amp; 등으로 사용 pidof : 실행중인 프로세스 아이디를 검색, 출력 fuser : 파일 또는 소켓을 사용하고 있는 프로세스를 출력 cron : 수퍼유저용/일반유저용 스케줄러 /etc/crontab 파일에 설정이 존재하며, 설정별 실행할 파일들은 /etc/cron.hourly 디렉터리에 위치 프로세스 관리와 부팅 init : 모든 프로세스의 부모 프로세스(PID 1번)이며 부팅시 /etc/inittab 파일에 런레벨을 결정 service : 시스템 서비스를 시작하고 중지하기 위해 사용 리눅스 시작스크립트 원본 경로는 /etc/rc.d/init.d 또는 심볼릭 링크된 /etc/init.d이며, 부팅 시 7개의 런레벨별 시작 스크립트 위치는 /etc/rc.d 디렉터리 아래에 런레벨별 디렉터리 명으로 존재 service cron restart 등으로 사용 네트워크 ifconfig : 네트워크 인터페이스 환경을 출력하고 튜닝하는 유틸리티. eth0은 첫번째 이더넷 카드를 의미하고, lo는 로컬호스트 루프백을 의미 ifconfig eht0 down : 네트워크 사용 중지. 시작은 up iwconfig : 무선랜 네트워크 인터페이스 환경을 출력해주는 명령이며, ifconfig와 유사하지만 무선랜 장치만 보여줌 ip : 라우팅, 디바이스, 라우팅과 터널 정책을 출력하고 조작할 때 사용 route : 커널 라우팅 테이블 정보를 출력하거나 변경. ip route list와 동일한 내용 출력 chkconfig : 시스템 서비스를 위한 런레벨 정보를 업데이트하고 검색 tcpdump : 네트워크 패킷을 실시간으로 출력해볼 수 있음. tcpdump tcp port 21 을 실행하면 21번 포트로 통신하는 패킷들을 출력 파일시스템 mount : 파일 시스템을 마운트하기 위해 사용. 파일 시스템을 가지는 디바이스는 하드 디스크의 파티션, 플로피 디스크, CDROM, USB 같은 외장 디바이스, 램디스크 등이 해당 etc/fstab 파일을 보면 여러 설정 내용을 볼 수 있는데, 이 설정 파일을 부팅시 자동으로 읽어들여 마운트하고 부팅 후 파일 시스템과 디바이스를 지정하지 않고 수동으로 마운트하기 위해 사용 /etc/mtab 파일에는 현재 마운트되어 있는 파일 시스템 또는 파티션 정보가 저장되어 있음 mount -o loop -t iso9660 -r cd_image.iso /media/cdimg umount : 마운트된 파일 시스템을 즉시 마운트 해제할 때 사용. 언마운트와 동시에 시디롬 드라이브에서 시디롬을 꺼내려면 eject 명령을 사용 sync : 업데이트된 모든 버퍼의 데이터를 강제로 하드 드라이버에 즉시 저장. 버퍼와 하드 드라이버 동기화 수행 losetup : 루프 장치를 정규 파일 또는 블럭 장치와 연결, 루프 장치와 분리, 루프 장치의 상태 파악을 하는 데 사용됨. 루프 장치 인자만 줄 경우에 해당하는 루프 장치의 상태를 보여줌 mkswap, dd : mkswap 명령은 스왑 파일 또는 스왑 파티션을 생성할 때 사용 dd if=/dev/zero of=swapfile bs=1024 count=8192 mkswap swapfile 8192 sync swapon swapfile swapon : 파일이나 파티션을 스왑으로 사용하도록 하는 명령 swapoff : 파일이나 파티션을 스왑으로 사용하지 않도록 하는 명령 mkfs.ext3 : 파티션이나 파일을 ext3 파일 시스템으로 만들 때 사용, mkfs 명령과 함께 -t 옵션값으로 ext3를 지정해 파일 시스템을 생성 hdparam : 하드 디스크의 설정을 보여주거나 설정을 조정 fdisk [디바이스명] : 저장용 디바이스의 파티션 테이븡를 생성하거나 변경 /dev/hda : 첫 IDE 하드 드라이브 /dev/sda : 첫 SATA, SCSI 하드 드라이브 fsck.ext3 : ext3 파일 시스템을 체크, 수리, 디버그하는 명령. 단 mount된 파티션에 대해 파일 시스템 체크를 하면 해당 파티션에 문제가 발생할 수 있으므로 마운트를 해제한 다음, 파일 시스템 체크를 해야함! badblocks : 저장 디바이스의 물리적인 배드 블록을 체크 lsusb : 장착되어 있는 USB 디바이스 목록 출력 lspci : 장착되어 있는 pci 디바이스 목록 출력 mkbootdisk : 시스템 구동을 위한 독립적인 부트 플로피 디스크를 만듬. -iso 옵션을 사용해 부팅이 가능한 iso 파일로 만들 수 있음 mkisofs : iso9660 파일 시스템, 즉 CD 이미지를 만들 수 있음 chroot : root 디렉터리를 변경. 지정한 루트 디렉터리를 사용해 명령과 인터렉티브 쉘을 실행. 보안적 측면에서 유용함 lockfile : procmail 패키지에 포함되어 있음. lcokfile 명령으로 세마포어 잠금 파일, 디바이스, 리소스 등을 생성해 파일 접근을 관리할 수 있음. 잠금 파일이 존재하면 다른 프로세스의 접근이 제한됨 /var/lock 디렉터리에 잠금 파일을 생성하고 체크하며 rm -f [잠금파일명] 명령으로 삭제할 수 있음 백업 dump : 정교한 파일 시스템(ext2/ext3) 백업 유틸리티이며, -f 옵션을 사용해 네트워크 파일 시스템도 백업할 수 있음. dump 명령은 저수준 디스크 파티션을 읽고 바이너리 포맷의 백업 파일을 만듬. 이 파일들은 restore를 사용해 복원 fdformat : 플로피 디스크를 로우레벨 저수준으로 포맷 시스템 리소스 ulimit : 시스템 리소스 사용의 상한 제한값을 설정. -f 옵션을 사용해 파일 크기 제한, -c 옵션으로 코어덤프 크기를 제한, -a옵션을 사용해 현재 시스템의 상한값을 출력해볼 수 있음 모듈 lsmod : 설치된 커널 모듈 목록을 출력. cat /proc/modules도 동일",
    "tags": "linux development",
    "url": "/development/2017/12/04/linux-1/"
  }]};