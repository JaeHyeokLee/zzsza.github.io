<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v6.4.0 (https://qwtel.com/hydejack/)
-->









<head>
  <!-- =============== -->
<!-- META            -->
<!-- =============== -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="naver-site-verification" content="22c5b4ef3bfda7fc6100671413989219de7a4ac8"/>
<meta property="og:title" content="Pytorch를 활용한 자연어 처리(NLP)">
<meta property="og:type" content="article">





  <meta property="og:image" content="http://localhost:4000/assets/img/logo.png">


<meta property="og:image:width" content="640" />
<meta property="og:image:height" content="360" />



  <title>Pytorch를 활용한 자연어 처리(NLP) &middot; 어쩐지 오늘은</title>



<meta name="description" content="김성동님의 Pytorch를 활용한 딥러닝 입문 중 자연어처리 파트 정리 파일입니다.

">
<meta property="og:description" content="김성동님의 Pytorch를 활용한 딥러닝 입문 중 자연어처리 파트 정리 파일입니다.

">


<!-- tipuesearch -->
<link rel="stylesheet" href="/assets/tipuesearch/css/tipuesearch.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">




<!-- =============== -->
<!-- LINKS           -->
<!-- =============== -->
<link rel="canonical" href="http://localhost:4000/data/2018/03/10/nlp-python/">
<meta property="og:url" content="http://localhost:4000/data/2018/03/10/nlp-python/">

<link rel="alternate" type="application/atom+xml" title="어쩐지 오늘은 Feed" href="http://localhost:4000/feed.xml">


  <link rel="prev" href="http://localhost:4000/etc/2018/03/01/okky-spectial-lecture/">



  <link rel="next" href="http://localhost:4000/data/2018/03/17/pytorch-rnn/">


<link rel="apple-touch-icon" href="http://localhost:4000/apple-touch-icon.png">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<!-- Place favicon.ico in the root directory -->

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->
<script>
  !function(n,e){function t(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function o(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}n._loaded=!1,n.loadJSDeferred=function(a,d){function r(){n._loaded=!0;var r=e.createElement("script");r.src=a,d&&(("onload"in r?t:o)(r,d),r.onload||t(r,d));var l=e.getElementsByTagName("script")[0];l.parentNode.insertBefore(r,l)}n._loaded?r():n.addEventListener?n.addEventListener("load",r,!1):n.attachEvent?n.attachEvent("onload",r):n.onload=r}}(window,document);

  !function(e){"use strict";var n=function(n,t,o){function i(e){if(a.body)return e();setTimeout(function(){i(e)})}function r(){l.addEventListener&&l.removeEventListener("load",r),l.media=o||"all"}var d,a=e.document,l=a.createElement("link");if(t)d=t;else{var f=(a.body||a.getElementsByTagName("head")[0]).childNodes;d=f[f.length-1]}var s=a.styleSheets;l.rel="stylesheet",l.href=n,l.media="only x",i(function(){d.parentNode.insertBefore(l,t?d:d.nextSibling)});var u=function(e){for(var n=l.href,t=s.length;t--;)if(s[t].href===n)return e();setTimeout(function(){u(e)})};return l.addEventListener&&l.addEventListener("load",r),l.onloadcssdefined=u,u(r),l};"undefined"!=typeof exports?exports.loadCSS=n:e.loadCSS=n}("undefined"!=typeof global?global:this);

  !function(t){if(t.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return t.document.createElement("link").relList.supports("preload")}catch(t){return!1}},e.poly=function(){for(var e=t.document.getElementsByTagName("link"),r=0;r<e.length;r++){var n=e[r];"preload"===n.rel&&"style"===n.getAttribute("as")&&(t.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var r=t.setInterval(e.poly,300);t.addEventListener&&t.addEventListener("load",function(){e.poly(),t.clearInterval(r)}),t.attachEvent&&t.attachEvent("onload",function(){t.clearInterval(r)})}}}(this);

  window.disablePushState = false;
  window.disableDrawer = false;
</script>

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-5105555504153863",
    enable_page_level_ads: true
  });
</script>

<!--[if lt IE 9]>
<script src="https://unpkg.com/html5shiv/dist/html5shiv.min.js"></script>
<![endif]-->

<!-- =============== -->
<!-- STYLES          -->
<!-- =============== -->
<!--[if gt IE 8]><!---->
<style>
  
  article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-size:16px;line-height:1.75}body{color:#333;background-color:#fff;overflow-y:scroll}a{text-decoration:none}.lead{margin-left:-1rem;margin-right:-1rem}img,.img{display:block;max-width:100%;margin-bottom:1rem;border:none}img.lead,.img.lead{max-width:calc(100% + 2rem);width:calc(100% + 2rem)}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-weight:bold;text-rendering:optimizeLegibility}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6{margin:1.6rem 0 1rem;line-height:1.6}h1,.h1{font-size:2rem;line-height:1.25}h2,.h2{font-size:1.5rem}h3,.h3{font-size:1.17em}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.25rem;font-weight:300;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee}.hr{padding-bottom:.5rem;border-bottom:1px solid #eee;margin-bottom:1.5rem}h4,h5,h6,.h4,.h5,.h6{margin-bottom:0.5rem;font-size:1rem}table{margin-bottom:1rem;width:100%;width:calc(100% + 2rem);margin-left:-1rem;border:1px solid #e5e5e5;border-collapse:collapse;border-spacing:0}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}td:first-child,th:first-child{padding-left:1rem}td:last-child,th:last-child{padding-right:1rem}thead+tbody,tbody+tbody,tfoot{border-top:3px double #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}footer{margin-bottom:2rem}.page,.post{margin-bottom:3em}.page li+li,.post li+li{margin-top:.25rem}.page>header,.post>header{margin-bottom:2rem}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-0.5rem;margin-bottom:1rem;color:#9a9a9a}.related-posts{padding-left:0;list-style:none}.related-posts>li,.related-posts>li+li{margin-top:1rem}.related-posts>li>small,.related-posts>li+li>small{font-size:75%;color:#9a9a9a}.message{margin-bottom:1rem;padding:1rem;color:#787878;background-color:#f9f9f9;margin-left:-1rem;margin-right:-1rem}body,main{position:relative;overflow-x:hidden}@media screen{body::before{content:'';background:#e5e5e5;position:absolute;left:0;top:0;bottom:0}}@media screen and (min-width: 40em){html{font-size:17px}}@media screen and (min-width: 54em){html{font-size:16px}}@media screen and (min-width: 88em){html{font-size:17px}}@media screen and (min-width: 125em){html{font-size:18px}}.sr-only{display:none}.clearfix,.sidebar-social::after,.clearafter::after{content:"";display:table;clear:both}a,.a{position:relative;padding-bottom:.15rem;border-style:hidden}.img{overflow:hidden;background-color:#f9f9f9}.img>img{margin:0;width:100%;height:100%}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}h1+hr,h2+hr,h3+hr,h4+hr,h5+hr,h6+hr{margin-top:0}.fade-in{animation-duration:500ms;animation-name:fade-in;animation-fill-mode:forwards}@keyframes fade-in{from{transform:translateY(-2rem);opacity:0}50%{transform:translateY(-2rem);opacity:0}to{transform:translateY(0);opacity:1}}.mb6{margin-bottom:10rem}.sidebar{color:rgba(255,255,255,0.75);text-align:left}.sidebar::before{content:"";position:absolute;z-index:2;top:0;left:0;bottom:0;right:0;background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 100%)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2)}.right-side{width:100%;margin-left:auto;margin-right:auto}.right-side .ad-first{text-align:center}.right-side .ad-second{text-align:center}@media screen{.right-side{max-width:38rem;min-height:100vh}}@media screen and (min-width: 54em){.right-side{margin-left:20rem;margin-right:1rem;padding:4rem 1rem 12rem}}@media screen and (min-width: 72em){.right-side{margin-left:22rem;max-width:42rem}}@media screen and (min-width: 88em){.right-side{width:162px;margin-left:0rem;margin-right:0rem;padding:0rem;margin-top:10rem;display:block;float:left}}@media screen and (min-width: 96em){.right-side{width:300px;margin-right:0rem}}#_yDrawer{position:relative}@media screen{#_yDrawer{min-height:640px;min-height:100vh}}@media screen and (min-width: 54em){#_yDrawer{width:18rem;margin-left:0}}.sidebar-bg{position:absolute;height:100%;overflow:hidden;top:0;right:0;bottom:0;left:0;background:#202020 center / cover}.sidebar-box{display:flex;justify-content:center}.sidebar-sticky{position:relative;z-index:3}@media screen{.sidebar-sticky{-ms-overflow-style:none;overflow:-moz-scrollbars-none;height:100%;overflow:auto;position:absolute;padding:3rem 0rem;right:2.5rem;left:2.5rem}}.sidebar-sticky::-webkit-scrollbar{display:none}.sidebar-about>h1{color:#fff;font-size:2rem}.sidebar-nav>ul{list-style:none;padding-left:0;margin-bottom:.5rem}a.sidebar-nav-item{width:100%;font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}a.sidebar-nav-subitem{font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}@media screen{.y-drawer-scrim{z-index:2}.y-drawer-content{width:18rem;left:-18rem;z-index:3}}.sidebar-social{margin-bottom:.5rem}.sidebar-social>ul{list-style:none;padding-left:0;margin:0 -.25rem}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.6rem;line-height:3rem;width:3.1249rem;height:4rem;padding:.5rem 0}.sidebar-social>ul li+li{margin-top:0}.fixed-top{position:fixed;top:0;left:0;width:100%;z-index:1}.navbar>.content{padding-top:0;padding-bottom:0;min-height:0;height:0}.menu{display:inline-block;padding:1.75rem 1.5rem;border-bottom:none;margin-left:-1.5rem;color:#9a9a9a !important}.menu::after{content:"\2630"}@media screen and (min-width: 54em){.menu{padding:1.25rem 1.5rem;position:absolute;left:-9999px}.menu:focus{position:static}}.animation-main{pointer-events:none}.loading{display:none}@media print{.menu{display:none}}.animation-main{opacity:0;will-change:opacity}.loading{position:absolute;top:0;right:0;padding:5.25rem 4.5rem;transform-origin:top right;transform:scale(0.33)}.content{position:relative;margin-left:auto;margin-right:auto;padding:5rem 1rem 12rem}@media screen{.content{min-height:100vh}}@media screen and (min-width: 54em){.content{padding:4rem 1rem 12rem;margin-left:19rem;margin-right:3rem}}@media screen and (min-width: 72em){.content{max-width:42rem;margin-left:21rem}}@media screen and (min-width: 88em){.content{float:left;width:100%;margin-left:22rem;margin-right:5rem}}@media screen and (min-width: 96em){.content{max-width:44rem}}@media screen and (min-width: 102em){.content{margin-left:25rem;margin-right:8rem}}.me{width:6.5rem;height:6.5rem;align-self:center;margin-right:20px;border-radius:100%;position:relative}@media screen and (min-width: 40em){.me{width:7rem;height:7rem}}@media screen and (min-width: 54em){.me{width:6.5rem;height:6.5rem}}@media screen and (min-width: 72em){.me{width:7rem;height:7rem}}main>footer{width:100%;position:absolute;bottom:0;left:0;right:0;padding:0 1rem;color:#9a9a9a;font-size:smaller;text-align:center}main>footer>p{margin-bottom:0}html{font-family:'Sans-serif'}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-family:'Sans-serif'}

</style>


<link rel="preload" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0" as="style" onload="this.rel='stylesheet'">

<style id="_pageStyle">

.content a{color:#4f86aa;border-color:rgba(79,134,170,0.2)}.content a:hover{border-color:#4f86aa}:focus{outline-color:#4f86aa}::selection{color:#fff;background:#4f86aa}::-moz-selection{color:#fff;background:#4f86aa}

</style>


<noscript>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0">
  
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <style>
      html { font-family: 'Lato', 'Sans-serif' }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: 'Lato', 'Sans-serif' }
    </style>
  

  
  <link rel="stylesheet" href="http://localhost:4000/assets/icomoon/style.css">
</noscript>
<!--<![endif]-->

</head>

<body>
  <!-- =============== -->
<!-- MENU            -->
<!-- =============== -->
<div class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <a id="_menu" class="menu no-hover" href="#_title">
      <span class="sr-only">Menu</span>
    </a>
  </div>
</div>

<!-- =============== -->
<!-- CONTENT         -->
<!-- =============== -->
<div id="_yPushState">
  <div class="fade-in">
    <main id="_main" class="content" role="main" data-color="#4f86aa" data-image="/assets/img/nap.jpg">
      

<article id="post-data/2018/03/10/nlp-python" class="post" role="article">
  <header>
    <h1 class="post-title">
      
        Pytorch를 활용한 자연어 처리(NLP)
      
    </h1>

    <p class="post-date heading">
      <time datetime="2018-03-10T00:00:00+09:00">10 Mar 2018</time>
      









in <a href="/category/data/" data-flip="title">Data</a>

      









on <a href="/tag/data-pytorch/" data-flip="title">Pytorch</a>

    </p>

    
  <div class="hr" style="padding-bottom:0"></div>


  </header>
  

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  
    <div class="markdown-body">


<style>
.myAd1190 { display:block; width:98%; height: 280px; }
@media(min-width: 600px) { .myAd1190 { display: none; } }
</style>
<ins class="adsbygoogle myAd1190"
    data-ad-client="ca-pub-5105555504153863"
    data-ad-slot="6481487416"
    ></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>



<br/>
      <p><a href="https://github.com/dsksd">김성동</a>님의 Pytorch를 활용한 딥러닝 입문 중 자연어처리 파트 정리 파일입니다.</p>

<h1 id="distributed-word-representation">Distributed Word Representation</h1>

<p>NLP Task는 지금까지 봤던 접근법이랑(CNN류) 많이 다릅니다. RNN을 배우기 전에 Word Vector를 알아야 하기 때문에 이 내용을 추가했습니다</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>파이토치가 짱이야. 
아닌데? 텐서플로우가 짱이지 구글이 만들었잖아
그래도 파이토치는 쉬워 페북도 장난 아니야
아마존이 만든 맥스넷이란 것도 좋더라
</code></pre></div></div>
<p>다음 대화에서 어떤 정보를 얻을 수 있을까요?</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>파이토치(페북) ≈ 텐서플로우(구글) ≈ 맥스넷(아마존)
</code></pre></div></div>

<p>사람은 문맥을 통해 각 단어의 유사함과 관계를 쉽게 추론할 수 있음! 컴퓨터에게 넣기 위해선 컴퓨터가 이해할 수 있도록 변형해줘야 합니다. 이 과정을 Word Representation이라고 합니다</p>

<h2 id="word-representation">Word Representation</h2>
<p>Idea/Thing을 여러 Symbol로 표현할 수 있습니다.(같은 것을 다르게 표현할 수 있습니다) 여기서 각 단어간의 관계도 알 수 있습니다(유사한지)</p>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-1.png?raw=true" /></p>

<h3 id="wordnet">WordNet</h3>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-2.png?raw=true" /></p>

<ul>
  <li>단어 의미를 그래프 형태로 출력</li>
  <li>사람이 직접 구축해야 함(비싼 비용)</li>
  <li>신조어의 의미를 이해 못함</li>
  <li>뉘앙스를 놓치기 쉬움</li>
  <li>주관적임</li>
  <li>단어 간의 정확한 유사도 계산이 어려움(얼마나 유사한지) 그래프상 몇 노드가 연결되는지 정도만 알 수 있음</li>
</ul>

<h3 id="one-hot-vector">One-hot Vector</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>파이토치		 [1,0,0,0,0,0,0,0]
짱		  [0,1,0,0,0,0,0,0]
텐서플로우		 [0,0,1,0,0,0,0,0]
구글		 [0,0,0,1,0,0,0,0]
쉽다		 [0,0,0,0,1,0,0,0]
페북		 [0,0,0,0,0,1,0,0]
맥스넷		 [0,0,0,0,0,0,1,0]
아마존		 [0,0,0,0,0,0,0,1]
</code></pre></div></div>

<ul>
  <li>단어 하나를 하나의 Discrete Variable로 취급</li>
  <li>각 단어의 구분은 가능하자 유사도를 측정할 수는 없음</li>
  <li>두 벡터를 내적해서 유사도를 측정!</li>
  <li>만약 1만개의 단어를 One-hot vector로 표현하면?
    <ul>
      <li>1만 차원의 벡터가 필요해 차원의 저주가 발생함</li>
    </ul>
  </li>
</ul>

<h3 id="distributed-word-representation-1">Distributed word representation</h3>
<ul>
  <li>그렇다면, 단어의 의미를 분산시켜 벡터로 표현하자!</li>
  <li>그 단어의 속성은 함꼐 쓰이는 단어(맥락)에 의해 결정될 것이다</li>
  <li>Dense한 Vector를 만들어봄!</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>파이토치	 [0.6, -0.2, 0.7, 0.3, 0.7, -0.2, 0.1, 0.1]
텐서플로우	[0.4, -0.1, 0.6, -0.2, 0.6, -0.2, 0.3, 0.4]
고양이	  [-0.3, 0.2, 0.1, 0.2, -0.2, 0.1, -0.3, 0.1]

유사도
파이토치^T * 텐서플로우 = 1.15
파이토치^T * 고양이 = -0.26
</code></pre></div></div>

<ul>
  <li>비교적 낮은 차원의(50 ~ 1000차원) 벡터에 단어의 의미를 분산해 Dense한 벡터로 표현</li>
  <li>Word vector라고 부름</li>
</ul>

<h2 id="nlp-task">NLP Task</h2>
<p><a href="https://github.com/DSKSD/Pytorch_Fast_Campus_2018/blob/master/week6/1_Bag_of_Words.ipynb">NLP Task</a> 연습</p>

<p>Bag of Words : Count 방식으로 표현</p>

<h3 id="1-tokenize">1. Tokenize</h3>
<ul>
  <li>문장을 단어 또는 형태소 단위로 토큰화
    <ul>
      <li>형태소 : 언어를 이루는 최소 단위</li>
    </ul>
  </li>
  <li>문장이란 것은 단어의 연속이기 때문에 리스트로 표현할 수 있음</li>
  <li>토큰은 문장, 단어, character, 형태소가 될 수 있음
    <ul>
      <li>한국어는 형태소로 쪼개는 방법에 유효함</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>token = nltk.word_tokenize("Hi, my name is sungdong. What's your name?")
print(token)
&gt;&gt;&gt; ['Hi', ',', 'my', 'name', 'is', 'sungdong', '.', 'What', "'s", 'your', 'name', '?']
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 꼬꼬마 형태소 분석기
token = kor_tagger.morphs("안녕하세요! 저는 파이토치를 공부하는 중입니다.")
print(token)
&gt;&gt;&gt; ['안녕', '하', '세요', '!', '저', '는', '파이', '토치', '를', '공부', '하', '는', '중', '이', 'ㅂ니다', '.']
</code></pre></div></div>

<h3 id="2-build-vocab">2. Build Vocab</h3>
<ul>
  <li>단어의 인덱스를 가지고 있어야 함(각 자리에 맞게 넣어주기 위해)</li>
  <li>따라서 Vocab을 구축
    <ul>
      <li>각 단어의 ID를 붙여주는 작업</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word2index={} # dictionary for indexing
for vo in token:
    if word2index.get(vo)==None:
        word2index[vo]=len(word2index)
print(word2index)
&gt;&gt;&gt; {'하': 1, '.': 13, '를': 8, '중': 10, '안녕': 0, '파이': 6, '세요': 2, '토치': 7, '저': 4, '!': 3, '공부': 9, 'ㅂ니다': 12, '이': 11, '는': 5}
</code></pre></div></div>

<h3 id="3-one-hot-encoding">3. One-hot Encoding</h3>
<ul>
  <li>자신의 인덱스에는 1을 채우고, 나머지엔 0을 채움</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def one_hot_encoding(word,word2index):
    tensor = torch.zeros(len(word2index))
    index = word2index[word]
    tensor[index]=1.
    return tensor
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch_vector = one_hot_encoding("토치",word2index)
print(torch_vector)
&gt;&gt;&gt; 
 0
 0
 0
 0
 0
 0
 0
 1
 0
 0
 0
 0
 0
 0
</code></pre></div></div>

<ul>
  <li>유사도를 계산해보면, (One Hot Encoding의 한계)</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>py_vector = one_hot_encoding("파이",word2index)
py_vector.dot(torch_vector)
&gt;&gt;&gt; 0.0
</code></pre></div></div>

<h3 id="bag-of-words를-통한-분류">Bag of Words를 통한 분류</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_data</span> <span class="p">=</span> <span class="p">[[</span><span class="s2">"배고프다 밥줘"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"뭐 먹을만한거 없냐"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"맛집 추천"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"이 근처 맛있는 음식점 좀"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"밥줘"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"뭐 먹지?"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"삼겹살 먹고싶어"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"영화 보고싶다"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"요즘 볼만한거 있어?"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"영화나 예능 추천"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"재밌는 드라마 보여줘"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"신과 함께 줄거리 좀 알려줘"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"고등랩퍼 다시보기 좀"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">"재밌는 영상 하이라이트만 보여줘"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">]]</span>

<span class="n">test_data</span> <span class="p">=</span> <span class="p">[[</span><span class="s2">"쭈꾸미 맛집 좀 찾아줘"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">"매콤한 떡볶이 먹고싶다"</span><span class="p">,</span><span class="s2">"FOOD"</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">"강남 씨지비 조조 영화 스케줄표 좀"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">],</span>
                   <span class="p">[</span><span class="s2">"효리네 민박 보고싶엉"</span><span class="p">,</span><span class="s2">"MEDIA"</span><span class="p">]]</span>


<span class="p">#</span> <span class="m">0.</span> <span class="n">Preprocessing</span>
<span class="n">train_X</span><span class="p">,</span><span class="n">train_y</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">zip</span><span class="p">(*</span><span class="n">train_data</span><span class="p">))</span>

<span class="p">#</span> <span class="m">1.</span> <span class="n">Tokenize</span>
<span class="n">train_X</span> <span class="p">=</span> <span class="p">[</span><span class="n">kor_tagger</span><span class="p">.</span><span class="n">morphs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="n">for</span> <span class="n">x</span> <span class="k">in</span> <span class="n">train_X</span><span class="p">]</span>

<span class="p">#</span> <span class="m">2.</span> <span class="n">Build</span> <span class="n">Vocab</span>
<span class="n">word2index</span><span class="p">={</span><span class="s1">'&lt;unk&gt;'</span> <span class="p">:</span> <span class="m">0</span><span class="p">}</span>
<span class="n">for</span> <span class="n">x</span> <span class="k">in</span> <span class="n">train_X</span><span class="p">:</span>
    <span class="n">for</span> <span class="n">token</span> <span class="k">in</span> <span class="n">x</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word2index</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">)==</span><span class="n">None</span><span class="p">:</span>
            <span class="n">word2index</span><span class="p">[</span><span class="n">token</span><span class="p">]=</span><span class="n">len</span><span class="p">(</span><span class="n">word2index</span><span class="p">)</span>
            
<span class="n">class2index</span> <span class="p">=</span> <span class="p">{</span><span class="s1">'FOOD'</span> <span class="p">:</span> <span class="m">0</span><span class="p">,</span> <span class="s1">'MEDIA'</span> <span class="p">:</span> <span class="m">1</span><span class="p">}</span>
<span class="n">print</span><span class="p">(</span><span class="n">word2index</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="n">class2index</span><span class="p">)</span>

<span class="p">#</span> <span class="m">3.</span> <span class="n">Prepare</span> <span class="n">Tensor</span>
<span class="n">def</span> <span class="n">make_BoW</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">word2index</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">word2index</span><span class="p">))</span>
    <span class="n">for</span> <span class="n">w</span> <span class="k">in</span> <span class="n">seq</span><span class="p">:</span>
        <span class="n">index</span> <span class="p">=</span> <span class="n">word2index</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">index</span><span class="c1">!=None:
</span>            <span class="n">tensor</span><span class="p">[</span><span class="n">index</span><span class="p">]+=</span><span class="m">1.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">index</span> <span class="p">=</span> <span class="n">word2index</span><span class="p">[</span><span class="s1">'&lt;unk&gt;'</span><span class="p">]</span>
            <span class="n">tensor</span><span class="p">[</span><span class="n">index</span><span class="p">]+=</span><span class="m">1.</span>
    
    <span class="n">return</span> <span class="n">tensor</span>
    
<span class="n">train_X</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">Variable</span><span class="p">(</span><span class="n">make_BoW</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">word2index</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span><span class="m">1</span><span class="p">,-</span><span class="m">1</span><span class="p">)</span> <span class="n">for</span> <span class="n">x</span> <span class="k">in</span> <span class="n">train_X</span><span class="p">])</span>
<span class="n">train_y</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">class2index</span><span class="p">[</span><span class="n">y</span><span class="p">]]))</span> <span class="n">for</span> <span class="n">y</span> <span class="k">in</span> <span class="n">train_y</span><span class="p">])</span>

<span class="p">#</span> <span class="m">4.</span> <span class="n">Modeling</span>
<span class="n">class</span> <span class="n">BoWClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">def</span> <span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">output_size</span><span class="p">):</span>
        <span class="n">super</span><span class="p">(</span><span class="n">BoWClassifier</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="p">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">output_size</span><span class="p">)</span>
    
    <span class="n">def</span> <span class="k">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">return</span> <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        
<span class="p">#</span> <span class="m">5.</span> <span class="n">Train</span>
<span class="n">STEP</span> <span class="p">=</span> <span class="m">100</span>
<span class="n">LR</span> <span class="p">=</span> <span class="m">0.1</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">BoWClassifier</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">word2index</span><span class="p">),</span><span class="m">2</span><span class="p">)</span>
<span class="n">loss_function</span> <span class="p">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="p">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="k">model</span><span class="p">.</span><span class="k">parameters</span><span class="p">(),</span><span class="n">lr</span><span class="p">=</span><span class="n">LR</span><span class="p">)</span>

<span class="n">for</span> <span class="n">step</span> <span class="k">in</span> <span class="k">range</span><span class="p">(</span><span class="n">STEP</span><span class="p">):</span>
    <span class="k">model</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">preds</span> <span class="p">=</span> <span class="k">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="p">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span><span class="n">train_y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="p">%</span> <span class="m">10</span> <span class="p">==</span> <span class="m">0</span><span class="p">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="m">0</span><span class="p">])</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    
<span class="p">#</span> <span class="m">6.</span> <span class="n">Test</span>
<span class="n">index2class</span> <span class="p">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="n">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="k">in</span> <span class="n">class2index</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">for</span> <span class="n">test</span> <span class="k">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">X</span> <span class="p">=</span> <span class="n">kor_tagger</span><span class="p">.</span><span class="n">morphs</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="m">0</span><span class="p">])</span>
    <span class="n">X</span> <span class="p">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">make_BoW</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">word2index</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span><span class="m">1</span><span class="p">,-</span><span class="m">1</span><span class="p">)</span>
    
    <span class="n">pred</span> <span class="p">=</span> <span class="k">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">pred</span> <span class="p">=</span> <span class="n">pred</span><span class="p">.</span><span class="k">max</span><span class="p">(</span><span class="m">1</span><span class="p">)[</span><span class="m">1</span><span class="p">].</span><span class="n">data</span><span class="p">[</span><span class="m">0</span><span class="p">]</span>
    <span class="n">print</span><span class="p">(</span><span class="s2">"Input : %s"</span> <span class="p">%</span> <span class="n">test</span><span class="p">[</span><span class="m">0</span><span class="p">])</span>
    <span class="n">print</span><span class="p">(</span><span class="s2">"Prediction : %s"</span> <span class="p">%</span> <span class="n">index2class</span><span class="p">[</span><span class="n">pred</span><span class="p">])</span>
    <span class="n">print</span><span class="p">(</span><span class="s2">"Truth : %s"</span> <span class="p">%</span> <span class="n">test</span><span class="p">[</span><span class="m">1</span><span class="p">])</span>
    <span class="n">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>           
</code></pre></div></div>

<ul>
  <li>&lt;unk&gt;란 단어는 모르는 단어를 처리하기 위해 넣은 단어(인덱스가 존재하지 않으면 unk!)</li>
  <li>Long Tensor로 변환</li>
</ul>

<h2 id="word2vec">Word2Vec</h2>
<ul>
  <li>우린 엄청 많은 Text를 가지고 있음(인터넷상에 텍스트는 많음)</li>
  <li>모든 단어는 fixed vocabulary (Train에 정의한 단어가 fixed voca가 됨)</li>
  <li>단어의 속성은 주변 단어로부터 결정된다라는 전제가 있었는데, 주변 단어가 Input이고 중심 단어가 Output이 나오는 CBOW 모델과 중심 단어가 Input이고 주변 단어가 Output인 Skip-gram이 있습니다</li>
</ul>

<h3 id="skip-gram">Skip-gram</h3>
<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-3.png?raw=true" /></p>

<ul>
  <li>중심 단어가 있으면 주변 단어가 나올 조건부 확률을 구할 수 있음</li>
  <li>윈도우 사이즈는 하이퍼 파라미터</li>
</ul>

<h3 id="learnable-embedding-matrix">Learnable Embedding Matrix</h3>
<ul>
  <li>7개의 단어를 5차원의 Vector로 임베딩하고 싶은 경우엔 7*5의 Embedding Matrix가 필요함</li>
  <li>1*7와 7*5를 행렬곱하면 자신의 Index로 인덱싱</li>
  <li>Embedding Matrix를 학습하는 것이 Word2Vec</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Pytorch
embed = nn.Embedding(총 단어의 갯수, 임베딩 시킬 벡터의 차원)
embed.weight
&gt;&gt;&gt; Parameter Containing : 학습 가능
</code></pre></div></div>

<ul>
  <li>Embedding 모듈은 index를 표현하는 LongTensor를 인풋으로 기대하고 해당 벡터로 인덱싱합니다</li>
  <li>따라서 원핫벡터로 명시적으로 바꿔주지 않아도 됩니다</li>
</ul>

<h3 id="object-function">Object Function</h3>
<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-4.png?raw=true" /></p>

<ul>
  <li>Corpus : 텍스트의 뭉치</li>
  <li>각 토큰마다 그 단어가 중심단어가 될 수 있음</li>
  <li>중심 단어가 등장했을 때, 맥락 단어가 함께 등장할 확률을최대화하는 방향으로 Parameter를 업데이트
    <ul>
      <li>최적화를 쉽게하기 위해 Log로 바꿔서 곱을 합으로 변경</li>
      <li>-를 붙여서 Log-Likelihood를 <strong>최소화</strong></li>
    </ul>
  </li>
</ul>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-5.png?raw=true" /></p>

<ul>
  <li>각 단어는 Center Word와 Context word가 될 수 있음
<img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-6.png?raw=true" /></li>
</ul>

<p><code class="MathJax_Preview">u_o^T*v_c</code><script type="math/tex">u_o^T*v_c</script> : 내적을 해서 유사도를 구함</p>

<ul>
  <li>데이터셋 예시
<img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-7.png?raw=true" /></li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>list(nltk.ngrams(tokenized, 5))
</code></pre></div></div>
<h3 id="예시">예시</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I have a puppy . His name is Bori . I love him .
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Corpus : 텍스트의 뭉치
T : 14 (코퍼스 내의 단어의 갯수)
m : 2 (Window size, 모델러가 정할 하이퍼 파라미터)
V : 11 (코퍼스 내의 단어의 집합. 중복을 제거한 집합)
</code></pre></div></div>

<ol>
  <li>Corpus에서 단어 집합<strong>(Vocabulary)</strong>을 구해서 <strong>Index</strong>를 매긴다.</li>
  <li><strong>Window size</strong>를 정하고 <strong>T개의 데이터셋</strong>를 준비한다.</li>
  <li><strong>Center word</strong>와 <strong>Context word</strong>를 표현할 <strong>2개의 Embedding Matrix</strong>를 선언한다.</li>
  <li><strong>P(o|c)</strong>를 구해서 <strong>Negative log-likelihood(loss)</strong>를 구한다.</li>
  <li><strong>Gradient Descent</strong>를 사용하여 <strong>loss</strong>를 최소화한다.</li>
  <li>학습이 끝난 뒤에는 <strong>Center vector와 Context vector를 평균</strong>해서 사용한다.</li>
</ol>

<h3 id="코드-예시">코드 예시</h3>
<ul>
  <li>1 Corpus에서 단어 집합을 구해 Index를 매김</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>corpus = "I have a puppy. His name is Bori. I love him."
tokenized = nltk.word_tokenize(corpus)
vocabulary = list(set(tokenized)) # 단어의 집합(중복 x)
print(tokenized)
print(vocabulary)

word2index={}
for voca in vocabulary:
    if word2index.get(voca)==None:
        word2index[voca]=len(word2index)
print(word2index)
</code></pre></div></div>

<ul>
  <li>2 Window size를 정하고 데이터를 준비</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WINDOW_SIZE = 2
windows = list(nltk.ngrams(['&lt;DUMMY&gt;'] * WINDOW_SIZE + tokenized + ['&lt;DUMMY&gt;'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1))

train_data = []

for window in windows:
    for i in range(WINDOW_SIZE * 2 + 1):
        if i == WINDOW_SIZE or window[i] == '&lt;DUMMY&gt;': 
            continue
        train_data.append((window[WINDOW_SIZE], window[i]))

print(train_data[:WINDOW_SIZE * 2])
# &gt;&gt;&gt; [('I', 'have'), ('I', 'a'), ('have', 'I'), ('have', 'a')]
# 각 단어를 index로 바꾸고 LongTensor로 바꿔주는 함수
def prepare_word(word, word2index):
    return Variable(torch.LongTensor([word2index[word]]))

X_p,y_p=[],[]

for (center,context) in train_data:
    X_p.append(prepare_word(center, word2index).view(1, -1))
    y_p.append(prepare_word(context, word2index).view(1, -1))
    
train_data = list(zip(X_p,y_p))
train_data[0]
</code></pre></div></div>

<ul>
  <li>3 Center word와 Context word를 표현할 2개의 Embedding Matrix를 선언</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>center_embed = nn.Embedding(len(word2index),3)
context_embed = nn.Embedding(len(word2index),3)

print(center_embed.weight)
print(context_embed.weight)

center,context = train_data[0]

center_vector = center_embed(center)
context_vector = context_embed(context)
print(center_vector)
print(context_vector)
# 배치 사이즈 : 1 
</code></pre></div></div>

<ul>
  <li>4 P(o|c)를 구해서 Negative log-likelihood(loss)를 구한다</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 분자값
score = torch.exp(context_vector.bmm(center_vector.transpose(1,2))).squeeze(2)
score

# 분모값
#  시퀀스(단어들의 연속된 리스트)가 들어오면 LongTensor로 매핑
def prepare_sequence(seq, word2index):
    idxs = list(map(lambda w: word2index[w], seq))
    return Variable(torch.LongTensor(idxs))

vocabulary_tensor = prepare_sequence(vocabulary,word2index).view(1,-1)
print(vocabulary_tensor)

vocabulary_vector = context_embed(vocabulary_tensor)

norm_scores = vocabulary_vector.bmm(center_vector.transpose(1, 2))
norm_scores = torch.sum(torch.exp(norm_scores,1))
print(norm_scores)

# 결과
score/norm_scores
</code></pre></div></div>

<ul>
  <li>이 정도의 문장이라면 T*2m 만큼의 배치 사이즈로 한번에
J(θ) 구할 수 있지만, 보통은 코퍼스의 크기가 매우 크기 때문에 미니 배치로 Negative log likelihood를 구해서 업데이트한다.(SGD)</li>
  <li>학습 후에는 두 벡터를 평균내서 최종 Word Vector로 사용함</li>
  <li>빈도 수가 적은 단어는 stopwords로 지정</li>
</ul>

<h3 id="실습"><a href="https://github.com/DSKSD/Pytorch_Fast_Campus_2018/blob/master/week6/2_Embedding_basic.ipynb">실습</a></h3>

<h2 id="negative-sampling">Negative Sampling</h2>
<ul>
  <li>word2vec의 비효율성을 개선하려고 한 논문
    <ul>
      <li>기존 방식의 문제점 : J(θ)를 계산하는 것이 매우 비싸고 비효율적</li>
      <li>보통의 코퍼스는 Vocabulary의 크기가 
1만개 이상이다. 즉, 이  Softmax 연산이 매우 비싼 연산이다. (== 학습이 느림)</li>
    </ul>
  </li>
  <li>문제점을 해소하기 위해 2가지 방법을 제시. 그 중 1개가 Negative Sampling</li>
</ul>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-8.png?raw=true" /></p>

<ul>
  <li>
    <pre class="MathJax_Preview"><code>P(w)=U(x)^(3/4)/Z</code></pre>
    <script type="math/tex; mode=display">P(w)=U(x)^(3/4)/Z</script>
  </li>
  <li>Unigram 분포!</li>
  <li>3/4라는 지수의 역할 : 빈도가 낮은 단어가 샘플링될 확률을 높여줌(Negative Sample)</li>
  <li>SoftMax는 비싼 연산이기 때문에 다른 방법이 많이 고안됨(면접 질문~)</li>
  <li>Negative Sampling을 하면 vector간 연산이 가능해짐</li>
</ul>

<p><img src="https://adriancolyer.files.wordpress.com/2016/04/word2vec-plural-relation.png?w=600" /></p>

<h2 id="glove--global-vector">Glove : Global Vector</h2>
<ul>
  <li>Timestep t 기준으로만 다른 주변단어와 co-occurrence를 포착했는데, 코퍼스(문서) 전체 기준으로 모든 Co-occurrence를 반영할 수는 없을까?</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I have a puppy . His name is Bori . I love him . 
And also I have a lovely cat. 
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>word</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>have</td>
      <td>2</td>
    </tr>
    <tr>
      <td>.</td>
      <td>1</td>
    </tr>
    <tr>
      <td>love</td>
      <td>1</td>
    </tr>
    <tr>
      <td>also</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<h3 id="co-occurrence">Co-occurrence</h3>
<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-9.png?raw=true" /></p>

<ul>
  <li>위 두 방식을 융합함! 통계적 정보를 반영해 fixed voca에 적용</li>
</ul>

<h3 id="object-function-1">Object Function</h3>
<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-10.png?raw=true" /></p>

<ul>
  <li>코퍼스 내의 모든 단어 W에 대해, 두 단어 벡터의 내적(유사도)과 두 단어의 Co-occurrence 확률(<code class="MathJax_Preview">P_{ij}</code><script type="math/tex">P_{ij}</script>)의 차이를 최소로 만들자</li>
  <li>특정 단어 간의 Co-occurrence는 다른 것들에 비해 과하게 높을 수가 있습니다. 그래서 이렇게 Co-occurrence가 너무 큰 경우의 영향을 줄이기 위한 Weighting Function 을 사용합니다(<code class="MathJax_Preview">f(P_{ij})</code><script type="math/tex">f(P_{ij})</script>)</li>
</ul>

<p><code class="MathJax_Preview">f(x)= if x &lt; x_{max} : (x/x_{max})^a</code><script type="math/tex">% <![CDATA[
f(x)= if x < x_{max} : (x/x_{max})^a %]]></script>
<code class="MathJax_Preview">otherwise : 1</code><script type="math/tex">otherwise : 1</script></p>

<h3 id="process">Process</h3>
<ol>
  <li>Corpus에서 단어 집합(Vocabulary)을 구해서 Index를 매긴다.</li>
  <li>Window size를 정하고 T개의 데이터셋를 준비한다.</li>
  <li>Center word와 Context word를 표현할 2개의 Embedding Matrix를 선언한다.</li>
  <li>전체 코퍼스에 대해 Co-occurrence matrix를 구축한다. =&gt; 단어셋이 클수록 메모리 에러가 남(VxV = 1억..)</li>
  <li>Objective function을 이용해 J(θ)를 구하고 여기에 -를 붙여서 loss로 만든다</li>
  <li>Gradient Descent를 사용하여 loss를 최소화한다.</li>
  <li>학습이 끝난 뒤에는 Center vector와 Context vector를 덧셈해서 사용한다.</li>
</ol>

<h3 id="pretrained-word-vector">Pretrained word vector</h3>
<p><a href="https://github.com/stanfordnlp/GloVe">https://github.com/stanfordnlp/GloVe</a><br />
<a href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a></p>

<ul>
  <li>대용량의 코퍼스에 미리 학습된 Word Vector들을 다운받아 사용할 수 있음. 많은 딥러닝 기반의 NLP 모델에선 이러한 Pre-trained word vector를 사용해 초기화함</li>
</ul>

<h2 id="word-vector-evaluation">Word Vector Evaluation</h2>
<ul>
  <li>Intrinsic
    <ul>
      <li>a:b == c:?</li>
      <li>데이터셋을 4개의 pair를 준비하고 구멍을 만듬</li>
      <li>cosine similarity를 사용해서 Distance가 가장 짧은 i를 찾음
  <code class="MathJax_Preview">d = argmax_i((x_b-x_a+x_c)^T*x_i)/(||x_b-x_a+x_c||)</code><script type="math/tex">d = argmax_i((x_b-x_a+x_c)^T*x_i)/(||x_b-x_a+x_c||)</script></li>
      <li>GloVe의 경우 더 오랜 훈련시킬수록 모델의 정확도가 더 높아지는 경향이 있음(수렴 역시 더 빠름!)</li>
      <li>무조건 GloVe가 좋다고할 순 없지만 대체로 좋은 편</li>
      <li>여러 사람이 두 단어의 관계에 대한 상관관계를 1~10 사이의 점수를 매기면 이를 평균내서 Test Set으로 구축하긴 함(But.. 하기 힘듬</li>
    </ul>
  </li>
  <li>Extrinsic
    <ul>
      <li>실제 task를 평가</li>
    </ul>
  </li>
</ul>

<h2 id="tensorboardx를-사용한-시각화">TensorboardX를 사용한 시각화</h2>
<p><a href="https://github.com/DSKSD/Pytorch_Fast_Campus_2018/blob/master/week6/5_Skip-gram-Negative-Sampling.ipynb">코드</a></p>

<ul>
  <li>필요한 데이터셋
    <ul>
      <li>학습이 끝난 weight matrix를 가지고 옴(345, 30)</li>
      <li>각 인덱스에 맞는 단어(라벨)</li>
    </ul>
  </li>
  <li>Search에 토치를 검색해서 볼 수 있음</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from tensorboardX import SummaryWriter
import pickle
import os

# 텐서보드 데이터 파일 초기화
try:
    shutil.rmtree('runs/')
except:
    pass

writer = SummaryWriter(comment='-embedding')
matrix = (model.embedding_u.weight.data + model.embedding_v.weight.data)/2
label = [index2word[i] for i in range(len(index2word))]

writer.add_embedding(matrix, metadata=label)
writer.close()

!tensorboard --logdir runs --port 6006    
</code></pre></div></div>

<ul>
  <li>이미지를 크게 보시려면 우측마우스 클릭 후 새 창에서 이미지를 열어주세요!</li>
</ul>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-12.png?raw=true" /></p>

<h2 id="using-pretrained-word-vector">Using Pretrained Word Vector</h2>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-11.png?raw=true" /></p>

<ul>
  <li>자신이 가지고 있는 코퍼스보다 더 큰 코퍼스로 학습 된 매트릭스!! using Word2Vec, GloVe, FastText….</li>
  <li>이전 Transfer Learning에서 Fine tuning했던 것과 
비슷한 효과를 기대함.</li>
  <li>단어 단위에서의 Representation Power 빌리기!
거의 모든 딥러닝 NLP 모델에서의 기본재료</li>
  <li>Unsupervised Learning! 레이블과 상관없이 큰 코퍼스로 학습을 시킨 후, 우리가 사용하려는 embedding matrix에 반영</li>
  <li><a href="https://github.com/DSKSD/Pytorch_Fast_Campus_2018/blob/master/week6/6_Using_Pretrained_Word_Vector.ipynb">코드</a></li>
  <li>Gensim : 오직 word vector만을 위한 라이브러리! 굉장히 빠름!</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">torch</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="k">from</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span> <span class="n">import</span> <span class="n">Variable</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="n">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">import</span> <span class="n">nltk</span>
<span class="n">import</span> <span class="n">torchtext</span>
<span class="k">from</span> <span class="n">konlpy</span><span class="p">.</span><span class="n">tag</span> <span class="n">import</span> <span class="n">Kkma</span>

<span class="n">tagger</span> <span class="p">=</span> <span class="n">Kkma</span><span class="p">()</span>
<span class="n">import</span> <span class="n">gensim</span>

<span class="n">torchtext</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">pretrained_aliases</span>

<span class="n">corpus</span> <span class="p">=</span> <span class="n">open</span><span class="p">(</span><span class="s1">'data/corpus.txt'</span><span class="p">,</span><span class="s1">'r'</span><span class="p">,</span><span class="n">encoding</span><span class="p">=</span><span class="s2">"utf-8"</span><span class="p">).</span><span class="n">readlines</span><span class="p">()</span>
<span class="n">corpus</span> <span class="p">=</span> <span class="p">[</span><span class="n">c</span><span class="p">[:-</span><span class="m">1</span><span class="p">]</span> <span class="n">for</span> <span class="n">c</span> <span class="k">in</span> <span class="n">corpus</span><span class="p">]</span>

<span class="n">tokenized</span> <span class="p">=</span> <span class="p">[</span><span class="n">tagger</span><span class="p">.</span><span class="n">morphs</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="n">for</span> <span class="n">c</span> <span class="k">in</span> <span class="n">corpus</span><span class="p">]</span>

<span class="k">model</span> <span class="p">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">tokenized</span><span class="p">,</span> <span class="n">size</span><span class="p">=</span><span class="m">15</span><span class="p">,</span> <span class="n">window</span><span class="p">=</span><span class="m">5</span><span class="p">,</span> <span class="n">min_count</span><span class="p">=</span><span class="m">2</span><span class="p">,</span> <span class="n">workers</span><span class="p">=</span><span class="m">4</span><span class="p">)</span>

<span class="k">model</span><span class="p">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">"토치"</span><span class="p">)</span>

<span class="k">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="s2">"data/word_vector_sample.bin"</span><span class="p">,</span><span class="n">binary</span><span class="p">=</span><span class="nb">True</span><span class="p">)</span> <span class="p">#</span> <span class="err">저장</span>

<span class="n">pretrained_vectors_model</span> <span class="p">=</span> <span class="n">gensim</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">KeyedVectors</span><span class="p">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s2">"data/word_vector_sample.bin"</span><span class="p">,</span><span class="n">binary</span><span class="p">=</span><span class="nb">True</span><span class="p">)</span>

<span class="n">pretrained_vectors_model</span><span class="p">[</span><span class="s1">'토치'</span><span class="p">]</span>

<span class="n">vocab</span> <span class="p">=</span> <span class="k">list</span><span class="p">(</span><span class="n">pretrained_vectors_model</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span> <span class="p">#</span> <span class="n">Word2Vec</span><span class="err">에서</span> <span class="err">사용한</span> <span class="n">vocab</span>

<span class="n">pretrained_vectors</span><span class="p">=[]</span>
<span class="n">for</span> <span class="n">vo</span> <span class="k">in</span> <span class="n">vocab</span><span class="p">:</span>
    <span class="n">pretrained_vectors</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pretrained_vectors_model</span><span class="p">[</span><span class="n">vo</span><span class="p">])</span>
    
<span class="n">pretrained_vectors</span> <span class="p">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">pretrained_vectors</span><span class="p">)</span>

<span class="n">pretrained_vectors</span><span class="p">.</span><span class="n">shape</span>

<span class="p">#</span> <span class="n">Init</span> <span class="n">embedding</span> <span class="n">matrix</span>
<span class="n">class</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">def</span> <span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">embed_size</span><span class="p">):</span>
        <span class="n">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="p">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span><span class="n">embed_size</span><span class="p">)</span>
        
    <span class="n">def</span> <span class="n">init_embed</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">pretrained_vectors</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="p">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pretrained_vectors</span><span class="p">).</span><span class="n">float</span><span class="p">()</span>
    
    <span class="n">def</span> <span class="k">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">return</span> <span class="n">self</span><span class="p">.</span><span class="n">embed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="k">model</span> <span class="p">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span><span class="m">15</span><span class="p">)</span>
<span class="k">model</span><span class="p">.</span><span class="n">embed</span><span class="p">.</span><span class="n">weight</span>
<span class="k">model</span><span class="p">.</span><span class="n">init_embed</span><span class="p">(</span><span class="n">pretrained_vectors</span><span class="p">)</span>
<span class="k">model</span><span class="p">.</span><span class="n">embed</span><span class="p">.</span><span class="n">weight</span>
    
</code></pre></div></div>

<h3 id="cnn을-사용한-sentence-classification-예시">CNN을 사용한 Sentence Classification 예시</h3>

<p><img src="https://github.com/zzsza/zzsza.github.io/blob/master/assets/img/nlp-13.png?raw=true" /></p>

<h3 id="fasttext">FastText</h3>
<ul>
  <li>Word2Vec이나 GloVe와 같은 Word level representation model의 문제점은 선정의한 단어셋에 대한 매트릭스만을 학습시킬 수 있다는 것입니다</li>
  <li>즉, 단어셋에 없는 단어를 만나면 아예 Indexing 자체를 할 수 없게 됩니다. 이러한 문제를 Out of Vocabulary(OOV)라고 부릅니다</li>
  <li>FastText는 Subword Information을 이용하여 Word representation을 시도합니다. OOV 문제를 어느 정도 중화시켰습니다</li>
</ul>

      <br/>
      <p><a href="https://bit.ly/kyleschool_blog">카일스쿨 유튜브</a> 채널을 만들었습니다. 데이터 사이언스, 성장, 리더십, BigQuery 등을 이야기할 예정이니, 관심 있으시면 구독 부탁드립니다 :)</p>
      <p>이 글이 도움이 되셨다면 추천 클릭을 부탁드립니다 :)</p>
      <br/>
      <br/>
      <div style="text-align:center">
        <style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFFFFF !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 0px 9px !important;font-size: 17px !important;letter-spacing:-0.08px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Lato', sans-serif !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style><link href="https://fonts.googleapis.com/css?family=Lato&subset=latin,latin-ext" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/zzsza"><img src="https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg" alt="Buy me a coffee"><span style="margin-left:5px">Buy me a coffee</span></a>
      </div>
      <br/>
      <br/>
       <!--블로그-하단-반응형 -->
      <ins class="adsbygoogle"
          style="display:block; width:100%; height:300px;"
          data-ad-client="ca-pub-5105555504153863"
          data-ad-slot="1037589043"
          data-ad-format="auto"></ins>
      <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
      </script>

    </div>
  

</article>

  <hr class="dingbat" />

  <div class="share">
      <h2>Share this post</h2>
      <div class="share-body">
        <a href="http://twitter.com/share?text=Pytorch를 활용한 자연어 처리(NLP)&amp;url=http://localhost:4000/data/2018/03/10/nlp-python/"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
            <span class="icon-twitter">
            </span>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/data/2018/03/10/nlp-python/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
            <span class="icon-facebook">
            </span>
        </a>
    </div>
  </div>
  <br/>






  <aside class="author" role="complementary">
    <div class="author">
  <h2 class="page-title hr">
    About
  </h2>
<div class="author-body">
  
    
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="seongyun byeon"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


  
  <div class="author-body-description">
    <p>Machine Learning Engineer</p>

  </div>
</div>
</div>

  </aside>





<aside class="related" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      
      
      
        
        
          


<li class="h4">
  <a href="/data/2019/12/15/rules-of-ml/" data-flip="title">
    <span>Rules of Machine Learning: Best Practices for ML Engineering 정리</span>
  </a>
  <small><time datetime="2019-12-15T00:00:00+09:00">
    15 Dec 2019
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/data/2019/12/03/cs224w-ml-with-graph/" data-flip="title">
    <span>CS224W - Machine Learning with Graphs 1강 정리</span>
  </a>
  <small><time datetime="2019-12-03T00:00:00+09:00">
    03 Dec 2019
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/data/2019/11/24/pydeck/" data-flip="title">
    <span>지도 데이터 시각화 : Uber의 pydeck 사용하기</span>
  </a>
  <small><time datetime="2019-11-24T00:00:00+09:00">
    24 Nov 2019
  </time></small>
</li>

        
      
        
        
      
    
  </ul>
</aside>



      
        <aside class="comments" role="complementary">
  <h2>Comments</h2>
  <hr/>

  <div id="disqus_thread"></div>

  <script>
    !function(s,i){function e(e){var t=s.pageYOffset||i.body.scrollTop;s.DISQUS&&!s._disqusThis&&!s._disqusFirst&&t+s.innerHeight>=s._disqusThreadOffsetTop&&(s._disqusThis=!0,s.DISQUS.reset({reload:!0,config:d}))}var d=function(){this.page.title="Pytorch를 활용한 자연어 처리(NLP)",this.page.identifier="/data/2018/03/10/nlp-python",this.page.url="http://localhost:4000/data/2018/03/10/nlp-python/"};s._disqusFirst=void 0===s._disqusFirst||s._disqusFirst,s._disqusLoading=void 0!==s._disqusLoading&&s._disqusLoading,s._disqusThis=!1,s._disqusThreadOffsetTop=i.getElementById("disqus_thread").offsetTop,s._disqusLoading?s._disqusFirst=!1:(s._disqusLoading=!0,loadJSDeferred("//zzsza.disqus.com/embed.js"),s.addEventListener?s.addEventListener("scroll",e,{passive:!0}):s.attachEvent?s.attachEvent("onscroll",e):s.onscroll=e)}(window,document);

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>

      

      <footer>
  <hr/>
  
    <p>© 2017. by Seongyun Byeon</p>

  
  <p>
    <code>Powered by <a href="https://zzsza.github.io/">zzsza</a></code>
  </p>
</footer>

    </main>
    <div class="right-side">
  <div class="ad-first">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 블로그-상단-모바일 -->
    <ins class="adsbygoogle"
         style="display:inline-block;width:100%;"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="9090558636"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
<br/>
<br/>
  <div class="ad-second">
    <!-- 블로그-스카이스크래퍼 -->
    <ins class="adsbygoogle"
         style="display:inline-block;max-width:320px;width:100%;height:600px"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="3646660262"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>


  </div>
  <div id="_yDrawer">
  <div id="_sidebar" class="sidebar">
    <div class="sidebar-bg" style="background-color:#4f86aa;background-image:url(/assets/img/nap.jpg)"></div>
    <header class="sidebar-sticky" role="banner">
      <br/>
      <div class="sidebar-about">
        <h1><a id="_title" href="/">어쩐지 오늘은</a></h1>
        <p>Machine Learning Engineer</p>

      </div>

      <br/>
      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  

  

  
  
  
  
  
    <li>
      <input type="checkbox" id="list-item-1"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/diary/">Diary</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/diary-diary/">diary</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-2"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/data/">Data</a>
       <label class="folder" for="list-item-2">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-ml/">Machine-Learning</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-dl/">Deep-Learning</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-mobility/">Mobility</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-engineering/">Engineering</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-optimization/">Optimization</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-paper/">Paper</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-pytorch/">Pytorch</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-cs231/">CS231n</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-rl/">Reinforcement-Learning</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-time-series/">Time Series</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-analytics/">Analytics</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-kaggle/">Kaggle</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-vision/">Vision</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-interpretable-ml/">Interpretable ML</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-simulation/">Simulation</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/data-graph/">Graph</a>
             </li>
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-3"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/mlops/">MLOps</a>
       <label class="folder" for="list-item-3">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-basic/">Basic</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-airflow/">Airflow</a>
             </li>
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-mlflow/">MLflow</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-tfx/">Tensorflow Extended</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-serving/">Serving</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-feature/">Feature</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/mlops-version/">Model & Pipeline version</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-4"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/gcp/">Google Cloud Platform</a>
       <label class="folder" for="list-item-4">▾</label>
    </div>
     <ul class="list-body">
       
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/gcp-basic/">Basic</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/gcp-bigquery/">BigQuery</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/gcp-aiplatform/">AI Platform</a>
             </li>
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/gcp-dataflow/">Dataflow</a>
             </li>
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/gcp-cloud-functions/">Cloud Functions</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-5"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/development/">Development</a>
       <label class="folder" for="list-item-5">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-linux/">Linux</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-web/">Web</a>
             </li>
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-python/">Python</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-sql/">SQL</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-scala/">Scala</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-os/">OS</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-kubernetes/">Kubernetes</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-devops/">DevOps</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-kotlin/">Kotlin</a>
             </li>
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-julia/">Julia</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/development-tools/">Tools</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-6"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/etc/">ETC</a>
       <label class="folder" for="list-item-6">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/etc-lecture/">Lecture</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/etc-book/">Book</a>
             </li>
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-7"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/about/">About</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
</ul>

      </nav>
    <!-- search box -->
    <form action="/search">
      <div class="tipue_search_box">
        <input type="text" name="q" id="tipue_search_input" placeholder="&#xF002;  Search" pattern=".{2,}" title="최소 2글자 이상" required></div>
      <div style="clear: both;"></div>
    </form>
    <br/>
      <div class="sidebar-box">
        
          
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="seongyun byeon"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


        
      </div>
      <p>메모가 습관인 데이터쟁이입니다</p>

      
      
        <div class="sidebar-social">
          <span class="sr-only">Social:</span>
<ul>
  
    









<li>
  <a href="https://facebook.com/zzsza">
    <span class="icon-facebook" title="Facebook"></span>
    <span class="sr-only">Facebook</span>
  </a>
</li>

  
    









<li>
  <a href="https://www.youtube.com/channel/UCYyAaezBGEVmXcnink6nY9Q">
    <span class="icon-youtube" title="YouTube"></span>
    <span class="sr-only">YouTube</span>
  </a>
</li>

  
    









<li>
  <a href="https://instagram.com/data.scientist">
    <span class="icon-instagram" title="Instagram"></span>
    <span class="sr-only">Instagram</span>
  </a>
</li>

  
    









<li>
  <a href="https://github.com/zzsza">
    <span class="icon-github" title="GitHub"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    









<li>
  <a href="https://www.linkedin.com/in/seong-yun-byeon-8183a8113">
    <span class="icon-linkedin2" title="LinkedIn"></span>
    <span class="sr-only">LinkedIn</span>
  </a>
</li>

  
    









<li>
  <a href="mailto:snugyun01@gmail.com">
    <span class="icon-mail" title="Email"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
    









<li>
  <a href="https://zzsza.github.io/feed.xml">
    <span class="icon-rss2" title="RSS"></span>
    <span class="sr-only">RSS</span>
  </a>
</li>

  
</ul>

        </div>
      
    </header>
  </div>
</div>

</div>

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->

<script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-110573232-1', 'auto');
  ga('send', 'pageview');
  loadJSDeferred('https://www.google-analytics.com/analytics.js');
</script>





<!--[if gt IE 8]><!---->
<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  WebFont.load({
    
    google: {
      families: 'Lato'.split('|')
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  });
</script>
<!--<![endif]-->


  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack.js?v=6.4.0');</script>

  
  <!--<![endif]-->



</body>

</html>
